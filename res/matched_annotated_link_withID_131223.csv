67<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/ : library providing high-performance, easy-to-use data structures and data analysis tools<|>7<|>32<|>0<|>6<|>67<|>software
70<|>http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable<|>Validatable<|>Validatable http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable : provides validations of email and password. It's optional and can be customized, so you're able to define your own validations.<|>12<|>91<|>0<|>11<|>70<|>software
155<|>http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#datasets<|>supervised dataset format<|>and  above have the now-standard scikit-learn supervised dataset format http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#datasets . This means we can use any classifier that satisfies the scikit-learn API. Below, we use a simple wrapper around the scikit-learn .<|>72<|>156<|>46<|>71<|>155<|>other
155<|>http://conda.pydata.org/miniconda.html<|>miniconda<|>Though you can install all the requirements yourself, as most are available in the Python Package Index (PyPI) and can be installed with simple commands, the easiest way to get up and running is to use miniconda http://conda.pydata.org/miniconda.html . Once you have the  command, you can create a fully-functional gala environment with  (inside the gala directory).<|>212<|>250<|>202<|>211<|>155<|>software
189<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>tutorial<|>Next we suggest you look at the comprehensive tutorial http://simongog.github.io/assets/data/sdsl-slides/tutorial  which describes all major features of the library or look at some of the provided examples examples .<|>55<|>113<|>46<|>54<|>189<|>other
189<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>tutorial slides and walk-through<|>We provide a large collection of supporting documentation consisting of examples, cheat sheet http://simongog.github.io/assets/data/sdsl-cheatsheet.pdf , tutorial slides and walk-through http://simongog.github.io/assets/data/sdsl-slides/tutorial .<|>187<|>245<|>154<|>186<|>189<|>other
189<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>presentation<|>A tutorial presentation http://simongog.github.io/assets/data/sdsl-slides/tutorial  with the example code tutorial/  using in the sides demonstrating all features of the library in a step-by-step walk-through.<|>24<|>82<|>11<|>23<|>189<|>other
231<|>http://www.tuhh.de/ibb/publications/databases-and-software.html<|>TUHH website<|>The program expects a network represented as an edgelist file. Such a file contains the list of all edges constituting the network, under the form of pairs of node ids. The nodes numbering must start from zero. For testing purposes, it is possible to generate a dummy network by setting  to  in . The original networks from [GA'05] are included in the folder data (or at least, networks very similar to the ones described in the article), also for testing purposes. They were retrieved from the TUHH website http://www.tuhh.de/ibb/publications/databases-and-software.html .<|>508<|>571<|>495<|>507<|>231<|>dataset_landing_page
269<|>http://grouplens.org/datasets/movielens/<|>Movielens dataset GroupLens<|>Movielens dataset GroupLens http://grouplens.org/datasets/movielens/<|>28<|>68<|>0<|>27<|>269<|>dataset_landing_page
269<|>http://law.di.unimi.it/datasets.php<|>Laboratory for Web Algorithms<|>Laboratory for Web Algorithms http://law.di.unimi.it/datasets.php<|>30<|>65<|>0<|>29<|>269<|>dataset_landing_page
269<|>http://stat-computing.org/dataexpo/2009/<|>Airline on time performance<|>Airline on time performance http://stat-computing.org/dataexpo/2009/<|>28<|>68<|>0<|>27<|>269<|>dataset_landing_page
269<|>http://snap.stanford.edu/data/index.html<|>Stanford Large Network Dataset (SNAP)<|>Stanford Large Network Dataset (SNAP) http://snap.stanford.edu/data/index.html<|>38<|>78<|>0<|>37<|>269<|>dataset_landing_page
269<|>http://aws.amazon.com/datasets<|>Amazon Web Services public datasets<|>Amazon Web Services public datasets http://aws.amazon.com/datasets<|>36<|>66<|>0<|>35<|>269<|>dataset_landing_page
301<|>http://shuyo.wordpress.com/2012/03/02/estimation-of-ldig-twitter-language-detection-for-liga-dataset/<|>Estimation of ldig (twitter Language Detection) for LIGA dataset<|>Estimation of ldig (twitter Language Detection) for LIGA dataset http://shuyo.wordpress.com/2012/03/02/estimation-of-ldig-twitter-language-detection-for-liga-dataset/<|>65<|>166<|>0<|>64<|>301<|>other
314<|>https://github.com/RDFLib/pymicrodata<|>pymicrodata<|>pymicrodata https://github.com/RDFLib/pymicrodata  - A module to extract RDF from an HTML5 page annotated with microdata.<|>12<|>49<|>0<|>11<|>314<|>software
346<|>https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_5050<|>Interpolation<|>Interpolation https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_5050<|>14<|>91<|>0<|>13<|>346<|>dataset_landing_page
346<|>https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_9010<|>Extrapolation<|>Extrapolation https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_9010<|>14<|>91<|>0<|>13<|>346<|>dataset_landing_page
374<|>http://nilm-metadata.readthedocs.org/en/latest/tutorial.html<|>tutorial<|>If you're new to NILM Metadata then please read this README and then dive into the tutorial http://nilm-metadata.readthedocs.org/en/latest/tutorial.html  to find out how to see a worked example.<|>92<|>152<|>83<|>91<|>374<|>other
374<|>https://github.com/nilmtk/nilm_metadata/tree/v0.1.0<|>version 0.1 of the schema<|>In version 0.1 of the schema https://github.com/nilmtk/nilm_metadata/tree/v0.1.0 , we wrote a very comprehensive (and complex) schema using JSON Schema http://json-schema.org/  in order to automate the validation of metadata instances. JSON Schema is a lovely language and can capture everything we need but, because our metadata is quite comprehensive, we found that using JSON Schema was a significant time drain and made it hard to move quickly and add new ideas to the metadata. As such, when we moved from v0.1 to v0.2, the JSON Schema has been dropped. Please use the human-readable documentation http://nilm-metadata.readthedocs.org  instead. If there is a real desire for automated validation then we could resurrect the JSON Schema, but it is a fair amount of work to maintain.<|>29<|>80<|>3<|>28<|>374<|>software
374<|>https://github.com/nilmtk/nilm_metadata/issues/6<|>issue #6<|>Please do  use  until I have updated  to copy the relevant  files. See issue #6 https://github.com/nilmtk/nilm_metadata/issues/6 .<|>80<|>128<|>71<|>79<|>374<|>other
374<|>http://nilm-metadata.readthedocs.org<|>human-readable documentation<|>In version 0.1 of the schema https://github.com/nilmtk/nilm_metadata/tree/v0.1.0 , we wrote a very comprehensive (and complex) schema using JSON Schema http://json-schema.org/  in order to automate the validation of metadata instances. JSON Schema is a lovely language and can capture everything we need but, because our metadata is quite comprehensive, we found that using JSON Schema was a significant time drain and made it hard to move quickly and add new ideas to the metadata. As such, when we moved from v0.1 to v0.2, the JSON Schema has been dropped. Please use the human-readable documentation http://nilm-metadata.readthedocs.org  instead. If there is a real desire for automated validation then we could resurrect the JSON Schema, but it is a fair amount of work to maintain.<|>603<|>639<|>574<|>602<|>374<|>other
374<|>http://nilm-metadata.readthedocs.org/en/latest/dataset_metadata.html<|>Dataset metadata<|>Or, if you are already familiar with NILM Metadata then perhaps you want direct access to the full description of the " Dataset metadata http://nilm-metadata.readthedocs.org/en/latest/dataset_metadata.html ".<|>137<|>205<|>120<|>136<|>374<|>other
374<|>http://nilm-metadata.readthedocs.org<|>documentation is available online<|>The documentation is available online http://nilm-metadata.readthedocs.org .<|>38<|>74<|>4<|>37<|>374<|>other
395<|>http://datatracker.ietf.org/doc/draft-ietf-ppsp-peer-protocol/<|>video streaming<|>Tribler https://github.com/Tribler/tribler/wiki  Aims to create a censorship-free Internet. Already deployed, used and incrementally improved for 8-years. Tribler uses an upcoming IETF Internet Standard for video streaming http://datatracker.ietf.org/doc/draft-ietf-ppsp-peer-protocol/  and is backward compatible with Bittorrent. Future aim is using smartphones to even bypass Internet kill switches. An early proof-of-principle Tribler-mobile https://play.google.com/store/apps/details?id=org.tribler.mobile  is available on the Android Market. Key principle http://www.foxnews.com/tech/2012/02/10/forget-megaupload-researchers-call-new-file-sharing-network-invincible/ : 'the only way to take it down is to take The Internet down'. Overview paper http://sigmm.org/records/records1201/featured03.html .<|>223<|>285<|>207<|>222<|>395<|>other
446<|>https://en.wikipedia.org/wiki/Rope_%28data_structure%29<|>rope data structure<|>RopeBWT2 keeps the entire BWT in six B+ trees with the -th tree storing the substring B[C(i)+1..C(i+1)], where C(i) equals the number of symbols lexicographically smaller than . In each B+ tree, an internal node keeps the count of symbols in the subtree descending from it; an external node keeps a BWT substring in the run-length encoding. The B+ tree achieve a similar purpose to the rope data structure https://en.wikipedia.org/wiki/Rope_%28data_structure%29 , which enables efficient query and insertion. RopeBWT2 uses this rope-like data structure to achieve incremental construction. This is where word 'rope' in ropeBWT2 comes from.<|>406<|>461<|>386<|>405<|>446<|>other
454<|>http://conda.pydata.org/miniconda.html<|>Miniconda<|>The Miniconda http://conda.pydata.org/miniconda.html  tool first needs to installed:<|>14<|>52<|>4<|>13<|>155<|>software
454<|>http://conda.pydata.org/<|>conda<|>To simplify the build we now use the conda-build http://conda.pydata.org/docs/build.html  tool. The resulting binary is uploaded to the flyem binstar channel https://binstar.org/flyem , and can be installed using the conda http://conda.pydata.org/  package manager. The installation will install all of the neuroproof binaries (including the interactive tool) and the python libraries.<|>223<|>247<|>217<|>222<|>454<|>software
454<|>http://conda.pydata.org/docs/build.html<|>conda-build<|>To simplify the build we now use the conda-build http://conda.pydata.org/docs/build.html  tool. The resulting binary is uploaded to the flyem binstar channel https://binstar.org/flyem , and can be installed using the conda http://conda.pydata.org/  package manager. The installation will install all of the neuroproof binaries (including the interactive tool) and the python libraries.<|>49<|>88<|>37<|>48<|>454<|>other
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m<|>region growing<|>The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m  pipeline.<|>19<|>113<|>4<|>18<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_MSTEPS.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>41<|>132<|>4<|>40<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m  pipeline.<|>49<|>146<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>49<|>140<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m<|>preprocessing<|>The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m  pipeline.<|>18<|>108<|>4<|>17<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m<|>region growing<|>The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m  pipeline.<|>19<|>114<|>4<|>18<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m<|>preprocessing<|>The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m  pipeline.<|>18<|>109<|>4<|>17<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>49<|>141<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m  pipeline, with regular grid of scales.<|>41<|>139<|>4<|>40<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m  pipeline.<|>49<|>147<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m<|>preprocessing<|>The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m  pipeline.<|>18<|>108<|>4<|>17<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>49<|>140<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m<|>Multiscale Statistical Parametric Connectome<|>The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m  pipeline.<|>49<|>146<|>4<|>48<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_MSTEPS.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>41<|>132<|>4<|>40<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_regular_grid.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_regular_grid.m  pipeline, with regular grid of scales.<|>41<|>138<|>4<|>40<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m<|>region growing<|>The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m  pipeline.<|>19<|>113<|>4<|>18<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_regular_grid.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_regular_grid.m  pipeline, with regular grid of scales.<|>41<|>138<|>4<|>40<|>559<|>dataset_landing_page
559<|>https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m<|>Boostrap Analysis of Stable Clusters<|>The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m  pipeline, with scales selected by MSTEPS.<|>41<|>133<|>4<|>40<|>559<|>dataset_landing_page
575<|>https://github.com/redpony/creg/tree/master/test_data<|>test_data<|>You will find example files for each type of model in the test_data https://github.com/redpony/creg/tree/master/test_data  directory.<|>68<|>121<|>58<|>67<|>575<|>software
685<|>http://www.bbci.de/competition/iii/#data_set_i<|>BCI Competition3, Data Set 1<|>An example for classification of motor imagery in ECoG recordings. For that example the BCI Competition3, Data Set 1 http://www.bbci.de/competition/iii/#data_set_i  was used.<|>117<|>163<|>88<|>116<|>685<|>dataset_landing_page
685<|>http://www.bbci.de/competition/iii/#data_set_ii<|>BCI Competition 3, Data Set 2<|>An example for classification with a P300 Matrix Speller in EEG recordings. The BCI Competition 3, Data Set 2 http://www.bbci.de/competition/iii/#data_set_ii  was used for that example.<|>110<|>157<|>80<|>109<|>685<|>dataset_landing_page
711<|>http://smileclinic.alwaysdata.net/<|>Novi Quadrianto<|>Novi Quadrianto http://smileclinic.alwaysdata.net/ : letter n followed by dot then quadrianto and the at, and also sussex.ac.uk<|>16<|>50<|>0<|>15<|>711<|>other
761<|>https://github.com/Spirals-Team/npe-dataset/<|>https://github.com/Spirals-Team/npe-dataset/<|>The evaluation dataset is at https://github.com/Spirals-Team/npe-dataset/ https://github.com/Spirals-Team/npe-dataset/<|>74<|>118<|>29<|>73<|>761<|>other
786<|>http://www.ams.org/mr-database<|><|>The code in this repo makes use of 24 years of data from http://www.ams.org/mr-database , available by request to the director of the American Mathematical Society. The data may require cleaning in order to match the format assumed by  and . The code also uses data included in the https://github.com/corybrunson/bitriad , specifically affiliation networks drawn from the studies http://books.google.com/books?id=Q3b9QTOgLFcC  and http://books.google.com/books?id=fR-LBQAAQBAJ .<|>57<|>87<|>0<|>0<|>786<|>other
791<|>http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools<|>SICK dataset<|>SICK dataset http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools  (semantic relatedness task)<|>13<|>78<|>0<|>12<|>791<|>dataset_landing_page
791<|>http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools<|>Sentences Involving Compositional Knowledge (SICK)<|>The goal of this task is to predict similarity ratings for pairs of sentences. We train and evaluate our models on the Sentences Involving Compositional Knowledge (SICK) http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools  dataset.<|>170<|>235<|>119<|>169<|>791<|>dataset_landing_page
831<|>http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data<|>Kaggle<|>Download the data files from Kaggle http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data . Place and extract the files in the following locations:<|>36<|>96<|>29<|>35<|>831<|>dataset_direct_link
843<|>http://www.micc.unifi.it/tagsurvey/#data<|>Florence data server<|>Data dashboard /data/ , Florence data server http://www.micc.unifi.it/tagsurvey/#data<|>45<|>85<|>24<|>44<|>843<|>dataset_landing_page
858<|>http://apps.who.int/gho/data/node.ebola-sitrep<|>WHO<|>contains data from the WHO http://apps.who.int/gho/data/node.ebola-sitrep  that compare sitrep case counts with patient database counts for select cities and countries.<|>27<|>73<|>23<|>26<|>858<|>dataset_landing_page
886<|>http://www.w3.org/2014/data-shapes/charter<|>W3C RDF data shapes working group charter<|>W3C RDF data shapes working group charter http://www.w3.org/2014/data-shapes/charter<|>42<|>84<|>0<|>41<|>886<|>other
924<|>http://cocodataset.org<|>COCO<|>This project includes the front end user interfaces that are used to annotate COCO dataset. For more details, please visit COCO http://cocodataset.org<|>128<|>150<|>123<|>127<|>924<|>dataset_landing_page
941<|>https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#<|>Assamese handwriting recognition<|>Assamese handwriting recognition https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#<|>33<|>120<|>0<|>32<|>941<|>dataset_landing_page
941<|>https://www.kaggle.com/c/datasciencebowl<|>Kaggle plankton recognition competition, 2015<|>Kaggle plankton recognition competition, 2015 https://www.kaggle.com/c/datasciencebowl  Third place. The competition solution is being adapted for research purposes in EcoTaxa http://ecotaxa.obs-vlfr.fr/ .<|>46<|>86<|>0<|>45<|>941<|>other
1004<|>https://github.com/klout/opendata/blob/master/wiki_annotation/README.md<|>DAWT: Densely Annotated Wikipedia Texts across multiple languages<|>DAWT: Densely Annotated Wikipedia Texts across multiple languages https://github.com/klout/opendata/blob/master/wiki_annotation/README.md  ( arxiv https://arxiv.org/pdf/1703.00948.pdf )<|>66<|>137<|>0<|>65<|>1004<|>dataset_landing_page
1021<|>https://github.com/wnzhang/make-ipinyou-data<|>make-ipinyou-data<|>For the large-scale experiment, please first check our GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data  for pre-processing the iPinYou data http://data.computational-advertising.org . After downloading the dataset, by simplying  you can generate the standardised data which will be used in the bid optimisation tasks.<|>88<|>132<|>70<|>87<|>1021<|>dataset_landing_page
1024<|>http://linkedgeodata.org<|>LinkedGeoData<|>Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the LinkedGeoData http://linkedgeodata.org  project.<|>117<|>141<|>103<|>116<|>1024<|>dataset_landing_page
1047<|>https://github.com/FasterXML/jackson-databind<|>jackson-databind<|>is a java serialization library. There are many https://github.com/eishay/jvm-serializers  like it, but this one is ours. Codec has its own reflection toolset, but we are moving to wards using more and more jackson in the back end. Currently, the most notable features are those that are built on top of jackson-databind https://github.com/FasterXML/jackson-databind  and typesafe-config https://github.com/typesafehub/config .<|>321<|>366<|>304<|>320<|>1047<|>software
1121<|>http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/<|>EDUB-Seg dataset<|>The training and validation of the code was performed using the EDUB-Seg dataset http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/ .<|>81<|>176<|>64<|>80<|>1121<|>dataset_landing_page
1127<|>http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/<|>Metadata Embeddings for User and Item Cold-start Recommendations<|>Metadata Embeddings for User and Item Cold-start Recommendations http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/<|>65<|>169<|>0<|>64<|>1127<|>other
1139<|>http://www.cs.utexas.edu/users/ml/nldata/geoquery.html<|>Geoquery<|>A lexicon of Geoquery http://www.cs.utexas.edu/users/ml/nldata/geoquery.html  is available in lex/<|>22<|>76<|>13<|>21<|>1139<|>dataset_landing_page
1185<|>https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type<|>Conflict-free Replicated Data Types<|>Swarm.js is a JavaScript client for the Swarm database. Swarm is like "git for data" except it's real-time and never has a merge conflict. Swarm is based on Replicated Object Notation http://github.com/gritzko/ron  (RON), a distributed live data format. In turn, RON is based on Conflict-free Replicated Data Types https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type  (CRDTs), a new math apparatus for distributed data.<|>315<|>379<|>279<|>314<|>1185<|>other
1185<|>https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(Positive-Negative_Counter)<|>Counter<|>[ ] Counter https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(Positive-Negative_Counter) , a positive-negative counter.<|>12<|>115<|>4<|>11<|>1185<|>other
1186<|>https://github.com/madvas/todomvc-omnext-datomic-datascript<|>OmNext TodoMVC<|>OmNext TodoMVC https://github.com/madvas/todomvc-omnext-datomic-datascript<|>15<|>74<|>0<|>14<|>1186<|>software
1186<|>http://thegeez.net/2014/04/30/datascript_clojure_web_app.html<|>blog post<|>clj-crud, demo CRUD app: sources https://github.com/thegeez/clj-crud , blog post http://thegeez.net/2014/04/30/datascript_clojure_web_app.html<|>81<|>142<|>71<|>80<|>1186<|>other
1186<|>http://tonsky.me/blog/datascript-chat/<|>code walkthrough<|>CatChat, chat demo app: sources https://github.com/tonsky/datascript-chat , code walkthrough http://tonsky.me/blog/datascript-chat/ , live http://tonsky.me/datascript-chat/<|>93<|>131<|>76<|>92<|>1186<|>other
1186<|>https://www.npmjs.org/package/datascript<|>npm page<|>or as a CommonJS module ( npm page https://www.npmjs.org/package/datascript ):<|>35<|>75<|>26<|>34<|>1186<|>software
1186<|>https://github.com/djjolicoeur/datamaps<|>Datamaps<|>Datamaps https://github.com/djjolicoeur/datamaps , lib designed to leverage datalog queries to query arbitrary maps.<|>9<|>48<|>0<|>8<|>1186<|>software
1186<|>https://github.com/typeetfunc/datascript-mori<|>DataScript-mori<|>DataScript-mori https://github.com/typeetfunc/datascript-mori , DataScript & Mori wrapper for use from JS<|>16<|>61<|>0<|>15<|>1186<|>software
1186<|>https://github.com/tonsky/datascript-transit<|>DataScript-Transit<|>DataScript-Transit https://github.com/tonsky/datascript-transit , transit serialization for database and datoms<|>19<|>63<|>0<|>18<|>1186<|>other
1186<|>https://github.com/tonsky/datascript-todo<|>sources<|>ToDo, task manager demo app (persistence via localStorage and transit, filtering, undo/redo): sources https://github.com/tonsky/datascript-todo , live http://tonsky.me/datascript-todo/<|>102<|>143<|>94<|>101<|>1186<|>other
1186<|>https://cljdoc.org/d/datascript/datascript/CURRENT<|><|>API Docs https://cljdoc.org/d/datascript/datascript/CURRENT<|>9<|>59<|>0<|>0<|>1186<|>software
1186<|>https://github.com/tonsky/datascript/wiki/Getting-started<|>Getting started<|>Getting started https://github.com/tonsky/datascript/wiki/Getting-started<|>16<|>73<|>0<|>15<|>1186<|>software
1186<|>https://github.com/tonsky/datascript-todo<|>app<|>“Building ToDo list with DataScript” webinar (ClojureScript NYC, Dec 2014): video http://vimeo.com/114688970 , app https://github.com/tonsky/datascript-todo<|>115<|>156<|>111<|>114<|>1186<|>other
1186<|>https://github.com/tonsky/datascript-chat<|>sources<|>CatChat, chat demo app: sources https://github.com/tonsky/datascript-chat , code walkthrough http://tonsky.me/blog/datascript-chat/ , live http://tonsky.me/datascript-chat/<|>32<|>73<|>24<|>31<|>1186<|>software
1186<|>http://www.learndatalogtoday.org/<|>Quick tutorial into Datalog<|>Quick tutorial into Datalog http://www.learndatalogtoday.org/<|>28<|>61<|>0<|>27<|>1186<|>other
1186<|>http://tonsky.me/blog/datascript-internals/<|>DataScript internals explained<|>DataScript internals explained http://tonsky.me/blog/datascript-internals/<|>31<|>74<|>0<|>30<|>1186<|>other
1186<|>https://github.com/kristianmandrup/datascript-tutorial<|>Tutorials<|>Tutorials https://github.com/kristianmandrup/datascript-tutorial<|>10<|>64<|>0<|>9<|>1186<|>other
1204<|>http://github.com/rug-compling/dep-brown-data<|>induced clusters and experimental details<|>Simon Šuster and Gertjan van Noord (2014) From neighborhood to parenthood: the advantages of dependency representation over bigrams in Brown clustering. http://www.let.rug.nl/suster/publications/DepBrown.pdf  COLING. See also induced clusters and experimental details http://github.com/rug-compling/dep-brown-data .<|>268<|>313<|>226<|>267<|>1204<|>dataset_landing_page
1221<|>http://grouplens.org/datasets/movielens/<|>MovieLens data<|>We have tested our matrix factorization algorithm on the MovieLens data http://grouplens.org/datasets/movielens/ . In particular, we have used the 1M, 10M, and 20M datasets (after a straightforward preprocessing step to make it compatible with the HAMSI input format).<|>72<|>112<|>57<|>71<|>269<|>dataset_landing_page
1258<|>http://www.nltk.org/data.html<|>here<|>(*) The stopword corpus is needed. Instructions here http://www.nltk.org/data.html .<|>53<|>82<|>48<|>52<|>1258<|>software
1268<|>http://corpus-texmex.irisa.fr/<|>SIFT<|>| data set | n | d | type of data | | -------- | ------- | ---- | -------------------------- | | MNIST http://yann.lecun.com/exdb/mnist/  | 60000 | 784 | images of hand-written digits | | News | 262144 | 1000 | web pages converted into TF-IDF representation | | GIST http://corpus-texmex.irisa.fr/  | 1000000 | 960 | global color GIST descriptors | | SIFT http://corpus-texmex.irisa.fr/  | 2500000 | 128 | local SIFT descriptors | | Trevi http://phototour.cs.washington.edu/patches/default.htm  | 101120 | 4096 | image patches | | STL-10 https://cs.stanford.edu/~acoates/stl10/  | 100000 | 9216 | images of different classes of objects | | Random | 50000 | 4096 | random samples from the 4096-dimensional unit sphere |<|>356<|>386<|>351<|>355<|>1268<|>dataset_landing_page
1268<|>http://corpus-texmex.irisa.fr/<|>GIST<|>| data set | n | d | type of data | | -------- | ------- | ---- | -------------------------- | | MNIST http://yann.lecun.com/exdb/mnist/  | 60000 | 784 | images of hand-written digits | | News | 262144 | 1000 | web pages converted into TF-IDF representation | | GIST http://corpus-texmex.irisa.fr/  | 1000000 | 960 | global color GIST descriptors | | SIFT http://corpus-texmex.irisa.fr/  | 2500000 | 128 | local SIFT descriptors | | Trevi http://phototour.cs.washington.edu/patches/default.htm  | 101120 | 4096 | image patches | | STL-10 https://cs.stanford.edu/~acoates/stl10/  | 100000 | 9216 | images of different classes of objects | | Random | 50000 | 4096 | random samples from the 4096-dimensional unit sphere |<|>267<|>297<|>262<|>266<|>1268<|>dataset_landing_page
1282<|>http://snap.stanford.edu/data/com-Amazon.html<|>http://snap.stanford.edu/data/com-Amazon.html<|>amazon dataset (available at http://snap.stanford.edu/data/com-Amazon.html http://snap.stanford.edu/data/com-Amazon.html )<|>75<|>120<|>29<|>74<|>1282<|>dataset_landing_page
1282<|>http://snap.stanford.edu/data/com-Amazon.html<|>SNAP<|>Here we use Amazon dataset (obtained from SNAP http://snap.stanford.edu/data/com-Amazon.html  website) as an illustration. You may switch to other datasets with corresponding file format as well. Note that some parameters might need to be adjusted accordingly based on the properties of network under test.<|>47<|>92<|>42<|>46<|>1282<|>dataset_landing_page
1301<|>http://ifcb-data.whoi.edu/mvco<|>IFCB Data Dashboard.<|>The images described here are part of a much larger data set (>700 million images) collected by IFCB at the Martha's Vineyard Coastal Observatory ( MVCO http://www.whoi.edu/mvco ) starting in 2006 and continuing to the present. Near real time image data and the complete archive are accessible for browse and download at the IFCB Data Dashboard. http://ifcb-data.whoi.edu/mvco<|>346<|>376<|>325<|>345<|>1301<|>dataset_landing_page
1313<|>https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip<|>link<|>You can download the GoogleRefexp data directly from this link https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip .<|>63<|>134<|>58<|>62<|>1313<|>dataset_direct_link
1351<|>http://dawenl.github.io/data/gowalla_pro.zip<|>here<|>Gowalla https://snap.stanford.edu/data/loc-gowalla.html : the pre-processed data that we used in the paper can be downloaded here http://dawenl.github.io/data/gowalla_pro.zip .<|>130<|>174<|>125<|>129<|>1351<|>dataset_direct_link
1351<|>https://snap.stanford.edu/data/loc-gowalla.html<|>Gowalla<|>Gowalla https://snap.stanford.edu/data/loc-gowalla.html : the pre-processed data that we used in the paper can be downloaded here http://dawenl.github.io/data/gowalla_pro.zip .<|>8<|>55<|>0<|>7<|>1351<|>dataset_landing_page
1355<|>http://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>403<|>453<|>391<|>402<|>1355<|>dataset_landing_page
1355<|>http://vision.in.tum.de/data/datasets/rgbd-dataset/tools<|>associate.py<|>Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:<|>75<|>131<|>62<|>74<|>1355<|>other
1355<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>304<|>358<|>290<|>303<|>1355<|>other
1355<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>503<|>578<|>489<|>502<|>1355<|>dataset_landing_page
1374<|>http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/<|>EDUB-Seg dataset<|>The EDUB-Seg dataset http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/  has been used in this paper. Please cite to this publication https://www.researchgate.net/publication/287901747_SR-Clustering_Semantic_Regularized_Clustering_for_Egocentric_Photo_Streams_Segmentation  if you use it. You might also want to check its project page https://github.com/MarcBS/SR-Clustering .<|>21<|>116<|>4<|>20<|>1121<|>dataset_landing_page
1374<|>https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application<|>BigGraph TEC2013-43935-R<|>| | | |:--|:-:| | We gratefully acknowledge the support of NVIDIA Corporation http://www.nvidia.com/content/global/global.php  with the donation of the GeoForce GTX Titan Z http://www.nvidia.com/gtx-700-graphics-cards/gtx-titan-z/  and Titan X http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x  used in this work. |  | | The Image Processing Group at UPC (SGR1421) and the Computer Vision Group at UB (SGR1219) are both Consolidated Research Groups recognized and sponsored by the Catalan Government (Generalitat de Catalunya) through its AGAUR http://agaur.gencat.cat/en/inici/index.html  office. Mariella Dimiccoli is supported by a Beatriu de Pinos grant, Marie-Curie COFUND action. |  | | This work has been developed in the framework of the project BigGraph TEC2013-43935-R https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application  and and TIN2012-38187-C03-01, funded by the Spanish Ministerio de Economía y Competitividad and the European Regional Development Fund (ERDF). |  |<|>794<|>917<|>769<|>793<|>1374<|>other
1435<|>http://opendatacommons.org/licenses/pddl/1-0/<|>Open Data Commons Public Domain Dedication and Licence (PDDL)<|>OpenMIIR is released under the Open Data Commons Public Domain Dedication and Licence (PDDL) http://opendatacommons.org/licenses/pddl/1-0/ , which means that you can freely use it without any restrictions.<|>93<|>138<|>31<|>92<|>1435<|>other
1435<|>https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab<|>in the wiki<|>This data format can, for instance, also be easily converted into the MAT format used by Matlab, which allows importing into EEGLab. A description on how to do this can be found in the wiki https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab .<|>190<|>273<|>178<|>189<|>1435<|>software
1506<|>https://github.com/Element-Research/dataload#dl.loadGBW<|>Google Billion Words dataset<|>Noise Contrastive Estimate examples/noise-contrastive-estimate.lua  for training multi-layer SeqLSTM #rnn.SeqLSTM  language models on the Google Billion Words dataset https://github.com/Element-Research/dataload#dl.loadGBW . The example uses MaskZero #rnn.MaskZero  to train independent variable length sequences using the NCEModule https://github.com/Element-Research/dpnn#nn.NCEModule  and NCECriterion https://github.com/Element-Research/dpnn#nn.NCECriterion . This script is our fastest yet boasting speeds of 20,000 words/second (on NVIDIA Titan X) with a 2-layer LSTM having 250 hidden units, a batchsize of 128 and sequence length of a 100. Note that you will need to have Torch installed with Lua instead of LuaJIT http://torch.ch/docs/getting-started.html#_ ;<|>167<|>222<|>138<|>166<|>1506<|>software
1506<|>https://github.com/Element-Research/dataload<|>dataload<|>dataload https://github.com/Element-Research/dataload  : a collection of torch dataset loaders;<|>9<|>53<|>0<|>8<|>1506<|>software
1574<|>https://github.com/v-m/PropagationAnalysis-dataset<|>here<|>Dataset generated for those papers can be found here https://github.com/v-m/PropagationAnalysis-dataset .<|>53<|>103<|>48<|>52<|>1574<|>dataset_landing_page
1591<|>http://www.nltk.org/data.html<|>NLTK Data<|>NLTK Data http://www.nltk.org/data.html : punkt<|>10<|>39<|>0<|>9<|>1258<|>software
1591<|>https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/<|>WikiQA: A Challenge Dataset for Open-Domain Question Answering<|>WikiQA: A Challenge Dataset for Open-Domain Question Answering https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/<|>63<|>178<|>0<|>62<|>1591<|>dataset_landing_page
1608<|>https://github.com/JJ/splash-volunteer/tree/data<|><|>Data files are in the https://github.com/JJ/splash-volunteer/tree/data  branch, divided by experiment sets.<|>22<|>70<|>0<|>0<|>1608<|>dataset_landing_page
1615<|>https://github.com/wnzhang/make-ipinyou-data<|>iPinYou data formalizing repository<|>iPinYou data formalizing repository https://github.com/wnzhang/make-ipinyou-data<|>36<|>80<|>0<|>35<|>1021<|>dataset_landing_page
1616<|>http://vowl.visualdataweb.org/webvowl/<|>WebVOWL<|>Integration with diagram creators ( WebVOWL http://vowl.visualdataweb.org/webvowl/ ).<|>44<|>82<|>36<|>43<|>1616<|>other
1616<|>https://oops.linkeddata.es/<|>OOPS! web service<|>Evaluation reports of your ontology (using the OOPS! web service https://oops.linkeddata.es/ )<|>65<|>92<|>47<|>64<|>1616<|>other
1620<|>http://wikidata.org/<|>Wikidata<|>There are many other projects developing schemas and ontologies for the Web, e.g. Wikidata http://wikidata.org/  or the vocabulary projects in the Linked Data http://lov.okfn.org/  community. Many of these projects go into more expressive detail than is possible for a project like Schema.org. To keep Schema.org manageable, we have a strong bias towards designs that are grounded in large scale usage on the Web, in particular usage https://github.com/schemaorg/schemaorg/issues/652  by data-consuming applications since these in turn motivate data publishers. Other schema initiatives have different priorities and make different tradeoffs.<|>91<|>111<|>82<|>90<|>1620<|>other
1624<|>http://lov.okfn.org/dataset/lov/vocabs/mv<|>@lov.okfn.org<|>LOV entry: @lov.okfn.org http://lov.okfn.org/dataset/lov/vocabs/mv<|>25<|>66<|>11<|>24<|>1624<|>other
1638<|>https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html<|>distributed data structures<|>Hazelcast also provides additional data structures such as ReplicatedMap, Set, MultiMap and List. For a full list, refer to the distributed data structures https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html  section of the docs.<|>156<|>248<|>128<|>155<|>1638<|>other
1638<|>https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction<|>Expiring items<|>Expiring items https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction  automatically based on certain criteria like TTL or last access time<|>15<|>96<|>0<|>14<|>1638<|>other
1638<|>https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html<|>docs<|>For examples in other languages, please refer to the docs https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html .<|>58<|>128<|>53<|>57<|>1638<|>other
1638<|>https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html<|>docs<|>For examples in other languages, please refer to the docs https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html .<|>58<|>128<|>53<|>57<|>1638<|>other
1638<|>https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data<|>Read-through and write-through<|>Read-through and write-through https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data  caching patterns<|>31<|>135<|>0<|>30<|>1638<|>other
1657<|>http://datatables.net<|>DataTables<|>A demo of Visual Event that is automatically loaded is available http://sprymedia.co.uk/VisualEvent/demo . This demo uses DataTables http://datatables.net  to create a test page with a number of events attached to different elements.<|>133<|>154<|>122<|>132<|>1657<|>software
1688<|>https://github.com/ekzhu/datasketch<|>Datasketch<|>Datasketch https://github.com/ekzhu/datasketch<|>11<|>46<|>0<|>10<|>1688<|>software
1688<|>http://corpus-texmex.irisa.fr/<|>GIST<|>| Dataset | Dimensions | Train size | Test size | Neighbors | Distance | Download | | ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- | | DEEP1B http://sites.skoltech.ru/compvision/noimi/  | 96 | 9,990,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/deep-image-96-angular.hdf5  (3.6GB) | Fashion-MNIST https://github.com/zalandoresearch/fashion-mnist  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5  (217MB) | | GIST http://corpus-texmex.irisa.fr/  | 960 | 1,000,000 | 1,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/gist-960-euclidean.hdf5  (3.6GB) | | GloVe http://nlp.stanford.edu/projects/glove/  | 25 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-25-angular.hdf5  (121MB) | | GloVe | 50 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-50-angular.hdf5  (235MB) | | GloVe | 100 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-100-angular.hdf5  (463MB) | | GloVe | 200 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-200-angular.hdf5  (918MB) | | Kosarak http://fimi.uantwerpen.be/data/  | 27,983 | 74,962 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/kosarak-jaccard.hdf5  (33MB) | | MNIST http://yann.lecun.com/exdb/mnist/  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/mnist-784-euclidean.hdf5  (217MB) | | MovieLens-10M https://grouplens.org/datasets/movielens/10m/  | 65,134 | 69,363 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/movielens10m-jaccard.hdf5  (63MB) | | NYTimes https://archive.ics.uci.edu/ml/datasets/bag+of+words  | 256 | 290,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/nytimes-256-angular.hdf5  (301MB) | | SIFT http://corpus-texmex.irisa.fr/  | 128 | 1,000,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/sift-128-euclidean.hdf5  (501MB) | | Last.fm https://github.com/erikbern/ann-benchmarks/pull/91  | 65 | 292,385 | 50,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/lastfm-64-dot.hdf5  (135MB) |<|>649<|>679<|>644<|>648<|>1268<|>dataset_landing_page
1688<|>http://corpus-texmex.irisa.fr/<|>SIFT<|>| Dataset | Dimensions | Train size | Test size | Neighbors | Distance | Download | | ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- | | DEEP1B http://sites.skoltech.ru/compvision/noimi/  | 96 | 9,990,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/deep-image-96-angular.hdf5  (3.6GB) | Fashion-MNIST https://github.com/zalandoresearch/fashion-mnist  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5  (217MB) | | GIST http://corpus-texmex.irisa.fr/  | 960 | 1,000,000 | 1,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/gist-960-euclidean.hdf5  (3.6GB) | | GloVe http://nlp.stanford.edu/projects/glove/  | 25 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-25-angular.hdf5  (121MB) | | GloVe | 50 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-50-angular.hdf5  (235MB) | | GloVe | 100 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-100-angular.hdf5  (463MB) | | GloVe | 200 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-200-angular.hdf5  (918MB) | | Kosarak http://fimi.uantwerpen.be/data/  | 27,983 | 74,962 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/kosarak-jaccard.hdf5  (33MB) | | MNIST http://yann.lecun.com/exdb/mnist/  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/mnist-784-euclidean.hdf5  (217MB) | | MovieLens-10M https://grouplens.org/datasets/movielens/10m/  | 65,134 | 69,363 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/movielens10m-jaccard.hdf5  (63MB) | | NYTimes https://archive.ics.uci.edu/ml/datasets/bag+of+words  | 256 | 290,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/nytimes-256-angular.hdf5  (301MB) | | SIFT http://corpus-texmex.irisa.fr/  | 128 | 1,000,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/sift-128-euclidean.hdf5  (501MB) | | Last.fm https://github.com/erikbern/ann-benchmarks/pull/91  | 65 | 292,385 | 50,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/lastfm-64-dot.hdf5  (135MB) |<|>1956<|>1986<|>1951<|>1955<|>1268<|>dataset_landing_page
1688<|>https://grouplens.org/datasets/movielens/10m/<|>MovieLens-10M<|>| Dataset | Dimensions | Train size | Test size | Neighbors | Distance | Download | | ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- | | DEEP1B http://sites.skoltech.ru/compvision/noimi/  | 96 | 9,990,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/deep-image-96-angular.hdf5  (3.6GB) | Fashion-MNIST https://github.com/zalandoresearch/fashion-mnist  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5  (217MB) | | GIST http://corpus-texmex.irisa.fr/  | 960 | 1,000,000 | 1,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/gist-960-euclidean.hdf5  (3.6GB) | | GloVe http://nlp.stanford.edu/projects/glove/  | 25 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-25-angular.hdf5  (121MB) | | GloVe | 50 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-50-angular.hdf5  (235MB) | | GloVe | 100 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-100-angular.hdf5  (463MB) | | GloVe | 200 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-200-angular.hdf5  (918MB) | | Kosarak http://fimi.uantwerpen.be/data/  | 27,983 | 74,962 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/kosarak-jaccard.hdf5  (33MB) | | MNIST http://yann.lecun.com/exdb/mnist/  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/mnist-784-euclidean.hdf5  (217MB) | | MovieLens-10M https://grouplens.org/datasets/movielens/10m/  | 65,134 | 69,363 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/movielens10m-jaccard.hdf5  (63MB) | | NYTimes https://archive.ics.uci.edu/ml/datasets/bag+of+words  | 256 | 290,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/nytimes-256-angular.hdf5  (301MB) | | SIFT http://corpus-texmex.irisa.fr/  | 128 | 1,000,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/sift-128-euclidean.hdf5  (501MB) | | Last.fm https://github.com/erikbern/ann-benchmarks/pull/91  | 65 | 292,385 | 50,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/lastfm-64-dot.hdf5  (135MB) |<|>1619<|>1664<|>1605<|>1618<|>1688<|>dataset_landing_page
1688<|>http://fimi.uantwerpen.be/data/<|>Kosarak<|>| Dataset | Dimensions | Train size | Test size | Neighbors | Distance | Download | | ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- | | DEEP1B http://sites.skoltech.ru/compvision/noimi/  | 96 | 9,990,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/deep-image-96-angular.hdf5  (3.6GB) | Fashion-MNIST https://github.com/zalandoresearch/fashion-mnist  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5  (217MB) | | GIST http://corpus-texmex.irisa.fr/  | 960 | 1,000,000 | 1,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/gist-960-euclidean.hdf5  (3.6GB) | | GloVe http://nlp.stanford.edu/projects/glove/  | 25 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-25-angular.hdf5  (121MB) | | GloVe | 50 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-50-angular.hdf5  (235MB) | | GloVe | 100 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-100-angular.hdf5  (463MB) | | GloVe | 200 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-200-angular.hdf5  (918MB) | | Kosarak http://fimi.uantwerpen.be/data/  | 27,983 | 74,962 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/kosarak-jaccard.hdf5  (33MB) | | MNIST http://yann.lecun.com/exdb/mnist/  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/mnist-784-euclidean.hdf5  (217MB) | | MovieLens-10M https://grouplens.org/datasets/movielens/10m/  | 65,134 | 69,363 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/movielens10m-jaccard.hdf5  (63MB) | | NYTimes https://archive.ics.uci.edu/ml/datasets/bag+of+words  | 256 | 290,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/nytimes-256-angular.hdf5  (301MB) | | SIFT http://corpus-texmex.irisa.fr/  | 128 | 1,000,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/sift-128-euclidean.hdf5  (501MB) | | Last.fm https://github.com/erikbern/ann-benchmarks/pull/91  | 65 | 292,385 | 50,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/lastfm-64-dot.hdf5  (135MB) |<|>1312<|>1343<|>1304<|>1311<|>1688<|>dataset_landing_page
1688<|>https://archive.ics.uci.edu/ml/datasets/bag+of+words<|>NYTimes<|>| Dataset | Dimensions | Train size | Test size | Neighbors | Distance | Download | | ----------------------------------------------------------------- | ---------: | ---------: | --------: | --------: | --------- | -------------------------------------------------------------------------- | | DEEP1B http://sites.skoltech.ru/compvision/noimi/  | 96 | 9,990,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/deep-image-96-angular.hdf5  (3.6GB) | Fashion-MNIST https://github.com/zalandoresearch/fashion-mnist  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/fashion-mnist-784-euclidean.hdf5  (217MB) | | GIST http://corpus-texmex.irisa.fr/  | 960 | 1,000,000 | 1,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/gist-960-euclidean.hdf5  (3.6GB) | | GloVe http://nlp.stanford.edu/projects/glove/  | 25 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-25-angular.hdf5  (121MB) | | GloVe | 50 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-50-angular.hdf5  (235MB) | | GloVe | 100 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-100-angular.hdf5  (463MB) | | GloVe | 200 | 1,183,514 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/glove-200-angular.hdf5  (918MB) | | Kosarak http://fimi.uantwerpen.be/data/  | 27,983 | 74,962 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/kosarak-jaccard.hdf5  (33MB) | | MNIST http://yann.lecun.com/exdb/mnist/  | 784 | 60,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/mnist-784-euclidean.hdf5  (217MB) | | MovieLens-10M https://grouplens.org/datasets/movielens/10m/  | 65,134 | 69,363 | 500 | 100 | Jaccard | HDF5 http://ann-benchmarks.com/movielens10m-jaccard.hdf5  (63MB) | | NYTimes https://archive.ics.uci.edu/ml/datasets/bag+of+words  | 256 | 290,000 | 10,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/nytimes-256-angular.hdf5  (301MB) | | SIFT http://corpus-texmex.irisa.fr/  | 128 | 1,000,000 | 10,000 | 100 | Euclidean | HDF5 http://ann-benchmarks.com/sift-128-euclidean.hdf5  (501MB) | | Last.fm https://github.com/erikbern/ann-benchmarks/pull/91  | 65 | 292,385 | 50,000 | 100 | Angular | HDF5 http://ann-benchmarks.com/lastfm-64-dot.hdf5  (135MB) |<|>1785<|>1837<|>1777<|>1784<|>1688<|>dataset_landing_page
1690<|>https://motchallenge.net/data/MOT15/<|>2D MOT 2015 benchmark dataset<|>Download the 2D MOT 2015 benchmark dataset https://motchallenge.net/data/MOT15/<|>43<|>79<|>13<|>42<|>1690<|>dataset_landing_page
1709<|>https://www.force11.org/group/joint-declaration-data-citation-principles-final<|>Joint Declaration of Data Citation Principle<|>See Joint Declaration of Data Citation Principle https://www.force11.org/group/joint-declaration-data-citation-principles-final  as a example of a similar deliverable.<|>49<|>127<|>4<|>48<|>1709<|>other
1734<|>http://numba.pydata.org<|><|>However, spectralDNS depends on two other modules in the spectralDNS https://github.com/spectralDNS  organization: shenfun https://github.com/spectralDNS/shenfun  and mpi4py-fft https://github.com/spectralDNS/mpi4py-fft . And besides that, it requires http://www.h5py.org  built with parallel HDF5, for visualizing the results, and http://cython.org , http://numba.pydata.org  or https://github.com/serge-sans-paille/pythran  are used to optimize a few routines. These dependencies are all available on https://conda-forge.org  and a proper environment would be<|>352<|>375<|>0<|>0<|>1734<|>software
1742<|>https://www.cs.jhu.edu/~mdredze/datasets/sentiment/<|>here<|>I got them from here https://www.cs.jhu.edu/~mdredze/datasets/sentiment/<|>21<|>72<|>16<|>20<|>1742<|>dataset_landing_page
1742<|>https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz<|>here<|>Available here https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz<|>15<|>95<|>10<|>14<|>1742<|>dataset_landing_page
1762<|>http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets<|>here<|>Due to the file size limit, we are not able to provide all those datasets in our Github repository. In fact, only the  dataset is provided as an example. To deploy a new benchmark dataset, download the zip file from here http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets  and then uncompress it. You will get a dataset folder including two files  and . Move this folder into the data directory https://github.com/yuyuz/FLASH/tree/master/data , just like the dataset folder  we already put there.<|>221<|>278<|>216<|>220<|>1762<|>dataset_landing_page
1762<|>https://github.com/yuyuz/FLASH/tree/master/data<|>data directory<|>Due to the file size limit, we are not able to provide all those datasets in our Github repository. In fact, only the  dataset is provided as an example. To deploy a new benchmark dataset, download the zip file from here http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets  and then uncompress it. You will get a dataset folder including two files  and . Move this folder into the data directory https://github.com/yuyuz/FLASH/tree/master/data , just like the dataset folder  we already put there.<|>402<|>449<|>387<|>401<|>1762<|>dataset_landing_page
1762<|>http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets<|>here<|>All the benchmark datasets are publicly available here http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets . These datasets were first introduced by Auto-WEKA http://www.cs.ubc.ca/labs/beta/Projects/autoweka  and have been widely used to evaluate Bayesian optimization methods.<|>55<|>112<|>50<|>54<|>1762<|>dataset_landing_page
1766<|>https://github.com/tdebatty/java-datasets/releases<|>GitHub releases<|>Or check the GitHub releases https://github.com/tdebatty/java-datasets/releases .<|>29<|>79<|>13<|>28<|>1766<|>software
1766<|>http://www.javadoc.io/doc/info.debatty/java-datasets<|><|> https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets https://travis-ci.org/tdebatty/java-datasets http://www.javadoc.io/doc/info.debatty/java-datasets<|>122<|>174<|>0<|>0<|>1766<|>other
1766<|>http://www.javadoc.io/doc/info.debatty/java-datasets<|>the documentation<|>For the other datasets, check the examples ./src/main/java/info/debatty/java/datasets/examples , or the documentation http://www.javadoc.io/doc/info.debatty/java-datasets .<|>118<|>170<|>100<|>117<|>1766<|>other
1766<|>https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets<|><|> https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets https://travis-ci.org/tdebatty/java-datasets http://www.javadoc.io/doc/info.debatty/java-datasets<|>1<|>76<|>0<|>0<|>1766<|>software
1786<|>http://bokeh.pydata.org/en/latest/docs/installation.html<|>Bokeh<|>Bokeh http://bokeh.pydata.org/en/latest/docs/installation.html  0.8.1+<|>6<|>62<|>0<|>5<|>1786<|>other
1801<|>https://github.com/facebook/zstd#the-case-for-small-data-compression<|>dictionary compression<|>LZ4 is also compatible with dictionary compression https://github.com/facebook/zstd#the-case-for-small-data-compression , both at API https://github.com/lz4/lz4/blob/v1.8.3/lib/lz4frame.h#L481  and CLI https://github.com/lz4/lz4/blob/v1.8.3/programs/lz4.1.md#operation-modifiers  levels. It can ingest any input file as dictionary, though only the final 64KB are used. This capability can be combined with the Zstandard Dictionary Builder https://github.com/facebook/zstd/blob/v1.3.5/programs/zstd.1.md#dictionary-builder , in order to drastically improve compression performance on small files.<|>51<|>119<|>28<|>50<|>1801<|>software
1815<|>https://coveralls.io/github/stanford-futuredata/macrobase?branch=master<|><|> https://travis-ci.org/stanford-futuredata/macrobase https://coveralls.io/github/stanford-futuredata/macrobase?branch=master<|>53<|>124<|>0<|>0<|>1815<|>other
1821<|>http://grouplens.org/datasets/movielens/<|>MovieLens<|>This script will turn an external raw dataset into torch format. The dataset will be split into a training/testing set by using the training ratio. When side inforamtion exist, they are automatically appended to the inputs. The MovieLens http://grouplens.org/datasets/movielens/  and Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  dataset are supported by default.<|>238<|>278<|>228<|>237<|>269<|>dataset_landing_page
1821<|>http://grouplens.org/datasets/movielens/10m/<|>MovieLens-10M<|>| Dataset | user info | item info | item tags | | :------- | --------: | :--------: | --------: | | MovieLens-1M http://grouplens.org/datasets/movielens/1m/  | true | true | false | | MovieLens-10M http://grouplens.org/datasets/movielens/10m/  | false | true | true | | MovieLens-20M http://grouplens.org/datasets/movielens/20m/  | false | true | true | | Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  | true | info | false |<|>198<|>242<|>184<|>197<|>1821<|>dataset_landing_page
1821<|>https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban<|>Douban<|>This script will turn an external raw dataset into torch format. The dataset will be split into a training/testing set by using the training ratio. When side inforamtion exist, they are automatically appended to the inputs. The MovieLens http://grouplens.org/datasets/movielens/  and Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  dataset are supported by default.<|>291<|>345<|>284<|>290<|>1821<|>dataset_landing_page
1821<|>http://grouplens.org/datasets/movielens/1m/<|>MovieLens-1M<|>| Dataset | user info | item info | item tags | | :------- | --------: | :--------: | --------: | | MovieLens-1M http://grouplens.org/datasets/movielens/1m/  | true | true | false | | MovieLens-10M http://grouplens.org/datasets/movielens/10m/  | false | true | true | | MovieLens-20M http://grouplens.org/datasets/movielens/20m/  | false | true | true | | Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  | true | info | false |<|>113<|>156<|>100<|>112<|>1821<|>dataset_landing_page
1821<|>http://grouplens.org/datasets/movielens/20m/<|>MovieLens-20M<|>| Dataset | user info | item info | item tags | | :------- | --------: | :--------: | --------: | | MovieLens-1M http://grouplens.org/datasets/movielens/1m/  | true | true | false | | MovieLens-10M http://grouplens.org/datasets/movielens/10m/  | false | true | true | | MovieLens-20M http://grouplens.org/datasets/movielens/20m/  | false | true | true | | Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  | true | info | false |<|>284<|>328<|>270<|>283<|>1821<|>dataset_landing_page
1821<|>https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban<|>Douban<|>| Dataset | user info | item info | item tags | | :------- | --------: | :--------: | --------: | | MovieLens-1M http://grouplens.org/datasets/movielens/1m/  | true | true | false | | MovieLens-10M http://grouplens.org/datasets/movielens/10m/  | false | true | true | | MovieLens-20M http://grouplens.org/datasets/movielens/20m/  | false | true | true | | Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  | true | info | false |<|>363<|>417<|>356<|>362<|>1821<|>dataset_landing_page
1822<|>http://saliency.mit.edu/datasets.html<|>MIT300<|>MIT300 http://saliency.mit.edu/datasets.html .<|>7<|>44<|>0<|>6<|>1822<|>dataset_landing_page
1822<|>https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application<|>BigGraph TEC2013-43935-R<|>| | | |:--|:-:| | We gratefully acknowledge the support of NVIDIA Corporation http://www.nvidia.com/content/global/global.php  with the donation of the GeoForce GTX Titan Z http://www.nvidia.com/gtx-700-graphics-cards/gtx-titan-z/  and Titan X http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x  used in this work. |  | | The Image ProcessingGroup at the UPC is a SGR14 Consolidated Research Group https://imatge.upc.edu/web/projects/sgr14-image-and-video-processing-group  recognized and sponsored by the Catalan Government (Generalitat de Catalunya) through its AGAUR http://agaur.gencat.cat/en/inici/index.html  office. |  | | This work has been developed in the framework of the project BigGraph TEC2013-43935-R https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application , funded by the Spanish Ministerio de Economía y Competitividad and the European Regional Development Fund (ERDF). |  | | This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/12/RC/2289. |  |<|>730<|>853<|>705<|>729<|>1374<|>other
1824<|>https://github.com/wnzhang/make-ipinyou-data<|>make-ipinyou-data<|>Please check our GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data . After downloading the dataset, by simplying  you can generate the standardised data which will be used in the bid optimisation tasks.<|>50<|>94<|>32<|>49<|>1021<|>dataset_landing_page
1830<|>https://github.com/hans/adversarial/tree/master/data/lfwcrop_color<|>in the<|>You can perform your own training runs using these YAML files. The paths in the YAML files reference my own local data; you'll need to download the LFW dataset and change these paths yourself. The "file-list" and embedding files referenced in the YAML files are available for LFW in the https://github.com/hans/adversarial/tree/master/data/lfwcrop_color . Once you have the paths in the YAML file, you can start training a model with the simple invocation of Pylearn2's  binary, e.g.<|>287<|>353<|>280<|>286<|>1830<|>dataset_landing_page
1830<|>https://github.com/hans/adversarial/blob/master/sampler/data_browser.py<|><|> https://github.com/hans/adversarial/blob/master/sampler/data_browser.py<|>1<|>72<|>0<|>0<|>1830<|>software
1857<|>https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt<|>here<|>WCRP assumes that your student data are in a space-delimited text file with one row per trial. The columns should correspond to a trial's student ID, item ID, and whether the student produced a correct response in the trial. The IDs should be integers beginning at 0, and the trials for each student should be ordered from least to most recent. An example data file is available here https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt<|>384<|>463<|>379<|>383<|>1857<|>software
1857<|>https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt<|>here<|>Our model's nonparametric prior distribution over skill labels can leverage skill labels provided by a human domain expert. If you want to provide them to our model, create a text file with one line per item. The ith line should be the expert-provided skill ID for the ith item. You can provide the file to WCRP via the command line argument --expertfile. An example file is available here https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt<|>390<|>475<|>385<|>389<|>1857<|>software
1878<|>http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools<|>SemEval 2014 Cross-level Semantic Similarity Task<|>[ ] SemEval 2014 Cross-level Semantic Similarity Task http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools  (TODO; 500 paragraph-to-sentence training items)<|>54<|>119<|>4<|>53<|>1878<|>dataset_landing_page
1878<|>http://allenai.org/data.html<|>AI2 8th Grade Science Questions<|>[X] AI2 8th Grade Science Questions http://allenai.org/data.html  are 641 school Science quiz questions (A/B/C/D test format), stemming from The Allen AI Science Challenge https://www.kaggle.com/c/the-allen-ai-science-challenge/  We are going to produce a dataset that merges questions and answers in a single sentence, and pairs each with potential-evidencing sentences from Wikipedia and CK12 textbooks. This will be probably the hardest dataset by far included in this repo for some time. (We may also want to include the Elementary dataset.)<|>36<|>64<|>4<|>35<|>1878<|>dataset_landing_page
1878<|>http://www.nist.gov/tac/data/<|>TAC tracks RTE-4 to RTE-7<|>[ ] TAC tracks RTE-4 to RTE-7 http://www.nist.gov/tac/data/ . Printed user agreement required.<|>30<|>59<|>4<|>29<|>1878<|>other
1885<|>https://carbondata.apache.org<|>Apache CarbonData<|>Apache CarbonData https://carbondata.apache.org ,<|>18<|>47<|>0<|>17<|>1885<|>software
1885<|>https://www.influxdata.com<|>InfluxDB<|>The YouTube SQL Engine, Google Procella https://research.google/pubs/pub48388/ , uses Roaring bitmaps for indexing. Apache Lucene http://lucene.apache.org/  uses Roaring bitmaps, though they have their own independent implementation https://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.java?view=markup&pathrev=1629606 . Derivatives of Lucene such as Solr and Elastic also use Roaring bitmaps. Other platforms such as Whoosh https://pypi.python.org/pypi/Whoosh/ , Microsoft Visual Studio Team Services (VSTS) https://www.visualstudio.com/team-services/  and Pilosa https://github.com/pilosa/pilosa  also use Roaring bitmaps with their own implementations. You find Roaring bitmaps in InfluxDB https://www.influxdata.com , Bleve http://www.blevesearch.com , Cloud Torrent https://github.com/jpillora/cloud-torrent , and so forth.<|>760<|>786<|>751<|>759<|>1885<|>software
1891<|>https://www.fc.up.pt/addi/ph2%20database.html<|>PH<|>In this example we use the tools provided by the micompr https://github.com/fakenmc/micompr  package to study the PH https://www.fc.up.pt/addi/ph2%20database.html  of dermoscopic images. This image database contains a total of 200 dermoscopic images of melanocytic lesions, including, from benign to more serious, 80 common nevi, 80 atypical nevi, and 40 melanomas. The goal is to verify if images of the three types of lesions form statistically distinguishable samples.<|>117<|>162<|>114<|>116<|>1891<|>dataset_landing_page
1903<|>https://github.com/rodrigo-pena/load-data<|>load-data<|>If you want to work with John Snow's GIS, or the ETEX data (as used in the paper https://arxiv.org/abs/1603.07584 ), you will need to clone another repository, load-data https://github.com/rodrigo-pena/load-data , and follow the installation instructions in the README therein.<|>170<|>211<|>160<|>169<|>1903<|>software
1915<|>http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz<|>Annotation<|>Annotation http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz<|>11<|>89<|>0<|>10<|>1915<|>dataset_direct_link
1915<|>http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz<|>Images<|>Images http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz<|>7<|>82<|>0<|>6<|>1915<|>dataset_direct_link
1968<|>https://snap.stanford.edu/data/egonets-Facebook.html<|>facebook<|>facebook https://snap.stanford.edu/data/egonets-Facebook.html , should find 5<|>9<|>61<|>0<|>8<|>1968<|>dataset_landing_page
1971<|>http://www.datasyslab.org/<|>DataSys Lab<|>Hippo index is one of the projects under DataSys Lab http://www.datasyslab.org/  at Arizona State University. The mission of DataSys Lab is designing and developing experimental data management systems (e.g., database systems).<|>53<|>79<|>41<|>52<|>1971<|>other
1980<|>https://github.com/countries/countries-data-yaml<|>YAML<|>The data used in this gem is also available as git submodules in YAML https://github.com/countries/countries-data-yaml  and JSON https://github.com/countries/countries-data-json  files.<|>70<|>118<|>65<|>69<|>1980<|>dataset_landing_page
1980<|>https://github.com/countries/countries-data-json<|>JSON<|>The data used in this gem is also available as git submodules in YAML https://github.com/countries/countries-data-yaml  and JSON https://github.com/countries/countries-data-json  files.<|>129<|>177<|>124<|>128<|>1980<|>dataset_landing_page
2003<|>https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip<|>2486 .lab files<|>Use 2486 .lab files https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip  to do even more interesting!<|>20<|>114<|>4<|>19<|>2003<|>software
2005<|>https://www.crossref.org/services/metadata-delivery/plus-service/<|>Metadata Plus subscribers<|>| filter | possible values | description| |:-----------|:----------------|:-----------| |  | | Member has made their references public for one or more of their prefixes | |  |  | Members who have made their references either ,  (to Metadata Plus subscribers https://www.crossref.org/services/metadata-delivery/plus-service/ ) or  | |  | {integer} | count of DOIs for material published more than two years ago | |  | {integer} | count of DOIs for material published within last two years |<|>258<|>323<|>232<|>257<|>2005<|>other
2005<|>https://www.crossref.org/services/metadata-delivery/<|>a variety of tools and APIs<|>The Crossref REST API is one of a variety of tools and APIs https://www.crossref.org/services/metadata-delivery/  that allow anybody to search and reuse our members' metadata in sophisticated ways.<|>60<|>112<|>32<|>59<|>2005<|>other
2005<|>https://www.crossref.org/services/metadata-delivery/plus-service/<|>have a service-level offering<|>What if you want to use our API for a production service that cannot depend on the performance uncertainties of the free and open public API? What if you don't want to be affected by impolite people who do not follow the API Etiquette #api-etiquette  guidelines? Well, if you’re interested in using these tools or APIs for production services, we have a service-level offering https://www.crossref.org/services/metadata-delivery/plus-service/  called "Plus". This service provides you with access to all supported APIs and metadata, but with extra service and support guarantees.<|>377<|>442<|>347<|>376<|>2005<|>other
2005<|>https://www.crossref.org/services/metadata-delivery/plus-service/<|>Metadata Plus subscribers<|>| filter | possible values | description| |:-----------|:----------------|:-----------| |  | | metadata which includes one or more funder entry | |  |  | metadata which include the  in FundRef data | |  |  | funder records where location = . Only works on  route | |  |  | metadata belonging to a DOI owner prefix  (e.g.  ) | |  |  | metadata belonging to a Crossref member | |  |  | metadata indexed since (inclusive)  | |  |  | metadata indexed before (inclusive)  | |  |  | metadata last (re)deposited since (inclusive)  | |  |  | metadata last (re)deposited before (inclusive)  | |  |  | Metadata updated since (inclusive) . Currently the same as . | |  |  | Metadata updated before (inclusive) . Currently the same as . | |  |  | metadata first deposited since (inclusive)  | |  |  | metadata first deposited before (inclusive)  | |  |  | metadata where published date is since (inclusive)  | |  |  | metadata where published date is before (inclusive)  | |  |  | metadata where online published date is since (inclusive)  | |  |  | metadata where online published date is before (inclusive)  | |  |  | metadata where print published date is since (inclusive)  | |  |  | metadata where print published date is before (inclusive)  | |  |  | metadata where posted date is since (inclusive)  | |  |  | metadata where posted date is before (inclusive)  | |  |  | metadata where accepted date is since (inclusive)  | |  |  | metadata where accepted date is before (inclusive)  | |  | | metadata that includes any  elements. | |  |  | metadata where  value equals  | |  |  | metadata where the 's  attribute is | |  |  | metadata where difference between publication date and the 's  attribute is <=  (in days)| |  | | metadata that includes any full text  elements. | |  |  | metadata where  element's  attribute is . | |  |  | metadata where  element's  attribute is  (e.g. ). | |  |  | metadata where  link has one of the following intended applications: ,  or  | |  | | metadata for works that have a list of references | |  |  | metadata for works where references are either ,  (to Metadata Plus subscribers https://www.crossref.org/services/metadata-delivery/plus-service/ ) or  | |  | | metadata which include name of archive partner | |  |  | metadata which where value of archive partner is  | |  | | metadata which includes one or more ORCIDs | |  | | metadata which includes one or more ORCIDs where the depositing publisher claims to have witness the ORCID owner authenticate with ORCID | |  |  | metadata where  element's value =  | |  |  | metadata where record has an ISSN = . Format is . | |  |  | metadata where record has an ISBN = . | |  |  | metadata records whose type = . Type must be an ID value from the list of types returned by the  resource | |  |  | metadata records whose article or serial are mentioned in the given . Currently the only supported value is . | |  |  | metadata describing the DOI  | |  |  | metadata for records that represent editorial updates to the DOI  | |  | | metadata for records that represent editorial updates | |  | | metadata for records that include a link to an editorial update policy | |  | | metadata for records with a publication title exactly with an exact match | |  | | metadata for records with an exact matching category label. Category labels come from this list https://www.elsevier.com/solutions/scopus/content  published by Scopus | |  | | metadata for records with type matching a type identifier (e.g. ) | |  | | metadata for records with an exactly matching type label | |  |  | metadata for records with a matching award nunber. Optionally combine with  | |  |  | metadata for records with an award with matching funder. Optionally combine with  | |  | | metadata for records with any assertions | |  | | metadata for records with an assertion in a particular group | |  | | metadata for records with a particular named assertion | |  | | metadata for records that have any affiliation information | |  | | metadata for records with the given alternative ID, which may be a publisher-specific ID, or any other identifier a publisher may have provided | |  | | metadata for records with a given article number | |  | | metadata for records which include an abstract | |  | | metadata for records which include a clinical trial number | |  | | metadata where the publisher records a particular domain name as the location Crossmark content will appear | |  | | metadata where the publisher records a domain name location for Crossmark content | |  | | metadata where the publisher restricts Crossmark usage to content domains | |  | | metadata for records that either assert or are the object of a relation | |  | | One of the relation types from the Crossref relations schema (e.g. , , ) | |  | | Relations where the object identifier matches the identifier provided | |  | | One of the identifier types from the Crossref relations schema (e.g. , ) |<|>2113<|>2178<|>2087<|>2112<|>2005<|>other
2006<|>https://github.com/kermitt2/article-dataset-builder<|>article-dataset-builder<|>Finally, the following python utilities can be used to create structured full text corpora of scientific articles. The tool simply takes a list of strong identifiers like DOI or PMID, performing the identification of online Open Access PDF, full text harvesting, metadata aggregation and Grobid processing in one workflow at scale: article-dataset-builder https://github.com/kermitt2/article-dataset-builder<|>356<|>407<|>332<|>355<|>2006<|>software
2006<|>https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat<|>small, high quality sets<|>GROBID does not use training data derived from existing publisher XML documents, but small, high quality sets https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat  of manually labeled training data.<|>110<|>200<|>85<|>109<|>2006<|>other
2006<|>https://github.com/kermitt2/datastet<|>datastet<|>datastet https://github.com/kermitt2/datastet : identification of named and implicit research datasets and associated attributes in scientific articles<|>9<|>45<|>0<|>8<|>2006<|>software
2006<|>https://github.com/dataseer/dataseer-ml<|>dataseer-ml<|>dataseer-ml https://github.com/dataseer/dataseer-ml : identification of sections and sentences introducing datasets in a scientific article, and classification of the type of these datasets<|>12<|>51<|>0<|>11<|>2006<|>software
2024<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101<|>Pre-computed optical flow images and resized rgb frames for the UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets<|>71<|>106<|>64<|>70<|>2024<|>dataset_landing_page
2024<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB51<|>Pre-computed optical flow images and resized rgb frames for the UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets<|>119<|>195<|>112<|>118<|>2024<|>dataset_landing_page
2031<|>https://www.kaggle.com/c/rossmann-store-sales/data<|>Kaggle<|>To run the code one needs first download and unzip the  and  files on Kaggle https://www.kaggle.com/c/rossmann-store-sales/data  and put them in this folder.<|>77<|>127<|>70<|>76<|>2031<|>dataset_landing_page
2046<|>http://snap.stanford.edu/data/index.html<|>SNAP format<|>converts a graph in SNAP format http://snap.stanford.edu/data/index.html  and converts it to Ligra's adjacency graph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name. The "-s" flag may be used to symmetrize the input file. This converter works for any format that lists the two endpoints of each edge separated by white space per line, with lines starting with '#' ignored.<|>32<|>72<|>20<|>31<|>269<|>dataset_landing_page
2046<|>http://snap.stanford.edu/data/index.html<|>SNAP format<|>converts a communities network in SNAP format http://snap.stanford.edu/data/index.html  and converts it to symmetric adjacency hypergraph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name.<|>46<|>86<|>34<|>45<|>269<|>dataset_landing_page
2047<|>http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip<|>3DPeS<|>3DPeS http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip<|>6<|>73<|>0<|>5<|>2047<|>dataset_direct_link
2104<|>http://caffe2.ai/docs/datasets.html<|>Datasets<|>Datasets http://caffe2.ai/docs/datasets.html<|>9<|>44<|>0<|>8<|>2104<|>dataset_landing_page
2111<|>https://www.coursera.org/specializations/tensorflow-data-and-deployment<|>TensorFlow: Data and Deployment from Coursera<|>TensorFlow: Data and Deployment from Coursera https://www.coursera.org/specializations/tensorflow-data-and-deployment<|>46<|>117<|>0<|>45<|>2111<|>other
2124<|>http://numba.pydata.org/<|>Numba<|>Numba http://numba.pydata.org/<|>6<|>30<|>0<|>5<|>2124<|>software
2210<|>https://archive.ics.uci.edu/ml/datasets/Adult<|>UCI Adult Data Set<|>We cannot release our dataset publicly, so the a toy dataset needs to be set up. We use the UCI Adult Data Set https://archive.ics.uci.edu/ml/datasets/Adult  for this purpose. Although it is a regular tabular dataset, and has a Gausian distribution instead of a heavy tailed one over the attributes, it is fine for experimenting. Then you can work on your own.<|>111<|>156<|>92<|>110<|>2210<|>dataset_landing_page
2211<|>http://google.github.io/rappor/doc/data-flow.html<|>RAPPOR Data Flow<|>RAPPOR Data Flow http://google.github.io/rappor/doc/data-flow.html<|>17<|>66<|>0<|>16<|>2211<|>other
2224<|>https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/<|>here<|>Some of the data file can be download at here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/<|>46<|>114<|>41<|>45<|>2224<|>dataset_landing_page
2224<|>https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json<|>here<|>if you use the vqa train+val https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/vqa_model/model_alternating_train-val_vgg.t7  model, you should use the corresponding json file form here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json<|>199<|>291<|>194<|>198<|>2224<|>dataset_direct_link
2224<|>http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py<|>here<|>To Evaluate VQA, you need to download the VQA evaluation tool https://github.com/VT-vision-lab/VQA . To evaluate COCO-QA, you can use script  under  folder. If you need to evaluate based on WUPS, download the evaluation script from here http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py<|>237<|>311<|>232<|>236<|>2224<|>software
2224<|>https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json<|>here<|>The pre-trained model can be download here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/ , if you use the vqa train https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/vqa_model/model_alternating_train_vgg.t7  model, you should use the corresponding json file form here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json<|>301<|>389<|>296<|>300<|>2224<|>dataset_direct_link
2252<|>http://conll.cemantix.org/2012/data.html<|>here<|>Download the CoNLL training data from here http://conll.cemantix.org/2012/data.html .<|>43<|>83<|>38<|>42<|>2252<|>dataset_landing_page
2264<|>http://www.opentracker.net/article/definitions-big-data<|>many<|>There is no formal definition of Big Data – though many http://www.opentracker.net/article/definitions-big-data  have tried. Most of them agree that it is more than 'large data', more than just its size.<|>56<|>111<|>51<|>55<|>2264<|>other
2270<|>https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh<|>dataset repository<|>dataset_forVD2D3D.spec https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec  defines path for image/label files of your dataset. For example images 1-12 refer to the images from the microscope, 13-24 refer to the output images from the VD2D part of the recursive "two-stage" approach of ZNN. The outputs of the VD2D part are provided in the dataset repository https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh , and can be used for re-training of the VD2D3D stage, or if you may you can obtain new VD2D outputs for your dataset if you wish.<|>416<|>534<|>397<|>415<|>2270<|>dataset_landing_page
2270<|>https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec<|>dataset_forVD2D3D.spec<|>dataset_forVD2D3D.spec https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec  defines path for image/label files of your dataset. For example images 1-12 refer to the images from the microscope, 13-24 refer to the output images from the VD2D part of the recursive "two-stage" approach of ZNN. The outputs of the VD2D part are provided in the dataset repository https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh , and can be used for re-training of the VD2D3D stage, or if you may you can obtain new VD2D outputs for your dataset if you wish.<|>23<|>131<|>0<|>22<|>2270<|>dataset_landing_page
2282<|>https://github.com/deepmind/rc-data<|>https://github.com/deepmind/rc-data<|>The original datasets can be downloaded from https://github.com/deepmind/rc-data https://github.com/deepmind/rc-data  or http://cs.nyu.edu/~kcho/DMQA/ http://cs.nyu.edu/~kcho/DMQA/ . Our processed ones are just simply concatenation of all data instances and keeping document, question and answer only for our inputs.<|>81<|>116<|>45<|>80<|>2282<|>dataset_landing_page
2285<|>https://github.com/harvardnlp/BSO/tree/master/data_prep<|>data_prep/<|>First prepare the data as in data_prep/ https://github.com/harvardnlp/BSO/tree/master/data_prep .<|>40<|>95<|>29<|>39<|>2285<|>dataset_landing_page
2334<|>http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip<|>here<|>Download the official SemEval evaluation materials from here http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip  and put the file  in the  directory.<|>61<|>139<|>56<|>60<|>2334<|>dataset_direct_link
2346<|>https://github.com/tensorpack/dataflow<|><|>Squeeze the best data loading performance of Python with https://github.com/tensorpack/dataflow .<|>57<|>95<|>0<|>0<|>2346<|>software
2346<|>https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions<|>does not<|>Symbolic programming (e.g. ) does not https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions  offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of  with various autoparallelization strategies.<|>38<|>140<|>29<|>37<|>2346<|>other
2354<|>http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/<|>here<|>The JIGSAWS dataset is available here http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/ . After registering, an automated system will send you a download link. To run this code, you'll need to download the Suturing Kinematics data.<|>38<|>101<|>33<|>37<|>2354<|>dataset_landing_page
2361<|>https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp<|>Recursive Length Prefix<|>| Command | Description | | :--------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | |  | Our main Ethereum CLI client. It is the entry point into the Ethereum network (main-, test- or private net), capable of running as a full node (default), archive node (retaining all historical state) or a light node (retrieving data live). It can be used by other processes as a gateway into the Ethereum network via JSON RPC endpoints exposed on top of HTTP, WebSocket and/or IPC transports.  and the CLI page https://geth.ethereum.org/docs/fundamentals/command-line-options  for command line options. | |  | Stand-alone signing tool, which can be used as a backend signer for . | |  | Utilities to interact with nodes on the networking layer, without running a full blockchain. | |  | Source code generator to convert Ethereum contract definitions into easy-to-use, compile-time type-safe Go packages. It operates on plain Ethereum contract ABIs https://docs.soliditylang.org/en/develop/abi-spec.html  with expanded functionality if the contract bytecode is also available. However, it also accepts Solidity source files, making development much more streamlined. Please see our Native DApps https://geth.ethereum.org/docs/developers/dapp-developer/native-bindings  page for details. | |  | Stripped down version of our Ethereum client implementation that only takes part in the network node discovery protocol, but does not run any of the higher level application protocols. It can be used as a lightweight bootstrap node to aid in finding peers in private networks. | |  | Developer utility version of the EVM (Ethereum Virtual Machine) that is capable of running bytecode snippets within a configurable environment and execution mode. Its purpose is to allow isolated, fine-grained debugging of EVM opcodes (e.g. ). | |  | Developer utility tool to convert binary RLP ( Recursive Length Prefix https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp ) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. ). |<|>2362<|>2434<|>2338<|>2361<|>2361<|>other
2370<|>http://buckeyecorpus.osu.edu/<|>buckeyecorpus.osu.edu<|>Buckeye corpus: buckeyecorpus.osu.edu http://buckeyecorpus.osu.edu/<|>38<|>67<|>16<|>37<|>2370<|>dataset_landing_page
2404<|>http://conda.pydata.org/miniconda.html<|>Install<|>Install http://conda.pydata.org/miniconda.html<|>8<|>46<|>0<|>7<|>155<|>software
2405<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>The program also provides the implementation of the embedding model TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data . See an overview of embedding models of entities and relationships for knowledge base completion at HERE https://arxiv.org/pdf/1703.08098.pdf .<|>75<|>166<|>68<|>74<|>2405<|>other
2414<|>https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava<|>guava corpus<|>guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava  and java grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java.g4<|>13<|>85<|>0<|>12<|>2414<|>software
2414<|>https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava<|>guava corpus<|>guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava  and java8 grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java8.g4<|>13<|>85<|>0<|>12<|>2414<|>software
2414<|>https://github.com/antlr/codebuff/tree/master/corpus<|>corpora<|>All input is completed squeezed of whitespace/newlines so only the output really matters when examining CodeBuff output. You can check out the output https://github.com/antlr/codebuff/tree/master/output  dir for leave-one-out formatting of the various corpora https://github.com/antlr/codebuff/tree/master/corpus . But, here are some sample formatting results.<|>260<|>312<|>252<|>259<|>2414<|>software
2414<|>https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training<|>antlr corpus<|>antlr corpus https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training  and antlr parser grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Parser.g4 , antlr lexer grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Lexer.g4<|>13<|>81<|>0<|>12<|>2414<|>software
2449<|>https://fasttext.cc/docs/en/dataset.html#content<|>YFCC100M data<|>The preprocessed YFCC100M data https://fasttext.cc/docs/en/dataset.html#content  used in [2].<|>31<|>79<|>17<|>30<|>2449<|>dataset_landing_page
2456<|>https://github.com/numenta/NAB/tree/master/data<|>data readme<|>The majority of the data is real-world from a variety of sources such as AWS server metrics, Twitter volume, advertisement clicking metrics, traffic data, and more. All data is included in the repository, with more details in the data readme https://github.com/numenta/NAB/tree/master/data . Please contact us at nab@numenta.org mailto:nab@numenta.org  if you have similar data (ideally with known anomalies) that you would like to see incorporated into NAB.<|>242<|>289<|>230<|>241<|>2456<|>dataset_landing_page
2468<|>https://star-datasets.github.io/vector/<|>Dataset<|>Gao, L., Liang, Y., Yang, J., Wu, S., Wang, C., Chen, J., Kneip, L., , IEEE Robotics and Automation Letters (RA-L), 7(3):8217-8224, 2022. PDF https://arxiv.org/abs/2207.01404 , Dataset https://star-datasets.github.io/vector/ , MPL Calibration Toolbox https://github.com/mgaoling/mpl_calibration_toolbox , MPL Dataset Toolbox https://github.com/mgaoling/mpl_dataset_toolbox .<|>185<|>224<|>177<|>184<|>2468<|>dataset_landing_page
2468<|>https://github.com/mgaoling/mpl_dataset_toolbox<|>MPL Dataset Toolbox<|>Gao, L., Liang, Y., Yang, J., Wu, S., Wang, C., Chen, J., Kneip, L., , IEEE Robotics and Automation Letters (RA-L), 7(3):8217-8224, 2022. PDF https://arxiv.org/abs/2207.01404 , Dataset https://star-datasets.github.io/vector/ , MPL Calibration Toolbox https://github.com/mgaoling/mpl_calibration_toolbox , MPL Dataset Toolbox https://github.com/mgaoling/mpl_dataset_toolbox .<|>325<|>372<|>305<|>324<|>2468<|>software
2468<|>https://github.com/mgaoling/mpl_dataset_toolbox<|>events_bag2h5<|>events_bag2h5 https://github.com/mgaoling/mpl_dataset_toolbox  Python code to convert event data from ROSbags to HDF5.<|>14<|>61<|>0<|>13<|>2468<|>software
2468<|>http://www.garrickorchard.com/datasets/n-mnist<|>Neuromorphic-MNIST (N-MNIST) dataset<|>Neuromorphic-MNIST (N-MNIST) dataset http://www.garrickorchard.com/datasets/n-mnist  is a spiking version of the original frame-based MNIST dataset (of handwritten digits). YouTube https://youtu.be/6qK97qM5aB4<|>37<|>83<|>0<|>36<|>2468<|>dataset_landing_page
2468<|>http://www.prophesee.ai/dataset-n-cars/<|>N-CARS Dataset<|>N-CARS Dataset http://www.prophesee.ai/dataset-n-cars/ : A large real-world event-based dataset for car classification. Sironi et al., CVPR 2018 #Sironi18cvpr .<|>15<|>54<|>0<|>14<|>2468<|>dataset_landing_page
2468<|>http://www.garrickorchard.com/datasets/n-caltech101<|>The Neuromorphic-Caltech101 (N-Caltech101) dataset<|>The Neuromorphic-Caltech101 (N-Caltech101) dataset http://www.garrickorchard.com/datasets/n-caltech101  is a spiking version of the original frame-based Caltech101 dataset. YouTube https://youtu.be/dxit9Ce5f_E<|>51<|>102<|>0<|>50<|>2468<|>dataset_landing_page
2468<|>https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox<|>Code<|>de Tournemire, P., Nitti, D., Perot, E., Migliore, D., Sironi, A., , arXiv, 2020. Code https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox , News https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/<|>87<|>155<|>82<|>86<|>2468<|>dataset_landing_page
2468<|>https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox<|>Code<|>Prophesee automotive dataset toolbox, Code https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox<|>43<|>111<|>38<|>42<|>2468<|>dataset_landing_page
2468<|>https://visibilitydataset.github.io/<|>ViViD++: Vision for Visibility Dataset<|>Lee, A. J., Cho, Y., Shin, Y., Kim, A., Myung, H., ViViD++: Vision for Visibility Dataset https://visibilitydataset.github.io/ , IEEE Robotics and Automation Letters (RA-L), 2022. Dataset https://visibilitydataset.github.io/<|>90<|>126<|>51<|>89<|>2468<|>dataset_landing_page
2468<|>https://visibilitydataset.github.io/<|>Dataset<|>Lee, A. J., Cho, Y., Shin, Y., Kim, A., Myung, H., ViViD++: Vision for Visibility Dataset https://visibilitydataset.github.io/ , IEEE Robotics and Automation Letters (RA-L), 2022. Dataset https://visibilitydataset.github.io/<|>188<|>224<|>180<|>187<|>2468<|>dataset_landing_page
2468<|>https://github.com/wl082013/ESIM_dataset<|>Dataset<|>Wang, L., Kim, T.-K., Yoon, K.-J., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. PDF https://arxiv.org/pdf/2003.07640 , YouTube https://youtu.be/OShS_MwHecs , Dataset https://github.com/wl082013/ESIM_dataset<|>188<|>228<|>180<|>187<|>2468<|>dataset_landing_page
2468<|>https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO<|>Dataset<|>Brebion, V., Moreau, J., Davoine, F., , IEEE Trans. Intell. Transp. Syst. (T-ITS), 2021. PDF https://arxiv.org/pdf/2112.10591.pdf , Code https://github.com/vbrebion/rt_of_low_high_res_event_cameras , Dataset https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO , YouTube https://youtube.com/playlist?list=PLLL0eWAd6OXBRXli-tB1NREdhBElAxisD .<|>208<|>257<|>200<|>207<|>2468<|>dataset_landing_page
2468<|>https://sites.google.com/view/davis-driving-dataset-2020/home<|>Dataset<|>Hu, Y., Binas, J., Neil, D., Liu, S.-C., Delbruck, T., , IEEE Intelligent Transportation Systems Conf. (ITSC), 2020. Dataset https://sites.google.com/view/davis-driving-dataset-2020/home , More datasets http://sensors.ini.uzh.ch/databases.html<|>125<|>186<|>117<|>124<|>2468<|>dataset_landing_page
2468<|>http://sensors.ini.uzh.ch/databases.html<|>More datasets<|>Hu, Y., Binas, J., Neil, D., Liu, S.-C., Delbruck, T., , IEEE Intelligent Transportation Systems Conf. (ITSC), 2020. Dataset https://sites.google.com/view/davis-driving-dataset-2020/home , More datasets http://sensors.ini.uzh.ch/databases.html<|>203<|>243<|>189<|>202<|>2468<|>dataset_landing_page
2468<|>http://sensors.ini.uzh.ch/databases.html<|>Dataset<|>Binas, J., Neil, D., Liu, S.-C., Delbruck, T.,  Int. Conf. Machine Learning, Workshop on Machine Learning for Autonomous Vehicles, 2017. Dataset http://sensors.ini.uzh.ch/databases.html<|>145<|>185<|>137<|>144<|>2468<|>dataset_landing_page
2468<|>http://sensors.ini.uzh.ch/databases.html<|>Datasets from the Sensors group at INI<|>Datasets from the Sensors group at INI http://sensors.ini.uzh.ch/databases.html  (Institute of Neuroinformatics), Zurich:<|>39<|>79<|>0<|>38<|>2468<|>dataset_landing_page
2468<|>https://sites.google.com/a/udayton.edu/issl/software/dataset<|>Dataset<|>Baldwin R.W., Almatrafi M., Asari V., Hirakawa K., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. PDF https://arxiv.org/abs/2003.08282 , Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset<|>165<|>225<|>157<|>164<|>2468<|>dataset_landing_page
2468<|>https://sites.google.com/a/udayton.edu/issl/software/dataset<|>DVSNOISE20<|>DVSNOISE20 https://sites.google.com/a/udayton.edu/issl/software/dataset  associated to the paper Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras #Baldwin20cvpr .<|>11<|>71<|>0<|>10<|>2468<|>dataset_landing_page
2468<|>https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0<|>Dataset<|>Almatrafi, M., Baldwin, R., Aizawa, K., Hirakawa, K., , IEEE Trans. Pattern Anal. Machine Intell. (TPAMI), 2020. PDF https://arxiv.org/pdf/2003.12680 , Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0<|>160<|>231<|>152<|>159<|>2468<|>dataset_landing_page
2468<|>https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0<|>DVSMOTION20 Dataset<|>Almatrafi et al. PAMI 2020 #Almatrafi20arxiv : . DVSMOTION20 Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0<|>69<|>140<|>49<|>68<|>2468<|>dataset_landing_page
2468<|>http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/<|>dataset<|>El Shair, Z., Rawashdeh, S.A., , Journal of Imaging, 2022. PDF https://www.mdpi.com/2313-433X/8/8/210/pdf , dataset http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/ .<|>116<|>186<|>108<|>115<|>2468<|>dataset_landing_page
2468<|>https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/<|>1Mpx Detection Dataset<|>Perot, E., de Tournemire, P., Nitti, D., Masci, J., Sironi, A., 1Mpx Detection Dataset https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/ : Learning to Detect Objects with a 1 Megapixel Event Camera. NeurIPS 2020 #Perot20nips .<|>87<|>164<|>64<|>86<|>2468<|>other
2468<|>https://sites.google.com/view/dnd21/datasets?authuser=0<|>DND21 DeNoising Dynamic vision sensors dataset<|>DND21 DeNoising Dynamic vision sensors dataset https://sites.google.com/view/dnd21/datasets?authuser=0  associated to the paper Low Cost and Latency Event Camera Background Activity Denoising #GuoDelbruck22pami<|>47<|>102<|>0<|>46<|>2468<|>dataset_landing_page
2468<|>https://n-rod-dataset.github.io/home/<|>Project page<|>Cannici, M., Plizzari, C., Planamente, M., Ciccone, M., Bottino, A., Caputo, B., Matteucci, M., , IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), 2021. Project page https://n-rod-dataset.github.io/home/ , YouTube https://youtu.be/IOZl8MxrfpQ , Poster https://tub-rip.github.io/eventvision2021/slides/CVPRW21_NROD_poster.pdf .<|>187<|>224<|>174<|>186<|>2468<|>dataset_landing_page
2468<|>https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/<|>News<|>de Tournemire, P., Nitti, D., Perot, E., Migliore, D., Sironi, A., , arXiv, 2020. Code https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox , News https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/<|>163<|>243<|>158<|>162<|>2468<|>dataset_landing_page
2468<|>http://wp.doc.ic.ac.uk/pb2114/datasets/<|>Four sequences<|>Bardow et al., CVPR2016 #Bardow16cvpr , Four sequences http://wp.doc.ic.ac.uk/pb2114/datasets/<|>55<|>94<|>40<|>54<|>2468<|>dataset_landing_page
2468<|>http://wp.doc.ic.ac.uk/pb2114/datasets/<|>Dataset: 4 sequences<|>Bardow, P. A., Davison, A. J., Leutenegger, S., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016. YouTube https://youtu.be/1zqJpiheaaI , YouTube 2 https://youtu.be/CASsIFuPxmc , Dataset: 4 sequences http://wp.doc.ic.ac.uk/pb2114/datasets/<|>216<|>255<|>195<|>215<|>2468<|>dataset_landing_page
2468<|>http://rpg.ifi.uzh.ch/davis_data.html<|>Dataset<|>E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, D. Scaramuzza,  Int. J. Robotics Research, 36:2, pp. 142-149, 2017. PDF https://arxiv.org/pdf/1610.08336.pdf , PDF IJRR http://dx.doi.org/10.1177/0278364917691115 , YouTube https://youtu.be/bVVBTQ7l36I , Dataset http://rpg.ifi.uzh.ch/davis_data.html .<|>261<|>298<|>253<|>260<|>2468<|>dataset_landing_page
2468<|>https://zhangjiqing.com/dataset/<|>dataset<|>Zhang, J., Yang, X., Fu, Y., Wei, X., Yin, B., Dong, B., , IEEE Int. Conf. Computer Vision (ICCV), 2021. Project https://zhangjiqing.com/publication/iccv21_fe108_tracking/ , PDF https://arxiv.org/abs/2109.09052 , code https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking , dataset https://zhangjiqing.com/dataset/ .<|>286<|>318<|>278<|>285<|>2468<|>dataset_landing_page
2485<|>http://www.nature.com/articles/sdata201555#data-records<|>http://www.nature.com/articles/sdata201555#data-records<|>is the name of a file where the weighted spatial tessellation is stored. The format of the  file must be the following: . Latitude and longitude are float numbers indicating the geographic position of a location and relevance is an integer indicating the importance of the location (e.g., the number of residents in the locations or the number of calls made in that location during a given period). We provide you an example of spatial tessellation in the Italian regione of Trentino, stored in the file . This spatial tessellation is released under a Open Data Commons Open Database License (ODbL) and it is obtained from the publicly available dataset described in this paper http://www.nature.com/articles/sdata201555#data-records http://www.nature.com/articles/sdata201555#data-records .<|>734<|>789<|>678<|>733<|>2485<|>other
2489<|>http://pandas.pydata.org/<|>Pandas<|>ProgressiVis relies on well known Python libraries, such as numpy http://www.numpy.org/ , scipy http://www.scipy.org/ , Pandas http://pandas.pydata.org/ , and Scikit-Learn http://scikit-learn.org/ .<|>127<|>152<|>120<|>126<|>67<|>software
2496<|>https://x-datainitiative.github.io/tick/auto_examples/index.html<|>https://x-datainitiative.github.io/tick/auto_examples/index.html<|>https://x-datainitiative.github.io/tick/auto_examples/index.html https://x-datainitiative.github.io/tick/auto_examples/index.html<|>65<|>129<|>0<|>64<|>2496<|>other
2496<|>https://x-datainitiative.github.io/tick/<|>https://x-datainitiative.github.io/tick<|>https://x-datainitiative.github.io/tick https://x-datainitiative.github.io/tick/<|>40<|>80<|>0<|>39<|>2496<|>software
2540<|>http://imageclef.org/SIAPRdata<|>imageCLEF<|>Besides, add "mscoco" into the  folder, which can be from mscoco http://mscoco.org/dataset/#overview  COCO's images are used for RefCOCO, RefCOCO+ and refCOCOg. For RefCLEF, please add  into  folder. We extracted the related 19997 images to our cleaned RefCLEF dataset, which is a subset of the original imageCLEF http://imageclef.org/SIAPRdata . Download the subset https://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip  and unzip it to .<|>314<|>344<|>304<|>313<|>2540<|>dataset_landing_page
2553<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB51<|>We experimented on two mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.<|>119<|>195<|>112<|>118<|>2024<|>dataset_landing_page
2553<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF-101<|>We experimented on two mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.<|>71<|>106<|>63<|>70<|>2024<|>dataset_landing_page
2617<|>https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html<|>UrbanSound8k dataset<|>This repository contains annotations in JAMS github.com/marl/jams/  format [1,2] for the UrbanSound8k dataset https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html  [3]. It also contains the extended JAMS files returned by the MUDA audio data augmentation library https://github.com/bmcfee/muda  [4], which contain the deformation parameters utilized to generate the augmented UrbanSound8K training set used in [5].<|>110<|>173<|>89<|>109<|>2617<|>dataset_landing_page
2678<|>http://dbpedia.org/page/Linked_data<|>subject pages<|>On today's Web, Linked Data is published in different ways, which include data dumps http://downloads.dbpedia.org/3.9/en/ , subject pages http://dbpedia.org/page/Linked_data , and results of SPARQL queries http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=CONSTRUCT+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D%0D%0AWHERE+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D&format=text%2Fturtle . We call each such part a http://linkeddatafragments.org/ .<|>138<|>173<|>124<|>137<|>2678<|>other
2678<|>https://linkeddatafragments.org/<|>Linked Data Fragments (LDF)<|>This repository contains modules for Linked Data Fragments (LDF) https://linkeddatafragments.org/  servers.<|>65<|>97<|>37<|>64<|>2678<|>other
2678<|>http://linkeddatafragments.org/<|><|>On today's Web, Linked Data is published in different ways, which include data dumps http://downloads.dbpedia.org/3.9/en/ , subject pages http://dbpedia.org/page/Linked_data , and results of SPARQL queries http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=CONSTRUCT+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D%0D%0AWHERE+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D&format=text%2Fturtle . We call each such part a http://linkeddatafragments.org/ .<|>422<|>453<|>0<|>0<|>2678<|>other
2678<|>https://linkeddatafragments.org/specification/quad-pattern-fragments/<|>Quad Pattern Fragments<|>Instead, this server offers Quad Pattern Fragments https://linkeddatafragments.org/specification/quad-pattern-fragments/  (a.k.a. ). Each Quad Pattern Fragment offers:<|>51<|>120<|>28<|>50<|>2678<|>other
2678<|>https://linkeddatafragments.org/specification/quad-pattern-fragments/<|>Quad Pattern Fragments<|>https://github.com/LinkedDataFragments/Server.js/tree/master/packages/feature-qpf : Feature that enables Quad Pattern Fragments https://linkeddatafragments.org/specification/quad-pattern-fragments/  (a.k.a. Triple Pattern Fragments https://linkeddatafragments.org/specification/triple-pattern-fragments/ ).<|>128<|>197<|>105<|>127<|>2678<|>other
2678<|>http://data.linkeddatafragments.org/<|>data.linkeddatafragments.org<|>An example server is available at data.linkeddatafragments.org http://data.linkeddatafragments.org/ .<|>63<|>99<|>34<|>62<|>2678<|>other
2678<|>https://linkeddatafragments.org/specification/triple-pattern-fragments/<|>Triple Pattern Fragments<|>https://github.com/LinkedDataFragments/Server.js/tree/master/packages/feature-qpf : Feature that enables Quad Pattern Fragments https://linkeddatafragments.org/specification/quad-pattern-fragments/  (a.k.a. Triple Pattern Fragments https://linkeddatafragments.org/specification/triple-pattern-fragments/ ).<|>232<|>303<|>207<|>231<|>2678<|>other
2683<|>https://codecov.io/gh/datactive/bigbang<|><|> http://conference.scipy.org/proceedings/scipy2015/sebastian_benthall.html https://codecov.io/gh/datactive/bigbang https://gitter.im/datactive/bigbang?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge<|>75<|>114<|>0<|>0<|>2683<|>other
2683<|>https://github.com/datactive/bigbang/issues/412<|>#412<|>Docstrings are preferred, so that auto-generated web-based documentation will be possible ( #412 https://github.com/datactive/bigbang/issues/412 ). You can follow the Google style guide for docstrings https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings .<|>97<|>144<|>92<|>96<|>2683<|>software
2683<|>https://gitter.im/datactive/bigbang<|>development chatroom<|>If you are interested in participating in BigBang development or would like support from the core development team, please subscribe to the bigbang-dev mailing list https://lists.ghserv.net/mailman/listinfo/bigbang-dev  and let us know your suggestions, questions, requests and comments. A development chatroom https://gitter.im/datactive/bigbang  is also available.<|>311<|>346<|>290<|>310<|>2683<|>other
2683<|>https://github.com/datactive/bigbang/wiki/Governance<|>Governance<|>MIT, see LICENSE LICENSE  for its text. This license may be changed at any time according to the principles of the project Governance https://github.com/datactive/bigbang/wiki/Governance .<|>134<|>186<|>123<|>133<|>2683<|>other
2689<|>https://en.wikipedia.org/wiki/Iris_flower_data_set<|><|>These examples are using the https://en.wikipedia.org/wiki/Iris_flower_data_set . More examples can be found in the Data Retriever documentation.<|>29<|>79<|>0<|>0<|>2689<|>dataset_landing_page
2689<|>https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery<|>Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative<|>Development of this software was funded by the Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery  through Grant GBMF4563 http://www.moore.org/grants/list/GBMF4563  to Ethan White and the National Science Foundation http://nsf.gov/  as part of a CAREER award to Ethan White http://nsf.gov/awardsearch/showAward.do?AwardNumber=0953694 .<|>116<|>199<|>47<|>115<|>2689<|>other
2689<|>http://www.data-retriever.org/<|>Data Retriever website<|>For more information see the Data Retriever website http://www.data-retriever.org/ .<|>52<|>82<|>29<|>51<|>2689<|>software
2741<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI stereo 2015<|>corresponds to the 200 official training set pairs from KITTI stereo 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo .<|>74<|>147<|>56<|>73<|>2741<|>dataset_landing_page
2757<|>https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable<|>https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable<|>While searching for a list of english words (for an auto-complete tutorial) I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable  (archived).<|>276<|>407<|>187<|>275<|>2757<|>dataset_landing_page
2785<|>https://www.physionet.org/physiobank/database/mitdb<|>MIT-BIH Arrhythmia Database<|>Perform a full interpretation of record  from the MIT-BIH Arrhythmia Database https://www.physionet.org/physiobank/database/mitdb  (the output will be stored in the  annotation file):<|>78<|>129<|>50<|>77<|>2785<|>dataset_landing_page
2785<|>https://www.physionet.org/physiobank/database/qtdb<|>QT database<|>Perform a delineation of the selected heartbeats in the  annotation file for the record  from the QT database https://www.physionet.org/physiobank/database/qtdb , and store the result in the  file.<|>110<|>160<|>98<|>109<|>2785<|>dataset_landing_page
2791<|>http://www.cvlibs.net/datasets/kitti/<|>http://www.cvlibs.net/datasets/kitti/<|>[1] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, "Vision meets robotics: The KITTI dataset," Int. J. Robot. Research (IJRR), vol. 32, no. 11, pp. 1231–1237, Sep. 2013. http://www.cvlibs.net/datasets/kitti/ http://www.cvlibs.net/datasets/kitti/<|>209<|>246<|>171<|>208<|>2791<|>software
2795<|>https://github.com/harvardnlp/sent-conv-torch/tree/master/data<|>HarvardNLP<|>More data in the paper can be found at HarvardNLP https://github.com/harvardnlp/sent-conv-torch/tree/master/data .<|>50<|>112<|>39<|>49<|>2795<|>dataset_landing_page
2797<|>https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/<|>intro guide<|>For a step-by-step guide to running text through the complete Petrarch2 processing pipeline, see our intro guide https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/ .<|>113<|>204<|>101<|>112<|>2797<|>other
2797<|>https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/<|>guide<|>Alternatively, see this guide https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/  on running the complete "phoenix_pipeline" https://github.com/openeventdata/phoenix_pipeline .<|>30<|>121<|>24<|>29<|>2797<|>other
2797<|>https://github.com/openeventdata/phoenix_pipeline<|>Phoenix pipeline<|>It is possible to run PETRARCH-2 as a stand-alone program. Most of our development work has gone into incorporating PETRARCH-2 into a full pipeline of utilities, through, e.g., the Phoenix pipeline https://github.com/openeventdata/phoenix_pipeline . There's also a RESTful wrapper around PETRARCH and CoreNLP named hypnos https://github.com/caerusassociates/hypnos . It's probably worthwhile to explore those options before trying to use PETRARCH as a stand-alone.<|>198<|>247<|>181<|>197<|>2797<|>software
2797<|>https://github.com/openeventdata/phoenix_pipeline<|>"phoenix_pipeline"<|>Alternatively, see this guide https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/  on running the complete "phoenix_pipeline" https://github.com/openeventdata/phoenix_pipeline .<|>166<|>215<|>147<|>165<|>2797<|>software
2797<|>http://eventdata.parusanalytics.com/software.dir/tabari.html<|>TABARI<|>Because it used the verb dictionaries from TABARI http://eventdata.parusanalytics.com/software.dir/tabari.html , a coder based on shallow parsing, PETRARCH-1 made relatively little use of the CoreNLP constituency parse beyond parts-of-speech markup and noun-phrase markup. PETRARCH-2 makes full use of the deep parse.<|>50<|>110<|>43<|>49<|>2797<|>software
2797<|>https://gitter.im/openeventdata/petrarch2<|>gitter<|>Detailed contribution guidlines can be found in the documentation https://petrarch2.readthedocs.io/en/latest/contributing.html . In general, we welcome contributions from anyone and everyone, be it in the form of pull requests, bug reports, or feature requests. If you would like to engage in more real-time conversation with us, please visit our gitter https://gitter.im/openeventdata/petrarch2  channel.<|>354<|>395<|>347<|>353<|>2797<|>other
2816<|>http://corpus-texmex.irisa.fr/<|>SIFT1M and GIST1M<|>SIFT1M and GIST1M http://corpus-texmex.irisa.fr/<|>18<|>48<|>0<|>17<|>1268<|>dataset_landing_page
2822<|>https://archive.ics.uci.edu/ml/datasets/Character+Trajectories<|>Character Trajectories Data Set<|>is the Character Trajectories Data Set https://archive.ics.uci.edu/ml/datasets/Character+Trajectories  from UCI<|>39<|>101<|>7<|>38<|>2822<|>dataset_landing_page
2822<|>https://github.com/ZZUTK/Delay_Embedding/tree/master/data<|>data<|>data https://github.com/ZZUTK/Delay_Embedding/tree/master/data<|>5<|>62<|>0<|>4<|>2822<|>dataset_landing_page
2832<|>https://github.com/hadiasghari/pyasn#ipasn-data-files<|>are online<|>The tool takes as input a FQDN and an ASN database. Instructions on how to build such a database are online https://github.com/hadiasghari/pyasn#ipasn-data-files .<|>108<|>161<|>97<|>107<|>2832<|>software
2851<|>http://smalldata.io<|>Small Data Lab<|>The Small Data Lab http://smalldata.io  ResearchStack Extensions package is the easiest way to include SDL visual surveys ( YADL http://yadl.smalldata.io , MEDL, PAM) into a ResearchStack application.<|>19<|>38<|>4<|>18<|>2851<|>other
2852<|>http://smalldata.io<|>Small Data Lab<|>The Small Data Lab http://smalldata.io  ResearchKit Extensions package is the easiest way to include SDL AVA ( YADL http://yadl.smalldata.io , MEDL, PAM) and Behavioral extensions (Go / No Go, Delayed Discounting, BART) into a ResearchKit application.<|>19<|>38<|>4<|>18<|>2851<|>other
2895<|>https://fcav.engin.umich.edu/sim-dataset/<|>our website<|>Create a directory and download the archive files for 10k images, annotations and image sets from our website https://fcav.engin.umich.edu/sim-dataset/ . Assuming you have downloaded these to a directory named  (driving in the matrix data):<|>110<|>151<|>98<|>109<|>2895<|>dataset_landing_page
2895<|>http://www.cvlibs.net/datasets/kitti/eval_object.php<|>KITTI<|>Specifically, we will train MXNet RCNN https://github.com/dmlc/mxnet/tree/master/example/rcnn  on our 10k dataset https://fcav.engin.umich.edu/sim-dataset  and evaluate on KITTI http://www.cvlibs.net/datasets/kitti/eval_object.php .<|>178<|>230<|>172<|>177<|>2895<|>dataset_landing_page
2895<|>http://www.cvlibs.net/datasets/kitti/eval_object.php<|>KITTI's object detection landing page<|>Visit KITTI's object detection landing page http://www.cvlibs.net/datasets/kitti/eval_object.php  and follow the links named:<|>44<|>96<|>6<|>43<|>2895<|>dataset_landing_page
2895<|>https://fcav.engin.umich.edu/sim-dataset<|>10k dataset<|>Specifically, we will train MXNet RCNN https://github.com/dmlc/mxnet/tree/master/example/rcnn  on our 10k dataset https://fcav.engin.umich.edu/sim-dataset  and evaluate on KITTI http://www.cvlibs.net/datasets/kitti/eval_object.php .<|>114<|>154<|>102<|>113<|>2895<|>dataset_landing_page
2925<|>http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering<|>"Exploring models and data for image question answering."<|>Ren, Mengye, Ryan Kiros, and Richard Zemel. "Exploring models and data for image question answering." http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering  In Advances in Neural Information Processing Systems, pp. 2935-2943. 2015. [code] http://gitxiv.com/posts/6pFP3b8gqxWZdBfjf/exploring-models-and-data-for-image-question-answering<|>102<|>189<|>44<|>101<|>2925<|>other
2927<|>https://github.com/yoosan/sentpair/releases/download/predata/data.zip<|>here<|>You can download the preprocessed data (recommend) from here https://github.com/yoosan/sentpair/releases/download/predata/data.zip . Alternatively you can process them by yourself. The original links are:<|>61<|>130<|>56<|>60<|>2927<|>dataset_direct_link
2930<|>https://docs.datastax.com/en/drivers/java/4.14<|>API docs<|>API docs https://docs.datastax.com/en/drivers/java/4.14<|>9<|>55<|>0<|>8<|>2930<|>other
2930<|>https://www.datastax.com/products/datastax-enterprise<|>DataStax Enterprise<|>A modern, feature-rich and highly tunable Java client library for Apache Cassandra® http://cassandra.apache.org/  (2.1+) and DataStax Enterprise https://www.datastax.com/products/datastax-enterprise  (4.7+), and DataStax Astra https://www.datastax.com/products/datastax-astra , using exclusively Cassandra's binary protocol and Cassandra Query Language (CQL) v3.<|>145<|>198<|>125<|>144<|>2930<|>other
2930<|>https://datastax-oss.atlassian.net/browse/JAVA<|>JIRA<|>Bug tracking: JIRA https://datastax-oss.atlassian.net/browse/JAVA<|>19<|>65<|>14<|>18<|>2930<|>other
2930<|>http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datastax.oss%22<|>com.datastax.oss<|>The driver artifacts are published in Maven central, under the group id com.datastax.oss http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datastax.oss%22 ; there are multiple modules, all prefixed with .<|>89<|>158<|>72<|>88<|>2930<|>other
2930<|>https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core<|><|> https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core<|>1<|>83<|>0<|>0<|>2930<|>software
2930<|>https://www.datastax.com/products/datastax-astra<|>DataStax Astra<|>A modern, feature-rich and highly tunable Java client library for Apache Cassandra® http://cassandra.apache.org/  (2.1+) and DataStax Enterprise https://www.datastax.com/products/datastax-enterprise  (4.7+), and DataStax Astra https://www.datastax.com/products/datastax-astra , using exclusively Cassandra's binary protocol and Cassandra Query Language (CQL) v3.<|>227<|>275<|>212<|>226<|>2930<|>other
2930<|>https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user<|>Mailing list<|>Mailing list https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user<|>13<|>90<|>0<|>12<|>2930<|>other
2960<|>https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst<|>these instructions<|>Setup  and download MNIST following these instructions https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst .<|>55<|>127<|>36<|>54<|>2960<|>dataset_landing_page
2961<|>https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst<|>these instructions<|>Setup  and download MNIST following these instructions https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst .<|>55<|>127<|>36<|>54<|>2960<|>dataset_landing_page
2961<|>https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py<|>script<|>Inpainting images with missing data locations given by a mask image is meant to be used with a dataset in the format created by this script https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py .<|>140<|>235<|>133<|>139<|>2961<|>software
2967<|>https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua<|>fb.resnet.torch<|>The pretrained ResNet-152 model and related scripts can be found in fb.resnet.torch https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua .<|>84<|>163<|>68<|>83<|>2967<|>software
3014<|>http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml<|>website<|>The version of the Taxi dataset that we used in the experiments is not open source, and therefore, we cannot make it available online. However, the Taxi and Limousine Commission has made the trip data available on their website http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml .<|>228<|>289<|>220<|>227<|>3014<|>dataset_landing_page
3014<|>https://www.citibikenyc.com/system-data<|>Citi Bike website<|>The original dataset is available at the Citi Bike website https://www.citibikenyc.com/system-data .<|>59<|>98<|>41<|>58<|>3014<|>dataset_landing_page
3046<|>https://github.com/deepmind/rc-data/<|>CNN/DailyMail<|>CNN/DailyMail https://github.com/deepmind/rc-data/ : This repository contains a script to download CNN and Daily Mail articles from the Wayback Machine.<|>14<|>50<|>0<|>13<|>3046<|>dataset_landing_page
3057<|>https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md<|>LICENSE<|>See the LICENSE https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md  file for details.<|>16<|>91<|>8<|>15<|>3057<|>dataset_landing_page
3057<|>https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets<|>assets directory<|>If you're mainly interest in the data, pre and post classified name data is available in the assets directory https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets . If you install the package these will not be included in the install as  files but will be included as compressed binaries (the data are identical).<|>110<|>181<|>93<|>109<|>3057<|>dataset_direct_link
3057<|>http://weblog.bocoup.com/global-name-data/<|>blog<|>You can read about some uses of this data along with code examples at the Bocoup blog http://weblog.bocoup.com/global-name-data/ .<|>86<|>128<|>81<|>85<|>3057<|>other
3057<|>https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md<|>license<|>We have collected birth record data from the United States and the United Kingdom across a number of years for all births in the two countries and are releasing the collected and cleaned up data here. We have also generated a simple gender classified based on incidence of gender by name. You can use this data for any purpose compatible with the license https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md .<|>355<|>430<|>347<|>354<|>3057<|>dataset_landing_page
3057<|>https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md<|>LICENSE<|>Processed data are provided under the Open Government License or the public domain where appropriate. See the LICENSE https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md  for details.<|>118<|>193<|>110<|>117<|>3057<|>dataset_landing_page
3060<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist<|><|>Users can also compare on the multi-class dataset https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist  with the follow command (Note that we only shuffle the training data once in this example, so the standard deviation is zero):<|>50<|>127<|>0<|>0<|>3060<|>dataset_landing_page
3081<|>http://www.di.ens.fr/data/software/<|>libsvm-compact<|>Keras https://github.com/fchollet/keras , Kerosene https://github.com/dribnet/kerosene , Blessings https://github.com/erikrose/blessings , and libsvm-compact http://www.di.ens.fr/data/software/ .<|>158<|>193<|>143<|>157<|>3081<|>software
3083<|>http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset<|>Million Song Dataset<|>A 10,000-song subset was downloaded from the Million Song Dataset http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset .<|>66<|>130<|>45<|>65<|>3083<|>dataset_landing_page
3084<|>https://github.com/Atomu2014/make-ipinyou-data<|>make-ipinyou-data<|>For simplicity, we provide iPinYou dataset at make-ipinyou-data https://github.com/Atomu2014/make-ipinyou-data . Follow the instructions and update the soft link :<|>64<|>110<|>46<|>63<|>3084<|>software
3119<|>http://fivethirtyeight.com/datalab/mta-new-york-lost-and-found-subway-most-common/<|>Mona's story<|>Python scrape for MTA current lost property index http://advisory.mtanyct.info/LPUWebServices/CurrentLostProperty.aspx , to revisit Mona's story http://fivethirtyeight.com/datalab/mta-new-york-lost-and-found-subway-most-common/  in a year (or some longer period of time)<|>145<|>227<|>132<|>144<|>3119<|>other
3134<|>https://github.com/tesseract-ocr/tessdata<|>tessdata<|>Tesseract 4 adds a new neural net (LSTM) based OCR engine https://en.wikipedia.org/wiki/Optical_character_recognition  which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0). It also needs traineddata https://tesseract-ocr.github.io/tessdoc/Data-Files.html  files which support the legacy engine, for example those from the tessdata https://github.com/tesseract-ocr/tessdata  repository.<|>522<|>563<|>513<|>521<|>3134<|>dataset_landing_page
3208<|>http://crcv.ucf.edu/data/UCF101.php<|>here<|>The data used to train this model is located here http://crcv.ucf.edu/data/UCF101.php .<|>50<|>85<|>45<|>49<|>2024<|>dataset_landing_page
3215<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>move validation images<|>Download the ImageNet http://image-net.org/download-images  dataset and move validation images https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to labeled subfolders<|>95<|>191<|>72<|>94<|>3215<|>dataset_landing_page
3219<|>http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow<|>KITTI2012<|>The code was developed on Ubuntu 14.04, using Theano+Lasagne+OpenCV. You can see the performance it achieved on the KITTI2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow , KITTI2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and MPI-Sintel http://sintel.is.tue.mpg.de/  optical flow scoreboards.<|>126<|>198<|>116<|>125<|>3219<|>dataset_landing_page
3219<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI2015<|>The code was developed on Ubuntu 14.04, using Theano+Lasagne+OpenCV. You can see the performance it achieved on the KITTI2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow , KITTI2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and MPI-Sintel http://sintel.is.tue.mpg.de/  optical flow scoreboards.<|>211<|>282<|>201<|>210<|>3219<|>dataset_landing_page
3220<|>https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets<|>MXNet Example<|>Prepare the corresponding datasets (CIFAR-10, CIFAR-100 or ImageNet) before training FBNs. In our experiments, we use RecordIO data format to generate  files for different datasets. Please refer to MXNet Example https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets .<|>212<|>299<|>198<|>211<|>3220<|>software
3223<|>http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/<|>here<|>The Freiburg Groceries Dataset consists of 5000 256x256 RGB images of 25 food classes. Examples for each class can be found below. The paper can be found here http://ais.informatik.uni-freiburg.de/publications/papers/jund16groceries.pdf  and the dataset here http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/ .<|>259<|>332<|>254<|>258<|>3223<|>dataset_landing_page
3245<|>https://www.cityscapes-dataset.com/<|>Cityscapes training set<|>: 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . [ Citation datasets/bibtex/cityscapes.tex ]<|>47<|>82<|>23<|>46<|>3245<|>dataset_landing_page
3248<|>http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/<|>http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/<|>For small norb dataset, please download the raw images in .MAT format from http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/ http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/  and run datasets_norb.convert_orig_to_np() to convert it into numpy format.<|>127<|>178<|>75<|>126<|>3248<|>dataset_landing_page
3263<|>https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>Flying Chairs<|>We trained our model based on the ImageNet pre-trained ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model and Flying Chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  pre-trained FlowNet https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz  model using a model converter https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter . The converted ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).<|>146<|>224<|>132<|>145<|>3263<|>dataset_landing_page
3296<|>https://github.com/fvisin/dataset_loaders<|>The dataset loader<|>The dataset loader https://github.com/fvisin/dataset_loaders . Thanks a lot to Francesco Visin for its data loader, please cite it if you use it.<|>19<|>60<|>0<|>18<|>3296<|>software
3311<|>https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results<|>leaderboard<|>For more information, refer to the official leaderboard https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results .<|>56<|>122<|>44<|>55<|>3311<|>other
3328<|>https://github.com/NorThanapon/dict-definition/tree/master/data<|>Data<|>For detail of the data, see Data https://github.com/NorThanapon/dict-definition/tree/master/data<|>33<|>96<|>28<|>32<|>3328<|>dataset_landing_page
3347<|>https://github.com/openeventdata/scraper<|><|>The EL:DIABLO event coding platform is comprised of two primary applications: a web scraper and a processing pipeline ( https://github.com/openeventdata/scraper  and https://github.com/openeventdata/phoenix_pipeline  specifically). The scraper is a simple web scraper that makes use of a whitelist of RSS feeds to pull stories from popular news outlets. The pipeline moves the news stories from storage in a database to the event coder, such as TABARI or PETRARCH, and outputs event data. More information about the details of these projects can be found in their respective documentation, linked to above. If you use the standard  script provided with EL:DIABLO, the web scraper will run once an hour, and the pipeline will run once a day at 01:00.<|>120<|>160<|>0<|>0<|>3347<|>software
3347<|>https://github.com/openeventdata/phoenix_pipeline<|><|>The EL:DIABLO event coding platform is comprised of two primary applications: a web scraper and a processing pipeline ( https://github.com/openeventdata/scraper  and https://github.com/openeventdata/phoenix_pipeline  specifically). The scraper is a simple web scraper that makes use of a whitelist of RSS feeds to pull stories from popular news outlets. The pipeline moves the news stories from storage in a database to the event coder, such as TABARI or PETRARCH, and outputs event data. More information about the details of these projects can be found in their respective documentation, linked to above. If you use the standard  script provided with EL:DIABLO, the web scraper will run once an hour, and the pipeline will run once a day at 01:00.<|>166<|>215<|>0<|>0<|>2797<|>software
3347<|>https://github.com/openeventdata/eldiablo<|>Github repository<|>As mentioned above, EL:DIABLO relies on Vagrant and VirtualBox for most of the heavy lifting. This means that the only things that a user needs to install on their local machine are these two pieces of software. The creators of this software describe the install process better than we can, so a user should look here https://www.vagrantup.com/downloads.html  for Vagrant and here https://www.virtualbox.org/wiki/Downloads  for VirtualBox. Once that software is installed, EL:DIABLO needs to be downloaded from the Github repository https://github.com/openeventdata/eldiablo . For those familiar with , a  should work fine. For those unfamiliar with , it is possible to download the repository as a zip file as shown in the picture below.<|>533<|>574<|>515<|>532<|>3347<|>software
3350<|>http://workshop.colips.org/dstc5/data.html<|>DSTC5<|>Although we only mentioned DSTC4 in our paper, users can also locate these sub-dialogs from DSTC5 http://workshop.colips.org/dstc5/data.html , since it provides all the same data in DSTC4, and plus two extra Chinese dialogs (055, 056).<|>98<|>140<|>92<|>97<|>3350<|>other
3372<|>https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad<|>Over 1.5 TB’s of Labeled Audio Datasets<|>Over 1.5 TB’s of Labeled Audio Datasets https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad , Towards Data Science, 2018-11-13.<|>40<|>119<|>0<|>39<|>3372<|>dataset_landing_page
3372<|>https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/<|>25 Open Datasets for Deep Learning Every Data Scientist Must Work With<|>25 Open Datasets for Deep Learning Every Data Scientist Must Work With https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/ , Analytics Vidhya, 2018-03-29.<|>71<|>164<|>0<|>70<|>3372<|>dataset_landing_page
3372<|>https://github.com/caesar0301/awesome-public-datasets<|>https://github.com/caesar0301/awesome-public-datasets<|>https://github.com/caesar0301/awesome-public-datasets https://github.com/caesar0301/awesome-public-datasets<|>54<|>107<|>0<|>53<|>3372<|>dataset_landing_page
3372<|>https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis<|>https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis<|>https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis<|>74<|>147<|>0<|>73<|>3372<|>dataset_landing_page
3372<|>http://www.audiocontentanalysis.org/data-sets<|>http://www.audiocontentanalysis.org/data-sets<|>http://www.audiocontentanalysis.org/data-sets http://www.audiocontentanalysis.org/data-sets<|>46<|>91<|>0<|>45<|>3372<|>dataset_landing_page
3372<|>https://github.com/ismir/mir-datasets<|>https://github.com/ismir/mir-datasets<|>https://github.com/ismir/mir-datasets https://github.com/ismir/mir-datasets<|>38<|>75<|>0<|>37<|>3372<|>dataset_landing_page
3372<|>https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research<|>https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research<|>https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research<|>77<|>153<|>0<|>76<|>3372<|>dataset_landing_page
3372<|>https://www.datasetlist.com<|>https://www.datasetlist.com<|>https://www.datasetlist.com https://www.datasetlist.com<|>28<|>55<|>0<|>27<|>3372<|>dataset_landing_page
3372<|>https://data-flair.training/blogs/deep-learning-project-ideas<|>https://data-flair.training/blogs/deep-learning-project-ideas<|>https://data-flair.training/blogs/deep-learning-project-ideas https://data-flair.training/blogs/deep-learning-project-ideas<|>62<|>123<|>0<|>61<|>3372<|>other
3372<|>https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb<|>Music Genre Classification With TensorFlow<|>Music Genre Classification With TensorFlow https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb , Towards Data Science, 2020-08-11.<|>43<|>129<|>0<|>42<|>3372<|>other
3372<|>https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58<|>Music Genre Classification: Transformers vs Recurrent Neural Networks<|>Music Genre Classification: Transformers vs Recurrent Neural Networks https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58 , Towards Data Science, 2020-06-14.<|>70<|>182<|>0<|>69<|>3372<|>dataset_landing_page
3372<|>https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af<|>Using CNNs and RNNs for Music Genre Recognition<|>Using CNNs and RNNs for Music Genre Recognition https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af , Towards Data Science, 2018-12-13.<|>48<|>139<|>0<|>47<|>3372<|>other
3372<|>https://pandas.pydata.org/<|>pandas<|>All metadata and features for all tracks are distributed in  (342 MiB). The below tables can be used with pandas https://pandas.pydata.org/  or any other data analysis tool. See the paper https://arxiv.org/abs/1612.01840  or the https://nbviewer.jupyter.org/github/mdeff/fma/blob/outputs/usage.ipynb  notebook for a description.<|>113<|>139<|>106<|>112<|>3372<|>software
3373<|>https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/<|>Flickr30k<|>Download the corresponding Vocabulary file for COCO https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/  and Flickr30k https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/<|>145<|>227<|>135<|>144<|>3373<|>dataset_direct_link
3373<|>https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/<|>COCO<|>Download the corresponding Vocabulary file for COCO https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/  and Flickr30k https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/<|>52<|>129<|>47<|>51<|>3373<|>dataset_direct_link
3399<|>http://conda.pydata.org/miniconda.html<|>Download<|>We won't need the entire distribution. Download http://conda.pydata.org/miniconda.html  a Python 3.7+ & install a minimal version of anaconda.<|>48<|>86<|>39<|>47<|>155<|>software
3400<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>flowers<|>Download the birds http://www.vision.caltech.edu/visipedia/CUB-200-2011.html  and flowers http://www.robots.ox.ac.uk/~vgg/data/flowers/102/  image data. Extract them to  and , respectively.<|>90<|>139<|>82<|>89<|>3400<|>dataset_landing_page
3470<|>http://pandas.pydata.org/<|>pandas<|>These files are suitable to be further processed with software such as pandas http://pandas.pydata.org/ .<|>78<|>103<|>71<|>77<|>67<|>software
3472<|>http://trec.nist.gov/data/docs_eng.html<|>TREC website<|>Change directory to IR for experimenting on information Retrieval task. IR Datasets mentioned in the paper can be downloaded from TREC website http://trec.nist.gov/data/docs_eng.html .<|>143<|>182<|>130<|>142<|>3472<|>other
3482<|>http://corpus-texmex.irisa.fr/<|>SIFT1M and GIST1M<|>SIFT1M and GIST1M http://corpus-texmex.irisa.fr/<|>18<|>48<|>0<|>17<|>1268<|>dataset_landing_page
3483<|>http://corpus-texmex.irisa.fr/<|>SIFT1M and GIST1M<|>SIFT1M and GIST1M http://corpus-texmex.irisa.fr/<|>18<|>48<|>0<|>17<|>1268<|>dataset_landing_page
3492<|>http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df<|>state-of-the-art<|>MultiNet is able to jointly perform road segmentation, car detection and street classification. The model achieves real-time speed and state-of-the-art http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df  performance in segmentation. Check out our paper https://arxiv.org/abs/1612.07695  for a detailed model description.<|>152<|>257<|>135<|>151<|>3492<|>other
3531<|>https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433<|>Using DataLoader to batch GraphQL requests<|>Using DataLoader to batch GraphQL requests https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433<|>43<|>116<|>0<|>42<|>3531<|>other
3531<|>https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/<|>Level up your serverless game with a GraphQL data-as-a-service layer<|>Level up your serverless game with a GraphQL data-as-a-service layer https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/<|>69<|>161<|>0<|>68<|>3531<|>other
3531<|>https://github.com/ardatan/graphql-toolkit<|>graphql-toolkit<|>graphql-toolkit https://github.com/ardatan/graphql-toolkit  - A set of utils for faster development of GraphQL tools (Schema and documents loading, Schema merging and more).<|>16<|>58<|>0<|>15<|>3531<|>software
3531<|>https://github.com/Yelp/dataloader-codegen<|>dataloader-codegen<|>dataloader-codegen https://github.com/Yelp/dataloader-codegen  - An opinionated JavaScript library for automatically generating predictable, type safe DataLoaders over a set of resources (e.g. HTTP endpoints).<|>19<|>61<|>0<|>18<|>3531<|>software
3531<|>https://github.com/engagingspaces/vertx-dataloader<|>vertx-dataloader<|>vertx-dataloader https://github.com/engagingspaces/vertx-dataloader  - Port of Facebook DataLoader for efficient, asynchronous batching and caching in clustered GraphQL environments.<|>17<|>67<|>0<|>16<|>3531<|>software
3552<|>https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data<|>LSHBOX-sample-data<|>You can get the sample dataset  from http://www.cs.princeton.edu/cass/audio.tar.gz http://www.cs.princeton.edu/cass/audio.tar.gz , if the link is invalid, you can also get it from LSHBOX-sample-data https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data .<|>199<|>254<|>180<|>198<|>3552<|>dataset_landing_page
3552<|>https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data<|>LSHBOX-sample datasets<|>LSHBOX-sample datasets https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data : a dataset for performance tests<|>23<|>78<|>0<|>22<|>3552<|>dataset_landing_page
3566<|>https://github.com/wnzhang/make-ipinyou-data<|>make-ipinyou-data<|>Note these results are produced from a subset (the first 350,000 lines of each campaign in iPinYou) under T = 1000 and c0 = 1/32. For the full small-scale evaluation and large-scale evaluation, please first check the GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data  for pre-processing the iPinYou data http://data.computational-advertising.org . After downloading the dataset, by "make all" you can generate the standardised data.<|>250<|>294<|>232<|>249<|>1021<|>dataset_landing_page
3591<|>http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html<|>FAKBA1<|>Freebase Annotations for the TREC KBA 2014 StreamCorpus: FAKBA1 http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html  (~200GB)<|>64<|>133<|>57<|>63<|>3591<|>dataset_direct_link
3613<|>https://github.com/IITDBGroup/gprom/wiki/datalog_prov<|>Provenance Graphs for Datalog<|>Provenance Graphs for Datalog https://github.com/IITDBGroup/gprom/wiki/datalog_prov<|>30<|>83<|>0<|>29<|>3613<|>software
3635<|>https://seaborn.pydata.org/index.html<|>seaborn<|>seaborn https://seaborn.pydata.org/index.html<|>8<|>45<|>0<|>7<|>3635<|>software
3695<|>https://github.com/yiling-chen/flickr-cropping-dataset<|>Flickr cropping dataset<|>We provide the evaluation script to reproduce our evaluation results on Flickr cropping dataset https://github.com/yiling-chen/flickr-cropping-dataset . For example,<|>96<|>150<|>72<|>95<|>3695<|>dataset_landing_page
3695<|>https://github.com/yiling-chen/flickr-cropping-dataset<|>Flickr cropping dataset<|>You will need to get  and the test images from the Flickr cropping dataset https://github.com/yiling-chen/flickr-cropping-dataset  and specify the path of your model when running . You can also try our pre-trained model, which can be downloaded from here https://drive.google.com/drive/folders/0B0sDVRDPL5zBd3ozNlFmZEZpY1k?resourcekey=0-8LArgbYDQT07L-Dob6Yo9w&usp=sharing .<|>75<|>129<|>51<|>74<|>3695<|>dataset_landing_page
3728<|>https://archive.ics.uci.edu/ml/datasets/Ionosphere<|>Ionosphere (UCI Machine Learning Repository)<|>Data set: Ionosphere (UCI Machine Learning Repository) https://archive.ics.uci.edu/ml/datasets/Ionosphere  (#features = 34, #classes = 2)<|>55<|>105<|>10<|>54<|>3728<|>dataset_landing_page
3737<|>http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools<|>SemEval-2016 official scorer<|>Evaluation scripts for SemEval were adapted & modified from SemEval-2016 official scorer http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools .<|>89<|>154<|>60<|>88<|>3737<|>dataset_landing_page
3766<|>http://wp.doc.ic.ac.uk/wbai/data<|>placing the landmarks<|>To initialise image registration, we use six landmarks and perform point-based registration, which is then followed by image-based registration. The landmarks are defined as in the placing the landmarks http://wp.doc.ic.ac.uk/wbai/data  section and they are manually selected using the rview https://www.doc.ic.ac.uk/~dr/software/download.html  software. Alternatively, the landmarks can be automatically detected using stratified decision forests https://www.doc.ic.ac.uk/~oo2113/publication/TMI_stratified/  developed by Ozan Oktay.<|>203<|>235<|>181<|>202<|>3766<|>dataset_landing_page
3802<|>http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip<|>3DPeS<|>3DPeS http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip<|>6<|>73<|>0<|>5<|>2047<|>dataset_direct_link
3844<|>http://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM RGBD datasets<|>The example application takes input frames and poses from the TUM RGBD datasets http://vision.in.tum.de/data/datasets/rgbd-dataset , and requires that your create an association file http://vision.in.tum.de/data/datasets/rgbd-dataset/tools  to associate the RGB, Depth, and pose information. Instructions for this process can be found here https://github.com/tum-vision/fastfusion .<|>80<|>130<|>62<|>79<|>1355<|>dataset_landing_page
3844<|>http://vision.in.tum.de/data/datasets/rgbd-dataset/tools<|>association file<|>The example application takes input frames and poses from the TUM RGBD datasets http://vision.in.tum.de/data/datasets/rgbd-dataset , and requires that your create an association file http://vision.in.tum.de/data/datasets/rgbd-dataset/tools  to associate the RGB, Depth, and pose information. Instructions for this process can be found here https://github.com/tum-vision/fastfusion .<|>183<|>239<|>166<|>182<|>1355<|>other
3863<|>https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/image_data_layer.cpp<|>image_data_layer.cpp<|>My Caffe (https://github.com/happynear/caffe-windows/tree/ms). If you don't want to train with class-balance sampling ( image_data_layer.cpp https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/image_data_layer.cpp ) and observing Pearson Correlation during training ( correlation_loss_layer.cpp https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/correlation_loss_layer.cpp ), you may use the official Caffe.<|>141<|>229<|>120<|>140<|>3863<|>software
3876<|>https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/#<|>Microsoft 7-Scenes<|>In air data: Any RGB-D dataset, e.g. Microsoft 7-Scenes https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/# , NYU Depth http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html , UW RGB-D Object https://www.cs.washington.edu/node/4229 , B3DO http://kinectdata.com/  Note: The current configuration expects 640x480 PNG images for in-air data.<|>56<|>129<|>37<|>55<|>3876<|>dataset_landing_page
3876<|>http://kinectdata.com/<|>B3DO<|>In air data: Any RGB-D dataset, e.g. Microsoft 7-Scenes https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/# , NYU Depth http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html , UW RGB-D Object https://www.cs.washington.edu/node/4229 , B3DO http://kinectdata.com/  Note: The current configuration expects 640x480 PNG images for in-air data.<|>263<|>285<|>258<|>262<|>3876<|>dataset_landing_page
3876<|>http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html<|>NYU Depth<|>In air data: Any RGB-D dataset, e.g. Microsoft 7-Scenes https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/# , NYU Depth http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html , UW RGB-D Object https://www.cs.washington.edu/node/4229 , B3DO http://kinectdata.com/  Note: The current configuration expects 640x480 PNG images for in-air data.<|>142<|>197<|>132<|>141<|>3876<|>dataset_landing_page
3883<|>https://code.google.com/p/data-shrinker<|>shrinker 0.1<|>shrinker 0.1 https://code.google.com/p/data-shrinker  - WARNING: it can throw SEGFAULT compiled with gcc 4.9+ -O3<|>13<|>52<|>0<|>12<|>3883<|>software
3884<|>https://datatracker.ietf.org/doc/html/rfc8878<|>RFC8878<|>Zstandard's format is stable and documented in RFC8878 https://datatracker.ietf.org/doc/html/rfc8878 . Multiple independent implementations are already available. This repository represents the reference implementation, provided as an open-source dual BSD LICENSE  and GPLv2 COPYING  licensed  library, and a command line utility producing and decoding , ,  and  files. Should your project require another programming language, a list of known ports and bindings is provided on Zstandard homepage https://facebook.github.io/zstd/#other-languages .<|>55<|>100<|>47<|>54<|>3884<|>other
3915<|>http://lmb.informatik.uni-freiburg.de/resources/datasets/<|>FBMS-59 dataset<|>The datasets included originate from the FBMS-59 dataset http://lmb.informatik.uni-freiburg.de/resources/datasets/ . The datasets are provided only for research purposes and without any warranty. When using the BMS-26 or FBMS-59 in your research work, you should cite the appropriate papers in the link above.<|>57<|>114<|>41<|>56<|>3915<|>dataset_landing_page
3922<|>https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp<|>SplitData<|>SplitData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp<|>10<|>133<|>0<|>9<|>3922<|>dataset_landing_page
3922<|>https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp<|>MergeData<|>MergeData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp<|>10<|>133<|>0<|>9<|>3922<|>dataset_landing_page
3922<|>https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp<|>RandomCropBoostedData<|>RandomCropBoostedData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp<|>22<|>159<|>0<|>21<|>3922<|>dataset_landing_page
3922<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>Oxford Flowers 102<|>This repository contains the codes and models described in the paper "Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning"(https://arxiv.org/abs/1702.08690). These models are those used in Stanford Dogs 120 http://vision.stanford.edu/aditya86/ImageNetDogs/ , Oxford Flowers 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/ , Caltech 256 http://authors.library.caltech.edu/7694/  and MIT Indoor 67 http://web.mit.edu/torralba/www/indoor.html .<|>323<|>372<|>304<|>322<|>3400<|>dataset_landing_page
3942<|>https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4<|>PMLB: a large benchmark suite for machine learning evaluation and comparison<|>Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4 . , page 36.<|>178<|>252<|>101<|>177<|>3942<|>other
3955<|>http://www.vs.inf.ethz.ch/res/show.html?what=eco-data<|>The ECO dataset<|>The ECO dataset http://www.vs.inf.ethz.ch/res/show.html?what=eco-data : Together with , a Swiss energy provider, we collected the ECO data set (Electricity Consumption and Occupancy). Using NILM-Eval, we evaluated the performance of four NILM algorithms on the ECO data set.<|>16<|>69<|>0<|>15<|>3955<|>dataset_landing_page
3956<|>http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database<|>here<|>The first thing you need to do is to download the data. You have to register here http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database  and download these two files:<|>82<|>152<|>77<|>81<|>3956<|>dataset_landing_page
3962<|>http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config<|>Makefile.config<|>You can download my Makefile.config http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config  for reference. 2. Python packages you might not have: , ,<|>36<|>98<|>20<|>35<|>3962<|>other
3997<|>https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax<|>here<|>This repository contains the code for the optimization step in the paper. The inference code is here https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax .<|>101<|>172<|>96<|>100<|>3997<|>software
4003<|>http://dataminingtutorial.com<|>dataminingtutorial.com<|>The data corpus used in the research is publicly available and can be requested at dataminingtutorial.com http://dataminingtutorial.com<|>106<|>135<|>83<|>105<|>4003<|>other
4027<|>http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm<|>Ucinet IV Datasets<|>Ucinet IV Datasets http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm .<|>19<|>82<|>0<|>18<|>4027<|>dataset_landing_page
4040<|>https://www.robots.ox.ac.uk/~vgg/data/lip_reading/<|>Lip Reading in the Wild<|>Download the Lip Reading in the Wild https://www.robots.ox.ac.uk/~vgg/data/lip_reading/  dataset<|>37<|>87<|>13<|>36<|>4040<|>dataset_landing_page
4042<|>https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption<|>https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption<|>The repository supports optimization of the above models on artifical multivariate noisy AR time series and household electricity conspumption dataset https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption  The dataset has to be specified alongside the paremeters in each of the files listed above.<|>239<|>326<|>151<|>238<|>4042<|>dataset_landing_page
4044<|>http://www.ltr-data.se/opencode.html/#ImDisk<|>IMDisk<|>Install a RAM disk like IMDisk http://www.ltr-data.se/opencode.html/#ImDisk  and set  to use it.<|>31<|>75<|>24<|>30<|>4044<|>software
4064<|>https://www.kaggle.com/c/ultrasound-nerve-segmentation/data<|>Provided data<|>Provided data https://www.kaggle.com/c/ultrasound-nerve-segmentation/data  is processed by  script. This script just loads the images and saves them into NumPy binary format files  for faster loading later.<|>14<|>73<|>0<|>13<|>4064<|>dataset_direct_link
4066<|>http://www.fc.up.pt/addi/ph2%20database.html<|>PH2 Dataset<|>The PH2 Dataset http://www.fc.up.pt/addi/ph2%20database.html  has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.<|>16<|>60<|>4<|>15<|>4066<|>dataset_landing_page
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py<|><|>To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example. Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .<|>156<|>259<|>0<|>0<|>4072<|>software
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md<|>data generators README<|>To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example. Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .<|>313<|>408<|>290<|>312<|>4072<|>software
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen<|><|>consist of features such as inputs and targets, and metadata such as each feature's modality (e.g. symbol, image, audio) and vocabularies. Problem features are given by a dataset, which is stored as a  file with  protocol buffers. All problems are imported in https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py  or are registered with . Run https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen  to see the list of available problems and download them.<|>392<|>477<|>0<|>0<|>4072<|>software
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir<|><|>See the https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir  for an example user directory.<|>8<|>103<|>0<|>0<|>4072<|>software
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py<|><|>To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example. Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .<|>31<|>127<|>0<|>0<|>4072<|>software
4072<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py<|><|>consist of features such as inputs and targets, and metadata such as each feature's modality (e.g. symbol, image, audio) and vocabularies. Problem features are given by a dataset, which is stored as a  file with  protocol buffers. All problems are imported in https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py  or are registered with . Run https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen  to see the list of available problems and download them.<|>260<|>361<|>0<|>0<|>4072<|>software
4075<|>https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine<|>alpine example chart<|>Take a look at the alpine example chart https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine  for reference when you're writing your first few charts.<|>40<|>116<|>19<|>39<|>4075<|>software
4095<|>http://www.census.gov/topics/population/genealogy/data/2000_surnames.html<|>Frequently Occurring Surnames from the Census 2000<|>Frequently Occurring Surnames from the Census 2000 http://www.census.gov/topics/population/genealogy/data/2000_surnames.html . Surnames occurring >= 100 more times in the 2000 census. Details here: http://www2.census.gov/topics/genealogy/2000surnames/surnames.pdf http://www2.census.gov/topics/genealogy/2000surnames/surnames.pdf<|>51<|>124<|>0<|>50<|>4095<|>dataset_landing_page
4100<|>https://archive.ics.uci.edu/ml/datasets/Wine+Quality<|>winequality dataset<|>Download winequality dataset https://archive.ics.uci.edu/ml/datasets/Wine+Quality , and other datasets and change utils.py to add new datasets to test out the pruning algorithm.<|>29<|>81<|>9<|>28<|>4100<|>dataset_landing_page
4113<|>http://datahub.io/dataset/amsterdam-museum-as-edm-lod<|>http://datahub.io/dataset/amsterdam-museum-as-edm-lod<|>Amsterdam Museum ( http://datahub.io/dataset/amsterdam-museum-as-edm-lod http://datahub.io/dataset/amsterdam-museum-as-edm-lod )<|>73<|>126<|>19<|>72<|>4113<|>other
4135<|>https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's%20Arrow%20Arrangements%20[SM5].zip<|>(Fraxtil) Fraxtil's Arrow Arrangements<|>(Fraxtil) Fraxtil's Arrow Arrangements https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's%20Arrow%20Arrangements%20[SM5].zip<|>39<|>136<|>0<|>38<|>4135<|>dataset_direct_link
4135<|>https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's%20Beast%20Beats%20[SM5].zip<|>(Fraxtil) Fraxtil's Beast Beats<|>(Fraxtil) Fraxtil's Beast Beats https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's%20Beast%20Beats%20[SM5].zip<|>32<|>115<|>0<|>31<|>4135<|>dataset_direct_link
4139<|>https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/<|>blog post<|>SMILES enumeration is the process of writing out all possible SMILES forms of a molecule. It's a useful technique for data augmentation before sequence based modeling of molecules. You can read more about the background in this blog post https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/  or this preprint on arxiv.org https://arxiv.org/abs/1703.07076<|>238<|>361<|>228<|>237<|>4139<|>other
4139<|>https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/<|><|> https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/<|>1<|>121<|>0<|>0<|>4139<|>other
4139<|>https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/<|>blog.<|>If you find it useful, feel welcome to leave a comment on the blog. https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/<|>68<|>191<|>62<|>67<|>4139<|>other
4148<|>https://motchallenge.net/data/MOT16/<|>MOT16 benchmark<|>The following example starts the tracker on one of the MOT16 benchmark https://motchallenge.net/data/MOT16/  sequences. We assume resources have been extracted to the repository root directory and the MOT16 benchmark data is in :<|>71<|>107<|>55<|>70<|>4148<|>dataset_landing_page
4158<|>http://www.nature.com/articles/sdata201635<|>paper<|>Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database ( paper http://www.nature.com/articles/sdata201635 , website http://mimic.physionet.org ). Our four clinical prediction tasks are critical care variants of four opportunities to transform health care using in "big clinical data" as described in Bates, et al, 2014 http://content.healthaffairs.org/content/33/7/1123.abstract :<|>217<|>259<|>211<|>216<|>4158<|>other
4159<|>http://www.nature.com/articles/sdata201635<|>paper<|>Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database ( paper http://www.nature.com/articles/sdata201635 , website http://mimic.physionet.org ). Our four clinical prediction tasks are critical care variants of four opportunities to transform health care using in "big clinical data" as described in Bates, et al, 2014 http://content.healthaffairs.org/content/33/7/1123.abstract :<|>217<|>259<|>211<|>216<|>4158<|>other
4180<|>https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java<|>source<|>source https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java cache examples/awips2-upc_14.4.1.zip<|>7<|>199<|>0<|>6<|>4180<|>software
4189<|>http://cvml.unige.ch/databases/emoMusic/<|>emoMusic<|>emoMusic http://cvml.unige.ch/databases/emoMusic/<|>9<|>49<|>0<|>8<|>4189<|>dataset_landing_page
4189<|>https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html<|>Urbansound8K<|>Urbansound8K https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html<|>13<|>83<|>0<|>12<|>4189<|>dataset_landing_page
4200<|>http://datashare.is.ed.ac.uk/handle/10283/1942<|>Edinburgh DataShare<|>The speech enhancement dataset used in this work (Valentini et al. 2016) http://ssw9.net/papers/ssw9_PS2-4_Valentini-Botinhao.pdf  can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . However, :<|>167<|>213<|>147<|>166<|>4200<|>dataset_landing_page
4207<|>https://github.com/escorciav/daps/tree/master/data/models<|>Pre-trained models<|>Pre-trained models https://github.com/escorciav/daps/tree/master/data/models . Our generalization experiment suggests that you may expect decent results for other kind of action classes with similar lengths. Check out the models trained on the validation set of THUMOS14.<|>19<|>76<|>0<|>18<|>4207<|>dataset_landing_page
4207<|>ihttp://conda.pydata.org/docs/index.html<|>conda<|>Ensure that you have gcc https://gcc.gnu.org/ , conda ihttp://conda.pydata.org/docs/index.html , CUDA and CUDNN https://developer.nvidia.com/cuda-downloads  (optional).<|>54<|>94<|>48<|>53<|>4207<|>software
4207<|>https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5<|>here<|>Download the C3D representation of a couple of videos from here https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5 .<|>64<|>141<|>59<|>63<|>4207<|>dataset_direct_link
4207<|>https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz<|>model<|>Download our model https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz .<|>19<|>97<|>13<|>18<|>4207<|>dataset_direct_link
4209<|>https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb<|>get_geographic_data.ipynb<|>Python 3 (and python 2 only for get_geographic_data.ipynb https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb  )<|>58<|>148<|>32<|>57<|>4209<|>dataset_landing_page
4212<|>https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>Flying Chairs<|>We trained our model based on the ImageNet pre-trained ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model and Flying Chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  pre-trained FlowNet https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz  model using a model converter https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter . The converted ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).<|>146<|>224<|>132<|>145<|>3263<|>dataset_landing_page
4223<|>https://www.alluxio.io/data-orchestration-summit-2019/<|>Data Orchestration Summit<|>Alluxio is used in production to manage Petabytes of data in many leading companies, with the largest deployment exceeding 3,000 nodes. You can find more use cases at Powered by Alluxio https://www.alluxio.io/powered-by-alluxio  or visit our first community conference ( Data Orchestration Summit https://www.alluxio.io/data-orchestration-summit-2019/ ) to learn from other community members!<|>297<|>351<|>271<|>296<|>4223<|>other
4233<|>http://numba.pydata.org/<|>Numba<|>PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy https://www.numpy.org/  / SciPy https://www.scipy.org/  / scikit-learn https://scikit-learn.org  etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython https://cython.org/  and Numba http://numba.pydata.org/ . Our goal is to not reinvent the wheel where appropriate.<|>422<|>446<|>416<|>421<|>2124<|>software
4233<|>https://pytorch.org/docs/stable/data.html<|><|>| Component | Description | | ---- | --- | | https://pytorch.org/docs/stable/torch.html  | A Tensor library like NumPy, with strong GPU support | | https://pytorch.org/docs/stable/autograd.html  | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch | | https://pytorch.org/docs/stable/jit.html  | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code | | https://pytorch.org/docs/stable/nn.html  | A neural networks library deeply integrated with autograd designed for maximum flexibility | | https://pytorch.org/docs/stable/multiprocessing.html  | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training | | https://pytorch.org/docs/stable/data.html  | DataLoader and other utility functions for convenience |<|>788<|>829<|>0<|>0<|>4233<|>software
4235<|>https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2<|>Github<|>The  and  datasets can be downloaded on Github https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2  or on Google Drive https://drive.google.com/open?id=0B6-YKFW-MnbOYWxUMTBEZ1FBam8 .<|>47<|>127<|>40<|>46<|>4235<|>dataset_direct_link
4254<|>http://lemire.me/data/integercompression2014.html<|>Document identifier data set<|>Document identifier data set http://lemire.me/data/integercompression2014.html<|>29<|>78<|>0<|>28<|>4254<|>dataset_landing_page
4254<|>https://github.com/zhenjl/encoding/tree/master/benchmark/data<|>Timestamps: ts.txt(sorted)<|>Test file Timestamps: ts.txt(sorted) https://github.com/zhenjl/encoding/tree/master/benchmark/data<|>37<|>98<|>10<|>36<|>4254<|>dataset_direct_link
4254<|>https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data<|>Test data<|>Raw 32 bits binary data file Test data https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data<|>39<|>110<|>29<|>38<|>4254<|>dataset_direct_link
4254<|>https://github.com/zhenjl/encoding/tree/master/benchmark/data<|>Test data: ts.txt(sorted) and lat.txt(unsorted)<|>Text file: 1 entry per line. Test data: ts.txt(sorted) and lat.txt(unsorted) https://github.com/zhenjl/encoding/tree/master/benchmark/data )<|>77<|>138<|>29<|>76<|>4254<|>dataset_direct_link
4270<|>https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime<|><|>However, if you're using a  Node build, you may see Lighthouse log messages about your locale not being available. To remedy this, you can manually install ICU data by using the https://www.npmjs.com/package/full-icu  module and the https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime  at launch.<|>233<|>300<|>0<|>0<|>4270<|>software
4279<|>http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database<|>Purge or recreate a Ruby on Rails database<|>Purge or recreate a Ruby on Rails database http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database<|>43<|>128<|>0<|>42<|>4279<|>other
4284<|>https://github.com/ryancotterell/sigmorphon2016/tree/master/data/<|>here<|>To run experiments for SIGMORPHON 2016 http://ryancotterell.github.io/sigmorphon2016/ , first download training, validatation and test data here https://github.com/ryancotterell/sigmorphon2016/tree/master/data/ .<|>145<|>210<|>140<|>144<|>4284<|>dataset_direct_link
4292<|>https://snap.stanford.edu/data/index.html<|>SNAP Database<|>Alternatively, you could run . Currently, this only supports files of the  and  forms. These are explained here https://gist.github.com/sampollard/f9169c4eb04669390a834884682c080d . It should accept any graph file you can find from SNAP Database https://snap.stanford.edu/data/index.html  or the KONECT Database http://konect.uni-koblenz.de/networks/ .<|>246<|>287<|>232<|>245<|>4292<|>dataset_landing_page
4294<|>http://corpus-tools.org/pepper/<|>Pepper<|>Luke Gessler has written a module for the Pepper http://corpus-tools.org/pepper/  tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See instructions for converting https://github.com/nert-nlp/streusle-pepper-importer .<|>49<|>80<|>42<|>48<|>4294<|>software
4304<|>http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz<|>Annotation<|>Annotation http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz<|>11<|>89<|>0<|>10<|>1915<|>dataset_direct_link
4304<|>http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz<|>Images<|>Images http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz<|>7<|>82<|>0<|>6<|>1915<|>dataset_direct_link
4309<|>http://people.ee.ethz.ch/~ihnatova/#dataset<|>DPED dataset<|>Download DPED dataset http://people.ee.ethz.ch/~ihnatova/#dataset  (patches for CNN training) and extract it into  folder. This folder should contain three subolders: ,  and<|>22<|>65<|>9<|>21<|>4309<|>dataset_landing_page
4328<|>http://numba.pydata.org/numba-doc/dev/user/installing.html<|>here<|>For numba instructions, you can find a tutorial and installation guideline here http://numba.pydata.org/numba-doc/dev/user/installing.html .<|>80<|>138<|>75<|>79<|>4328<|>other
4336<|>https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md<|>here<|>[Optional] If you want to use COCO, please see the notes here https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md .<|>62<|>167<|>57<|>61<|>4336<|>software
4353<|>https://github.com/davidstutz/superpixel-benchmark-data<|>data repository<|>The converted (i.e. pre-processed) NYUV2, SBD and SUNRGBD datasets are now available in the data repository https://github.com/davidstutz/superpixel-benchmark-data .<|>108<|>163<|>92<|>107<|>4353<|>dataset_landing_page
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html<|>[Ratio]<|>Quantification: [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html  · [Ratio] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html  · [Complex] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html<|>133<|>230<|>125<|>132<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html<|>[Existential]<|>Logical: [Existential] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html<|>23<|>119<|>9<|>22<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html<|>[Positive]<|>Selection: [Positive] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html  · [Superlative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html<|>22<|>117<|>11<|>21<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html<|>[Spatial]<|>Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  · [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  · [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  · [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>152<|>247<|>142<|>151<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html<|>[Attribute]<|>Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  · [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  · [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  · [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>263<|>360<|>251<|>262<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html<|>[Full]<|>Quantification: [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html  · [Ratio] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html  · [Complex] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html<|>241<|>337<|>234<|>240<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html<|>[Full]<|>Existential: [One shape] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html  · [Collision-free] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html  · [Chinese] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html<|>250<|>343<|>243<|>249<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html<|>[Complex]<|>Quantification: [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html  · [Ratio] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html  · [Complex] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html<|>351<|>455<|>341<|>350<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>[Full]<|>Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  · [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  · [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  · [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>488<|>580<|>481<|>487<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html<|>[Spatial two shapes]<|>Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  · [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  · [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  · [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>33<|>138<|>12<|>32<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html<|>[Count]<|>Shape: [Single] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html  · [Multi] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html  · [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html<|>227<|>320<|>219<|>226<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html<|>[Count]<|>Quantification: [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html  · [Ratio] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html  · [Complex] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html<|>24<|>121<|>16<|>23<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html<|>[Superlative]<|>Selection: [Positive] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html  · [Superlative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html<|>135<|>233<|>121<|>134<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html<|>[Full]<|>Selection: [Positive] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html  · [Superlative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html<|>244<|>335<|>237<|>243<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html<|>[Comparative]<|>Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  · [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  · [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  · [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html<|>378<|>477<|>364<|>377<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html<|>[One shape]<|>Existential: [One shape] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html  · [Collision-free] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html  · [Chinese] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html<|>25<|>122<|>13<|>24<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html<|>[Chinese]<|>Existential: [One shape] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html  · [Collision-free] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html  · [Chinese] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html<|>357<|>453<|>347<|>356<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html<|>[Collision-free]<|>Existential: [One shape] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html  · [Collision-free] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html  · [Chinese] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html<|>143<|>239<|>126<|>142<|>4366<|>other
4366<|>https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets<|>dataset arguments<|>: Additional dataset configuration values passed as command line arguments (  with  being a string or in JSON format, put in single quotes  if necessary, see dataset arguments https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets  for details)<|>176<|>248<|>158<|>175<|>4366<|>software
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html<|>[Full]<|>Logical: [Existential] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html  · [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html<|>130<|>219<|>123<|>129<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html<|>[Multi]<|>Shape: [Single] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html  · [Multi] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html  · [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html<|>122<|>215<|>114<|>121<|>4366<|>other
4366<|>https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html<|>[Single]<|>Shape: [Single] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html  · [Multi] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html  · [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html<|>16<|>110<|>7<|>15<|>4366<|>other
4376<|>http://www.cs.cmu.edu/~glai1/data/race/<|>here<|>RACE: Please submit a data request here http://www.cs.cmu.edu/~glai1/data/race/ . The data will be automatically sent to you. Create a "data" directory alongside "src" directory and download the data.<|>40<|>79<|>35<|>39<|>4376<|>dataset_landing_page
4403<|>https://github.com/ppuliu/GloRE/tree/master/data<|><|>We provide the following pre-processed files in https://github.com/ppuliu/GloRE/tree/master/data :<|>48<|>96<|>0<|>0<|>4403<|>dataset_direct_link
4427<|>http://www.cs.columbia.edu/~andrews/mil/datasets.html<|>here<|>This script will download datasets from here http://www.cs.columbia.edu/~andrews/mil/datasets.html  and make necessary changes.<|>45<|>98<|>40<|>44<|>4427<|>dataset_landing_page
4438<|>http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser<|>total<|>English Wiktionary: total http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser , semantic relations http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Semantic_relations , translations http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Translations , part of speech http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:POS<|>26<|>139<|>20<|>25<|>4438<|>other
4438<|>http://en.wikipedia.org/wiki/Semi-structured_data<|>semi-structured information<|>The goal of this project is to extract semi-structured information http://en.wikipedia.org/wiki/Semi-structured_data  from Wiktionary and construct machine-readable dictionary http://en.wikipedia.org/wiki/Machine-readable_dictionary  (database + API http://en.wikipedia.org/wiki/API  + GUI http://en.wikipedia.org/wiki/GUI ).<|>67<|>116<|>39<|>66<|>4438<|>other
4438<|>https://github.com/componavt/wikokit/blob/wiki/File_wikt_parsed_empty_sql.md#machine-readable-database-schema<|>Machine-readable database schema<|>start from the paper describing the database (tables and relations) of machine-readable Wiktionary: Transformation of Wiktionary entry structure into tables and relations in a relational database schema http://arxiv.org/abs/1011.1368 . 2010. But there are new tables (absent in the publication) related to  and , see Machine-readable database schema https://github.com/componavt/wikokit/blob/wiki/File_wikt_parsed_empty_sql.md#machine-readable-database-schema ;<|>350<|>459<|>317<|>349<|>4438<|>software
4457<|>https://www.kaggle.com/c/GiveMeSomeCredit/data/<|>Credit scoring<|>Credit scoring https://www.kaggle.com/c/GiveMeSomeCredit/data/<|>15<|>62<|>0<|>14<|>4457<|>dataset_direct_link
4458<|>https://github.com/databricks/spark-perf/issues<|>open an issue on GitHub<|>For questions, bug reports, or feature requests, please open an issue on GitHub https://github.com/databricks/spark-perf/issues .<|>80<|>127<|>56<|>79<|>4458<|>software
4461<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>here<|>For the pose experiments, we used the KITTI odometry split, which can be downloaded here http://www.cvlibs.net/datasets/kitti/eval_odometry.php . Then you can change  option to  when preparing the data.<|>89<|>143<|>84<|>88<|>1355<|>other
4461<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . Then run the following command<|>15<|>50<|>4<|>14<|>3245<|>dataset_landing_page
4461<|>https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation<|>TUM evaluation toolkit<|>Notice that all the predictions and ground-truth are 5-frame snippets with the format of  consistent with the TUM evaluation toolkit https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation . Then you could run<|>133<|>201<|>110<|>132<|>4461<|>other
4461<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI<|>For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command<|>10<|>59<|>4<|>9<|>2741<|>software
4469<|>https://pandas.pydata.org<|>pandas<|>pandas https://pandas.pydata.org<|>7<|>32<|>0<|>6<|>4469<|>software
4495<|>http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html<|>Blur Detection Dataset<|>Test images are from Blur Detection Dataset http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html  [2].<|>44<|>111<|>21<|>43<|>4495<|>dataset_landing_page
4549<|>https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26<|>CrisisLexT26 Dataset<|>CrisisLexT26 Dataset https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26<|>21<|>85<|>0<|>20<|>4549<|>dataset_landing_page
4549<|>http://www.crowdflower.com/data-for-everyone<|>CrowdFlower10K Dataset<|>CrowdFlower10K Dataset http://www.crowdflower.com/data-for-everyone<|>23<|>67<|>0<|>22<|>4549<|>other
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>BShift<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>683<|>752<|>676<|>682<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>TreeDepth<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>421<|>490<|>411<|>420<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>TopConst<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>550<|>619<|>541<|>549<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SentLen<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>172<|>241<|>164<|>171<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|><|>SentEval also includes a series of https://github.com/facebookresearch/SentEval/tree/master/data/probing  to evaluate what linguistic properties are encoded in your sentence embeddings:<|>35<|>104<|>0<|>0<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>WC<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>291<|>360<|>288<|>290<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>Tense<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>807<|>876<|>801<|>806<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>CoordInv<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1322<|>1391<|>1313<|>1321<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SOMO<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1194<|>1263<|>1189<|>1193<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>ObjNum<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1066<|>1135<|>1059<|>1065<|>4567<|>dataset_landing_page
4567<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SubjNum<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>935<|>1004<|>927<|>934<|>4567<|>dataset_landing_page
4612<|>http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html<|>site<|>ECSSD results (link1) https://www.dropbox.com/s/zets1xsne570bgl/ECSSD_ELD.zip?dl=1 (link2) http://pan.baidu.com/s/1i4QslAP  (ECSSD dataset site http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html )<|>144<|>209<|>139<|>143<|>4612<|>dataset_landing_page
4633<|>https://github.com/ctuning/ctuning-datasets-min<|>'ctuning-datasets-min' repository<|>Note that you will be asked to select a JPEG image from available CK data sets. We have added standard demo images ( , , , ) to the 'ctuning-datasets-min' repository https://github.com/ctuning/ctuning-datasets-min .<|>166<|>213<|>132<|>165<|>4633<|>software
4644<|>https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/<|>can be found here<|>Final raw bounding box results can be found here https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/ .<|>49<|>110<|>31<|>48<|>4644<|>dataset_landing_page
4647<|>https://github.com/Lab41/Magnolia/tree/master/data<|><|>Directory: https://github.com/Lab41/Magnolia/tree/master/data<|>11<|>61<|>0<|>0<|>4647<|>software
4661<|>https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets<|>"When to use what data set for your self-driving car algorithm: An overview of publicly available driving datasets"<|>H. Yin, C. Berger: "When to use what data set for your self-driving car algorithm: An overview of publicly available driving datasets" https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets<|>135<|>298<|>19<|>134<|>4661<|>other
4705<|>http://datadryad.org/resource/doi:10.5061/dryad.jp917<|>data set<|>: Embeddings of UMLS https://www.nlm.nih.gov/research/umls/  concept unique identifiers (CUIs), derived from 20 million clinical notes spanning 19 years of data from Stanford Hospital and Clinics, using a data set http://datadryad.org/resource/doi:10.5061/dryad.jp917  released in a paper http://www.nature.com/articles/sdata201432  by Finlayson, LePendu & Shah.<|>214<|>267<|>205<|>213<|>4705<|>dataset_landing_page
4705<|>http://www.nature.com/articles/sdata201432<|>paper<|>: Embeddings of UMLS https://www.nlm.nih.gov/research/umls/  concept unique identifiers (CUIs), derived from 20 million clinical notes spanning 19 years of data from Stanford Hospital and Clinics, using a data set http://datadryad.org/resource/doi:10.5061/dryad.jp917  released in a paper http://www.nature.com/articles/sdata201432  by Finlayson, LePendu & Shah.<|>289<|>331<|>283<|>288<|>4705<|>other
4709<|>https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray<|>link<|>Histopathology Gray: the dataset could be downloaded from link https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray ;<|>63<|>152<|>58<|>62<|>4709<|>software
4709<|>https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST<|>link<|>static MNIST: links to the datasets can found at link https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST ;<|>54<|>117<|>49<|>53<|>4709<|>dataset_direct_link
4711<|>https://pytorch.org/vision/stable/datasets.html#fashion-mnist<|>Pytorch<|>Pytorch https://pytorch.org/vision/stable/datasets.html#fashion-mnist<|>8<|>69<|>0<|>7<|>4711<|>software
4711<|>https://www.tensorflow.org/datasets/catalog/fashion_mnist<|>TensorFlow Datasets<|>TensorFlow Datasets https://www.tensorflow.org/datasets/catalog/fashion_mnist<|>20<|>77<|>0<|>19<|>4711<|>dataset_landing_page
4711<|>https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST<|>Apache MXNet Gluon<|>Apache MXNet Gluon https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST<|>19<|>147<|>0<|>18<|>4711<|>other
4711<|>https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js<|>TensorFlow.js<|>TensorFlow.js https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js<|>14<|>95<|>0<|>13<|>4711<|>software
4711<|>https://docs.activeloop.ai/datasets/fashion-mnist-dataset<|>Activeloop Hub<|>Activeloop Hub https://docs.activeloop.ai/datasets/fashion-mnist-dataset<|>15<|>72<|>0<|>14<|>4711<|>dataset_landing_page
4711<|>https://github.com/AbhirajHinge/CNN-with-Fashion-MNIST-dataset<|>:link:<|>| Classifier | Preprocessing | Fashion test accuracy | MNIST test accuracy | Submitter| Code | | --- | --- | --- | --- | --- |--- | |2 Conv+pooling | None | 0.876 | - | Kashif Rasul https://twitter.com/krasul  | :link: https://gist.github.com/kashif/76792939dd6f473b7404474989cb62a8  | |2 Conv+pooling | None | 0.916| - | Tensorflow's doc https://www.tensorflow.org/tutorials/layers  | :link: /benchmark/convnet.py | |2 Conv+pooling+ELU activation (PyTorch)| None| 0.903| - | @AbhirajHinge https://github.com/AbhirajHinge  | :link: https://github.com/AbhirajHinge/CNN-with-Fashion-MNIST-dataset | |2 Conv | Normalization, random horizontal flip, random vertical flip, random translation, random rotation. | 0.919 |0.971 | Kyriakos Efthymiadis https://github.com/kefth | :link: https://github.com/kefth/fashion-mnist | |2 Conv <100K parameters | None | 0.925 | 0.992 | @hardmaru https://twitter.com/hardmaru  | :link: https://github.com/hardmaru/pytorch_notebooks/blob/master/pytorch_tiny_custom_mnist_adam.ipynb | |2 Conv ~113K parameters | Normalization | 0.922| 0.993 | Abel G. https://github.com/abelusha  | :link: https://github.com/abelusha/MNIST-Fashion-CNN/blob/master/Fashon_MNIST_CNN_using_Keras_10_Runs.ipynb | |2 Conv+3 FC ~1.8M parameters| Normalization | 0.932 | 0.994 | @Xfan1025 https://github.com/Xfan1025  | :link: https://github.com/Xfan1025/Fashion-MNIST/blob/master/fashion-mnist.ipynb  | |2 Conv+3 FC ~500K parameters | Augmentation, batch normalization | 0.934 | 0.994 | @cmasch https://github.com/cmasch  | :link: https://github.com/cmasch/zalando-fashion-mnist  | |2 Conv+pooling+BN | None | 0.934 | - | @khanguyen1207 https://github.com/khanguyen1207  | :link: https://github.com/khanguyen1207/My-Machine-Learning-Corner/blob/master/Zalando%20MNIST/fashion.ipynb | |2 Conv+2 FC| Random Horizontal Flips| 0.939| -| @ashmeet13 https://github.com/ashmeet13 | :link: https://github.com/ashmeet13/FashionMNIST-CNN | |3 Conv+2 FC | None | 0.907 | - | @Cenk Bircanoğlu https://github.com/cenkbircanoglu  | :link: https://github.com/cenkbircanoglu/openface/tree/master/fashion_mnist | |3 Conv+pooling+BN | None | 0.903 | 0.994 | @meghanabhange https://github.com/meghanabhange  | :link: https://github.com/meghanabhange/FashionMNIST-3-Layer-CNN  | |3 Conv+pooling+2 FC+dropout | None | 0.926 | - | @Umberto Griffo https://github.com/umbertogriffo  | :link: https://github.com/umbertogriffo/Fashion-mnist-cnn-keras | |3 Conv+BN+pooling|None|0.921|0.992| @gchhablani https://github.com/gchhablani | :link: https://github.com/gchhablani/CNN-with-FashionMNIST | |5 Conv+BN+pooling|None|0.931|-| @Noumanmufc1 https://github.com/Noumanmufc1 | :link: https://gist.github.com/Noumanmufc1/60f00e434f0ce42b6f4826029737490a | |CNN with optional shortcuts, dense-like connectivity| standardization+augmentation+random erasing | 0.947 |-| @kennivich https://github.com/Dezhic  | :link: https://github.com/Dezhic/fashion-classifier | |GRU+SVM | None| 0.888 | 0.965 | @AFAgarap https://github.com/AFAgarap  | :link: https://gist.githubusercontent.com/AFAgarap/92c1c4a5dd771999b0201ec0e7edfee0/raw/828fbda0e466dacb1fad66549e0e3022e1c7263a/gru_svm_zalando.py | |GRU+SVM with dropout | None| 0.897 | 0.988 | @AFAgarap https://github.com/AFAgarap  | :link: https://gist.githubusercontent.com/AFAgarap/92c1c4a5dd771999b0201ec0e7edfee0/raw/58dbe7cd8b0d83e4386cd6896766113b1a9af096/gru_svm_zalando_dropout.py | |WRN40-4 8.9M params | standard preprocessing (mean/std subtraction/division) and augmentation (random crops/horizontal flips)| 0.967 | - | @ajbrock https://github.com/ajbrock  | :link: https://github.com/xternalz/WideResNet-pytorch :link: https://github.com/ajbrock/FreezeOut  | |DenseNet-BC 768K params| standard preprocessing (mean/std subtraction/division) and augmentation (random crops/horizontal flips) | 0.954 | - | @ajbrock https://github.com/ajbrock  | :link: https://github.com/bamos/densenet.pytorch :link: https://github.com/ajbrock/FreezeOut  | |MobileNet | augmentation (horizontal flips)| 0.950|- | @苏剑林 https://github.com/bojone | :link: http://kexue.fm/archives/4556/ | |ResNet18 | Normalization, random horizontal flip, random vertical flip, random translation, random rotation. | 0.949 | 0.979 | Kyriakos Efthymiadis https://github.com/kefth | :link: https://github.com/kefth/fashion-mnist | |GoogleNet with cross-entropy loss | None | 0.937 | - | @Cenk Bircanoğlu https://github.com/cenkbircanoglu  | :link: https://github.com/cenkbircanoglu/openface/tree/master/fashion_mnist | |AlexNet with Triplet loss| None | 0.899 | - | @Cenk Bircanoğlu https://github.com/cenkbircanoglu  | :link: https://github.com/cenkbircanoglu/openface/tree/master/fashion_mnist | |SqueezeNet with cyclical learning rate 200 epochs| None| 0.900| - | @snakers4 https://github.com/snakers4  | :link: https://github.com/zalandoresearch/fashion-mnist/files/1263340/squeeze_net_mnist.zip | |Dual path network with wide resnet 28-10|standard preprocessing (mean/std subtraction/division) and augmentation (random crops/horizontal flips) |0.957|-| @Queequeg https://github.com/Queequeg92 | :link: https://github.com/Queequeg92/DualPathNet | |MLP 256-128-100| None | 0.8833| - | @heitorrapela https://github.com/heitorrapela | :link: https://github.com/heitorrapela/fashion-mnist-mlp | |VGG16 26M parameters | None | 0.935| - | @QuantumLiu https://github.com/QuantumLiu | :link: https://github.com/QuantumLiu/fashion-mnist-demo-by-Keras :link: https://zhuanlan.zhihu.com/p/28968219 | |WRN-28-10| standard preprocessing (mean/std subtraction/division) and augmentation (random crops/horizontal flips) | 0.959 | -| @zhunzhong07 https://github.com/zhunzhong07 | :link: https://github.com/zhunzhong07/Random-Erasing | |WRN-28-10 + Random Erasing| standard preprocessing (mean/std subtraction/division) and augmentation (random crops/horizontal flips) | 0.963 | -| @zhunzhong07 https://github.com/zhunzhong07 | :link: https://github.com/zhunzhong07/Random-Erasing | |Human Performance| Crowd-sourced evaluation of human (with no fashion expertise) performance. 1000 randomly sampled test images, 3 labels per image, majority labelling. | 0.835 | - | Leo | - | |Capsule Network 8M parameters| Normalization and shift at most 2 pixel and horizontal flip | 0.936 | - | @XifengGuo https://github.com/XifengGuo  | :link: https://github.com/XifengGuo/CapsNet-Fashion-MNIST | |HOG+SVM| HOG | 0.926 | - | @subalde https://github.com/subalde  | :link: https://github.com/subalde/fashion-mnist | |XgBoost| scaling the pixel values to mean=0.0 and var=1.0| 0.898| 0.958| @anktplwl91 https://github.com/anktplwl91 | :link: https://github.com/anktplwl91/fashion_mnist.git | |DENSER| - | 0.953| 0.997| @fillassuncao https://github.com/fillassuncao | :link: https://github.com/fillassuncao/denser-models :link: https://arxiv.org/pdf/1801.01563.pdf | |Dyra-Net| Rescale to unit interval | 0.906| -| @Dirk Schäfer https://github.com/disc5 | :link: https://github.com/disc5/dyra-net :link: https://dl.acm.org/citation.cfm?id=3204176.3204200 | |Google AutoML|24 compute hours (higher quality)| 0.939|-| @Sebastian Heinz https://github.com/sebastianheinz  | :link: https://www.statworx.com/de/blog/a-performance-benchmark-of-google-automl-vision-using-fashion-mnist/ | |Fastai| Resnet50+Fine-tuning+Softmax on last layer's activations| 0.9312| - | @Sayak https://github.com/sayakpaul  | :link: https://github.com/sayakpaul/Experiments-on-Fashion-MNIST/ |<|>532<|>594<|>525<|>531<|>4711<|>software
4711<|>https://keras.io/api/datasets/fashion_mnist/<|>Keras<|>Keras https://keras.io/api/datasets/fashion_mnist/<|>6<|>50<|>0<|>5<|>4711<|>software
4711<|>https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist<|>Tensorflow<|>Tensorflow https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist<|>11<|>85<|>0<|>10<|>4711<|>other
4711<|>https://huggingface.co/datasets/fashion_mnist<|>HuggingFace Datasets<|>HuggingFace Datasets https://huggingface.co/datasets/fashion_mnist<|>21<|>66<|>0<|>20<|>4711<|>dataset_landing_page
4711<|>https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html<|>Chainer<|>Chainer https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html<|>8<|>102<|>0<|>7<|>4711<|>other
4746<|>https://github.com/davidsandberg/facenet/wiki/Training-using-the-VGGFace2-dataset<|>wiki<|>| Date | Update | |----------|--------| | 2018-04-10 | Added new models trained on Casia-WebFace and VGGFace2 (see below). Note that the models uses fixed image standardization (see wiki https://github.com/davidsandberg/facenet/wiki/Training-using-the-VGGFace2-dataset ). | | 2018-03-31 | Added a new, more flexible input pipeline as well as a bunch of minor updates. | | 2017-05-13 | Removed a bunch of older non-slim models. Moved the last bottleneck layer into the respective models. Corrected normalization of Center Loss. | | 2017-05-06 | Added code to train a classifier on your own images https://github.com/davidsandberg/facenet/wiki/Train-a-classifier-on-own-images . Renamed facenet_train.py to train_tripletloss.py and facenet_train_classifier.py to train_softmax.py. | | 2017-03-02 | Added pretrained models that generate 128-dimensional embeddings.| | 2017-02-22 | Updated to Tensorflow r1.0. Added Continuous Integration using Travis-CI.| | 2017-02-03 | Added models where only trainable variables has been stored in the checkpoint. These are therefore significantly smaller. | | 2017-01-27 | Added a model trained on a subset of the MS-Celeb-1M dataset. The LFW accuracy of this model is around 0.994. | | 2017‑01‑02 | Updated to run with Tensorflow r0.12. Not sure if it runs with older versions of Tensorflow though. |<|>187<|>268<|>182<|>186<|>4746<|>software
4747<|>http://jmcauley.ucsd.edu/data/amazon/<|>Amazon dataset<|>Amazon dataset http://jmcauley.ucsd.edu/data/amazon/<|>15<|>52<|>0<|>14<|>4747<|>dataset_landing_page
4748<|>https://dataskeptic.com/blog/episodes/2017/pix2code<|>Data Skeptic<|>Data Skeptic https://dataskeptic.com/blog/episodes/2017/pix2code  (podcast)<|>13<|>64<|>0<|>12<|>4748<|>other
4752<|>https://bmcfee.github.io/data/aotm2011.html<|>AotM-2011<|>The paper presents a thorough off-line evaluation conducted on two playlist datasets: the publicly available AotM-2011 https://bmcfee.github.io/data/aotm2011.html  dataset (derived from the Art of the Mix http://www.artofthemix.org  platform), and a private collection that 8tracks https://8tracks.com/  shared with us for research purposes. The playlist collections are enriched with song features derived from the publicly available Million Song Dataset https://labrosa.ee.columbia.edu/millionsong/ .<|>119<|>162<|>109<|>118<|>4752<|>dataset_landing_page
4770<|>http://bokeh.pydata.org<|>bokeh<|>bokeh http://bokeh.pydata.org  for training visualization<|>6<|>29<|>0<|>5<|>4770<|>software
4770<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for logging to csv<|>7<|>32<|>0<|>6<|>67<|>software
4789<|>http://vision.jhu.edu/data/<|>Hopkings dataset<|>Download Hopkings dataset http://vision.jhu.edu/data/  into  folder<|>26<|>53<|>9<|>25<|>4789<|>dataset_landing_page
4789<|>https://cs.adelaide.edu.au/users/hwong/doku.php?id=data<|>AdelaideRMF dataset<|>Download the AdelaideRMF dataset https://cs.adelaide.edu.au/users/hwong/doku.php?id=data  into  folder<|>33<|>88<|>13<|>32<|>4789<|>other
4795<|>http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html<|>digits<|>To train the model on a custom dataset you need to define a class with a specific interface. Suppose we want to train the model on the digits http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html  dataset. This datasets consists of 8x8 images of digits. Let's suppose that the data is stored in , ,  and  files. We will assume that  and  have shapes of the form . We can then define the class corresponding to this dataset in  as follows.<|>142<|>224<|>135<|>141<|>4795<|>dataset_landing_page
4807<|>http://www.let.rug.nl/rikvannoord/AMR/silver_data/<|>here<|>The silver data that I used in the experiments for the CLIN paper can be downloaded here http://www.let.rug.nl/rikvannoord/AMR/silver_data/ . The silver data was obtained by parsing all sentences in the Groningen Meaning Bank http://gmb.let.rug.nl/  with the parsers CAMR https://github.com/c-amr/camr  and JAMR https://github.com/jflanigan/jamr . The data folder contains seven files: all CAMR and JAMR parses (1.25 million, aligned with each other) and sets of AMRs (20k, 50k, 75k, 100k, 500k) that were used in our experiments (CAMR only). For more details please see our CLIN paper https://clinjournal.org/clinj/article/view/72/64 .<|>89<|>139<|>84<|>88<|>4807<|>dataset_direct_link
4847<|>https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs<|>data/generator/inputs<|>The values associated to the keys can be either one value or a list of values. In the case of a list of values, the PIMC generator will consider all the possible combinations of values between all the keys. Some examples of configuration files for PIMC generation can be found in data/generator/config https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config . Some examples of PRISM files can be found in data/generator/inputs https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs  (from PRISM benchmarks http://www.prismmodelchecker.org/benchmarks/models.php#dtmcs ).<|>447<|>522<|>425<|>446<|>4847<|>software
4847<|>https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config<|>data/generator/config<|>The values associated to the keys can be either one value or a list of values. In the case of a list of values, the PIMC generator will consider all the possible combinations of values between all the keys. Some examples of configuration files for PIMC generation can be found in data/generator/config https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config . Some examples of PRISM files can be found in data/generator/inputs https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs  (from PRISM benchmarks http://www.prismmodelchecker.org/benchmarks/models.php#dtmcs ).<|>302<|>377<|>280<|>301<|>4847<|>other
4851<|>https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/<|>(mirror)<|>We added more aligned speech data (630h total now), thanks to the m-ailabs speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ . We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.<|>161<|>218<|>152<|>160<|>4851<|>dataset_landing_page
4851<|>https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/<|>(mirror)<|>This recipe and collection of scripts enables you to train large vocabulary German acoustic models for speaker-independent automatic speech recognition (ASR) with Kaldi http://kaldi.sourceforge.net/ . The scripts currently use three freely available German speech corpora: The Tuda-De corpus is recorded with a Microsoft Kinect and two other microphones in parallel at Technische Universität Darmstadt and has been released under a permissive license (CC-BY 4.0) http://creativecommons.org/licenses/by/4.0/ . This corpus compromises ~31h of training data per microphone and ~5h separated into development and test partitions. We also make use of the German subset from the Spoken Wikipedia Corpora (SWC) https://nats.gitlab.io/swc/ , containing about 285h of additional data and the German subset of m-ailabs read speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/  (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.<|>900<|>957<|>891<|>899<|>4851<|>dataset_landing_page
4869<|>https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv<|>in the repo<|>F1 scores of models against secret blind data in the STUART and CRAWFORD wells. The logs for those wells are available in the repo https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv , but contestants do not have access to the facies.<|>131<|>210<|>119<|>130<|>4869<|>dataset_direct_link
4904<|>https://github.com/amarasovic/abstract-anaphora-data<|>repo<|>Check this repo https://github.com/amarasovic/abstract-anaphora-data .<|>16<|>68<|>11<|>15<|>4904<|>software
4968<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM_RGBD<|>Two short RGB-D sequences are included as examples. To add more sequences, download from: TUM_RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset  or ICL-NUIM https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html  and place them inside the directory  the same way it has been done for the examples. Then add the respective camera parameters in the same format as the examples as<|>99<|>150<|>90<|>98<|>4968<|>dataset_landing_page
4969<|>https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json<|>trains-json folder<|>JSON-format information in the trains-json folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json  is fetched from: http://rata.digitraffic.fi/api/v1/history?departure_date=2016-08-26<|>50<|>136<|>31<|>49<|>4969<|>dataset_direct_link
4969<|>https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql<|>psql folder<|>CSV-versions of the data are available in the csv folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv . The psql folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql  contains instructions and scripts to import the data into a PostgreSQL database and produce reports and comparison tables between the manual log and recognised trips. Output from three sample methods is included for testing.<|>154<|>233<|>142<|>153<|>4969<|>other
4969<|>https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv<|>csv folder<|>CSV-versions of the data are available in the csv folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv . The psql folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql  contains instructions and scripts to import the data into a PostgreSQL database and produce reports and comparison tables between the manual log and recognised trips. Output from three sample methods is included for testing.<|>57<|>135<|>46<|>56<|>4969<|>dataset_direct_link
4969<|>http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic#.V9BlOxB96Ho<|>Digitraffic<|>The train data is licensed under the Creative Commons BY 4.0 licence http://creativecommons.org/licenses/by/4.0/  from Digitraffic http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic#.V9BlOxB96Ho  offered by the Finnish Traffic Agency http://www.liikennevirasto.fi/web/en .<|>131<|>211<|>119<|>130<|>4969<|>other
4969<|>https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/transit-data-bounding-box.png<|>around the coordinates<|>A series of positions of the Helsinki Regional Transport https://www.hsl.fi/en  fleet obtained by sampling http://dev.hsl.fi/siriaccess/vm/json every 30 seconds, restricting the timeperiod to the time of the trial and geoboxing the area around the coordinates https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/transit-data-bounding-box.png  sampled from the test participants.<|>260<|>368<|>237<|>259<|>4969<|>other
4969<|>https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/example-positioning-problem.png<|>here<|>The mobile client uses the Android fused location provider https://developers.google.com/android/reference/com/google/android/gms/location/FusedLocationProviderApi , which combines data from satellite, WLAN and cellular positioning. Despite that, sometimes there can be problematic positioning fixes, which should be taken care of by filtering. One example is shown here https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/example-positioning-problem.png .<|>371<|>481<|>366<|>370<|>4969<|>dataset_landing_page
4976<|>http://data.caida.org/datasets/as-relationships/serial-1/<|>here<|>as_relationship : CAIDAs AS relationship files found here http://data.caida.org/datasets/as-relationships/serial-1/<|>58<|>115<|>53<|>57<|>4976<|>dataset_direct_link
5016<|>http://pytorch.org/audio/main/datasets.html<|>Dataloaders for common audio datasets<|>Dataloaders for common audio datasets http://pytorch.org/audio/main/datasets.html<|>38<|>81<|>0<|>37<|>5016<|>dataset_landing_page
5053<|>http://datashare.is.ed.ac.uk/handle/10283/1942<|>Download here<|>Download here http://datashare.is.ed.ac.uk/handle/10283/1942<|>14<|>60<|>0<|>13<|>4200<|>dataset_landing_page
5063<|>https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1<|>this link<|>The Personalized Dialog dataset can be downloaded using . Alternatively, it is accessable using this link https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1  or through the ParlAI framework http://parl.ai/  for dialog AI research.<|>106<|>187<|>96<|>105<|>5063<|>dataset_direct_link
5074<|>https://github.com/caesar0301/awesome-public-datasets<|>Awesome public datasets<|>Awesome public datasets https://github.com/caesar0301/awesome-public-datasets<|>24<|>77<|>0<|>23<|>3372<|>dataset_landing_page
5074<|>https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215<|>Wikipedia's list of datasets for machine learning research<|>Wikipedia's list of datasets for machine learning research https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215<|>59<|>148<|>0<|>58<|>5074<|>dataset_landing_page
5074<|>https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0<|>Estimating Optimal Learning Rate<|>Estimating Optimal Learning Rate https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0  - Blog post on the learning rate optimisation<|>33<|>135<|>0<|>32<|>5074<|>other
5074<|>http://www.audiocontentanalysis.org/data-sets/<|>AudioContentAnalysis nearly exhaustive list of music-related datasets<|>AudioContentAnalysis nearly exhaustive list of music-related datasets http://www.audiocontentanalysis.org/data-sets/<|>70<|>116<|>0<|>69<|>5074<|>dataset_landing_page
5074<|>https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750<|>Battle of the Deep Learning frameworks<|>Battle of the Deep Learning frameworks https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750  - DL frameworks comparison and evolution<|>39<|>128<|>0<|>38<|>5074<|>other
5102<|>https://webscope.sandbox.yahoo.com/catalog.php?datatype=c<|>link<|>| Data | Task | Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs | Binary classification | link https://archive.ics.uci.edu/ml/datasets/HIGGS  |10,500,000|28| use last 500,000 samples as test set | | Epsilon | Binary classification | link http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | 400,000 | 2,000 | use the provided test set | | Bosch | Binary classification | link https://www.kaggle.com/c/bosch-production-line-performance/data  | 1,000,000 | 968 | use the provided test set | | Yahoo LTR| Learning to rank | link https://webscope.sandbox.yahoo.com/catalog.php?datatype=c  |473,134|700| set1.train as train, set1.test as test | | MS LTR | Learning to rank | link http://research.microsoft.com/en-us/projects/mslr/  |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo | Binary classification (Categorical) | link http://stat-computing.org/dataexpo/2009/  |11,000,000|700| use last 1,000,000 as test set |<|>607<|>664<|>602<|>606<|>5102<|>dataset_landing_page
5102<|>http://stat-computing.org/dataexpo/2009/<|>link<|>| Data | Task | Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs | Binary classification | link https://archive.ics.uci.edu/ml/datasets/HIGGS  |10,500,000|28| use last 500,000 samples as test set | | Epsilon | Binary classification | link http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | 400,000 | 2,000 | use the provided test set | | Bosch | Binary classification | link https://www.kaggle.com/c/bosch-production-line-performance/data  | 1,000,000 | 968 | use the provided test set | | Yahoo LTR| Learning to rank | link https://webscope.sandbox.yahoo.com/catalog.php?datatype=c  |473,134|700| set1.train as train, set1.test as test | | MS LTR | Learning to rank | link http://research.microsoft.com/en-us/projects/mslr/  |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo | Binary classification (Categorical) | link http://stat-computing.org/dataexpo/2009/  |11,000,000|700| use last 1,000,000 as test set |<|>920<|>960<|>915<|>919<|>269<|>dataset_landing_page
5102<|>http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>LibSVM Datasets<|>We also tested our implementation with the  dataset, available at LibSVM Datasets http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html .<|>82<|>148<|>66<|>81<|>5102<|>dataset_landing_page
5102<|>https://www.kaggle.com/c/bosch-production-line-performance/data<|>link<|>| Data | Task | Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs | Binary classification | link https://archive.ics.uci.edu/ml/datasets/HIGGS  |10,500,000|28| use last 500,000 samples as test set | | Epsilon | Binary classification | link http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | 400,000 | 2,000 | use the provided test set | | Bosch | Binary classification | link https://www.kaggle.com/c/bosch-production-line-performance/data  | 1,000,000 | 968 | use the provided test set | | Yahoo LTR| Learning to rank | link https://webscope.sandbox.yahoo.com/catalog.php?datatype=c  |473,134|700| set1.train as train, set1.test as test | | MS LTR | Learning to rank | link http://research.microsoft.com/en-us/projects/mslr/  |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo | Binary classification (Categorical) | link http://stat-computing.org/dataexpo/2009/  |11,000,000|700| use last 1,000,000 as test set |<|>457<|>520<|>452<|>456<|>5102<|>dataset_direct_link
5102<|>http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>link<|>| Data | Task | Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs | Binary classification | link https://archive.ics.uci.edu/ml/datasets/HIGGS  |10,500,000|28| use last 500,000 samples as test set | | Epsilon | Binary classification | link http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | 400,000 | 2,000 | use the provided test set | | Bosch | Binary classification | link https://www.kaggle.com/c/bosch-production-line-performance/data  | 1,000,000 | 968 | use the provided test set | | Yahoo LTR| Learning to rank | link https://webscope.sandbox.yahoo.com/catalog.php?datatype=c  |473,134|700| set1.train as train, set1.test as test | | MS LTR | Learning to rank | link http://research.microsoft.com/en-us/projects/mslr/  |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo | Binary classification (Categorical) | link http://stat-computing.org/dataexpo/2009/  |11,000,000|700| use last 1,000,000 as test set |<|>302<|>368<|>297<|>301<|>5102<|>dataset_landing_page
5102<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>link<|>| Data | Task | Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs | Binary classification | link https://archive.ics.uci.edu/ml/datasets/HIGGS  |10,500,000|28| use last 500,000 samples as test set | | Epsilon | Binary classification | link http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | 400,000 | 2,000 | use the provided test set | | Bosch | Binary classification | link https://www.kaggle.com/c/bosch-production-line-performance/data  | 1,000,000 | 968 | use the provided test set | | Yahoo LTR| Learning to rank | link https://webscope.sandbox.yahoo.com/catalog.php?datatype=c  |473,134|700| set1.train as train, set1.test as test | | MS LTR | Learning to rank | link http://research.microsoft.com/en-us/projects/mslr/  |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo | Binary classification (Categorical) | link http://stat-computing.org/dataexpo/2009/  |11,000,000|700| use last 1,000,000 as test set |<|>159<|>204<|>154<|>158<|>5102<|>dataset_landing_page
5113<|>https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb<|>dataset_class.ipynb<|>dataset_class.ipynb https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb  examples for using the data generator<|>20<|>102<|>0<|>19<|>5113<|>dataset_landing_page
5116<|>https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m<|>this script<|>For image denoise, launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_test_demo.m  to see the denoise result by pre-train models. For training, you need to generate the data yourself since the data used in the training is large. Please do the following steps to generate data: a) download MIT saliency dataset from here http://saliency.mit.edu/BenchmarkIMAGES.zip  and put all the image files here https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency ; b) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m  to generate training data; c) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m  to generate validation data; d) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_train_demo.m  to start the training.<|>571<|>686<|>559<|>570<|>5116<|>software
5116<|>https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m<|>this script<|>For image denoise, launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_test_demo.m  to see the denoise result by pre-train models. For training, you need to generate the data yourself since the data used in the training is large. Please do the following steps to generate data: a) download MIT saliency dataset from here http://saliency.mit.edu/BenchmarkIMAGES.zip  and put all the image files here https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency ; b) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m  to generate training data; c) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m  to generate validation data; d) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_train_demo.m  to start the training.<|>737<|>847<|>725<|>736<|>5116<|>software
5116<|>https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency<|>here<|>For image denoise, launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_test_demo.m  to see the denoise result by pre-train models. For training, you need to generate the data yourself since the data used in the training is large. Please do the following steps to generate data: a) download MIT saliency dataset from here http://saliency.mit.edu/BenchmarkIMAGES.zip  and put all the image files here https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency ; b) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m  to generate training data; c) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m  to generate validation data; d) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_train_demo.m  to start the training.<|>461<|>546<|>456<|>460<|>5116<|>dataset_landing_page
5125<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>here<|>Download KITTI dataset from here http://www.cvlibs.net/datasets/kitti/eval_tracking.php . We need left color images http://www.cvlibs.net/download.php?file=data_tracking_image_2.zip  and tracking labels http://www.cvlibs.net/download.php?file=data_tracking_label_2.zip .<|>33<|>87<|>28<|>32<|>5125<|>dataset_landing_page
5127<|>http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>This code implements the state-of-the-art Knowledge Graph Embedding algorithms http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf  such as RESCAL http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf , TransE http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data , HOLE http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12484/11828 , and S-RESCAL with added Type Regularizer for Freebase. Computational Graphs are implemented using Theano http://deeplearning.net/software/theano/ . Available SGD Algorithms : ADAM, Adagrad. Algorithms are hand coded and implementing SGD variations should be straightforward.<|>226<|>316<|>219<|>225<|>5127<|>other
5131<|>http://data.iana.org/TLD/tlds-alpha-by-domain.txt<|>IANA<|>The valid TLDs file (get if from IANA http://data.iana.org/TLD/tlds-alpha-by-domain.txt )<|>38<|>87<|>33<|>37<|>5131<|>dataset_direct_link
5140<|>http://nlp.stanford.edu/data/glove.6B.100d.zip<|>Download<|>Second, this project uses pretrained GloVe word embeddings https://nlp.stanford.edu/projects/glove/  of 100 dimensions, trained on 6B tokens. Download http://nlp.stanford.edu/data/glove.6B.100d.zip  and extract under .<|>151<|>197<|>142<|>150<|>5140<|>dataset_direct_link
5140<|>https://github.com/Noahs-ARK/semafor/tree/master/training/data<|>frame-elements file format<|>The output, in a CoNLL 2009-like format will be written to  and in the frame-elements file format https://github.com/Noahs-ARK/semafor/tree/master/training/data  to  for frame and argument identification.<|>98<|>160<|>71<|>97<|>5140<|>software
5166<|>http://corpus-texmex.irisa.fr/<|>SIFT1M and GIST1M<|>SIFT1M and GIST1M http://corpus-texmex.irisa.fr/<|>18<|>48<|>0<|>17<|>1268<|>dataset_landing_page
5167<|>http://corpus-texmex.irisa.fr/<|>SIFT1M and GIST1M<|>SIFT1M and GIST1M http://corpus-texmex.irisa.fr/<|>18<|>48<|>0<|>17<|>1268<|>dataset_landing_page
5189<|>https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset<|>MediaPipe inference demo<|>To run inference with your model in MediaPipe inference demo https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset , you need to export your checkpoint to a SavedModel.<|>61<|>211<|>36<|>60<|>5189<|>software
5194<|>http://spatialrelations.cs.uni-freiburg.de/#dataset<|>dataset<|>Download the dataset http://spatialrelations.cs.uni-freiburg.de/#dataset<|>21<|>72<|>13<|>20<|>5194<|>dataset_landing_page
5222<|>https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/<|>Oxford English Corpus<|>This repo is useful as a corpus for typing training programs. According to analysis of the Oxford English Corpus https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/ , the 7,000 most common English lemmas account for approximately 90% of usage, so a 10,000 word training corpus is more than sufficient for practical training applications.<|>113<|>194<|>91<|>112<|>5222<|>other
5237<|>https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv<|>data/sensor_graph/graph_sensor_locations.csv<|>Besides, the locations of sensors in Los Angeles, i.e., METR-LA, are available at data/sensor_graph/graph_sensor_locations.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv , and the locations of sensors in PEMS-BAY are available at data/sensor_graph/graph_sensor_locations_bay.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv .<|>127<|>218<|>82<|>126<|>5237<|>dataset_direct_link
5237<|>https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv<|>data/sensor_graph/graph_sensor_locations_bay.csv<|>Besides, the locations of sensors in Los Angeles, i.e., METR-LA, are available at data/sensor_graph/graph_sensor_locations.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv , and the locations of sensors in PEMS-BAY are available at data/sensor_graph/graph_sensor_locations_bay.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv .<|>328<|>423<|>279<|>327<|>5237<|>dataset_direct_link
5264<|>http://pandas.pydata.org/<|>Pandas<|>Pandas http://pandas.pydata.org/  ( )<|>7<|>32<|>0<|>6<|>67<|>software
5293<|>https://github.com/vered1986/LexNET/tree/v2/datasets<|>LexNet<|>To reproduce our experimental results, you need dictionaries ( LexNet https://github.com/vered1986/LexNET/tree/v2/datasets  for English, Parsed Wiktionary http://ustalov.imm.uran.ru/pub/projlearn-ruwikt.tar.gz  for Russian) and word embeddings ( Google News https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing  for English, Russian Distributional Thesaurus http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz500-w10-cb0-it3-min5.w2v  for Russian). Since our implementation uses Python 3 and TensorFlow 0.12, please install them, too.<|>70<|>122<|>63<|>69<|>5293<|>dataset_landing_page
5295<|>http://curtis.ml.cmu.edu/datasets/quasar/<|>here<|>Both Quasar-S and Quasar-T are available for download here http://curtis.ml.cmu.edu/datasets/quasar/ . See the accompanying  for a description of the included files. The release includes:<|>59<|>100<|>54<|>58<|>5295<|>dataset_direct_link
5306<|>https://github.com/datafl4sh/diskpp<|>DiSk++<|>The code for handling the meshes is a prelimary version of what became the DiSk++ https://github.com/datafl4sh/diskpp  library by Matteo Cicuttin. The code for assembling the high order operators is based on code written by Danielle Di Pietro for similar HHO schemes.<|>82<|>117<|>75<|>81<|>5306<|>software
5322<|>http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz<|><|>incremental subsamples of 150K, 700K, 1M and 1.7M (~all news2016) parallel back-translation corpora used in the  where the target (TR) side samples are from monolingual Turkish data http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz . The sentences are translated into EN with a single TR->EN NMT system (~14 BLEU on newstest2016):<|>182<|>252<|>0<|>0<|>5322<|>dataset_direct_link
5359<|>https://github.com/cocodataset/cocoapi<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi<|>12<|>50<|>0<|>11<|>5359<|>software
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/open_in_sv.sh<|>open_in_sv.sh<|>verify annotations of note onsets and beats. Correct manually some imprecise vocal annotations. Open as note layer in Sonic Visualiser http://www.sonicvisualiser.org/  by script 'sh open_in_sv.sh https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/open_in_sv.sh '<|>196<|>284<|>182<|>195<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py<|>this script<|>derive beat annotations by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py<|>39<|>140<|>27<|>38<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/match.py<|>by this script<|>find recording MSD_TRACK_id from this list https://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_tracks.txt . Then match by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/match.py<|>169<|>252<|>154<|>168<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py<|>shift time of annotation<|>if systematic delay/advance of timestamps, measure the difference to onsets with SV's measure tool and run shift time of annotation https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py<|>132<|>230<|>107<|>131<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts<|>similar repository<|>scripts: python scripts for loading data, more scripts are in the similar repository https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts<|>85<|>159<|>66<|>84<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py<|>fetch_midi<|>get the matched MIDI from lakh-matched MIDI fetch_midi https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py  (if more than one match, pick the MIDI for the best match)<|>55<|>143<|>44<|>54<|>5374<|>dataset_landing_page
5374<|>https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py<|>this script<|>derive singing voice note annotations by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py  Since MIDI standard does not define an instrument for singing voice, the singing voice track is given a different program # in a random channel in each MIDI. Thus one needs to manually identify the MIDI channel # that corresponds to the melody of the singing voice track Optionally, doing in advance an annotation of segments with active vocal is helpful.<|>53<|>154<|>41<|>52<|>5374<|>dataset_landing_page
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>171<|>255<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1191<|>1275<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1106<|>1190<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1021<|>1105<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>451<|>540<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>361<|>450<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1361<|>1445<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1276<|>1360<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1446<|>1530<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>86<|>170<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>181<|>270<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>271<|>360<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>256<|>340<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>426<|>510<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>341<|>425<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>511<|>595<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1<|>85<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>596<|>680<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>721<|>810<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>541<|>630<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>936<|>1020<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>1531<|>1615<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>1<|>90<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>91<|>180<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip<|>631<|>720<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>766<|>850<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>681<|>765<|>0<|>0<|>5385<|>dataset_direct_link
5385<|>https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip<|><|> https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip<|>851<|>935<|>0<|>0<|>5385<|>dataset_direct_link
5399<|>https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh<|>Yoon Kim's script<|>The non-English data (Czech, French, German, Russian, and Spanish) can be downloaded from Jan Botha's website https://bothameister.github.io . For ease of use you can use the Yoon Kim's script https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh , which downloads these data and saves them into the relevant folders.<|>193<|>257<|>175<|>192<|>5399<|>dataset_landing_page
5457<|>https://github.com/deepmind/rc-data<|>https://github.com/deepmind/rc-data<|>http://cs.stanford.edu/~danqi/data/cnn.tar.gz http://cs.stanford.edu/~danqi/data/cnn.tar.gz  already preprocessed, and the original one from https://github.com/deepmind/rc-data https://github.com/deepmind/rc-data  for the CNN news article<|>177<|>212<|>141<|>176<|>2282<|>dataset_landing_page
5459<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>models and results reflecting the newly cleaned up data in the boxscore-data repo https://github.com/harvardnlp/boxscore-data  are now given below.<|>82<|>125<|>63<|>81<|>5459<|>dataset_landing_page
5459<|>https://github.com/ratishsp/data2text-plan-py<|>data2text-plan-py<|>For an improved implementation of the extractive evaluation metrics (and improved models), please see the data2text-plan-py https://github.com/ratishsp/data2text-plan-py  repo associated with the Puduppully et al. (AAAI 2019) paper https://arxiv.org/abs/1809.00582 .<|>124<|>169<|>106<|>123<|>5459<|>dataset_landing_page
5459<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>The boxscore-data associated with the above paper can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data , and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar.<|>96<|>139<|>77<|>95<|>5459<|>dataset_landing_page
5464<|>https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus<|>GenX corpus<|>Data is from the GenX corpus https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus  produced by FitzGerald http://nfitz.net/  et al.<|>29<|>95<|>17<|>28<|>5464<|>dataset_landing_page
5484<|>https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth<|>here<|>Following our text recognition experiments might be a little difficult, because we can not offer the entire dataset used by us. But it is possible to perform the experiments based on the Synth-90k dataset provided by Jaderberg et al. here https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth . After downloading and extracting this file you'll need to adapt the groundtruth file provided with this dataset to fit to the format used by our code. Our format is quite easy. You need to create a  file with tabular separated values. The first column is the absolute path to the image and the rest of the line are the labels corresponding to this image.<|>239<|>292<|>234<|>238<|>5484<|>dataset_landing_page
5495<|>https://hanlp.hankcs.com/docs/data_format.html<|>《格式规范》<|>关于标注集含义，请参考 《语言学标注规范》 https://hanlp.hankcs.com/docs/annotations/index.html 及 《格式规范》 https://hanlp.hankcs.com/docs/data_format.html 。我们购买、标注或采用了世界上量级最大、种类最多的语料库用于联合多语种多任务学习，所以HanLP的标注集也是覆盖面最广的。<|>84<|>130<|>77<|>83<|>5495<|>other
5496<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google NGrams<|>Create a dictionary with inverse document frequency (idf) values from the Google NGrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html  dataset.<|>88<|>152<|>74<|>87<|>5496<|>dataset_landing_page
5496<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google NGrams<|>Download the 1-gram files and the  files for your language from the Google NGrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html  site into a common folder.<|>82<|>146<|>68<|>81<|>5496<|>dataset_landing_page
5507<|>http://bactra.org/notebooks/network-data-analysis.html<|>Analysis of Network Data<|>Analysis of Network Data http://bactra.org/notebooks/network-data-analysis.html .<|>25<|>79<|>0<|>24<|>5507<|>other
5507<|>http://snap.stanford.edu/data/index.html<|>Stanford Large Network Dataset Collection<|>Stanford Large Network Dataset Collection http://snap.stanford.edu/data/index.html .<|>42<|>82<|>0<|>41<|>269<|>dataset_landing_page
5507<|>https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/<|>Working with Bipartite/Affiliation Network Data in R<|>Working with Bipartite/Affiliation Network Data in R https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/  (2012).<|>53<|>153<|>0<|>52<|>5507<|>other
5507<|>http://eh.net/database/international-currencies-1890-1910/<|>data<|>The Ties that Divide: A Network Analysis of the International Monetary System, 1890–1910 http://www.stats.ox.ac.uk/~snijders/FlandreauJobst2005.pdf  ( , 2005) and The Empirics of International Currencies: Network Externalities, History and Persistence http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02219.x/abstract  ( , 2009), both by Marc Flandreau and Clemens Jobst - Network analysis of the foreign exchange system in the late 19th century ( data http://eh.net/database/international-currencies-1890-1910/ ).<|>464<|>522<|>459<|>463<|>5507<|>dataset_landing_page
5507<|>http://eh.net/database/international-currencies-1890-1910/<|>International Currencies 1890-1910<|>International Currencies 1890-1910 http://eh.net/database/international-currencies-1890-1910/  - Historical data on the international connections between 45 currencies.<|>35<|>93<|>0<|>34<|>5507<|>dataset_landing_page
5507<|>https://CRAN.R-project.org/package=igraphdata<|>igraphdata<|>igraphdata https://CRAN.R-project.org/package=igraphdata  - R data-centric package.<|>11<|>56<|>0<|>10<|>5507<|>software
5507<|>http://math.bu.edu/people/kolaczyk/datasets.html<|>Eric D. Kolaczyk’s Network Datasets<|>Eric D. Kolaczyk’s Network Datasets http://math.bu.edu/people/kolaczyk/datasets.html .<|>36<|>84<|>0<|>35<|>5507<|>dataset_landing_page
5507<|>http://networksciencebook.com/translations/en/resources/data.html<|>Network Science Book - Network Datasets<|>Network Science Book - Network Datasets http://networksciencebook.com/translations/en/resources/data.html  - Network data sets from Albert-László Barabási’s  book. Includes data on IMDB actors, arXiv scientific collaboration, network of routers, the US power grid, protein-protein interactions, cell phone users, citation networks, metabolic reactions, e-mail networks, and nd.edu Web pages.<|>40<|>105<|>0<|>39<|>5507<|>dataset_landing_page
5507<|>http://www-personal.umich.edu/~mejn/netdata/<|>Mark E.J. Newman’s Network Data<|>Mark E.J. Newman’s Network Data http://www-personal.umich.edu/~mejn/netdata/  ( example visualizations http://www-personal.umich.edu/~mejn/networks/ ).<|>32<|>76<|>0<|>31<|>5507<|>dataset_landing_page
5507<|>https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/<|>Using Metadata to Find Paul Revere<|>Using Metadata to Find Paul Revere https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/  and The Other Ride of Paul Revere: The Brokerage Role in the Making of the American Revolution http://www.sscnet.ucla.edu/polisci/faculty/chwe/ps269/han.pdf  - Network analysis applied to American revolutionaries.<|>35<|>119<|>0<|>34<|>5507<|>other
5507<|>http://vlado.fmf.uni-lj.si/pub/networks/data/<|>Pajek Datasets<|>Pajek Datasets http://vlado.fmf.uni-lj.si/pub/networks/data/ .<|>15<|>60<|>0<|>14<|>5507<|>dataset_landing_page
5507<|>https://www.data-imaginist.com/2017/introducing-tidygraph/<|>Introducing tidygraph<|>Introducing tidygraph https://www.data-imaginist.com/2017/introducing-tidygraph/<|>22<|>80<|>0<|>21<|>5507<|>software
5507<|>https://toreopsahl.com/datasets/<|>tnet Datasets<|>tnet Datasets https://toreopsahl.com/datasets/  - Weighted network data.<|>14<|>46<|>0<|>13<|>5507<|>dataset_landing_page
5507<|>http://www.pfeffer.at/data/visposter/<|>poster version<|>Methods of Social Network Visualization http://moreno.ss.uci.edu/90.pdf  ( , 2009; poster version http://www.pfeffer.at/data/visposter/ ).<|>98<|>135<|>83<|>97<|>5507<|>other
5507<|>https://github.com/schochastics/networkdata<|>networkdata<|>networkdata https://github.com/schochastics/networkdata  - Includes 979 network datasets containing 2135 networks.<|>12<|>55<|>0<|>11<|>5507<|>dataset_landing_page
5507<|>http://www.sociopatterns.org/datasets/<|>SocioPatterns Datasets<|>SocioPatterns Datasets http://www.sociopatterns.org/datasets/  - Network data obtained through the SocioPatterns http://www.sociopatterns.org/  sensing platform.<|>23<|>61<|>0<|>22<|>5507<|>dataset_landing_page
5507<|>https://sites.google.com/site/ucinetsoftware/datasets<|>UCINET Datasets<|>UCINET Datasets https://sites.google.com/site/ucinetsoftware/datasets  - Network data in UCINET format.<|>16<|>69<|>0<|>15<|>5507<|>dataset_landing_page
5507<|>http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm<|>Siena Datasets<|>Siena Datasets http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm .<|>15<|>75<|>0<|>14<|>5507<|>dataset_landing_page
5530<|>http://www.cs.ucr.edu/~eamonn/time_series_data/<|>UCR Time Series Repository<|>We have provided a DROP demo script ( ) and sample dataset from the UCR Time Series Repository http://www.cs.ucr.edu/~eamonn/time_series_data/  ( ).<|>95<|>142<|>68<|>94<|>5530<|>dataset_landing_page
5539<|>https://github.com/rbgirshick/fast-rcnn/blob/master/data/scripts/fetch_fast_rcnn_models.sh<|>selective search windows<|>Download the VOC 2007 http://host.robots.ox.ac.uk/pascal/VOC/voc2007/  dataset and Koen van de Sande's selective search windows http://koen.me/research/selectivesearch/  for VOC 2007 and the VGG-F https://gist.github.com/ksimonyan/a32c9063ec8e1118221a  model by running the first command. Optionally download the VOC 2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/  and Ross Girshick's selective search windows https://github.com/rbgirshick/fast-rcnn/blob/master/data/scripts/fetch_fast_rcnn_models.sh  by manually downloading the VOC 2012 test data tarball http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar  to  and then running the second command:<|>416<|>506<|>391<|>415<|>5539<|>software
5544<|>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html<|>documentation repository<|>Product documentation including an architecture overview, platform support, installation and usage guides can be found in the documentation repository https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html .<|>151<|>230<|>126<|>150<|>5544<|>other
5544<|>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html<|>user guide<|>The user guide https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html  provides information on the configuration and command line options available when running GPU containers with Docker.<|>15<|>96<|>4<|>14<|>5544<|>other
5544<|>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker<|>installation guide<|>For instructions on getting started with the NVIDIA Container Toolkit, refer to the installation guide https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker .<|>103<|>194<|>84<|>102<|>5544<|>other
5564<|>https://wiki.haskell.org/Algebraic_data_type<|>Haskell algebraic data types<|>The input specification format mimics that of Haskell algebraic data types https://wiki.haskell.org/Algebraic_data_type  where in addition each type constructor may be annotated with an additional  parameter. For instance:<|>75<|>119<|>46<|>74<|>5564<|>other
5587<|>https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list<|>Here<|>Standardized train https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list , valid https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list , and test https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list  sets are also available. Here https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list  is a list of all the files.<|>266<|>329<|>261<|>265<|>5587<|>dataset_direct_link
5587<|>https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list<|>valid<|>Standardized train https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list , valid https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list , and test https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list  sets are also available. Here https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list  is a list of all the files.<|>93<|>158<|>87<|>92<|>5587<|>dataset_direct_link
5587<|>https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list<|>test<|>Standardized train https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list , valid https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list , and test https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list  sets are also available. Here https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list  is a list of all the files.<|>170<|>234<|>165<|>169<|>5587<|>dataset_direct_link
5587<|>https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list<|>train<|>Standardized train https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list , valid https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list , and test https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list  sets are also available. Here https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list  is a list of all the files.<|>19<|>84<|>13<|>18<|>5587<|>dataset_direct_link
5592<|>https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json<|>block data retrieved directly from the Wikipedia block log<|>The full corpus of Wikipedia talk page conversations, based on the reconstruction described in this paper http://www.cs.cornell.edu/~cristian/index_files/wikiconv-conversation-corpus.pdf . Note that due to the large size of the data, it is split up by year. We separately provide block data retrieved directly from the Wikipedia block log https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json , for reproducing the Trajectories of Blocked Community Members http://www.cs.cornell.edu/~cristian/Recidivism_online_files/recidivism_online.pdf  paper.<|>339<|>419<|>280<|>338<|>5592<|>dataset_direct_link
5592<|>http://zissou.infosci.cornell.edu/convokit/datasets/<|>here<|>ConvoKit ships with several datasets ready for use "out-of-the-box". These datasets can be downloaded using the helper function https://github.com/CornellNLP/ConvoKit/blob/master/convokit/util.py . Alternatively you can access them directly here http://zissou.infosci.cornell.edu/convokit/datasets/ .<|>246<|>298<|>241<|>245<|>5592<|>dataset_direct_link
5592<|>https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb<|>This example script<|>In addition to the provided datasets, you may also use ConvoKit with your own custom datasets by loading them into a  object. This example script https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb  shows how to construct a Corpus from custom data.<|>146<|>235<|>126<|>145<|>5592<|>software
5592<|>https://github.com/CornellNLP/ConvoKit#datasets<|>conversational datasets<|>This toolkit contains tools to extract conversational features and analyze social phenomena in conversations, using a single unified interface https://convokit.cornell.edu/documentation/architecture.html  inspired by (and compatible with) scikit-learn. Several large conversational datasets https://github.com/CornellNLP/ConvoKit#datasets  are included together with scripts exemplifying the use of the toolkit on these datasets. The latest version is 2.5.3 https://github.com/CornellNLP/ConvoKit/releases/tag/v2.5.2  (released 16 Jan 2022); follow the project on GitHub https://github.com/CornellNLP/ConvoKit  to keep track of updates.<|>291<|>338<|>267<|>290<|>5592<|>dataset_landing_page
5619<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
5620<|>https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries<|>Frequency dictionaries<|>Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>160<|>247<|>137<|>159<|>5620<|>dataset_direct_link
5620<|>https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/<|>Very fast Data cleaning of product names, company names & street names<|>1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/ Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast-approximate-string-matching/ Very fast Data cleaning of product names, company names & street names https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/ Sub-millisecond compound aware automatic spelling correction https://seekstorm.com/blog/sub-millisecond-compound-aware-automatic.spelling-correction/ SymSpell vs. BK-tree: 100x faster fuzzy string search & spell checking https://seekstorm.com/blog/symspell-vs-bk-tree/ Fast Word Segmentation for noisy text https://seekstorm.com/blog/fast-word-segmentation-noisy-text/ The Pruning Radix Trie — a Radix trie on steroids https://seekstorm.com/blog/pruning-radix-trie/<|>300<|>390<|>229<|>299<|>5620<|>other
5620<|>https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>Frequency dictionaries<|>Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>271<|>340<|>248<|>270<|>5620<|>dataset_direct_link
5620<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google Books Ngram data<|>Google Books Ngram data http://storage.googleapis.com/books/ngrams/books/datasetsv2.html (License) https://creativecommons.org/licenses/by/3.0/  : Provides representative word frequencies<|>24<|>88<|>0<|>23<|>5496<|>dataset_landing_page
5624<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>For this example, we'll use the "a1a" dataset, acquired from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . Currently the Photon ML dataset converter supports only the LibSVM format.<|>129<|>196<|>61<|>128<|>5624<|>dataset_landing_page
5628<|>http://ltdata1.informatik.uni-hamburg.de/sensegram/<|>pre-trained models for English, German, and Russian<|>You can downlooad pre-trained models for English, German, and Russian http://ltdata1.informatik.uni-hamburg.de/sensegram/ . Note that to run examples from the QuickStart you only need files with extensions , , and . Other files are supplementary.<|>70<|>121<|>18<|>69<|>5628<|>dataset_direct_link
5643<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC MAV Dataset<|>Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets . Although it contains stereo cameras, we only use one camera. The system also works with ETH-asl cla dataset http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/ . We take EuRoC as the example.<|>27<|>102<|>9<|>26<|>1355<|>dataset_landing_page
5643<|>http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/<|>ETH-asl cla dataset<|>Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets . Although it contains stereo cameras, we only use one camera. The system also works with ETH-asl cla dataset http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/ . We take EuRoC as the example.<|>213<|>289<|>193<|>212<|>5643<|>dataset_direct_link
5679<|>https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes<|>this link<|>This example uses the famous Pima diabetes study at the UCI data repository. The following table shows the 9 variables in , followed by their descriptions from this link https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes :<|>170<|>231<|>160<|>169<|>5679<|>dataset_landing_page
5686<|>http://grupolys.org/software/UUUSA/sisa-data.zip<|>here<|>Data/Resources used for our SISA (Syntactic Iberian Polarity classification) model can be found here http://grupolys.org/software/UUUSA/sisa-data.zip<|>101<|>149<|>96<|>100<|>5686<|>dataset_direct_link
5693<|>https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0<|>link<|>| No. | LiDAR Model | Camera Model | Pattern Size | Grid Length[cm] | Distance Range[m] | Data source | Author | |:---:|:----------------:|:------------:|:------------:|:---------------:|:-----------------:|:--------------------------------------------------------------------------------------:|:---------------------------------:| | 1 | Velodyne  HDL-32e | Ladybug3 (panoramic) | 8 6 | 7.5 | 1.2 ~ 2.6 | link https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0  | mfxox https://github.com/mfxox  |<|>411<|>490<|>406<|>410<|>5693<|>dataset_landing_page
5693<|>https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0<|>here<|>The sample data and processing results of detected corners can be downloaded from here https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0  (181M) for panoramic image and here https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0  (29M) for perspective image.  These data are acquired with the chessboard file readme_files/chessboard_A0_0.75_6_8.pdf  which contains 6*8 patterns and the length of one grid is 7.5cm if it is printed by A0 size.<|>203<|>282<|>198<|>202<|>5693<|>dataset_landing_page
5693<|>https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0<|>here<|>The sample data and processing results of detected corners can be downloaded from here https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0  (181M) for panoramic image and here https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0  (29M) for perspective image.  These data are acquired with the chessboard file readme_files/chessboard_A0_0.75_6_8.pdf  which contains 6*8 patterns and the length of one grid is 7.5cm if it is printed by A0 size.<|>87<|>165<|>82<|>86<|>5693<|>dataset_landing_page
5714<|>http://lxcenter.di.fc.ul.pt/datasets/models/<|>All models<|>All models http://lxcenter.di.fc.ul.pt/datasets/models/<|>11<|>55<|>0<|>10<|>5714<|>dataset_direct_link
5714<|>http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/<|>LX-DSemVectors 2.2b<|>LX-DSemVectors 2.2b http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/<|>20<|>69<|>0<|>19<|>5714<|>dataset_direct_link
5717<|>http://www.cs.cornell.edu/people/pabo/movie-review-data/<|>Cornell Movie Review<|>Cornell Movie Review http://www.cs.cornell.edu/people/pabo/movie-review-data/  -- MovieReview.ipynb emoint/examples/MovieReview.ipynb<|>21<|>77<|>0<|>20<|>5717<|>dataset_landing_page
5734<|>http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>This code implements the state-of-the-art Knowledge Graph Embedding algorithms http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf  such as RESCAL http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf , TransE http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data , COMPLEX http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12484/11828 , Computational Graphs are implemented using PyTorch https://pytorch.org/ .<|>226<|>316<|>219<|>225<|>5127<|>other
5753<|>https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py<|>KG generation script<|>DBpedia 2016-10 knowledge graph represented by three files: nodes.txt, relations.txt, and abbrev_cup_dbpedia_filtered.nt, which have been derived from raw data on the DBpedia downloads page: DBpedia 2016-10 http://wiki.dbpedia.org/downloads-2016-10 . Additionally, it contains a directory named  which contains datastructures in binary format as required by the code. If you are interested in applying methods in this repository on your own knowledge graph, you may use the following script to generate the required graph files (.npy): KG generation script https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py<|>557<|>650<|>536<|>556<|>5753<|>dataset_landing_page
5772<|>http://graphics.stanford.edu/data/3Dscanrep/<|>Stanford dragon_stand dataset<|>This Python script https://github.com/bing-jian/gmmreg/blob/master/expts/dragon_expts.py  can be used to reproduce the results described in Section 6.1 of Jian&Vemuri PAMI'11 paper https://github.com/bing-jian/gmmreg/blob/master/gmmreg_PAMI_preprint.pdf  using the Stanford dragon_stand dataset http://graphics.stanford.edu/data/3Dscanrep/ .<|>295<|>339<|>265<|>294<|>4833<|>dataset_landing_page
5772<|>https://github.com/bing-jian/gmmreg/tree/master/data<|>this folder<|>The executable (named "gmmreg_demo") takes a configuration file (in INI format) and a tag string from command line. For more on usage, please check this file https://github.com/bing-jian/gmmreg/blob/master/C%2B%2B/gmmreg_api.cpp . For examples of config INI file, please check this folder https://github.com/bing-jian/gmmreg/tree/master/data .<|>289<|>341<|>277<|>288<|>5772<|>dataset_landing_page
5772<|>http://qianyi.info/scenedata.html<|>Stanford lounge dataset<|>This Python script https://github.com/bing-jian/gmmreg/blob/master/expts/lounge_expts.py  can be used to register depth frames in the Stanford lounge dataset http://qianyi.info/scenedata.html . Please see section below for results.<|>158<|>191<|>134<|>157<|>5772<|>dataset_landing_page
5780<|>https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>12<|>84<|>0<|>11<|>5780<|>software
5808<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB51<|>We experimented on three mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and Hollywood2 http://www.di.ens.fr/~laptev/actions/hollywood2/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.<|>118<|>194<|>111<|>117<|>2024<|>dataset_landing_page
5808<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF-101<|>We experimented on three mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and Hollywood2 http://www.di.ens.fr/~laptev/actions/hollywood2/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.<|>73<|>108<|>65<|>72<|>2024<|>dataset_landing_page
5840<|>https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/<|>See the results.<|>We organized a data challenge about recommender systems with Kyoto University. See the results. https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/<|>96<|>169<|>79<|>95<|>5840<|>other
5855<|>https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset<|>exp-src/data/sketch-dataset<|>For instructions to get annotated sketch dataset, navigate to exp-src/data/sketch-dataset https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset . Pose dataset is present in https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt . Find instruction regarding pose dataset here https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists .<|>90<|>170<|>62<|>89<|>5855<|>dataset_landing_page
5855<|>https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt<|><|>For instructions to get annotated sketch dataset, navigate to exp-src/data/sketch-dataset https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset . Pose dataset is present in https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt . Find instruction regarding pose dataset here https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists .<|>200<|>290<|>0<|>0<|>5855<|>dataset_direct_link
5855<|>https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists<|>here<|>For instructions to get annotated sketch dataset, navigate to exp-src/data/sketch-dataset https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset . Pose dataset is present in https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt . Find instruction regarding pose dataset here https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists .<|>338<|>409<|>333<|>337<|>5855<|>dataset_landing_page
5879<|>https://anndata.readthedocs.io/en/latest/<|>AnnData<|>scvi-tools https://scvi-tools.org/  (single-cell variational inference tools) is a package for probabilistic modeling and analysis of single-cell omics data, built on top of PyTorch https://pytorch.org  and AnnData https://anndata.readthedocs.io/en/latest/ .<|>215<|>256<|>207<|>214<|>5879<|>software
5881<|>https://github.com/gmatht/leviathan/blob/master/samples/benchmark_data_DOC.txt<|>Documentation<|>This modification allows Leviathan to divide work between multiple independant jobs that can be run in parallel. This was be presented http://staffhome.ecm.uwa.edu.au/~00061811/GandALF2017a  at GandALF 2017 http://eptcs.web.cse.unsw.edu.au/paper.cgi?GANDALF2017:10.pdf . The raw benchmark data (705MB) http://staffhome.ecm.uwa.edu.au/~00061811/parallel_benchdata.tar.gz , used in this paper is available online, including the comparison with PolSAT (26MB) http://staffhome.ecm.uwa.edu.au/~00061811/polsat_benchdata.tar.gz . There is some Documentation https://github.com/gmatht/leviathan/blob/master/samples/benchmark_data_DOC.txt  for the benchmark data.<|>552<|>630<|>538<|>551<|>5881<|>dataset_landing_page
5893<|>https://github.com/openvenues/libpostal/tree/master/scripts/geodata<|>geodata<|>Libpostal is a bit different because it's trained on open data that's available to everyone, so we've released the entire training pipeline (the geodata https://github.com/openvenues/libpostal/tree/master/scripts/geodata  package in this repo), as well as the resulting training data itself on the Internet Archive. It's over 100GB unzipped.<|>153<|>220<|>145<|>152<|>5893<|>software
5893<|>https://github.com/openvenues/libpostal/tree/master/scripts/geodata<|>geodata<|>The geodata https://github.com/openvenues/libpostal/tree/master/scripts/geodata  Python package in the libpostal repo contains the pipeline for preprocessing the various geo data sets and building training data for the C models to use. This package shouldn't be needed for most users, but for those interested in generating new types of addresses or improving libpostal's training data, this is where to look.<|>12<|>79<|>4<|>11<|>5893<|>software
5893<|>https://github.com/openvenues/libpostal/blob/master/scripts/geodata/addresses/components.py<|>normalizations<|>: Conditional Random Field http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/  which parses "123 Main Street New York New York" into {"house_number": 123, "road": "Main Street", "city": "New York", "state": "New York"}. The parser works for a wide variety of countries and languages, not just US/English. The model is trained on over 1 billion addresses and address-like strings, using the templates in the OpenCage address formatting repo https://github.com/OpenCageData/address-formatting  to construct formatted, tagged traning examples for every inhabited country in the world. Many types of normalizations https://github.com/openvenues/libpostal/blob/master/scripts/geodata/addresses/components.py  are performed to make the training data resemble real messy geocoder input as closely as possible.<|>635<|>726<|>620<|>634<|>5893<|>software
5898<|>https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0<|>Dropbox<|>We provide demo attack data and classifier on Dropbox https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0  and 百度网盘 https://pan.baidu.com/s/1gfpcB5p  (密码: yzt4). Please download and put the unzipped files in . You may also use your own data for test.<|>54<|>124<|>46<|>53<|>5898<|>dataset_landing_page
5899<|>https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT<|>Dataverse<|>The data is hosted on Dataverse https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT  and Google Drive https://drive.google.com/drive/folders/1wZwDIR18IHPPTiH1C0dyBbGPR-3MktI7?usp=sharing .<|>32<|>111<|>22<|>31<|>5899<|>dataset_landing_page
5916<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI odometry benchmark<|>To use the KITTI odometry benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to train DPC-Net, you can use the scripts  and  as starting points. If you use our framework, you'll need to save your estimator's poses in a  object.<|>36<|>90<|>11<|>35<|>1355<|>other
5954<|>https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data<|>mismatched<|>Then, manually download download MultiNLI 0.9 matched https://www.kaggle.com/c/multinli-matched-open-evaluation/data  and mismatched https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data  test set under data/multinli_0.9 folder<|>133<|>198<|>122<|>132<|>5954<|>dataset_direct_link
5954<|>https://www.kaggle.com/c/multinli-matched-open-evaluation/data<|>matched<|>Then, manually download download MultiNLI 0.9 matched https://www.kaggle.com/c/multinli-matched-open-evaluation/data  and mismatched https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data  test set under data/multinli_0.9 folder<|>54<|>116<|>46<|>53<|>5954<|>dataset_direct_link
5956<|>https://www.audiocontentanalysis.org/data-sets/<|>MIR datasets<|>MIR datasets https://www.audiocontentanalysis.org/data-sets/ : An awesome list of MIR datasets<|>13<|>60<|>0<|>12<|>5956<|>dataset_landing_page
5956<|>http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset<|>MagnaTagATune<|>MagnaTagATune http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset : (29s, 188 tags, 25,880 mp3) for tagging and triplet similarity<|>14<|>71<|>0<|>13<|>5956<|>dataset_landing_page
5988<|>http://exploredata.net<|>Maximal Information Coefficient (MIC)<|>Our best model has an accuracy greater than 99.0% with mean squared error of ~6.00 watts. For correlation analysis, we applied a metric called Maximal Information Coefficient (MIC) http://exploredata.net  that has the equitability and generality properties.<|>181<|>203<|>143<|>180<|>5988<|>dataset_landing_page
6028<|>http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag<|>slider_depth.bag<|>Download a squence of the dataset, such as slider_depth.bag http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag<|>60<|>113<|>43<|>59<|>6028<|>dataset_direct_link
6028<|>http://rpg.ifi.uzh.ch/davis_data.html<|>The Event Camera Dataset and Simulator<|>, such as those of The Event Camera Dataset and Simulator http://rpg.ifi.uzh.ch/davis_data.html . :<|>58<|>95<|>19<|>57<|>2468<|>dataset_landing_page
6029<|>https://github.com/remega/LEDOV-eye-tracking-database<|>LEDOV<|>As introduced in our paper, our model is trained by our newly-established eye-tracking database, LEDOV https://github.com/remega/LEDOV-eye-tracking-database , which is also available at Dropbox https://www.dropbox.com/s/pc8symd9i3cky1q/LEDOV.zip?dl=0  and BaiduYun http://pan.baidu.com/s/1pLmfjCZ<|>103<|>156<|>97<|>102<|>6029<|>dataset_landing_page
6046<|>https://coveralls.io/r/Wikidata-lib/PropertySuggester?branch=master<|><|> https://travis-ci.org/Wikidata-lib/PropertySuggester https://coveralls.io/r/Wikidata-lib/PropertySuggester?branch=master<|>54<|>121<|>0<|>0<|>6046<|>other
6046<|>https://github.com/Wikidata-lib/PropertySuggester-Python<|>PropertySuggester-Python<|>This extension adds a new table "wbs_propertypairs" that contains the information that is needed to generate suggestions. You can use PropertySuggester-Python https://github.com/Wikidata-lib/PropertySuggester-Python  to generate this data from a wikidata dump.<|>159<|>215<|>134<|>158<|>6046<|>software
6065<|>https://sites.google.com/site/iitaffdataset/<|>IIT-AFF dataset<|>Download pretrained weights ( Google Drive https://drive.google.com/file/d/0Bx3H_TbKFPCjNlMtSGJlQ0dxVzQ/view?usp=sharing , One Drive https://studenthcmusedu-my.sharepoint.com/:u:/g/personal/nqanh_mso_hcmus_edu_vn/ETD6q64-L1lCgtNEryA42NwBNM9vNoyE8QyxAYzgt8NqnA?e=uRCxPg ). This weight is trained on the training set of the IIT-AFF dataset https://sites.google.com/site/iitaffdataset/ :<|>338<|>382<|>322<|>337<|>6065<|>dataset_landing_page
6065<|>https://sites.google.com/site/iitaffdataset/<|>IIT-AFF dataset<|>If you use IIT-AFF dataset https://sites.google.com/site/iitaffdataset/ , please consider citing:<|>27<|>71<|>11<|>26<|>6065<|>dataset_landing_page
6065<|>https://sites.google.com/site/iitaffdataset/<|>IIT-AFF dataset<|>We train AffordanceNet on IIT-AFF dataset https://sites.google.com/site/iitaffdataset/<|>42<|>86<|>26<|>41<|>6065<|>dataset_landing_page
6073<|>http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html<|>NYU Depth V2<|>Download the preprocessed NYU Depth V2 http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html  and/or KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  datasets in HDF5 formats and place them under the  folder. The downloading process might take an hour or so. The NYU dataset requires 32G of storage space, and KITTI requires 81G.<|>39<|>94<|>26<|>38<|>6073<|>dataset_landing_page
6073<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI<|>Download the preprocessed NYU Depth V2 http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html  and/or KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  datasets in HDF5 formats and place them under the  folder. The downloading process might take an hour or so. The NYU dataset requires 32G of storage space, and KITTI requires 81G.<|>109<|>163<|>103<|>108<|>1355<|>other
6075<|>https://github.com/tesseradata/install-vagrant<|>Vagrant<|>Probably the easiest way to build RHIPE is to provision a Vagrant https://github.com/tesseradata/install-vagrant  machine that has all the prerequisites configured. Another option is to set up a local pseudo-distributed Hadoop cluster, for example see here https://github.com/hafen/RHIPE/blob/master/cdh5-on-mac.md .<|>66<|>112<|>58<|>65<|>6075<|>software
6079<|>https://www.caida.org/data/internet-topology-data-kit/<|>Macroscopic Internet Topology Data Kit<|>The Macroscopic Internet Topology Data Kit https://www.caida.org/data/internet-topology-data-kit/  from Caida including measured real world internet topologies<|>43<|>97<|>4<|>42<|>6079<|>dataset_landing_page
6091<|>https://github.com/tesseract-ocr/tessdata<|>tessdata repository<|>If you want to find a language data set to run Tesseract, then look at our tessdata repository https://github.com/tesseract-ocr/tessdata  instead.<|>95<|>136<|>75<|>94<|>3134<|>dataset_landing_page
6113<|>https://datastax-oss.atlassian.net/projects/SPARKC/issues<|>SPARKC JIRA<|>Create a SPARKC JIRA https://datastax-oss.atlassian.net/projects/SPARKC/issues<|>21<|>78<|>9<|>20<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html<|>26<|>142<|>0<|>25<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package<|>26<|>111<|>0<|>25<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html<|>26<|>142<|>0<|>25<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html<|>26<|>142<|>0<|>25<|>6113<|>other
6113<|>https://github.com/datastax/spark-cassandra-connector/tree/b3.0<|>b3.0<|>Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).<|>315<|>378<|>310<|>314<|>6113<|>software
6113<|>http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/<|>26<|>118<|>0<|>25<|>6113<|>other
6113<|>http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/<|>Embedded-Cassandra<|>Embedded-Cassandra http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/<|>19<|>120<|>0<|>18<|>6113<|>other
6113<|>https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster<|><|> https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster<|>1<|>84<|>0<|>0<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector<|>Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html<|>26<|>142<|>0<|>25<|>6113<|>other
6113<|>https://github.com/datastax/spark-cassandra-connector/tree/master<|>master<|>Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).<|>73<|>138<|>66<|>72<|>6113<|>software
6113<|>https://github.com/datastax/spark-cassandra-connector/tree/b3.1<|>b3.1<|>Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).<|>235<|>298<|>230<|>234<|>6113<|>software
6113<|>https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.3.0/jar<|>3.3.0<|>| What | Where | | ---------- |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Community | Chat with us at Datastax and Cassandra Q&A https://community.datastax.com/index.html  | | Scala Docs | Most Recent Release (3.3.0): Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html , Spark-Cassandra-Connector-Driver https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/driver/com/datastax/spark/connector/index.html  | | Latest Production Release | 3.3.0 https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.3.0/jar  |<|>837<|>930<|>831<|>836<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/driver/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector-Driver<|>| What | Where | | ---------- |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Community | Chat with us at Datastax and Cassandra Q&A https://community.datastax.com/index.html  | | Scala Docs | Most Recent Release (3.3.0): Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html , Spark-Cassandra-Connector-Driver https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/driver/com/datastax/spark/connector/index.html  | | Latest Production Release | 3.3.0 https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.3.0/jar  |<|>684<|>797<|>651<|>683<|>6113<|>other
6113<|>https://github.com/datastax/spark-cassandra-connector/tree/b2.5<|>b2.5<|>Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).<|>398<|>461<|>393<|>397<|>6113<|>software
6113<|>https://github.com/datastax/spark-cassandra-connector/tree/b3.2<|>b3.2<|>Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).<|>155<|>218<|>150<|>154<|>6113<|>software
6113<|>https://datastax-oss.atlassian.net/browse/SPARKC/<|>JIRA<|>New issues may be reported using JIRA https://datastax-oss.atlassian.net/browse/SPARKC/ . Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.<|>38<|>87<|>33<|>37<|>6113<|>other
6113<|>https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html<|>Spark-Cassandra-Connector<|>| What | Where | | ---------- |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Community | Chat with us at Datastax and Cassandra Q&A https://community.datastax.com/index.html  | | Scala Docs | Most Recent Release (3.3.0): Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html , Spark-Cassandra-Connector-Driver https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/driver/com/datastax/spark/connector/index.html  | | Latest Production Release | 3.3.0 https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.3.0/jar  |<|>532<|>648<|>506<|>531<|>6113<|>other
6113<|>https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user<|>user mailing list<|>Questions and requests for help may be submitted to the user mailing list https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user .<|>74<|>155<|>56<|>73<|>6113<|>other
6122<|>https://github.com/rasvaan/accurator/wiki/2.-Collection-data<|>Collection data<|>Collection data https://github.com/rasvaan/accurator/wiki/2.-Collection-data<|>16<|>76<|>0<|>15<|>6122<|>software
6123<|>https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data<|>Collection data<|>Collection data https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data<|>16<|>100<|>0<|>15<|>6123<|>dataset_landing_page
6129<|>https://github.com/zephyr-data-specs/GMNS<|>General Modeling Network Specification<|>General Modeling Network Specification https://github.com/zephyr-data-specs/GMNS  - GMNS defines a common human and machine readable format for sharing routable road network files. It is designed to be used in multi-modal static and dynamic transportation planning and operations models.<|>39<|>80<|>0<|>38<|>6129<|>other
6129<|>https://github.com/osplanning-data-standards/GTFS-PLUS<|>GTFS-PLUS<|>GTFS-PLUS https://github.com/osplanning-data-standards/GTFS-PLUS  - GTFS-based data transit network data standard suitable for dynamic transit modeling<|>10<|>64<|>0<|>9<|>6129<|>other
6143<|>https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data<|>M. Rudolph, F. Ruiz, S. Athey, D. Blei,<|>M. Rudolph, F. Ruiz, S. Athey, D. Blei, https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data<|>40<|>118<|>0<|>39<|>6143<|>other
6165<|>https://www.openhub.net/p/Wikidata-Toolkit<|><|> https://codecov.io/gh/Wikidata/Wikidata-Toolkit http://search.maven.org/#search|ga|1|g%3A%22org.wikidata.wdtk%22 https://www.openhub.net/p/Wikidata-Toolkit<|>114<|>156<|>0<|>0<|>6165<|>software
6165<|>https://www.mediawiki.org/wiki/Wikidata_Toolkit<|>Wikidata Toolkit homepage<|>Wikidata Toolkit homepage https://www.mediawiki.org/wiki/Wikidata_Toolkit : project homepage with basic user documentation, including guidelines on how to setup your Java IDE for using Maven and git.<|>26<|>73<|>0<|>25<|>6165<|>other
6165<|>https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit<|>Wikibase Toolkit Individual Engagement Grant<|>The development of Wikidata Toolkit has been partially funded by the Wikimedia Foundation under the Wikibase Toolkit Individual Engagement Grant https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit , and by the German Research Foundation (DFG) under Emmy Noether grant KR 4381/1-1 "DIAMOND" https://ddll.inf.tu-dresden.de/web/DIAMOND/en .<|>145<|>204<|>100<|>144<|>6165<|>other
6165<|>https://codecov.io/gh/Wikidata/Wikidata-Toolkit<|><|> https://codecov.io/gh/Wikidata/Wikidata-Toolkit http://search.maven.org/#search|ga|1|g%3A%22org.wikidata.wdtk%22 https://www.openhub.net/p/Wikidata-Toolkit<|>1<|>48<|>0<|>0<|>6165<|>other
6165<|>https://github.com/Wikidata/Wikidata-Toolkit-Examples<|>Wikidata Toolkit examples<|>Wikidata Toolkit examples https://github.com/Wikidata/Wikidata-Toolkit-Examples : stand-alone Java project that shows how to use Wikidata Toolkit as a library for your own code.<|>26<|>79<|>0<|>25<|>6165<|>software
6165<|>http://search.maven.org/#search|ga|1|g%3A%22org.wikidata.wdtk%22<|><|> https://codecov.io/gh/Wikidata/Wikidata-Toolkit http://search.maven.org/#search|ga|1|g%3A%22org.wikidata.wdtk%22 https://www.openhub.net/p/Wikidata-Toolkit<|>49<|>113<|>0<|>0<|>6165<|>other
6165<|>http://wikidata.github.io/Wikidata-Toolkit/<|>Wikidata Toolkit Javadocs<|>Wikidata Toolkit Javadocs http://wikidata.github.io/Wikidata-Toolkit/ : API documentation<|>26<|>69<|>0<|>25<|>6165<|>other
6165<|>https://github.com/Wikidata/Wikidata-Toolkit/graphs/contributors<|>other contributors<|>Authors: Markus Kroetzsch http://korrekt.org , Julian Mendez https://julianmendez.github.io/ , Fredo Erxleben https://github.com/fer-rum , Michael Guenther https://github.com/guenthermi , Markus Damm https://github.com/mardam , Antonin Delpeuch http://antonin.delpeuch.eu/ , Thomas Pellissier Tanon https://thomas.pellissier-tanon.fr/  and other contributors https://github.com/Wikidata/Wikidata-Toolkit/graphs/contributors<|>359<|>423<|>340<|>358<|>6165<|>software
6185<|>https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design<|>here<|>Available here https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design .<|>15<|>96<|>10<|>14<|>6185<|>software
6186<|>https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets<|>here<|>For uploading your datasets, you can follow the instructions explained here https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets .<|>76<|>159<|>71<|>75<|>6186<|>other
6212<|>http://cocodataset.org/#download<|>MSCOCO<|>MSCOCO http://cocodataset.org/#download<|>7<|>39<|>0<|>6<|>6212<|>dataset_landing_page
6230<|>https://www.webmproject.org/docs/container/#cueing-data<|>cueing data<|>Can parse cueing data https://www.webmproject.org/docs/container/#cueing-data  elements for DASH's SegmentBase@indexRange and SegmentTemplate@index<|>22<|>77<|>10<|>21<|>6230<|>other
6264<|>http://www.nltk.org/data.html<|>the data installed<|>The code in this repository is compatible with Python 2 and Python 3. Its only other external dependency is NLTK http://www.nltk.org/install.html , with the data installed http://www.nltk.org/data.html  so that WordNet is available.<|>172<|>201<|>153<|>171<|>1258<|>software
6287<|>https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt<|>categories<|>Our annotations https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt  and categories https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt  are available in the  directory of this repository.<|>126<|>218<|>115<|>125<|>6287<|>dataset_direct_link
6287<|>https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt<|>annotations<|>Our annotations https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt  and categories https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt  are available in the  directory of this repository.<|>16<|>109<|>4<|>15<|>6287<|>dataset_direct_link
6315<|>https://github.com/haozhenWu/lightchem/tree/master/datasets<|>Datasets<|>Datasets https://github.com/haozhenWu/lightchem/tree/master/datasets<|>9<|>68<|>0<|>8<|>6315<|>dataset_landing_page
6315<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  version = 0.18.1<|>7<|>32<|>0<|>6<|>67<|>software
6322<|>https://github.com/vs-uulm/vnc2017-CACC-data<|>here<|>To save some time, we've also published the data set generated by this code in a separate (very large!) repository, which can be found here https://github.com/vs-uulm/vnc2017-CACC-data .<|>140<|>184<|>135<|>139<|>6322<|>dataset_landing_page
6325<|>https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset<|>dataset<|>We use a subset of ImageNet validation set containing 1000 images, most of which are correctly classified by those models. Our dataset can be downloaded at http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip . You can alternatively use the NIPS 2017 competition official dataset https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset .<|>351<|>451<|>343<|>350<|>6325<|>software
6325<|>http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip<|>http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip<|>We use a subset of ImageNet validation set containing 1000 images, most of which are correctly classified by those models. Our dataset can be downloaded at http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip . You can alternatively use the NIPS 2017 competition official dataset https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset .<|>218<|>279<|>156<|>217<|>6325<|>dataset_direct_link
6336<|>https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>12<|>84<|>0<|>11<|>5780<|>software
6338<|>http://www.mohamedaly.info/datasets/caltech-lanes<|>Caltech Lanes Dataset<|>Download Caltech Lanes Dataset http://www.mohamedaly.info/datasets/caltech-lanes .<|>31<|>80<|>9<|>30<|>6338<|>dataset_landing_page
6346<|>http://groups.csail.mit.edu/vision/datasets/ADE20K/<|>ADE20k<|>Download the ADE20k http://groups.csail.mit.edu/vision/datasets/ADE20K/  dataset and put it in .<|>20<|>71<|>13<|>19<|>6346<|>dataset_landing_page
6347<|>https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html<|>Speech Commands Dataset<|>Honk is a PyTorch reimplementation of Google's TensorFlow convolutional neural networks for keyword spotting, which accompanies the recent release of their Speech Commands Dataset https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html . For more details, please consult our writeup:<|>180<|>258<|>156<|>179<|>6347<|>other
6348<|>https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb<|>RAE_on_Skoda_dataset.ipynb<|>Codes and files are available under "skoda" folder: RAE_on_Skoda_dataset.ipynb https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb<|>79<|>178<|>52<|>78<|>6348<|>dataset_landing_page
6350<|>https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split<|>MSD_split<|>Here are examples of how to run the code. (To run 1. and 2., you need MSD audio files and its related metadata from msd-artist-split https://github.com/jiyoungpark527/msd-artist-split , MSD_split https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split )<|>196<|>267<|>186<|>195<|>6350<|>dataset_landing_page
6356<|>https://www.wikidata.org<|>Wikidata<|>The aim of the SLING project is to learn to read and understand Wikipedia articles in many languages for the purpose of knowledge base completion, e.g. adding facts mentioned in Wikipedia (and other sources) to the Wikidata https://www.wikidata.org  knowledge base. We use frame semantics doc/guide/frames.md  as a common representation for both knowledge representation and document annotation. The SLING parser can be trained to produce frame semantic representations of text directly without any explicit intervening linguistic representation.<|>224<|>248<|>215<|>223<|>6356<|>other
6362<|>http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration<|><|> http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration<|>1<|>85<|>0<|>0<|>6362<|>dataset_landing_page
6365<|>http://www.cvlibs.net/datasets/kitti/<|>kitti dataset<|>In this example we create a cblox map using the lidar data and "ground-truth" pose estimates from the kitti dataset http://www.cvlibs.net/datasets/kitti/ . This simple example demonstrates the  and  of submaps using c-blox - because we use drift-free pose estimates (rather than a SLAM system), no submap correction is required/used.<|>116<|>153<|>102<|>115<|>2791<|>software
6365<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>kitti raw dataset<|>To run the example download a kitti raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . To produce the map above, we ran the "2011_09_30_drive_0018" dataset under the catagory "residential". Convert the data to a rosbag using kitti_to_rosbag https://github.com/ethz-asl/kitti_to_rosbag .<|>48<|>97<|>30<|>47<|>2741<|>software
6367<|>https://www.microsoft.com/en-us/research/project/figureqa-dataset/<|>The dataset is available for download here<|>Code to generate the FigureQA dataset. The dataset is available for download here https://www.microsoft.com/en-us/research/project/figureqa-dataset/ .<|>82<|>148<|>39<|>81<|>6367<|>dataset_landing_page
6378<|>https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat<|>/Data/TCK_data.mat<|>We can see that the matrix has a block structure: the first larger block on the diagonal are the similarities between MTS of class 1, the second smaller block is relative to the elements of class 2. Results are saved in /Data/TCK_data.mat https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat  and they are used in the next section to train the dkAE.<|>239<|>304<|>220<|>238<|>6378<|>dataset_landing_page
6391<|>http://cocodataset.org/#home<|>COCO<|>Multi-label Image Annotation(COCO) Please download COCO http://cocodataset.org/#home  dataset first. Specifically, 2014 train, val and test splits are used in the paper. To prepare a well trained multi-label model for feedback-prop, you can either train a new model by running train.py https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel/train.py  in coco_multilabel https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel  folder or directly download the pretrained model http://www.cs.virginia.edu/~tw8cb/files/model_best.pth.tar . We recommend to train a multi-label model by first fixing all CNN layers for a few epochs and then finetuning the model end-to-end. To apply feedback-prop, please go through COCO-Feedback-prop.ipynb https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel/COCO-Feedback-prop.ipynb .<|>56<|>84<|>51<|>55<|>6391<|>dataset_landing_page
6401<|>https://angular.io/guide/displaying-data<|>Components and Templates<|>Components and Templates https://angular.io/guide/displaying-data<|>25<|>65<|>0<|>24<|>6401<|>other
6409<|>https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip<|><|>Download the surface forms file which is available at https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip .<|>54<|>130<|>0<|>0<|>6409<|>dataset_direct_link
6414<|>https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m<|>dataGeneration.m<|>dataGeneration.m https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m : Generating data for numerical simulations<|>17<|>80<|>0<|>16<|>6414<|>software
6424<|>https://pandas.pydata.org/<|><|>The notebooks use https://pandas.pydata.org/  for data analysis. We used v0.20.3 but anything above and some below should do as well. For plotting,  and  were used. Finally, you need  in version  or above.<|>18<|>44<|>0<|>0<|>3372<|>software
6433<|>http://vowl.visualdataweb.org/<|>VOWL<|>WebVOWL http://visualdataweb.de/webvowl/#  for visualizing OWL ontologies online. It is very good. The VOWL http://vowl.visualdataweb.org/  project includes online querying an endpoint for visualizing the structure of data and other tools.<|>108<|>138<|>103<|>107<|>6433<|>other
6433<|>https://www.coursera.org/specializations/genomic-data-science<|>MOOC specialization<|>What are the most useful analysis and visualizations for biological and medical data? MOOC https://www.coursera.org/specializations/bioinformatics , MOOC specialization https://www.coursera.org/learn/bioinformatics-pku/ . This other MOOC specialization https://www.coursera.org/specializations/genomic-data-science  has a Python for BioInformatics course. A MOOC for big data and bioinformatics https://www.coursera.org/learn/data-genes-medicine .<|>253<|>314<|>233<|>252<|>6433<|>other
6433<|>https://www.coursera.org/learn/data-genes-medicine<|>MOOC for big data and bioinformatics<|>What are the most useful analysis and visualizations for biological and medical data? MOOC https://www.coursera.org/specializations/bioinformatics , MOOC specialization https://www.coursera.org/learn/bioinformatics-pku/ . This other MOOC specialization https://www.coursera.org/specializations/genomic-data-science  has a Python for BioInformatics course. A MOOC for big data and bioinformatics https://www.coursera.org/learn/data-genes-medicine .<|>395<|>445<|>358<|>394<|>6433<|>other
6433<|>http://vowl.visualdataweb.org/<|>VOWL<|>(See VOWL http://vowl.visualdataweb.org/  for good online tools for visualizing OWL and data from endpoints as node-edge diagrams, but no textual visualization.)<|>10<|>40<|>5<|>9<|>6433<|>other
6444<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for logging to csv<|>7<|>32<|>0<|>6<|>67<|>software
6444<|>http://bokeh.pydata.org<|>bokeh<|>bokeh http://bokeh.pydata.org  for training visualization<|>6<|>29<|>0<|>5<|>4770<|>software
6447<|>https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/<|>Download<|>CIFAR100: Download https://www.cs.toronto.edu/~kriz/cifar.html  CIFAR10: Download https://www.cs.toronto.edu/~kriz/cifar.html  MNIST: Download http://yann.lecun.com/exdb/mnist/  LCSTS: Download http://icrc.hitsz.edu.cn/Article/show/139.html  IWSLT2015: Download https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/<|>262<|>319<|>253<|>261<|>6447<|>dataset_direct_link
6450<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
6450<|>http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset<|>here<|>Download audio data and tag annotations from here http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset . Then you should see 3  files and 1  file:<|>50<|>107<|>45<|>49<|>5956<|>dataset_landing_page
6458<|>https://github.com/ramey/datamicroarray/wiki/Gravier-%282010%29<|>Gravier (2010)<|>Gravier (2010) https://github.com/ramey/datamicroarray/wiki/Gravier-%282010%29<|>15<|>78<|>0<|>14<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Chowdary-%282006%29<|>Chowdary (2006)<|>Chowdary (2006) https://github.com/ramey/datamicroarray/wiki/Chowdary-%282006%29<|>16<|>80<|>0<|>15<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Su-%282002%29<|>Su (2002)<|>Su (2002) https://github.com/ramey/datamicroarray/wiki/Su-%282002%29<|>10<|>68<|>0<|>9<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Subramanian-%282005%29<|>Subramanian (2005)<|>Subramanian (2005) https://github.com/ramey/datamicroarray/wiki/Subramanian-%282005%29<|>19<|>86<|>0<|>18<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Sorlie-%282001%29<|>Sorlie (2001)<|>Sorlie (2001) https://github.com/ramey/datamicroarray/wiki/Sorlie-%282001%29<|>14<|>76<|>0<|>13<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/West-%282001%29<|>West (2001)<|>West (2001) https://github.com/ramey/datamicroarray/wiki/West-%282001%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29<|>Golub (1999)<|>Golub (1999) https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29<|>13<|>74<|>0<|>12<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Singh-%282002%29<|>Singh (2002)<|>Singh (2002) https://github.com/ramey/datamicroarray/wiki/Singh-%282002%29<|>13<|>74<|>0<|>12<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Nakayama-%282007%29<|>Nakayama (2007)<|>Nakayama (2007) https://github.com/ramey/datamicroarray/wiki/Nakayama-%282007%29<|>16<|>80<|>0<|>15<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Khan-%282001%29<|>Khan (2001)<|>Khan (2001) https://github.com/ramey/datamicroarray/wiki/Khan-%282001%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Tian-%282003%29<|>Tian (2003)<|>Tian (2003) https://github.com/ramey/datamicroarray/wiki/Tian-%282003%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29<|>Alon et al. (1999) Colon Cancer data set<|>Once you have installed and loaded the  package, you can load a data set with the  command. For example, to load the well-known Alon et al. (1999) Colon Cancer data set https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29 , type the following at the R console:<|>169<|>229<|>128<|>168<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29<|>Alon (1999)<|>Alon (1999) https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Burczynski-%282006%29<|>Burczynski (2006)<|>Burczynski (2006) https://github.com/ramey/datamicroarray/wiki/Burczynski-%282006%29<|>18<|>84<|>0<|>17<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Gordon-%282002%29<|>Gordon (2002)<|>Gordon (2002) https://github.com/ramey/datamicroarray/wiki/Gordon-%282002%29<|>14<|>76<|>0<|>13<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Shipp-%282002%29<|>Shipp (2002)<|>Shipp (2002) https://github.com/ramey/datamicroarray/wiki/Shipp-%282002%29<|>13<|>74<|>0<|>12<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Yeoh-%282002%29<|>Yeoh (2002)<|>Yeoh (2002) https://github.com/ramey/datamicroarray/wiki/Yeoh-%282002%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Chin-%282006%29<|>Chin (2006)<|>Chin (2006) https://github.com/ramey/datamicroarray/wiki/Chin-%282006%29<|>12<|>72<|>0<|>11<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29<|>Alon et al. (1999) Colon Cancer data set<|>Here is a summary for the Alon et al. (1999) Colon Cancer data set https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29 .<|>67<|>127<|>26<|>66<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Sun-%282006%29<|>Sun (2006)<|>Sun (2006) https://github.com/ramey/datamicroarray/wiki/Sun-%282006%29<|>11<|>70<|>0<|>10<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Borovecki-%282005%29<|>Borovecki (2005)<|>Borovecki (2005) https://github.com/ramey/datamicroarray/wiki/Borovecki-%282005%29<|>17<|>82<|>0<|>16<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Chiaretti-%282004%29<|>Chiaretti (2004)<|>Chiaretti (2004) https://github.com/ramey/datamicroarray/wiki/Chiaretti-%282004%29<|>17<|>82<|>0<|>16<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Christensen-%282009%29<|>Christensen (2009)<|>Christensen (2009) https://github.com/ramey/datamicroarray/wiki/Christensen-%282009%29<|>19<|>86<|>0<|>18<|>6458<|>software
6458<|>https://github.com/ramey/datamicroarray/wiki/Pomeroy-%282002%29<|>Pomeroy (2002)<|>Pomeroy (2002) https://github.com/ramey/datamicroarray/wiki/Pomeroy-%282002%29<|>15<|>78<|>0<|>14<|>6458<|>software
6471<|>[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>End-to-end training with on-the-fly data processing<|>End-to-end training with on-the-fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>52<|>158<|>0<|>51<|>6471<|>other
6475<|>https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data<|>answer sentence selection dataset<|>We use the answer sentence selection dataset https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data  from TREC QA as our source of indirect supervision. We ran Stanford NER to extract entity mentions on both question and answer sentences and process the dataset into JSON format containing QA-pairs. Details of how we construct QA-pairs can be found in our paper.<|>45<|>136<|>11<|>44<|>6475<|>dataset_landing_page
6488<|>https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py<|>pytorch/tnt<|>ConcatDataset from pytorch/tnt https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py .<|>31<|>107<|>19<|>30<|>6488<|>software
6503<|>https://rose1.ntu.edu.sg/dataset/actionRecognition/<|>https://rose1.ntu.edu.sg/dataset/actionRecognition/<|>https://rose1.ntu.edu.sg/dataset/actionRecognition/ https://rose1.ntu.edu.sg/dataset/actionRecognition/<|>52<|>103<|>0<|>51<|>6503<|>dataset_landing_page
6504<|>https://github.com/OSGeo/PROJ-data<|>PROJ-data GitHub repository<|>More info on the contents of the proj-data package can be found at the PROJ-data GitHub repository https://github.com/OSGeo/PROJ-data .<|>99<|>133<|>71<|>98<|>6504<|>dataset_landing_page
6516<|>http://www.fc.up.pt/addi/ph2%20database.html<|>PH2 Dataset<|>The PH2 Dataset http://www.fc.up.pt/addi/ph2%20database.html  has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.<|>16<|>60<|>4<|>15<|>4066<|>dataset_landing_page
6516<|>https://github.com/learningtitans/data-depth-design/issues<|>submitting an issue<|>Please, help us to improve this code, by submitting an issue https://github.com/learningtitans/data-depth-design/issues  if you find any problems.<|>61<|>119<|>41<|>60<|>6516<|>other
6546<|>https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip<|>data<|>Download data https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip<|>14<|>74<|>9<|>13<|>6546<|>dataset_direct_link
6551<|>http://opendatacommons.org/licenses/by/1.0/<|>Open Data Commons Attribution License (ODC-By) v1.0<|>Open Data Commons Attribution License (ODC-By) v1.0 http://opendatacommons.org/licenses/by/1.0/<|>52<|>95<|>0<|>51<|>6551<|>other
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb<|>Introduction to MNIST Dataset<|>Introduction to MNIST Dataset https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb .<|>30<|>159<|>0<|>29<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb<|>notebook<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb ). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).<|>11<|>132<|>2<|>10<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb<|>this notebook<|>Some examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples. MNIST is a database of handwritten digits, for a quick description of that dataset, you can check this notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb .<|>256<|>371<|>242<|>255<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb<|>Introduction to MNIST Dataset<|>Introduction to MNIST Dataset https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb .<|>30<|>159<|>0<|>29<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb<|>notebook<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py ). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.<|>11<|>145<|>2<|>10<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb<|>notebook<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py ). Introducing TensorFlow Dataset API for optimizing the input data pipeline.<|>11<|>145<|>2<|>10<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb<|>notebook<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb ). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).<|>11<|>132<|>2<|>10<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py<|>code<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py ). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.<|>155<|>285<|>150<|>154<|>6568<|>software
6568<|>https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py<|>code<|>( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py ). Introducing TensorFlow Dataset API for optimizing the input data pipeline.<|>155<|>285<|>150<|>154<|>6568<|>software
6572<|>https://www.indigo-datacloud.eu/<|>INDIGO-DataCloud<|>The present document describes how Apache Mesos is used by the INDIGO-DataCloud https://www.indigo-datacloud.eu/  PaaS layer.  INDIGO-DataCloud (start date: 01/04/2015, end date: 30/09/2017) is a project funded under the Horizon2020 framework program of the European Union and led by the National Institute for Nuclear Physics (INFN). It developed a data and computing platform targeting scientific communities, deployable on multiple hardware and provisioned over hybrid (private or public) e-infrastructures. The INDIGO solutions are being evolved in the context of other European projects like DEEP Hybrid-DataCloud https://deep-hybrid-datacloud.eu , eXtreme-DataCloud http://www.extreme-datacloud.eu/  and EOSC-Hub https://www.eosc-hub.eu/<|>80<|>112<|>63<|>79<|>6572<|>other
6572<|>https://deep-hybrid-datacloud.eu<|>DEEP Hybrid-DataCloud<|>The present document describes how Apache Mesos is used by the INDIGO-DataCloud https://www.indigo-datacloud.eu/  PaaS layer.  INDIGO-DataCloud (start date: 01/04/2015, end date: 30/09/2017) is a project funded under the Horizon2020 framework program of the European Union and led by the National Institute for Nuclear Physics (INFN). It developed a data and computing platform targeting scientific communities, deployable on multiple hardware and provisioned over hybrid (private or public) e-infrastructures. The INDIGO solutions are being evolved in the context of other European projects like DEEP Hybrid-DataCloud https://deep-hybrid-datacloud.eu , eXtreme-DataCloud http://www.extreme-datacloud.eu/  and EOSC-Hub https://www.eosc-hub.eu/<|>619<|>651<|>597<|>618<|>6572<|>other
6572<|>http://www.extreme-datacloud.eu/<|>eXtreme-DataCloud<|>The present document describes how Apache Mesos is used by the INDIGO-DataCloud https://www.indigo-datacloud.eu/  PaaS layer.  INDIGO-DataCloud (start date: 01/04/2015, end date: 30/09/2017) is a project funded under the Horizon2020 framework program of the European Union and led by the National Institute for Nuclear Physics (INFN). It developed a data and computing platform targeting scientific communities, deployable on multiple hardware and provisioned over hybrid (private or public) e-infrastructures. The INDIGO solutions are being evolved in the context of other European projects like DEEP Hybrid-DataCloud https://deep-hybrid-datacloud.eu , eXtreme-DataCloud http://www.extreme-datacloud.eu/  and EOSC-Hub https://www.eosc-hub.eu/<|>672<|>704<|>654<|>671<|>6572<|>other
6573<|>http://www.extreme-datacloud.eu/<|>eXtreme-DataCloud project<|>eXtreme-DataCloud project http://www.extreme-datacloud.eu/  (Horizon 2020) under Grant number 777367.<|>26<|>58<|>0<|>25<|>6572<|>other
6573<|>https://deep-hybrid-datacloud.eu/<|>DEEP-HybridDataCloud project<|>DEEP-HybridDataCloud project https://deep-hybrid-datacloud.eu/  (Horizon 2020) under Grant number 777435.<|>29<|>62<|>0<|>28<|>6573<|>other
6573<|>https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/<|><|> https://travis-ci.org/indigo-dc/orchestrator https://codecov.io/gh/indigo-dc/orchestrator https://sonarcloud.io/dashboard?id=it.reply%3Aorchestrator https://snyk.io/test/github/indigo-dc/orchestrator https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/<|>201<|>286<|>0<|>0<|>6573<|>other
6573<|>https://www.indigo-datacloud.eu/<|>INDIGO-DataCloud project<|>INDIGO-DataCloud project https://www.indigo-datacloud.eu/  (Horizon 2020) under Grant number 653549.<|>25<|>57<|>0<|>24<|>6572<|>other
6581<|>https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/<|><|> http://jenkins.i3m.upv.es/job/indigo/job/im-unit/ https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/ LICENSE https://imdocs.readthedocs.io/en/latest/<|>51<|>126<|>0<|>0<|>6581<|>other
6582<|>https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html<|>Onepanel<|>Onepanel https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html  - administration and configuration interface for  and  components,<|>9<|>82<|>0<|>8<|>6582<|>other
6582<|>https://onedata.org/docs/doc/using_onedata/oneclient.html<|>Oneclient<|>Oneclient https://onedata.org/docs/doc/using_onedata/oneclient.html  - command line tool which enables transparent access to users data spaces through Fuse https://github.com/libfuse/libfuse  virtual filesystem,<|>10<|>67<|>0<|>9<|>6582<|>other
6582<|>https://onedata.org/docs/doc/administering_onedata/provider_overview.html<|>Oneprovider<|>Oneprovider https://onedata.org/docs/doc/administering_onedata/provider_overview.html  - the main data management component of Onedata, deployed at each storage provider site, responsible for unifying and controlling access to data over low level storage resources of the provider,<|>12<|>85<|>0<|>11<|>6582<|>other
6582<|>https://onedata.org/docs/doc/administering_onedata/onezone_overview.html<|>Onezone<|>Onezone https://onedata.org/docs/doc/administering_onedata/onezone_overview.html  - allows to connect multiple storage providers into a larger distributed domain and provides users with Graphical User Interface for typical data management tasks,<|>8<|>80<|>0<|>7<|>6582<|>other
6582<|>http://onedata.org<|>Onedata<|>This is the main code repository of Onedata http://onedata.org  - a global data management system, providing easy access to distributed storage resources, supporting wide range of use cases from personal data management to data-intensive scientific computations.<|>44<|>62<|>36<|>43<|>6582<|>other
6582<|>https://hub.docker.com/u/onedata/<|>Docker Hub<|>The best way to use Onedata is to use our Docker images available at Docker Hub https://hub.docker.com/u/onedata/  or the binary packages available here https://get.onedata.org/ . Currently the binary packages are only available for  component.<|>80<|>113<|>69<|>79<|>6582<|>other
6582<|>https://onedata.org/docs/index.html<|>documentation<|>The easiest way to get started with using or deploying Onedata is to start with our official documentation https://onedata.org/docs/index.html .<|>107<|>142<|>93<|>106<|>6582<|>other
6582<|>https://onedata.org/docs/doc/administering_onedata/luma.html<|>LUMA<|>LUMA https://onedata.org/docs/doc/administering_onedata/luma.html  - service which allows mapping of between Onedata user accounts and local storage ID's, here we provide an example implementation of this service.<|>5<|>65<|>0<|>4<|>6582<|>other
6582<|>https://github.com/onedata/getting-started<|>example configurations and scenarios<|>In order to try deploying Onedata, or specific components we have prepared a set of example configurations and scenarios https://github.com/onedata/getting-started .<|>121<|>163<|>84<|>120<|>6582<|>software
6582<|>https://github.com/onedata/onedata/issues<|>GitHub issues<|>Please use GitHub issues https://github.com/onedata/onedata/issues  mechanism as the main channel for reporting bugs and requesting support or new features.<|>25<|>66<|>11<|>24<|>6582<|>other
6582<|>https://onedata.org/support<|>here<|>More information about support can be found here https://onedata.org/support .<|>49<|>76<|>44<|>48<|>6582<|>other
6582<|>https://get.onedata.org/<|>here<|>The best way to use Onedata is to use our Docker images available at Docker Hub https://hub.docker.com/u/onedata/  or the binary packages available here https://get.onedata.org/ . Currently the binary packages are only available for  component.<|>153<|>177<|>148<|>152<|>6582<|>software
6585<|>https://deep-hybrid-datacloud.eu<|>https://deep-hybrid-datacloud.eu<|>DEEP-Hybrid-DataCloud https://deep-hybrid-datacloud.eu https://deep-hybrid-datacloud.eu<|>55<|>87<|>22<|>54<|>6572<|>other
6585<|>https://www.indigo-datacloud.eu<|>https://www.indigo-datacloud.eu<|>INDIGO DataCloud https://www.indigo-datacloud.eu https://www.indigo-datacloud.eu<|>49<|>80<|>17<|>48<|>6585<|>other
6591<|>https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets<|>corpus1<|>Katrin Wisniewski et al.  2013 corpus1 https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets corpus2 https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/<|>39<|>109<|>31<|>38<|>6591<|>dataset_landing_page
6591<|>https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/<|>corpus2<|>Katrin Wisniewski et al.  2013 corpus1 https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets corpus2 https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/<|>118<|>206<|>110<|>117<|>6591<|>dataset_landing_page
6605<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>presentation<|>A tutorial presentation http://simongog.github.io/assets/data/sdsl-slides/tutorial  with the example code tutorial/  using in the sides demonstrating all features of the library in a step-by-step walk-through.<|>24<|>82<|>11<|>23<|>189<|>other
6605<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>tutorial slides and walk-through<|>We provide a large collection of supporting documentation consisting of examples, cheat sheet http://simongog.github.io/assets/data/sdsl-cheatsheet.pdf , tutorial slides and walk-through http://simongog.github.io/assets/data/sdsl-slides/tutorial .<|>187<|>245<|>154<|>186<|>189<|>other
6605<|>http://simongog.github.io/assets/data/sdsl-slides/tutorial<|>tutorial<|>Next we suggest you look at the comprehensive tutorial http://simongog.github.io/assets/data/sdsl-slides/tutorial  which describes all major features of the library or look at some of the provided examples examples .<|>55<|>113<|>46<|>54<|>189<|>other
6611<|>https://research.fb.com/category/data-science/<|>Core Data Science team<|>Prophet is open source software https://code.facebook.com/projects/  released by Facebook's Core Data Science team https://research.fb.com/category/data-science/ . It is available for download on CRAN https://cran.r-project.org/package=prophet  and PyPI https://pypi.python.org/pypi/prophet/ .<|>115<|>161<|>92<|>114<|>6611<|>other
6621<|>https://github.com/ziqizhang/data#ate<|>TTC<|>JATE 2.0 Beta.11 released. The main changes include: 1) migration to Solr 7.2.1. : the index files created by this version of Solr is not compatible with the previous versions; 2) fixing a couple of minor bugs documented in the Issues page; 3) added two more example configrations for the TTC https://github.com/ziqizhang/data#ate  corpora; 4) added two new algorithms, Basic https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/Basic.java  and ComboBasic https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/ComboBasic.java ; 5) improved introduction page.<|>293<|>330<|>289<|>292<|>6621<|>dataset_landing_page
6621<|>https://github.com/ziqizhang/data#ate<|>research data page<|>Ziqi Zhang's research data page https://github.com/ziqizhang/data#ate  contains 4 datasets used for ATE research.<|>32<|>69<|>13<|>31<|>6621<|>dataset_landing_page
6623<|>https://github.com/ziqizhang/data/tree/master/ontology%20mapping<|>/ontology mapping<|>: dataset used for evaluating mapping relations collected from DBpedia : ontology mapping, ontology alignment, DBpedia : Ontology alignment https://en.wikipedia.org/wiki/Ontology_alignment : LODIE http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/J019488/1 : /ontology mapping https://github.com/ziqizhang/data/tree/master/ontology%20mapping<|>281<|>345<|>263<|>280<|>6623<|>dataset_direct_link
6623<|>https://github.com/ziqizhang/data/tree/master/terminology%20extraction<|>/terminology extraction<|>: dataset used for evaluating automatic term extraction/recognition. : automatic term extraction or recognition, ATE, ATR, text mining, terminology, thesaurus, glossary, ontology engineering : Terminology extraction https://en.wikipedia.org/wiki/Terminology_extraction : SemRe-Rank https://github.com/ziqizhang/semrerank : /terminology extraction https://github.com/ziqizhang/data/tree/master/terminology%20extraction<|>347<|>417<|>323<|>346<|>6623<|>dataset_direct_link
6623<|>https://github.com/ziqizhang/data/tree/master/procedural%20knowledge<|>/procedural knowledge<|>: dataset containing annotated instructions that describe procedures (e.g., how to cook a recipe, how to mount snow chain on wheels etc. : procedure, instruction, annotation, classification : Procedural knowledge https://en.wikipedia.org/wiki/Procedural_knowledge : /procedural knowledge https://github.com/ziqizhang/data/tree/master/procedural%20knowledge<|>288<|>356<|>266<|>287<|>6623<|>dataset_direct_link
6623<|>http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html<|>Entity Deduplication on ScholarlyData<|>Z. Zhang, A. N. Nuzzolese, and A. L. Gentile. Entity Deduplication on ScholarlyData http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html . In Proceedings of ESWC 2017, pp 85-100, Lecture Notes in Computer Science. Springer, 2017.<|>84<|>146<|>46<|>83<|>6623<|>other
6623<|>https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking<|>/scholarly data linking<|>: dataset used for evaluating author name and organisation linking in scholarly data : author name disambiguation, link discovery, entity linking, entity disambiguation : Author name disambiguation https://en.wikipedia.org/wiki/Author_Name_Disambiguation : scholarlydata http://www.scholarlydata.org/ : /scholarly data linking https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking<|>327<|>399<|>303<|>326<|>6623<|>dataset_landing_page
6623<|>https://github.com/ziqizhang/data/tree/master/hate%20speech<|>/hate speech<|>: dataset used for evaluating hate speech on Twitter. : hate speech, Twitter, social media, abusive language, classification : chase https://github.com/ziqizhang/chase : /hate speech https://github.com/ziqizhang/data/tree/master/hate%20speech<|>183<|>242<|>170<|>182<|>6623<|>dataset_direct_link
6623<|>http://www.scholarlydata.org/<|>scholarlydata<|>: dataset used for evaluating author name and organisation linking in scholarly data : author name disambiguation, link discovery, entity linking, entity disambiguation : Author name disambiguation https://en.wikipedia.org/wiki/Author_Name_Disambiguation : scholarlydata http://www.scholarlydata.org/ : /scholarly data linking https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking<|>271<|>300<|>257<|>270<|>6623<|>other
6623<|>https://github.com/ziqizhang/data/tree/master/webtable%20entity%20linking<|>/webtable entity linking<|>: dataset used for evaluating entity linking in webtables, and also table header classification and relation annotation; contains 16,000+ annotated relational tables that can be used for many studies related to webtables. : Entity linking https://en.wikipedia.org/wiki/Entity_linking : webtable, web table, entity linking, classification, relation extraction : sti https://github.com/ziqizhang/sti : /webtable entity linking https://github.com/ziqizhang/data/tree/master/webtable%20entity%20linking<|>425<|>498<|>400<|>424<|>6623<|>dataset_landing_page
6649<|>https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet<|>Ravi and Larochelle - splits<|>The splits  can be downloaded from Ravi and Larochelle - splits https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet . For more information on how to obtain the images check the original source Ravi and Larochelle - github https://github.com/twitter/meta-learning-lstm<|>64<|>139<|>35<|>63<|>6649<|>dataset_direct_link
6663<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>train<|>SQuAD: train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json , dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>13<|>79<|>7<|>12<|>6663<|>dataset_direct_link
6663<|>http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2<|>train<|>WebQuestions: train http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2 , test http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2 , entities https://dl.fbaipublicfiles.com/drqa/freebase-entities.txt.gz<|>20<|>154<|>14<|>19<|>6663<|>dataset_direct_link
6663<|>http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2<|>test<|>WebQuestions: train http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2 , test http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2 , entities https://dl.fbaipublicfiles.com/drqa/freebase-entities.txt.gz<|>162<|>295<|>157<|>161<|>6663<|>dataset_direct_link
6663<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>dev<|>SQuAD: train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json , dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>86<|>150<|>82<|>85<|>6663<|>dataset_direct_link
6706<|>https://www.cityscapes-dataset.com/<|>Cityscapes training set<|>: 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ .<|>47<|>82<|>23<|>46<|>3245<|>dataset_landing_page
6719<|>https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare<|>Flickr30k<|>Prepocess the dataset. Follow the instruction in . You can choose one dataset to run. Three datasets need different prepocessing. I write the instruction for Flickr30k https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare , MSCOCO https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare  and CUHK-PEDES https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare .<|>168<|>252<|>158<|>167<|>6719<|>dataset_landing_page
6719<|>https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare<|>CUHK-PEDES<|>Prepocess the dataset. Follow the instruction in . You can choose one dataset to run. Three datasets need different prepocessing. I write the instruction for Flickr30k https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare , MSCOCO https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare  and CUHK-PEDES https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare .<|>360<|>445<|>349<|>359<|>6719<|>dataset_landing_page
6719<|>https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare<|>MSCOCO<|>Prepocess the dataset. Follow the instruction in . You can choose one dataset to run. Three datasets need different prepocessing. I write the instruction for Flickr30k https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare , MSCOCO https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare  and CUHK-PEDES https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare .<|>262<|>343<|>255<|>261<|>6719<|>dataset_landing_page
6724<|>https://www.cityscapes-dataset.com/<|>Cityscapes training set<|>| dataset | example | | --- | --- | |  400 images from CMP Facades dataset http://cmp.felk.cvut.cz/~tylecr1/facade/ . (31MB)  Pre-trained: BtoA https://mega.nz/#!H0AmER7Y!pBHcH4M11eiHBmJEWvGr-E_jxK4jluKBUlbfyLSKgpY  |  | |  2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . (113M)  Pre-trained: AtoB https://mega.nz/#!K1hXlbJA!rrZuEnL3nqOcRhjb-AnSkK0Ggf9NibhDymLOkhzwuQk BtoA https://mega.nz/#!y1YxxB5D!1817IXQFcydjDdhk_ILbCourhA6WSYRttKLrGE97q7k  |  | |  1096 training images scraped from Google Maps (246M)  Pre-trained: AtoB https://mega.nz/#!7oxklCzZ!8fRZoF3jMRS_rylCfw2RNBeewp4DFPVE_tSCjCKr-TI BtoA https://mega.nz/#!S4AGzQJD!UH7B5SV7DJSTqKvtbFKqFkjdAh60kpdhTk9WerI-Q1I  |  | |  50k training images from UT Zappos50K dataset http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ . Edges are computed by HED https://github.com/s9xie/hed  edge detector + post-processing. (2.2GB)  Pre-trained: AtoB https://mega.nz/#!u9pnmC4Q!2uHCZvHsCkHBJhHZ7xo5wI-mfekTwOK8hFPy0uBOrb4  |  | |  137K Amazon Handbag images from iGAN project https://github.com/junyanz/iGAN . Edges are computed by HED https://github.com/s9xie/hed  edge detector + post-processing. (8.6GB)  Pre-trained: AtoB https://mega.nz/#!G1xlDCIS!sFDN3ZXKLUWU1TX6Kqt7UG4Yp-eLcinmf6HVRuSHjrM  |  |<|>269<|>304<|>245<|>268<|>3245<|>dataset_landing_page
6754<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101 website<|>The release of the DeepMind Kinetics dataset www.deepmind.com/kinetics  only included the YouTube IDs and the start and end times of the clips. For the sample data here, we use a video from the UCF101 dataset, for which all the videos are provided in full. The video used is  which can be downloaded from the UCF101 website http://crcv.ucf.edu/data/UCF101.php .<|>324<|>359<|>309<|>323<|>2024<|>dataset_landing_page
6754<|>https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py<|>this<|>For additional details on preprocessing, check this https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py , refer to our paper or contact the authors.<|>52<|>161<|>47<|>51<|>6754<|>software
6774<|>https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py<|><|>Change the ‘self._data_path’ in https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py  to yours.<|>32<|>108<|>0<|>0<|>6774<|>software
6774<|>https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md<|>data/VOC0712/README.md<|>Follow the data/VOC0712/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md  to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2007 training and testing.<|>34<|>107<|>11<|>33<|>6774<|>dataset_landing_page
6774<|>https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py<|><|>Change the ‘self._devkit_path’ in https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py  to yours.<|>34<|>116<|>0<|>0<|>6774<|>dataset_landing_page
6774<|>https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md<|>data/coco/README.md<|>Follow the data/coco/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md  to download MS COCO dataset and create the LMDB file for the COCO training and testing.<|>31<|>101<|>11<|>30<|>6774<|>dataset_landing_page
6774<|>https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md<|>data/VOC0712Plus/README.md<|>Follow the data/VOC0712Plus/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md  to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2012 training and testing.<|>38<|>115<|>11<|>37<|>6774<|>dataset_landing_page
6786<|>http://cocodataset.org/#download<|>http://cocodataset.org/#download<|>Download MSCOCO images from http://cocodataset.org/#download http://cocodataset.org/#download . We train in COCO trainvalminusminival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset and validate in minival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset. Then put the data and evaluation PythonAPI https://github.com/cocodataset/cocoapi/tree/master/PythonAPI  in $CPN_ROOT/data/COCO/MSCOCO. All paths are defined in config.py and you can modify them as you wish.<|>61<|>93<|>28<|>60<|>6212<|>dataset_landing_page
6786<|>https://github.com/cocodataset/cocoapi/tree/master/PythonAPI<|>PythonAPI<|>Download MSCOCO images from http://cocodataset.org/#download http://cocodataset.org/#download . We train in COCO trainvalminusminival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset and validate in minival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset. Then put the data and evaluation PythonAPI https://github.com/cocodataset/cocoapi/tree/master/PythonAPI  in $CPN_ROOT/data/COCO/MSCOCO. All paths are defined in config.py and you can modify them as you wish.<|>390<|>450<|>380<|>389<|>6786<|>software
6794<|>https://visualdialog.org/data<|>VisDial v1.0<|>VisDial v1.0 https://visualdialog.org/data  dataset can be downloaded and preprocessed as specified below. The path provided as  must have four subdirectories - http://images.cocodataset.org/zips/train2014.zip  and http://images.cocodataset.org/zips/val2014.zip  as per COCO dataset,  and  which can be downloaded from here https://visualdialog.org/data .<|>13<|>42<|>0<|>12<|>6794<|>dataset_landing_page
6794<|>https://visualdialog.org/data<|>here<|>VisDial v1.0 https://visualdialog.org/data  dataset can be downloaded and preprocessed as specified below. The path provided as  must have four subdirectories - http://images.cocodataset.org/zips/train2014.zip  and http://images.cocodataset.org/zips/val2014.zip  as per COCO dataset,  and  which can be downloaded from here https://visualdialog.org/data .<|>324<|>353<|>319<|>323<|>6794<|>dataset_landing_page
6794<|>https://visualdialog.org/data<|>Visdial v0.9<|>To download and preprocess Visdial v0.9 https://visualdialog.org/data  dataset, provide an extra  argument while execution.<|>40<|>69<|>27<|>39<|>6794<|>dataset_landing_page
6800<|>http://conda.pydata.org/<|>Conda<|>An additional method using Conda http://conda.pydata.org/  is also possible:<|>33<|>57<|>27<|>32<|>454<|>software
6801<|>https://github.com/pcyin/pytorch_nmt/tree/master/data<|>this repo<|>NOTE: this script requires Lua and luaTorch. As an alternative, you can download all necessary files from this repo https://github.com/pcyin/pytorch_nmt/tree/master/data<|>116<|>169<|>106<|>115<|>6801<|>dataset_landing_page
6801<|>https://github.com/harvardnlp/BSO/tree/master/data_prep/MT<|>Harvard NLP repo<|>Run the script (borrowed from Harvard NLP repo https://github.com/harvardnlp/BSO/tree/master/data_prep/MT ) to download and preprocess IWSLT'14 dataset:<|>47<|>105<|>30<|>46<|>6801<|>software
6803<|>https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md<|>instructions<|>Install the MS COCO dataset at /path/to/coco from official website http://mscoco.org/ , default is ~/data/COCO. Following the instructions https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md  to prepare  and  annotations. All label files (.json) should be under the COCO/annotations/ folder. It should have this basic structure<|>139<|>244<|>126<|>138<|>6803<|>software
6816<|>http://cocodataset.org/#home<|>COCO Dataset<|>Download COCO2017 image from COCO Dataset http://cocodataset.org/#home<|>42<|>70<|>29<|>41<|>6391<|>dataset_landing_page
6833<|>https://seaborn.pydata.org/<|>Seaborn<|>Seaborn https://seaborn.pydata.org/<|>8<|>35<|>0<|>7<|>6833<|>software
6833<|>http://pandas.pydata.org<|>pandas<|>pandas http://pandas.pydata.org<|>7<|>31<|>0<|>6<|>6833<|>software
6836<|>https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0<|>Dropbox<|>We provide demo attack data and classifier on Dropbox https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0  and 百度网盘 https://pan.baidu.com/s/1gfpcB5p  (密码: yzt4). Please download and put the unzipped files in . You may also use your own data for test.<|>54<|>124<|>46<|>53<|>5898<|>dataset_landing_page
6838<|>http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php<|>here<|>HitL-SLAM can be used on other datasets as well, as long as they are 2D, and based on depth scans or depth images. Many well-known datasets of this nature can be found here http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php . After downloading a dataset or generating some data yourself, it needs to be put into the right format.<|>173<|>238<|>168<|>172<|>6838<|>dataset_landing_page
6846<|>https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html<|>Cora<|>In the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph. For this, we load the Cora https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html  dataset, and create a simple 2-layer GCN model using the pre-defined https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html :<|>135<|>235<|>130<|>134<|>6846<|>other
6846<|>https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py<|><|>It consists of various methods for deep learning on graphs and other irregular structures, also known as , from a variety of published papers. In addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu , https://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html  support, https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py  support, a large number of common benchmark datasets (based on simple interfaces to create your own), the GraphGym https://pytorch-geometric.readthedocs.io/en/latest/advanced/graphgym.html  experiment manager, and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.<|>435<|>513<|>0<|>0<|>6846<|>software
6854<|>http://cocodataset.org/#download<|>MSCOCO<|>Download train2014, val2014 images and their annotations from the MSCOCO http://cocodataset.org/#download  webpage and put them in ./data/coco<|>73<|>105<|>66<|>72<|>6212<|>dataset_landing_page
6871<|>http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html<|>notMNIST dataset<|>notMNIST ./notebooks/notMNIST.ipynb  does the same accuracy comparisons, but for the notMNIST dataset http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html . We omit the textual explanations since it would be redundant with what's in the MNIST notebook.<|>102<|>162<|>85<|>101<|>6871<|>dataset_landing_page
6884<|>http://crcv.ucf.edu/data/UCF101.php<|>here<|>Download videos and train/test splits here http://crcv.ucf.edu/data/UCF101.php .<|>43<|>78<|>38<|>42<|>2024<|>dataset_landing_page
6884<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>here<|>Download videos and train/test splits here http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ .<|>43<|>119<|>38<|>42<|>2024<|>dataset_landing_page
6887<|>https://github.com/EdinburghNLP/spot-data<|>link<|>The SPOT dataset used in the above paper is available on the EdinburghNLP github page: link https://github.com/EdinburghNLP/spot-data<|>92<|>133<|>87<|>91<|>6887<|>dataset_landing_page
6888<|>https://github.com/DavidGrangier/wikipedia-biography-dataset<|>WIKIBIO<|>The dataset for evaluation is WIKIBIO https://github.com/DavidGrangier/wikipedia-biography-dataset  from Lebret et al. 2016 https://arxiv.org/abs/1603.07771 . We preprocess the dataset in a easy-to-use way.<|>38<|>98<|>30<|>37<|>6888<|>dataset_landing_page
6909<|>https://www.kaggle.com/c/datasciencebowl<|>Kaggle plankton recognition competition, 2015<|>Kaggle plankton recognition competition, 2015 https://www.kaggle.com/c/datasciencebowl  Third place. The competition solution is being adapted for research purposes in EcoTaxa http://ecotaxa.obs-vlfr.fr/ .<|>46<|>86<|>0<|>45<|>941<|>other
6909<|>https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#<|>Assamese handwriting recognition<|>Assamese handwriting recognition https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#<|>33<|>120<|>0<|>32<|>941<|>dataset_landing_page
6910<|>https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen<|>this page<|>Prepare datasets following this page https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen .<|>37<|>107<|>27<|>36<|>6910<|>dataset_landing_page
6929<|>https://www.re3data.org<|>many available options<|>In the interest of facilitating research reproducibility and thereby increasing the value of your MATPOWER-related research publications, we strongly encourage you to also publish, whenever possible, all of the code and data required to generate the results you are publishing. Zenodo/GitHub https://guides.github.com/activities/citable-code/  and IEEE DataPort https://ieee-dataport.org  are two of many available options https://www.re3data.org .<|>423<|>446<|>400<|>422<|>6929<|>other
6929<|>https://ieee-dataport.org<|>IEEE DataPort<|>In the interest of facilitating research reproducibility and thereby increasing the value of your MATPOWER-related research publications, we strongly encourage you to also publish, whenever possible, all of the code and data required to generate the results you are publishing. Zenodo/GitHub https://guides.github.com/activities/citable-code/  and IEEE DataPort https://ieee-dataport.org  are two of many available options https://www.re3data.org .<|>362<|>387<|>348<|>361<|>6929<|>other
6943<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC<|>Once the  is built and sourced (via ), there are two launch files prepared for the EuRoC https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  and UPenn fast flight https://github.com/KumarRobotics/msckf_vio/wiki/Dataset  dataset named  and  respectively. Each launch files instantiates two ROS nodes:<|>89<|>165<|>83<|>88<|>6943<|>dataset_landing_page
6943<|>http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag<|>Vicon Room 1 01<|>Vicon Room 1 01 http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag<|>16<|>114<|>0<|>15<|>6943<|>dataset_direct_link
6943<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC<|>First obtain either the EuRoC https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  or the UPenn fast flight https://github.com/KumarRobotics/msckf_vio/wiki/Dataset  dataset.<|>30<|>106<|>24<|>29<|>6943<|>dataset_landing_page
6943<|>http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag<|>Vicon Room 1 02<|>Vicon Room 1 02 http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag<|>16<|>114<|>0<|>15<|>6943<|>dataset_direct_link
6962<|>http://cocodataset.org/#download<|>MS-COCO website<|>In all MS-COCO experiments, we use  for training, and  (a.k.a. ) for validation. Follow MS-COCO website http://cocodataset.org/#download  to download images/annotations, and set-up the COCO API.<|>104<|>136<|>88<|>103<|>6212<|>dataset_landing_page
6980<|>https://github.com/bredele/datastore<|>datastore<|>This project is based on asyncCall https://github.com/dielc/asyncCall.js  and uses parts of datastore https://github.com/bredele/datastore . The two projects are combined such that changes two types of data (observable and replicated data) can be used. Changes to these data types are propagated to server or clients via remote procedure calls. This way, custom failure handling can be added when for example no network is available. Proxies are used to ensure that every assignment on these objects are propagated to the server and every connected client.<|>102<|>138<|>92<|>101<|>6980<|>software
6989<|>https://mobilitydata.org/cities-gbfs-v2-2-is-here-for-you/<|>v2.2 Article<|>| Version | Type | Release Date | Status | JSON Schema | Release Notes | |:---:|:---:|---|---|---|---| | v2.2 https://github.com/NABSA/gbfs/blob/v2.2/gbfs.md  | MINOR | March 19, 2021 |:white_check_mark:  | v2.2 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.2 | v2.2 Article https://mobilitydata.org/cities-gbfs-v2-2-is-here-for-you/  | v2.1 https://github.com/NABSA/gbfs/blob/v2.1/gbfs.md  | MINOR | March 18, 2021 |:white_check_mark:  | v2.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.1 | v2.1 Article https://mobilitydata.org/gbfs-now-fully-supports-dockless-systems-%f0%9f%9b%b4%f0%9f%91%8f/  | v2.0 https://github.com/NABSA/gbfs/blob/v2.0/gbfs.md  | MAJOR | March 16, 2020 | :white_check_mark:  | v2.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.0  | v2.0 Article https://mobilitydata.org/whats-new-in-gbfs-v2-0-%f0%9f%9a%b2%f0%9f%9b%b4/  | | v1.1 https://github.com/NABSA/gbfs/blob/v1.1/gbfs.md  | MINOR | March 16, 2020 |:white_check_mark:  | v1.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.1  | | | v1.0 https://github.com/NABSA/gbfs/blob/v1.0/gbfs.md  | MAJOR | Prior to October 2019 | :x:  | v1.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.0 | |<|>300<|>358<|>287<|>299<|>6989<|>other
6989<|>https://mobilitydata.org/whats-new-in-gbfs-v2-0-%f0%9f%9a%b2%f0%9f%9b%b4/<|>v2.0 Article<|>| Version | Type | Release Date | Status | JSON Schema | Release Notes | |:---:|:---:|---|---|---|---| | v2.2 https://github.com/NABSA/gbfs/blob/v2.2/gbfs.md  | MINOR | March 19, 2021 |:white_check_mark:  | v2.2 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.2 | v2.2 Article https://mobilitydata.org/cities-gbfs-v2-2-is-here-for-you/  | v2.1 https://github.com/NABSA/gbfs/blob/v2.1/gbfs.md  | MINOR | March 18, 2021 |:white_check_mark:  | v2.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.1 | v2.1 Article https://mobilitydata.org/gbfs-now-fully-supports-dockless-systems-%f0%9f%9b%b4%f0%9f%91%8f/  | v2.0 https://github.com/NABSA/gbfs/blob/v2.0/gbfs.md  | MAJOR | March 16, 2020 | :white_check_mark:  | v2.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.0  | v2.0 Article https://mobilitydata.org/whats-new-in-gbfs-v2-0-%f0%9f%9a%b2%f0%9f%9b%b4/  | | v1.1 https://github.com/NABSA/gbfs/blob/v1.1/gbfs.md  | MINOR | March 16, 2020 |:white_check_mark:  | v1.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.1  | | | v1.0 https://github.com/NABSA/gbfs/blob/v1.0/gbfs.md  | MAJOR | Prior to October 2019 | :x:  | v1.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.0 | |<|>849<|>922<|>836<|>848<|>6989<|>other
6989<|>https://bit.ly/mobilitydata-slack<|>public GBFS Slack channel<|>GBFS is an open source project developed under a consensus-based governance model. Contributors come from across the shared mobility industry, public sector, civic technology and elsewhere. Comments or questions can be addressed to the community by opening an issue https://github.com/NABSA/gbfs/issues . Proposals for changes or additions to the specification can be made through pull requests https://github.com/NABSA/gbfs/pulls .  Questions can also be addressed to the community via the public GBFS Slack channel https://bit.ly/mobilitydata-slack  or to the shared mobility staff at MobilityData: sharedmobility@mobilitydata.org mailto:sharedmobility@mobilitydata.org<|>517<|>550<|>491<|>516<|>6989<|>other
6989<|>https://mobilitydata.org/<|>MobilityData<|>The copyright for GBFS is held by the MobilityData https://mobilitydata.org/ .<|>51<|>76<|>38<|>50<|>6989<|>other
6989<|>https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement<|>requirement<|>There are many similarities between GBFS and MDS https://github.com/openmobilityfoundation/mobility-data-specification  (Mobility Data Specification), however, their intended use cases are different. GBFS is a real-time or near real-time specification for public data primarily intended to provide transit advice through consumer-facing applications. MDS is not public data and is intended for use only by mobility regulators. Publishing a public GBFS feed is a requirement https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement  of all MDS compatible  APIs.<|>474<|>560<|>462<|>473<|>6989<|>software
6989<|>https://gbfs.mobilitydata.org/toolbox/resources/<|>here<|>Including APIs, datasets, validators, research, and software can be found here https://gbfs.mobilitydata.org/toolbox/resources/ .<|>79<|>127<|>74<|>78<|>6989<|>other
6989<|>https://github.com/openmobilityfoundation/mobility-data-specification<|>MDS<|>There are many similarities between GBFS and MDS https://github.com/openmobilityfoundation/mobility-data-specification  (Mobility Data Specification), however, their intended use cases are different. GBFS is a real-time or near real-time specification for public data primarily intended to provide transit advice through consumer-facing applications. MDS is not public data and is intended for use only by mobility regulators. Publishing a public GBFS feed is a requirement https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement  of all MDS compatible  APIs.<|>49<|>118<|>45<|>48<|>6989<|>software
6989<|>https://mobilitydata-io.slack.com<|>GBFS Slack channel<|>The person calling for the vote should announce the vote in the GBFS Slack channel https://mobilitydata-io.slack.com  with a link to the PR. The message should conform to this template:<|>83<|>116<|>64<|>82<|>6989<|>other
6989<|>https://mobilitydata.org/gbfs-now-fully-supports-dockless-systems-%f0%9f%9b%b4%f0%9f%91%8f/<|>v2.1 Article<|>| Version | Type | Release Date | Status | JSON Schema | Release Notes | |:---:|:---:|---|---|---|---| | v2.2 https://github.com/NABSA/gbfs/blob/v2.2/gbfs.md  | MINOR | March 19, 2021 |:white_check_mark:  | v2.2 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.2 | v2.2 Article https://mobilitydata.org/cities-gbfs-v2-2-is-here-for-you/  | v2.1 https://github.com/NABSA/gbfs/blob/v2.1/gbfs.md  | MINOR | March 18, 2021 |:white_check_mark:  | v2.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.1 | v2.1 Article https://mobilitydata.org/gbfs-now-fully-supports-dockless-systems-%f0%9f%9b%b4%f0%9f%91%8f/  | v2.0 https://github.com/NABSA/gbfs/blob/v2.0/gbfs.md  | MAJOR | March 16, 2020 | :white_check_mark:  | v2.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v2.0  | v2.0 Article https://mobilitydata.org/whats-new-in-gbfs-v2-0-%f0%9f%9a%b2%f0%9f%9b%b4/  | | v1.1 https://github.com/NABSA/gbfs/blob/v1.1/gbfs.md  | MINOR | March 16, 2020 |:white_check_mark:  | v1.1 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.1  | | | v1.0 https://github.com/NABSA/gbfs/blob/v1.0/gbfs.md  | MAJOR | Prior to October 2019 | :x:  | v1.0 Schema https://github.com/MobilityData/gbfs-json-schema/tree/master/v1.0 | |<|>557<|>648<|>544<|>556<|>6989<|>other
6989<|>https://bit.ly/mobilitydata-slack<|>public GBFS Slack channel<|>GBFS was created in 2014 by Mitch Vars https://github.com/mplsmitch  with collaboration from public, private sector and non-profit shared mobility system owners and operators, application developers, and technology vendors. Michael Frumin https://github.com/fruminator , Jesse Chan-Norris https://github.com/jcn  and others made significant contributions of time and expertise toward the development of v1.0 on behalf of Motivate International LLC (now Lyft). The North American Bikeshare Association’s http://www.nabsa.net  endorsement, support, and hosting was key to its success starting in 2015. In 2019, NABSA chose MobilityData to govern and facilitate the improvement of GBFS. MobilityData hosts a GBFS Resource Center https://gbfs.mobilitydata.org/  and a public GBFS Slack channel https://bit.ly/mobilitydata-slack  - you are welcome to contact us there or at sharedmobility@mobilitydata.org mailto:sharedmobility@mobilitydata.org  with questions.<|>790<|>823<|>764<|>789<|>6989<|>other
6989<|>https://gbfs.mobilitydata.org/<|>GBFS Resource Center<|>GBFS was created in 2014 by Mitch Vars https://github.com/mplsmitch  with collaboration from public, private sector and non-profit shared mobility system owners and operators, application developers, and technology vendors. Michael Frumin https://github.com/fruminator , Jesse Chan-Norris https://github.com/jcn  and others made significant contributions of time and expertise toward the development of v1.0 on behalf of Motivate International LLC (now Lyft). The North American Bikeshare Association’s http://www.nabsa.net  endorsement, support, and hosting was key to its success starting in 2015. In 2019, NABSA chose MobilityData to govern and facilitate the improvement of GBFS. MobilityData hosts a GBFS Resource Center https://gbfs.mobilitydata.org/  and a public GBFS Slack channel https://bit.ly/mobilitydata-slack  - you are welcome to contact us there or at sharedmobility@mobilitydata.org mailto:sharedmobility@mobilitydata.org  with questions.<|>726<|>756<|>705<|>725<|>6989<|>other
6999<|>http://www.robots.ox.ac.uk/~vgg/data/dtd/<|>Describable Textures Dataset<|>get a dataset with backgrounds, e.g. the Describable Textures Dataset http://www.robots.ox.ac.uk/~vgg/data/dtd/<|>70<|>111<|>41<|>69<|>6999<|>dataset_landing_page
7008<|>https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME<|>Dataverse<|>The data set is available on Dataverse https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME  (361 MB). This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in our experiments.<|>39<|>118<|>29<|>38<|>7008<|>dataset_landing_page
7039<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>this page<|>Go to this page https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to prepare ImageNet 1K data.<|>16<|>112<|>6<|>15<|>3215<|>dataset_landing_page
7106<|>https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html<|>UrbanSound8K<|>Parses the UrbanSound8K https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html  data set. This parser requires the following directory structure below the data set base directory.<|>24<|>94<|>11<|>23<|>4189<|>dataset_landing_page
7166<|>http://www.nltk.org/data.html<|>Installing NLTK Data<|>Additionally, the NLTK data package  needs to be downloaded. For installing packages, see the official guide Installing NLTK Data http://www.nltk.org/data.html .<|>130<|>159<|>109<|>129<|>1258<|>software
7166<|>https://github.com/deepmind/rc-data<|>DeepMind<|>The dataset for query-based abstractive summarization is created by converting an existing dataset for question answering, released by DeepMind https://github.com/deepmind/rc-data . Archives containing the processed DeepMind dataset can be downloaded at http://cs.nyu.edu/~kcho/DMQA/ http://cs.nyu.edu/~kcho/DMQA/ , which we used. Both the  and  archives are required for the conversion, from either news organization, or both. To use both, merge the extracted directories, for  and  separately.<|>144<|>179<|>135<|>143<|>2282<|>dataset_landing_page
7191<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>HIGGS<|>download data from HIGGS https://archive.ics.uci.edu/ml/datasets/HIGGS  and uncompress gz file.<|>25<|>70<|>19<|>24<|>5102<|>dataset_landing_page
7528<|>https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md<|>here<|>: We use  to name COCO's API as inheritance. Download the annotations and images http://cocodataset.org/#download  into . Note the valminusminival and minival can be downloaded here https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md .<|>182<|>287<|>177<|>181<|>6803<|>software
7528<|>http://cocodataset.org/#download<|>annotations and images<|>: We use  to name COCO's API as inheritance. Download the annotations and images http://cocodataset.org/#download  into . Note the valminusminival and minival can be downloaded here https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md .<|>81<|>113<|>58<|>80<|>6212<|>dataset_landing_page
7641<|>http://conda.pydata.org/miniconda.html<|>conda<|>We highly recommended to use conda http://conda.pydata.org/miniconda.html  as your Python distribution. Once downloading and installing conda http://conda.pydata.org/miniconda.html , this project can be installed by using the  file as follows:<|>35<|>73<|>29<|>34<|>155<|>software
7641<|>http://conda.pydata.org/miniconda.html<|>conda<|>We highly recommended to use conda http://conda.pydata.org/miniconda.html  as your Python distribution. Once downloading and installing conda http://conda.pydata.org/miniconda.html , this project can be installed by using the  file as follows:<|>142<|>180<|>136<|>141<|>155<|>software
7642<|>http://cocodataset.org/#download<|>here<|>Download MS-COCO 2017 dataset from here http://cocodataset.org/#download .<|>40<|>72<|>35<|>39<|>6212<|>dataset_landing_page
7670<|>http://corpus-texmex.irisa.fr/<|>Datasets for approximate nearest neighbor search<|>dataset, Datasets for approximate nearest neighbor search http://corpus-texmex.irisa.fr/<|>58<|>88<|>9<|>57<|>1268<|>dataset_landing_page
7782<|>https://archive.ics.uci.edu/ml/datasets/Adult<|>link<|>adult.csv link https://archive.ics.uci.edu/ml/datasets/Adult<|>15<|>60<|>10<|>14<|>2210<|>dataset_landing_page
7848<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>epsilon<|>|Dataset Name| #Training | #Testing | #Features | Task | Link | |------------|------------|----------|-----------|----------------|------| | Higgs | 10,000,000 | 500,000 | 28 | Classification | higgs https://archive.ics.uci.edu/ml/datasets/HIGGS  | | Epsilon | 400,000 | 100,000 | 2000 | Classification | epsilon https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | | HEPMASS | 7,000,000 | 3,500,000 | 28 | Classification | hepmass https://archive.ics.uci.edu/ml/datasets/HEPMASS | | SUSY | 4,000,000 | 1,000,000 | 18 | Classification | susy https://archive.ics.uci.edu/ml/datasets/SUSY | | CASP | 29,999 | 15,731 | 9 | Regression | casp https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure  | | SGEMM | 193,280 | 48,320 | 14 | Regression | sgemm https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance  | | SUPERCONDUCTOR | 17,008 | 4,255 | 81 | Regression | superconductor https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data  | | CT | 29,999 | 15,731 | 384 | Regression | ct https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis  | | Energy | 29,999 | 15,788 | 27 | Regression | energy https://archive.ics.uci.edu/ml/datasets/Energy+efficiency  | | Year | 412,206 | 103,139 | 90 | Regression | year https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD  |<|>313<|>380<|>305<|>312<|>5624<|>dataset_landing_page
7848<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>higgs<|>|Dataset Name| #Training | #Testing | #Features | Task | Link | |------------|------------|----------|-----------|----------------|------| | Higgs | 10,000,000 | 500,000 | 28 | Classification | higgs https://archive.ics.uci.edu/ml/datasets/HIGGS  | | Epsilon | 400,000 | 100,000 | 2000 | Classification | epsilon https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html  | | HEPMASS | 7,000,000 | 3,500,000 | 28 | Classification | hepmass https://archive.ics.uci.edu/ml/datasets/HEPMASS | | SUSY | 4,000,000 | 1,000,000 | 18 | Classification | susy https://archive.ics.uci.edu/ml/datasets/SUSY | | CASP | 29,999 | 15,731 | 9 | Regression | casp https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure  | | SGEMM | 193,280 | 48,320 | 14 | Regression | sgemm https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance  | | SUPERCONDUCTOR | 17,008 | 4,255 | 81 | Regression | superconductor https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data  | | CT | 29,999 | 15,731 | 384 | Regression | ct https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis  | | Energy | 29,999 | 15,788 | 27 | Regression | energy https://archive.ics.uci.edu/ml/datasets/Energy+efficiency  | | Year | 412,206 | 103,139 | 90 | Regression | year https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD  |<|>200<|>245<|>194<|>199<|>5102<|>dataset_landing_page
7908<|>http://grouplens.org/datasets/movielens/<|>Movielens 1M<|>A special effort was made to make a , which stresses on the ease of use of the framework. For example, that's how you build a pure SVD recommender on top of the Movielens 1M http://grouplens.org/datasets/movielens/  dataset:<|>174<|>214<|>161<|>173<|>269<|>dataset_landing_page
7931<|>http://cocodataset.org/#home<|>MS COCO<|>The original translation corpora can be downloaded from ( IWLST'16 En-De https://wit3.fbk.eu/ , WMT'16 En-Ro http://www.statmt.org/wmt16/translation-task.html , WMT'15 En-De http://www.statmt.org/wmt15/translation-task.html , MS COCO http://cocodataset.org/#home ). For the preprocessed corpora and pre-trained models, see below.<|>234<|>262<|>226<|>233<|>6391<|>dataset_landing_page
8004<|>https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh<|>Yoon Kim's script<|>The non-English data (Czech, French, German, Russian, and Spanish) can be downloaded from Jan Botha's website https://bothameister.github.io . For ease of use you can use the Yoon Kim's script https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh , which downloads these data and saves them into the relevant folders.<|>193<|>257<|>175<|>192<|>5399<|>dataset_landing_page
8045<|>http://cocodataset.org/#home<|>Link<|>Coco: Link http://cocodataset.org/#home<|>11<|>39<|>6<|>10<|>6391<|>dataset_landing_page
8045<|>https://www.cityscapes-dataset.com<|>Link<|>Cityscapes: Link https://www.cityscapes-dataset.com<|>17<|>51<|>12<|>16<|>2741<|>dataset_landing_page
8050<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>LIBSVM library<|>Each .ipynb file corresponds to the particular set of experiments with given dimension of the problem (for Nesterov's function) or given dataset. All datasets are taken from LIBSVM library https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html .<|>189<|>256<|>174<|>188<|>5624<|>dataset_landing_page
8054<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>Higgs<|>Higgs https://archive.ics.uci.edu/ml/datasets/HIGGS<|>6<|>51<|>0<|>5<|>5102<|>dataset_landing_page
8054<|>https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption<|>Power<|>Power https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption<|>6<|>93<|>0<|>5<|>4042<|>dataset_landing_page
8084<|>http://conda.pydata.org/<|>Conda<|>An additional method using Conda http://conda.pydata.org/  is also possible:<|>33<|>57<|>27<|>32<|>454<|>software
8096<|>https://www.cityscapes-dataset.com/<|>Cityscapes Dataset<|>Download the Cityscapes Dataset https://www.cityscapes-dataset.com/  as the target domain, and put it in the  folder<|>32<|>67<|>13<|>31<|>3245<|>dataset_landing_page
8119<|>http://www.cs.cmu.edu/~glai1/data/race/<|>RACE dataset<|>Pretrain our model with RACE dataset http://www.cs.cmu.edu/~glai1/data/race/  for 10 epochs.<|>37<|>76<|>24<|>36<|>4376<|>dataset_landing_page
8141<|>http://datashare.is.ed.ac.uk/handle/10283/1942<|>Edinburgh DataShare<|>In this work, two speech enhancement datasets are used. The first one is device recorded version of VCTK corpus which can be found in Edinburgh DataShare https://doi.org/10.7488/ds/2316 . The second dataset is (Valentini et al. 2016) http://ssw9.net/papers/ssw9_PS2-4_Valentini-Botinhao.pdf  which also can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . However, the following script downloads and prepares the second dataset for TensorFlow format:<|>339<|>385<|>319<|>338<|>4200<|>dataset_landing_page
8196<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI raw dataset<|>For  and  tasks, the training data is KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  and you can download them by the official script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip ;<|>56<|>105<|>38<|>55<|>2741<|>software
8209<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>Kitti Stereo 2015<|>Please go to directory , and run  to download Kitti Stereo 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  and Kitti Raw http://www.cvlibs.net/datasets/kitti/raw_data.php  datasets.<|>64<|>137<|>46<|>63<|>2741<|>dataset_landing_page
8209<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>Kitti Raw<|>Please go to directory , and run  to download Kitti Stereo 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  and Kitti Raw http://www.cvlibs.net/datasets/kitti/raw_data.php  datasets.<|>153<|>202<|>143<|>152<|>2741<|>software
8214<|>https://www.cityscapes-dataset.com/<|>CityScapes dataset for urban scenes<|>Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In RTSeg, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The code and the experimental results are presented on the CityScapes dataset for urban scenes https://www.cityscapes-dataset.com/ .<|>659<|>694<|>623<|>658<|>3245<|>dataset_landing_page
8259<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry benchmark<|>KITTI Odometry benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  contains 22 stereo sequences, in which 11 sequences are provided with ground truth. The 11 sequences are used for evaluation or training of visual odometry.<|>25<|>79<|>0<|>24<|>1355<|>other
8259<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI Driving Dataset<|>The main dataset used in this project is KITTI Driving Dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . Please follow the instruction in  to prepare the required dataset.<|>63<|>112<|>41<|>62<|>2741<|>software
8273<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
8465<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>Leaderboard Link<|>Leaderboard Link http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>17<|>90<|>0<|>16<|>2741<|>dataset_landing_page
8646<|>http://cocodataset.org/#download<|>coco<|>Download the coco http://cocodataset.org/#download  image data. Extract them to .<|>18<|>50<|>13<|>17<|>6212<|>dataset_landing_page
8650<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from http://www.cvlibs.net/datasets/kitti/eval_odometry.php http://www.cvlibs.net/datasets/kitti/eval_odometry.php  using:<|>144<|>198<|>89<|>143<|>1355<|>other
8775<|>https://www.cityscapes-dataset.com/<|>Cityscapes training set<|>| dataset | example | | --- | --- | |  400 images from CMP Facades dataset http://cmp.felk.cvut.cz/~tylecr1/facade/ . (31MB)  Pre-trained: BtoA https://mega.nz/#!H0AmER7Y!pBHcH4M11eiHBmJEWvGr-E_jxK4jluKBUlbfyLSKgpY  |  | |  2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . (113M)  Pre-trained: AtoB https://mega.nz/#!K1hXlbJA!rrZuEnL3nqOcRhjb-AnSkK0Ggf9NibhDymLOkhzwuQk BtoA https://mega.nz/#!y1YxxB5D!1817IXQFcydjDdhk_ILbCourhA6WSYRttKLrGE97q7k  |  | |  1096 training images scraped from Google Maps (246M)  Pre-trained: AtoB https://mega.nz/#!7oxklCzZ!8fRZoF3jMRS_rylCfw2RNBeewp4DFPVE_tSCjCKr-TI BtoA https://mega.nz/#!S4AGzQJD!UH7B5SV7DJSTqKvtbFKqFkjdAh60kpdhTk9WerI-Q1I  |  | |  50k training images from UT Zappos50K dataset http://vision.cs.utexas.edu/projects/finegrained/utzap50k/ . Edges are computed by HED https://github.com/s9xie/hed  edge detector + post-processing. (2.2GB)  Pre-trained: AtoB https://mega.nz/#!u9pnmC4Q!2uHCZvHsCkHBJhHZ7xo5wI-mfekTwOK8hFPy0uBOrb4  |  | |  137K Amazon Handbag images from iGAN project https://github.com/junyanz/iGAN . Edges are computed by HED https://github.com/s9xie/hed  edge detector + post-processing. (8.6GB)  Pre-trained: AtoB https://mega.nz/#!G1xlDCIS!sFDN3ZXKLUWU1TX6Kqt7UG4Yp-eLcinmf6HVRuSHjrM  |  |<|>269<|>304<|>245<|>268<|>3245<|>dataset_landing_page
8911<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>train-v1.1.json<|>train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>16<|>82<|>0<|>15<|>6663<|>dataset_direct_link
8911<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>dev-v1.1.json<|>dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>14<|>78<|>0<|>13<|>6663<|>dataset_direct_link
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>Tense<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>807<|>876<|>801<|>806<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>TopConst<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>550<|>619<|>541<|>549<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>BShift<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>683<|>752<|>676<|>682<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SubjNum<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>935<|>1004<|>927<|>934<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>WC<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>291<|>360<|>288<|>290<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|><|>SentEval also includes a series of https://github.com/facebookresearch/SentEval/tree/master/data/probing  to evaluate what linguistic properties are encoded in your sentence embeddings:<|>35<|>104<|>0<|>0<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SentLen<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>172<|>241<|>164<|>171<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>SOMO<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1194<|>1263<|>1189<|>1193<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>ObjNum<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1066<|>1135<|>1059<|>1065<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>CoordInv<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>1322<|>1391<|>1313<|>1321<|>4567<|>dataset_landing_page
8933<|>https://github.com/facebookresearch/SentEval/tree/master/data/probing<|>TreeDepth<|>| Task | Type | #train | #test | needs_train | set_classifier | |---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:| | SentLen https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Length prediction | 100k | 10k | 1 | 1 | | WC https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word Content analysis | 100k | 10k | 1 | 1 | | TreeDepth https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Tree depth prediction | 100k | 10k | 1 | 1 | | TopConst https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Top Constituents prediction | 100k | 10k | 1 | 1 | | BShift https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Word order analysis | 100k | 10k | 1 | 1 | | Tense https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Verb tense prediction | 100k | 10k | 1 | 1 | | SubjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Subject number prediction | 100k | 10k | 1 | 1 | | ObjNum https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Object number prediction | 100k | 10k | 1 | 1 | | SOMO https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Semantic odd man out | 100k | 10k | 1 | 1 | | CoordInv https://github.com/facebookresearch/SentEval/tree/master/data/probing  | Coordination Inversion | 100k | 10k | 1 | 1 |<|>421<|>490<|>411<|>420<|>4567<|>dataset_landing_page
9134<|>https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html<|>(link)<|>: download the data (link) https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html . Partitions were already defined by the dataset authors, and we have some code to get those! Just list all the audios in a file, and set the config file: , , , .<|>27<|>97<|>20<|>26<|>4189<|>dataset_landing_page
9148<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen<|>data generator<|>are all standardized on  files with  protocol buffers. All datasets are registered and generated with the data generator https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen  and many common sequence datasets are already available for generation and use.<|>121<|>206<|>106<|>120<|>4072<|>software
9148<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py<|><|>To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example.<|>156<|>259<|>0<|>0<|>4072<|>software
9148<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py<|><|>To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example.<|>31<|>127<|>0<|>0<|>4072<|>software
9148<|>https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md<|>data generators README<|>Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .<|>36<|>131<|>13<|>35<|>4072<|>software
9285<|>http://www.nltk.org/data.html<|>the data installed<|>The code in this repository is compatible with Python 2 and Python 3. Its only other external dependency is NLTK http://www.nltk.org/install.html , with the data installed http://www.nltk.org/data.html  so that WordNet is available.<|>172<|>201<|>153<|>171<|>1258<|>software
9294<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>HIGGS<|>HIGGS https://archive.ics.uci.edu/ml/datasets/HIGGS<|>6<|>51<|>0<|>5<|>5102<|>dataset_landing_page
9294<|>http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>Other datasets<|>Other datasets http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>15<|>81<|>0<|>14<|>5102<|>dataset_landing_page
9329<|>https://www.cityscapes-dataset.com/<|>official website<|>We use the Cityscapes dataset. To train a model on the full dataset, please download it from the official website https://www.cityscapes-dataset.com/  (registration required). After downloading, please put it under the  folder in the same way the example images are provided.<|>114<|>149<|>97<|>113<|>3245<|>dataset_landing_page
9392<|>https://snap.stanford.edu/data/index.html<|>SNAP dataset collection<|>Graphs from the SNAP dataset collection https://snap.stanford.edu/data/index.html  are commonly used for graph algorithm benchmarks. We provide a tool that converts the most common SNAP graph format to the adjacency graph format that GBBS accepts. Usage example:<|>40<|>81<|>16<|>39<|>4292<|>dataset_landing_page
9473<|>https://seaborn.pydata.org/<|>seaborn<|>seaborn https://seaborn.pydata.org/  - helper plotting library for some charts<|>8<|>35<|>0<|>7<|>6833<|>software
9473<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/  - helper data manipulation library<|>7<|>33<|>0<|>6<|>3372<|>software
9503<|>http://www.garrickorchard.com/datasets/n-mnist<|>the link<|>Get the N-MNIST dataset by the link http://www.garrickorchard.com/datasets/n-mnist . Then unzip the ''Test.zip'' and ''Train.zip''.<|>36<|>82<|>27<|>35<|>2468<|>dataset_landing_page
9526<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>this repo<|>ImageNet dataset should be stored in  path and set up in the usual way (separate  and  folders with 1000 subfolders each). See this repo https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  for detailed instructions how to download and set up the dataset.<|>137<|>233<|>127<|>136<|>3215<|>dataset_landing_page
9588<|>http://cocodataset.org/#download<|>official COCO dataset website<|>Please follow the official COCO dataset website http://cocodataset.org/#download  to download the dataset. After downloading the dataset you should have the following directory structure:<|>48<|>80<|>18<|>47<|>6212<|>dataset_landing_page
9610<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI<|>For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command.<|>10<|>59<|>4<|>9<|>2741<|>software
9610<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . You will probably need to contact the administrators to be able to get it.<|>15<|>50<|>4<|>14<|>3245<|>dataset_landing_page
9610<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry<|>For pose evaluation, you need to download KITTI Odometry http://www.cvlibs.net/datasets/kitti/eval_odometry.php  dataset.<|>57<|>111<|>42<|>56<|>1355<|>other
9610<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI2015<|>For testing optical flow ground truths on KITTI, download KITTI2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  dataset. You need to download 1)  data set (2 GB), 2)  (14 GB), and 3)  (1 MB) . In addition, download semantic labels from here https://keeper.mpdl.mpg.de/f/239c2dda94e54c449401/?dl=1 . You should have the following directory structure:<|>68<|>139<|>58<|>67<|>3219<|>dataset_landing_page
9929<|>http://corpus-texmex.irisa.fr/<|>BigANN<|>To benchmark our method, we use the two standard benchmark datasets BigANN http://corpus-texmex.irisa.fr/  and Deep1b https://yadi.sk/d/11eDCm7Dsn9GA , see here https://github.com/facebookresearch/faiss/tree/master/benchs#getting-bigann  for more info on how to download. You need to indicate the path to these in lib/data.py:<|>75<|>105<|>68<|>74<|>1268<|>dataset_landing_page
9943<|>http://cocodataset.org/#home<|>MS COCO Dataset<|>MS COCO Dataset http://cocodataset.org/#home<|>16<|>44<|>0<|>15<|>6391<|>dataset_landing_page
9967<|>http://www.cs.cmu.edu/~glai1/data/race/<|>RACE: Large-scale ReAding Comprehension Dataset From Examinations<|>RACE: Large-scale ReAding Comprehension Dataset From Examinations http://www.cs.cmu.edu/~glai1/data/race/<|>66<|>105<|>0<|>65<|>4376<|>dataset_landing_page
10079<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
10208<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  -- for data structuring and manipulation,<|>7<|>32<|>0<|>6<|>67<|>software
10262<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google format<|>The -gram counts files follow the Google format http://storage.googleapis.com/books/ngrams/books/datasetsv2.html , i.e., one separate file for each distinct value of  (order) listing one gram per row. We enrich this format with a file header indicating the total number of -grams in the file (rows):<|>48<|>112<|>34<|>47<|>5496<|>dataset_landing_page
10358<|>https://github.com/wnzhang/make-ipinyou-data<|>make-ipinyou-data<|>This project is forked from make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data , slightly changing the data format and feature alignment for future use.<|>46<|>90<|>28<|>45<|>1021<|>dataset_landing_page
10359<|>https://github.com/wnzhang/make-ipinyou-data<|>make-ipinyou-data<|>The feature engineering is contributed by @weinan zhang. On his benchmark make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data , we re-organized the feature alignment and removed the  feature considering leaky problems make-ipinyou-data-refined https://github.com/Atomu2014/make-ipinyou-data .<|>92<|>136<|>74<|>91<|>1021<|>dataset_landing_page
10359<|>https://github.com/Atomu2014/make-ipinyou-data<|>make-ipinyou-data-refined<|>The feature engineering is contributed by @weinan zhang. On his benchmark make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data , we re-organized the feature alignment and removed the  feature considering leaky problems make-ipinyou-data-refined https://github.com/Atomu2014/make-ipinyou-data .<|>255<|>301<|>229<|>254<|>3084<|>software
10368<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101<|>You must have downloaded the UCF101 http://crcv.ucf.edu/data/UCF101.php  (Action Recognition Data Set)<|>36<|>71<|>29<|>35<|>2024<|>dataset_landing_page
10381<|>http://cocodataset.org<|>COCO image<|>COCO image http://cocodataset.org :<|>11<|>33<|>0<|>10<|>924<|>dataset_landing_page
10408<|>http://www.cvlibs.net/datasets/kitti/eval_object.php<|>KITTI<|>For training on KITTI http://www.cvlibs.net/datasets/kitti/eval_object.php , run:<|>22<|>74<|>16<|>21<|>2895<|>dataset_landing_page
10408<|>http://cocodataset.org/#home<|>MS COCO<|>For training on MS COCO http://cocodataset.org/#home , run:<|>24<|>52<|>16<|>23<|>6391<|>dataset_landing_page
10437<|>https://pandas.pydata.org/<|>pandas<|>To aggregate the data and convert them to various formats we use a pandas https://pandas.pydata.org/  data frame.<|>74<|>100<|>67<|>73<|>3372<|>software
10546<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>Flowers 102: http://www.robots.ox.ac.uk/~vgg/data/flowers/102/ http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>63<|>112<|>13<|>62<|>3400<|>dataset_landing_page
10550<|>http://cocodataset.org<|>COCO Dataset<|>We have tested our method on COCO Dataset http://cocodataset.org<|>42<|>64<|>29<|>41<|>924<|>dataset_landing_page
10642<|>http://pandas.pydata.org/<|>Pandas<|>Pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
10642<|>http://cocodataset.org/#home<|>COCO<|>This repository holds the code used for our recent work on domain randomization. We try to study how we can use this technique in the creation of large domain-specific synthetic datasets. We conclude that this approach can be preferable to fine-tuning  object detection CNNs on small real image datasets, after being pre-trained on huge and available datasets such as COCO http://cocodataset.org/#home .<|>373<|>401<|>368<|>372<|>6391<|>dataset_landing_page
10672<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  >= 0.18<|>7<|>32<|>0<|>6<|>67<|>software
10817<|>https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/<|>(mirror)<|>We added more aligned speech data (630h total now), thanks to the m-ailabs speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ . We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.<|>161<|>218<|>152<|>160<|>4851<|>dataset_landing_page
10817<|>https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/<|>(mirror)<|>This recipe and collection of scripts enables you to train large vocabulary German acoustic models for speaker-independent automatic speech recognition (ASR) with Kaldi http://kaldi.sourceforge.net/ . The scripts currently use three freely available German speech corpora: The Tuda-De corpus is recorded with a Microsoft Kinect and two other microphones in parallel at Technische Universität Darmstadt and has been released under a permissive license (CC-BY 4.0) http://creativecommons.org/licenses/by/4.0/ . This corpus compromises ~31h of training data per microphone and ~5h separated into development and test partitions. We also make use of the German subset from the Spoken Wikipedia Corpora (SWC) https://nats.gitlab.io/swc/ , containing about 285h of additional data and the German subset of m-ailabs read speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/  (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.<|>900<|>957<|>891<|>899<|>4851<|>dataset_landing_page
10851<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google Books Ngrams<|>Google Books Ngrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html : available also in hadoop format on amazon s3 (2.2 TB)<|>20<|>84<|>0<|>19<|>5496<|>dataset_landing_page
10889<|>https://pandas.pydata.org/<|>Pandas<|>Python 3 https://www.python.org/ , Pandas https://pandas.pydata.org/ , Matplotlib https://matplotlib.org/  and NumPy http://www.numpy.org/  for producing the plots and analyse the data.<|>42<|>68<|>35<|>41<|>3372<|>software
10916<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>COCO API https://github.com/cocodataset/cocoapi<|>9<|>47<|>0<|>8<|>5359<|>software
10924<|>http://cocodataset.org/#download<|>here<|>Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download<|>64<|>96<|>59<|>63<|>6212<|>dataset_landing_page
10928<|>http://jmcauley.ucsd.edu/data/amazon/<|>amazon-review dataset<|>For example, a Transformer https://arxiv.org/abs/1706.03762  language model for unsupervised modeling of large text datasets, such as the amazon-review dataset http://jmcauley.ucsd.edu/data/amazon/ , is implemented in PyTorch. We also support other tokenization methods, such as character or sentencepiece tokenization, and language models using various recurrent architectures.<|>160<|>197<|>138<|>159<|>4747<|>dataset_landing_page
10928<|>http://jmcauley.ucsd.edu/data/amazon/<|>amazon review dataset<|>This project uses the amazon review dataset http://jmcauley.ucsd.edu/data/amazon/  collected by J. McAuley<|>44<|>81<|>22<|>43<|>4747<|>dataset_landing_page
10928<|>http://jmcauley.ucsd.edu/data/amazon/<|>site<|>In the  folder we've provided processed copies of the Binary Stanford Sentiment Treebank (Binary SST) https://nlp.stanford.edu/sentiment/index.html , IMDB Movie Review http://ai.stanford.edu/~amaas/data/sentiment/ , and the SemEval2018 Tweet Emotion https://competitions.codalab.org/competitions/17751  datasets as part of this repository. In order to train on the amazon dataset please download the "aggressively deduplicated data" version from Julian McAuley's original site http://jmcauley.ucsd.edu/data/amazon/ . Access requests to the dataset should be approved instantly. While using the dataset make sure to load it with the  flag.<|>477<|>514<|>472<|>476<|>4747<|>dataset_landing_page
10929<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>The indoor Synthetic Dataset renders from SUNCG http://suncg.cs.princeton.edu/  and indoor Realistic Dataset comes from NYUv2 https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html . The outdooe Synthetic Dataset is vKITTI http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds  and outdoor Realistic dataset is KITTI http://www.cvlibs.net/datasets/kitti/<|>343<|>380<|>337<|>342<|>2791<|>software
10965<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
10965<|>http://www.cs.ucr.edu/~eamonn/time_series_data/<|>UCR archive<|>The data used in this project comes from the UCR archive http://www.cs.ucr.edu/~eamonn/time_series_data/ , which contains the 85 univariate time series datasets we used in our experiements.<|>57<|>104<|>45<|>56<|>5530<|>dataset_landing_page
11101<|>https://github.com/IITDBGroup/gprom/wiki/datalog_prov<|>Provenance Graphs for Datalog<|>Provenance Graphs for Datalog https://github.com/IITDBGroup/gprom/wiki/datalog_prov<|>30<|>83<|>0<|>29<|>3613<|>software
11112<|>https://github.com/caesar0301/awesome-public-datasets<|>Awesome Public Datasets<|>Awesome Public Datasets https://github.com/caesar0301/awesome-public-datasets  - An awesome list of public datasets.<|>24<|>77<|>0<|>23<|>3372<|>dataset_landing_page
11171<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>Facebook process of ImageNet<|>We follow the Facebook process of ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset . Two subfolders ("train" and "val") are included in the "/path/to/ImageNet2012". The correspding code is here https://github.com/he-y/soft-filter-pruning/blob/master/pruning_train.py#L129-L130 .<|>43<|>139<|>14<|>42<|>3215<|>dataset_landing_page
11296<|>http://ltdata1.informatik.uni-hamburg.de/sensegram/<|>pre-trained models for English, German, and Russian<|>You can downlooad pre-trained models for English, German, and Russian http://ltdata1.informatik.uni-hamburg.de/sensegram/ . Note that to run examples from the QuickStart you only need files with extensions , , and . Other files are supplementary.<|>70<|>121<|>18<|>69<|>5628<|>dataset_direct_link
11368<|>https://github.com/DavidGrangier/wikipedia-biography-dataset<|>here<|>The WikiBio data is available here https://github.com/DavidGrangier/wikipedia-biography-dataset , and the preprocessed version of the target-side data used for training is at data/wb_aligned.tar.gz https://github.com/harvardnlp/neural-template-gen/blob/master/data/wb_aligned.tar.gz . This target-side data is again preprocessed to annotate spans appearing in the corresponding database. Code for this annotation is at data/make_wikibio_labedata.py https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_wikibio_labedata.py . The source-side data can be downloaded directly from the WikiBio repo https://github.com/DavidGrangier/wikipedia-biography-dataset , and we used it unchanged; in particular the  files become our  files mentioned below.<|>35<|>95<|>30<|>34<|>6888<|>dataset_landing_page
11368<|>https://github.com/DavidGrangier/wikipedia-biography-dataset<|>WikiBio repo<|>The WikiBio data is available here https://github.com/DavidGrangier/wikipedia-biography-dataset , and the preprocessed version of the target-side data used for training is at data/wb_aligned.tar.gz https://github.com/harvardnlp/neural-template-gen/blob/master/data/wb_aligned.tar.gz . This target-side data is again preprocessed to annotate spans appearing in the corresponding database. Code for this annotation is at data/make_wikibio_labedata.py https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_wikibio_labedata.py . The source-side data can be downloaded directly from the WikiBio repo https://github.com/DavidGrangier/wikipedia-biography-dataset , and we used it unchanged; in particular the  files become our  files mentioned below.<|>613<|>673<|>600<|>612<|>6888<|>dataset_landing_page
11422<|>[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>End-to-end training with on-the-fly data processing<|>End-to-end training with on-the-fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>52<|>158<|>0<|>51<|>6471<|>other
11462<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>The boxscore-data json files can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data .<|>75<|>118<|>56<|>74<|>5459<|>dataset_landing_page
11599<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and convert the dataset to 19 categories https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py .<|>24<|>59<|>13<|>23<|>3245<|>dataset_landing_page
11694<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
11733<|>http://www.cs.ucr.edu/~eamonn/time_series_data/<|>Dr. Eamonn Keogh at University of California Riverside<|>The work of Dr. Eamonn Keogh at University of California Riverside http://www.cs.ucr.edu/~eamonn/time_series_data/  has shown that a good way to classify time series is with a k-NN algorithm using a dynamic time warping similarity measure.<|>67<|>114<|>12<|>66<|>5530<|>dataset_landing_page
11848<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>These images are from the Cityscapes https://www.cityscapes-dataset.com/  and the SVS https://svsdataset.github.io/  datasets.<|>37<|>72<|>26<|>36<|>3245<|>dataset_landing_page
11848<|>https://www.cityscapes-dataset.com/<|>Cityscapes dataset<|>For better adaptation to real world images, we have used the Cityscapes dataset https://www.cityscapes-dataset.com/ .<|>80<|>115<|>61<|>79<|>3245<|>dataset_landing_page
11948<|>https://www.cityscapes-dataset.com/<|>The Cityscapes Dataset<|>Download The Cityscapes Dataset https://www.cityscapes-dataset.com/<|>32<|>67<|>9<|>31<|>3245<|>dataset_landing_page
11962<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from http://www.cvlibs.net/datasets/kitti/eval_odometry.php http://www.cvlibs.net/datasets/kitti/eval_odometry.php  using:<|>144<|>198<|>89<|>143<|>1355<|>other
12049<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM RGB-D benchmark<|>The program supports datasets in the format of the TUM RGB-D benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  with two small additions:<|>71<|>122<|>51<|>70<|>4968<|>dataset_landing_page
12056<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry sequences<|>We use KITTI Odometry sequences http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and Oxford Robotcar dataset http://robotcar-dataset.robots.ox.ac.uk/datasets/  to evaluate the robustness of our method.<|>32<|>86<|>7<|>31<|>1355<|>other
12076<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>We trained our FutureGAN on three different datasets, MovingMNIST https://github.com/emansim/unsupervised-videos  (64×64 px), KTH Action http://www.nada.kth.se/cvap/actions/  (bicubically resized to 128×128 px), and Cityscapes https://www.cityscapes-dataset.com/  (bicubically resized to 128×128 px). For MovingMNIST and KTH Action the networks were trained to predict 6 frames conditioned on 6 input frames. For Cityscapes they were trained to predict 5 frames based on 5 input frames. All pre-trained models are available upon request, please contact sandra.aigner@tum.de.<|>227<|>262<|>216<|>226<|>3245<|>dataset_landing_page
12102<|>http://cocodataset.org/#home<|><|>we introduce  a visual complexity dataset that compromises of more than 1,400 images from seven image categories namely http://places2.csail.mit.edu/ , http://people.cs.pitt.edu/~kovashka/ads/ , http://massvis.mit.edu/ , http://cocodataset.org/#home , , https://github.com/BathVisArtData/PeopleArt , and https://github.com/BathVisArtData/PeopleArt/tree/master/JPEGImages/Suprematism . The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. The ground truth for SAVOIAS is obtained by crowdsourcing more than 37,000 pairwise comparisons of images using the forced-choice methodology and with more than 1,600 contributors using the Figure-Eight http://figure-eight.com/  crowdplatform. The resulting relative scores are then converted to absolute visual complexity scores using the Bradley-Terry method and matrix completion. When applying five state-of-the-art algorithms to analyze the visual complexity of the images in the SAVOIAS dataset, we found that the scores obtained from these baseline tools only correlate well with crowd-sourced labels for abstract patterns in the Suprematism category (Pearson correlation r=0.84). For the other categories, in particular, the objects and advertisement categories, low correlation coefficients were revealed (r=0.3 and 0.56, respectively). These findings suggest that (1) state-of-the-art approaches are mostly insufficient and (2) SAVOIAS enables category-specific method development, which is likely to improve the impact of visual complexity analysis on specific application areas, including computer vision.<|>221<|>249<|>0<|>0<|>6391<|>dataset_landing_page
12125<|>https://www.cityscapes-dataset.com/<|>Cityscapes training set<|>: 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . [ Citation datasets/bibtex/cityscapes.tex ]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to  for more detail.<|>47<|>82<|>23<|>46<|>3245<|>dataset_landing_page
12185<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . You will probably need to contact the administrators to be able to get it. Then run the following command<|>15<|>50<|>4<|>14<|>3245<|>dataset_landing_page
12185<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI<|>For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command. The  option will save resized copies of groundtruth to help you setting hyper parameters. The  will dump the sequence pose in the same format as Odometry dataset (see pose evaluation)<|>10<|>59<|>4<|>9<|>2741<|>software
12185<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>Odometry dataset<|>Pose evaluation is also available on Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php . Be sure to download both color images and pose !<|>54<|>108<|>37<|>53<|>1355<|>other
12208<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI 2015<|>You would need to download all of the KITTI raw data http://www.cvlibs.net/datasets/kitti/raw_data.php  and calibration files to train the model. You would also need the training files of KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo  and KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  for validating the models.<|>290<|>363<|>279<|>289<|>2741<|>dataset_landing_page
12208<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI raw data<|>You would need to download all of the KITTI raw data http://www.cvlibs.net/datasets/kitti/raw_data.php  and calibration files to train the model. You would also need the training files of KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo  and KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  for validating the models.<|>53<|>102<|>38<|>52<|>2741<|>software
12273<|>http://pandas.pydata.org/<|>pandas<|>Sequeval requires numpy http://www.numpy.org/ , pandas http://pandas.pydata.org/ , pytimeparse https://github.com/wroberts/pytimeparse , and scipy http://www.scipy.org/ .<|>55<|>80<|>48<|>54<|>67<|>software
12293<|>http://cocodataset.org<|>COCO<|>In Mayo, we decouple the description of each neural network application into three separate components: the dataset, model and trainer, each written in YAML http://yaml.org . The reason for decoupling is to encourage reuse, for instance a ResNet-50 model can not only be used for ImageNet http://www.image-net.org  classification, but also object detection with COCO http://cocodataset.org , or even customized tasks.<|>367<|>389<|>362<|>366<|>924<|>dataset_landing_page
12361<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>Euroc<|>To run StructVIO on the Euroc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  datasets, download the configuration file euroc_data.yaml https://github.com/danping/structvio/blob/master/euroc_data.yaml  and type<|>30<|>106<|>24<|>29<|>6943<|>dataset_landing_page
12383<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>Code is included for training the PredNet on the raw KITTI http://www.cvlibs.net/datasets/kitti/  dataset. We include code for downloading and processing the data, as well as training and evaluating the model. The preprocessed data and can also be downloaded directly using  and the  by running . The model download will include the original weights trained for t+1 prediction, the fine-tuned weights trained to extrapolate predictions for multiple timesteps, and the "L all " weights trained with an 0.1 loss weight on upper layers (see paper for details).<|>59<|>96<|>53<|>58<|>2791<|>software
12453<|>http://www.cvlibs.net/datasets/kitti/eval_object.php<|>Kitti Object Detection Dataset<|>KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset http://www.cvlibs.net/datasets/kitti/eval_object.php . A detailed description of Fastbox can be found in our MultiNet paper https://arxiv.org/abs/1612.07695 .<|>101<|>153<|>70<|>100<|>2895<|>dataset_landing_page
12585<|>https://fasttext.cc/docs/en/dataset.html#content<|>YFCC100M data<|>The preprocessed YFCC100M data https://fasttext.cc/docs/en/dataset.html#content  used in [2].<|>31<|>79<|>17<|>30<|>2449<|>dataset_landing_page
12590<|>https://motchallenge.net/data/MOT15/<|>MOT15<|>Our method can be evaluated on MOT17 https://motchallenge.net/data/MOT17/ , MOT15 https://motchallenge.net/data/MOT15/  and UA-DETRAC https://detrac-db.rit.albany.edu/ .<|>82<|>118<|>76<|>81<|>1690<|>dataset_landing_page
12619<|>http://graphics.stanford.edu/data/3Dscanrep/<|>Stanford bunny<|>We apply our method to implicit surface reconstruction. We reconstruct the Stanford bunny http://graphics.stanford.edu/data/3Dscanrep/  from noisy surface normals (i.e. gradients), which is difficult to do using traditional spline methods. Note that, as we are using roughly 40 thousand points and gradients, reconstruction will take a bit of time. Run  located in  to recreate the following plot (figure 8 in the appendix).<|>90<|>134<|>75<|>89<|>4833<|>dataset_landing_page
12633<|>http://cocodataset.org/#download<|>here<|>Download the MSCOCO2014 dataset here http://cocodataset.org/#download .<|>37<|>69<|>32<|>36<|>6212<|>dataset_landing_page
12650<|>http://groups.csail.mit.edu/vision/datasets/ADE20K/<|>ADE20K<|>: We provide ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K/  as an example.<|>20<|>71<|>13<|>19<|>6346<|>dataset_landing_page
12705<|>https://github.com/NorThanapon/dict-definition/tree/master/data<|>Data<|>For detail of the data, see Data https://github.com/NorThanapon/dict-definition/tree/master/data<|>33<|>96<|>28<|>32<|>3328<|>dataset_landing_page
12738<|>https://pandas.pydata.org<|><|> https://pandas.pydata.org<|>1<|>26<|>0<|>0<|>4469<|>software
12776<|>https://github.com/harvardnlp/sent-conv-torch/tree/master/data<|>https://github.com/harvardnlp/sent-conv-torch/tree/master/data<|>For most dataset, it could be downloaded from https://github.com/harvardnlp/sent-conv-torch/tree/master/data https://github.com/harvardnlp/sent-conv-torch/tree/master/data<|>109<|>171<|>46<|>108<|>2795<|>dataset_landing_page
12793<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
12900<|>http://numba.pydata.org/<|>numba<|>spyn is build upon numpy http://www.numpy.org/ , sklearn http://scikit-learn.org/stable/ , scipy http://www.scipy.org/ , numba http://numba.pydata.org/ , matplotlib http://matplotlib.org/  and theano http://deeplearning.net/software/theano/ .<|>127<|>151<|>121<|>126<|>2124<|>software
13006<|>http://conda.pydata.org/<|>conda<|>If you are new to Python, the easiest way of installing the prerequisites is via conda https://conda.io/docs/index.html . After installing conda http://conda.pydata.org/ , run the following command to create a new environment https://conda.io/docs/user-guide/tasks/manage-environments.html  named  and install all prerequisites:<|>145<|>169<|>139<|>144<|>454<|>software
13050<|>http://cocodataset.org/#download<|>MS COCO 2017<|>Download the datasets of Pascal VOC 2007 & 2012 http://host.robots.ox.ac.uk/pascal/VOC/ , MS COCO 2017 http://cocodataset.org/#download  and COCO-Text http://rrc.cvc.uab.es/?ch=5&com=introduction .<|>103<|>135<|>90<|>102<|>6212<|>dataset_landing_page
13128<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and convert the dataset to 19 categories https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py . It should have this basic structure.<|>24<|>59<|>13<|>23<|>3245<|>dataset_landing_page
13147<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB51<|>We have successfully trained on Kinetics https://deepmind.com/research/open-source/open-source-datasets/kinetics/ , UCF101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ , Something-Something-V1 https://20bn.com/datasets/something-something/v1  and V2 https://20bn.com/datasets/something-something/v2 , Jester https://20bn.com/datasets/jester  datasets with this codebase. Basically, the processing of video data can be summarized into 3 steps:<|>168<|>244<|>161<|>167<|>2024<|>dataset_landing_page
13147<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101<|>We have successfully trained on Kinetics https://deepmind.com/research/open-source/open-source-datasets/kinetics/ , UCF101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ , Something-Something-V1 https://20bn.com/datasets/something-something/v1  and V2 https://20bn.com/datasets/something-something/v2 , Jester https://20bn.com/datasets/jester  datasets with this codebase. Basically, the processing of video data can be summarized into 3 steps:<|>123<|>158<|>116<|>122<|>2024<|>dataset_landing_page
13170<|>http://cocodataset.org/#download<|>Link<|>The resources required to reproduce results are kept in the directory . For training and testing, we used MSCOCO-2014 train images from  and validation images from . These zipped archives are downloadable from MSCOCO website ( Link http://cocodataset.org/#download ). Please find the exact list of images (with annotations) used for "training" in . The lists of images used for "testing" different ZSL settings are:<|>232<|>264<|>227<|>231<|>6212<|>dataset_landing_page
13170<|>http://cocodataset.org/#download<|>Link<|>Extract the dataset  and  inside the folder Dataset. These files are downloadable from Link http://cocodataset.org/#download . Make sure the pre-trained model is present inside the Model folder ('Model/resnet50_csv_50_focal_seen_w2v.h5'). This pre-trained model is trained by focal loss on 65 seen classes without considering any vocabulary metric. This model is available to download from ( Link to pre-trained model for training (h5 format) https://www.dropbox.com/s/dc0vit1dj83rd56/resnet50_csv_50_focal_seen_w2v.h5?dl=0 ). Also, make sure the  folder is already created to store intermediate models of each epoch. Then, run the following commands for training and testing.<|>92<|>124<|>87<|>91<|>6212<|>dataset_landing_page
13221<|>http://cocodataset.org/#download<|>COCO2017<|>1.3 Download COCO2017 http://cocodataset.org/#download .<|>22<|>54<|>13<|>21<|>6212<|>dataset_landing_page
13225<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB<|>Download UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and organize the image files (from the videos) as follows:<|>62<|>138<|>57<|>61<|>2024<|>dataset_landing_page
13225<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101<|>Download UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and organize the image files (from the videos) as follows:<|>16<|>51<|>9<|>15<|>2024<|>dataset_landing_page
13329<|>http://cocodataset.org/#home<|>MS COCO<|>The model requires MS COCO http://cocodataset.org/#home  and the CocoAPI https://github.com/waleedka/coco  to be added to .<|>27<|>55<|>19<|>26<|>6391<|>dataset_landing_page
13349<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>here<|>Download the dataset (colour images) from here http://www.cvlibs.net/datasets/kitti/eval_odometry.php .<|>47<|>101<|>42<|>46<|>1355<|>other
13349<|>https://www.cityscapes-dataset.com<|>Cityscapes Dataset<|>There are 2 separate weights files. There are files for both  and  Bayesian SegNet trained on the KITTI Semantic Dataset http://www.cvlibs.net/datasets/kitti/eval_semantics.php ; these weights were first trained using the Cityscapes Dataset https://www.cityscapes-dataset.com , and were then fine tuned. All weights have the batch normalization layer merged with the preceding convolutional layer in order to speed up inference.<|>241<|>275<|>222<|>240<|>2741<|>dataset_landing_page
13349<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI dataset<|>SIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see here #slam-and-localization-modes .<|>361<|>415<|>347<|>360<|>1355<|>other
13349<|>http://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM dataset<|>SIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see here #slam-and-localization-modes .<|>460<|>510<|>448<|>459<|>1355<|>dataset_landing_page
13349<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC dataset<|>SIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see here #slam-and-localization-modes .<|>560<|>635<|>546<|>559<|>1355<|>dataset_landing_page
13350<|>https://www.cityscapes-dataset.com/<|>here<|>Cityscapes is a dataset that can be used to train SegNet/Bayesian SegNet, but a few steps must be done first. You can download the dataset here https://www.cityscapes-dataset.com/  and the Cityscape scripts repo here https://github.com/mcordts/cityscapesScripts . Once downloaded, follow these steps:<|>144<|>179<|>139<|>143<|>3245<|>dataset_landing_page
13382<|>http://cocodataset.org/#download<|>MSCOCO2017<|>DensePoseData https://drive.google.com/open?id=1WiTLYVIgMyCDENXHPVEWW7qbZ-3EBjbt (using original MSCOCO2017 http://cocodataset.org/#download  images)<|>108<|>140<|>97<|>107<|>6212<|>dataset_landing_page
13541<|>https://visualdialog.org/data<|>here<|>batra-mlp-lab https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch  provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VisDial v1.0 images from here https://visualdialog.org/data  instead. Extracted features for v1.0 train, val and test are available for download at these links. Note that these files do not contain the bounding box information.<|>304<|>333<|>299<|>303<|>6794<|>dataset_landing_page
13541<|>http://cocodataset.org/#download<|>MSCOCO<|>Prepare the MSCOCO http://cocodataset.org/#download  and Flickr https://visualdialog.org/data  images.<|>19<|>51<|>12<|>18<|>6212<|>dataset_landing_page
13541<|>https://visualdialog.org/data<|>Flickr<|>Prepare the MSCOCO http://cocodataset.org/#download  and Flickr https://visualdialog.org/data  images.<|>64<|>93<|>57<|>63<|>6794<|>dataset_landing_page
13541<|>https://visualdialog.org/data<|>here<|>Download the VisDial v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.<|>54<|>83<|>49<|>53<|>6794<|>dataset_landing_page
13584<|>https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions<|>does not<|>Symbolic programming (e.g. ) does not https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions  offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of  with various autoparallelization strategies.<|>38<|>140<|>29<|>37<|>2346<|>other
13584<|>https://github.com/tensorpack/dataflow<|><|>Squeeze the best data loading performance of Python with https://github.com/tensorpack/dataflow .<|>57<|>95<|>0<|>0<|>2346<|>software
13597<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>COCO API https://github.com/cocodataset/cocoapi<|>9<|>47<|>0<|>8<|>5359<|>software
13597<|>https://github.com/cocodataset/cocoapi<|>cocoapi<|>As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use cocoapi https://github.com/cocodataset/cocoapi  or poseval https://github.com/leonid-pishchulin/poseval  to evaluate result on the MS COCO or PoseTrack dataset. You have to convert the produced  file to MPII  format to evaluate on MPII dataset following this http://human-pose.mpi-inf.mpg.de/#evaluation .<|>101<|>139<|>93<|>100<|>5359<|>software
13598<|>https://github.com/cocodataset/cocoapi<|>cocoapi<|>As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use cocoapi https://github.com/cocodataset/cocoapi  or poseval https://github.com/leonid-pishchulin/poseval  to evaluate result on the MS COCO or PoseTrack dataset. You have to convert the produced  file to MPII  format to evaluate on MPII dataset following this http://human-pose.mpi-inf.mpg.de/#evaluation .<|>101<|>139<|>93<|>100<|>5359<|>software
13598<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>COCO API https://github.com/cocodataset/cocoapi<|>9<|>47<|>0<|>8<|>5359<|>software
13604<|>https://pandas.pydata.org/<|>pandas<|>The script takes the meshes generated in the previous step and evaluates them using a standardized protocol. The output will be written to /  files in the corresponding generation folder which can be processed using pandas https://pandas.pydata.org/ .<|>223<|>249<|>216<|>222<|>3372<|>software
13632<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
13635<|>https://www.wikidata.org<|>Wikidata<|>We first downloaded from Wikidata https://www.wikidata.org  the URIs of all the cities and the associated countries available on GeoNames https://www.geonames.org , as well as their population, if available. To this purpose, we exploited the  script.<|>34<|>58<|>25<|>33<|>6356<|>other
13654<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>KITTI http://www.cvlibs.net/datasets/kitti/ :<|>6<|>43<|>0<|>5<|>2791<|>software
13654<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM-RGBD<|>Try it out on NYU-Depth https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html , ScanNet http://www.scan-net.org/ , TUM-RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset , or KITTI http://www.cvlibs.net/datasets/kitti/ . Using more keyframes  reduces drift but results in slower tracking.<|>127<|>178<|>118<|>126<|>4968<|>dataset_landing_page
13654<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>Try it out on NYU-Depth https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html , ScanNet http://www.scan-net.org/ , TUM-RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset , or KITTI http://www.cvlibs.net/datasets/kitti/ . Using more keyframes  reduces drift but results in slower tracking.<|>190<|>227<|>184<|>189<|>2791<|>software
13661<|>https://fcav.engin.umich.edu/sim-dataset/<|>Sim10k<|>: Website Sim10k https://fcav.engin.umich.edu/sim-dataset/<|>17<|>58<|>10<|>16<|>2895<|>dataset_landing_page
13661<|>https://fcav.engin.umich.edu/sim-dataset/<|>Sim10k<|>All codes are written to fit for the format of PASCAL_VOC. For example, the dataset Sim10k https://fcav.engin.umich.edu/sim-dataset/  is stored as follows.<|>91<|>132<|>84<|>90<|>2895<|>dataset_landing_page
13661<|>https://www.cityscapes-dataset.com/<|>Cityscape<|>: Download website Cityscape https://www.cityscapes-dataset.com/ , see dataset preparation code in DA-Faster RCNN https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data<|>29<|>64<|>19<|>28<|>3245<|>dataset_landing_page
13746<|>https://github.com/v-m/PropagationAnalysis-dataset<|>here<|>Dataset generated for those papers can be found here https://github.com/v-m/PropagationAnalysis-dataset .<|>53<|>103<|>48<|>52<|>1574<|>dataset_landing_page
13818<|>http://cocodataset.org<|>COCO val2017<|>measures average inference time per image on COCO val2017 http://cocodataset.org  dataset using a AWS p3.2xlarge https://aws.amazon.com/ec2/instance-types/p3/  V100 instance at batch-size 32.<|>58<|>80<|>45<|>57<|>924<|>dataset_landing_page
13818<|>http://cocodataset.org<|>COCO val2017<|>denotes mAP@0.5:0.95 metric measured on the 5000-image COCO val2017 http://cocodataset.org  dataset over various inference sizes from 256 to 1536.<|>68<|>90<|>55<|>67<|>924<|>dataset_landing_page
13818<|>http://cocodataset.org<|>COCO val2017<|>values are for single-model single-scale on COCO val2017 http://cocodataset.org  dataset. Reproduce by<|>57<|>79<|>44<|>56<|>924<|>dataset_landing_page
13863<|>http://corpus-texmex.irisa.fr/<|>SIFT1M<|>Quicker ADC achieves excellent performance outperforming polysemous codes in numerous configurations. We evaluated its performance for exhaustive search in the SIFT1M http://corpus-texmex.irisa.fr/  dataset with both 64-bit and 128-bit codes.<|>167<|>197<|>160<|>166<|>1268<|>dataset_landing_page
13863<|>http://corpus-texmex.irisa.fr/<|>SIFT1000M<|>We also evaluated its performance for index-based (i.e., non-exhaustive) search on the SIFT1000M http://corpus-texmex.irisa.fr/  dataset and Deep1B http://sites.skoltech.ru/compvision/noimi/  dataset. For this evaluation, we used three different type of indexes ( standard IVF https://doi.org/10.1109/TPAMI.2010.57 , Inverted Multi-Index https://doi.org/10.1109/TPAMI.2014.2361319 , and HNSW-based IVFs https://arxiv.org/abs/1603.09320 ). For each combination of a product quantizer implementation with an index, we plot the curve that gives the achievable recall (R@1 and R@100) given a query time budget. Quicker ADC is particularly efficient for low query times, R@100 and Deep1B dataset.<|>97<|>127<|>87<|>96<|>1268<|>dataset_landing_page
13969<|>http://people.ee.ethz.ch/~ihnatova/#dataset<|>DPED dataset<|>Download DPED dataset http://people.ee.ethz.ch/~ihnatova/#dataset  (patches for CNN training) and extract it into  folder. This folder should contain three subolders: ,  and , but only  is needed.<|>22<|>65<|>9<|>21<|>4309<|>dataset_landing_page
13972<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>KITTI http://www.cvlibs.net/datasets/kitti/ : copy the raw data to a folder with the path '../kitti'. Our method expects dense input depth maps, therefore, you need to run a depth inpainting method https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html  on the Lidar data. For our experiments, we used our Python re-implmentaiton https://gist.github.com/ialhashim/be6235489a9c43c6d240e8331836586a  of the Matlab code provided with NYU Depth V2 toolbox. The entire 80K images took 2 hours on an 80 nodes cluster for inpainting. For our training, we used the subset defined here https://s3-eu-west-1.amazonaws.com/densedepth/kitti_train.csv .<|>6<|>43<|>0<|>5<|>2791<|>software
13985<|>http://cocodataset.org/#download<|>COCO website<|>Download images from COCO website http://cocodataset.org/#download , and put train2014/val2014 splits into  respectively.<|>34<|>66<|>21<|>33<|>6212<|>dataset_landing_page
13985<|>https://github.com/cocodataset/cocoapi<|>cocoapi website<|>Install COCOAPI referring to cocoapi website https://github.com/cocodataset/cocoapi , or:<|>45<|>83<|>29<|>44<|>5359<|>software
14016<|>http://cocodataset.org/#download<|>here<|>obtain the train and validation images from the 2014 split here http://cocodataset.org/#download , extract and save them in  and<|>64<|>96<|>59<|>63<|>6212<|>dataset_landing_page
14101<|>http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html<|>notMNIST<|>You need to download the notMNIST http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html  dataset from here http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz .<|>34<|>94<|>25<|>33<|>6871<|>dataset_landing_page
14163<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry dataset<|>Download KITTI Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to YOUR_DATASET_FOLDER. Take sequences 00 for example, Open two terminals, run vins and rviz respectively. (We evaluated odometry on KITTI benchmark without loop closure funtion)<|>32<|>86<|>9<|>31<|>1355<|>other
14163<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI raw dataset<|>Download KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  to YOUR_DATASET_FOLDER. Take 2011_10_03_drive_0027_synced https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_sync.zip  for example. Open three terminals, run vins, global fusion and rviz respectively. Green path is VIO odometry; blue path is odometry under GPS global fusion.<|>27<|>76<|>9<|>26<|>2741<|>software
14163<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry Benchmark<|>We are the  open-sourced stereo algorithm on KITTI Odometry Benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  (12.Jan.2019).<|>70<|>124<|>45<|>69<|>1355<|>other
14163<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC MAV Dataset<|>Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  to YOUR_DATASET_FOLDER. Take MH_01 for example, you can run VINS-Fusion with three sensor types (monocular camera + IMU, stereo cameras + IMU and stereo cameras). Open four terminals, run vins odometry, visual loop closure(optional), rviz and play the bag file respectively. Green path is VIO odometry; red path is odometry under visual loop closure.<|>27<|>102<|>9<|>26<|>1355<|>dataset_landing_page
14168<|>http://onedata.org<|>Onedata<|>Onedata http://onedata.org<|>8<|>26<|>0<|>7<|>6582<|>other
14270<|>http://cocodataset.org/#download<|>here<|>Download the COCO train2014 and val2014 data here http://cocodataset.org/#download . Put the COCO train2014 images in the folder , and put the file  in the folder . Similarly, put the COCO val2014 images in the folder , and put the file  in the folder . Furthermore, download the pretrained VGG16 net here https://app.box.com/s/idt5khauxsamcg3y69jz13w6sc6122ph  or ResNet50 net here https://app.box.com/s/17vthb1zl0zeh340m4gaw0luuf2vscne  if you want to use it to initialize the CNN part.<|>50<|>82<|>45<|>49<|>6212<|>dataset_landing_page
14300<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
14300<|>https://seaborn.pydata.org/<|>Seaborn<|>Seaborn https://seaborn.pydata.org/<|>8<|>35<|>0<|>7<|>6833<|>software
14340<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>UCI Machine Learning Repository<|>In our experiments, we used a random subset of the HIGGS dataset from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/HIGGS . This subset can be downloaded in HDF5 format from here https://www.dropbox.com/s/x7qdf9bmsvfezl9/HIGGSsubset.zip?dl=0 . HDF5 format is convenient for sequential tests since it allows constant time sampling.<|>106<|>151<|>74<|>105<|>5102<|>dataset_landing_page
14353<|>https://snap.stanford.edu/data/egonets-Facebook.html<|>Facebook<|>Facebook https://snap.stanford.edu/data/egonets-Facebook.html  (combined network)<|>9<|>61<|>0<|>8<|>1968<|>dataset_landing_page
14380<|>http://cocodataset.org/#download<|>coco website<|>Download annotation files (2017 train/val and test image info) from coco website http://cocodataset.org/#download .<|>81<|>113<|>68<|>80<|>6212<|>dataset_landing_page
14380<|>http://cocodataset.org/#download<|>coco website<|>Download the images (2017 Train, 2017 Val, 2017 Test) from coco website http://cocodataset.org/#download .<|>72<|>104<|>59<|>71<|>6212<|>dataset_landing_page
14399<|>http://cocodataset.org<|>COCO<|>Currently, models are mostly implemented on Gluon and then ported to other frameworks. Some models are pretrained on ImageNet-1K http://www.image-net.org , CIFAR-10/100 https://www.cs.toronto.edu/~kriz/cifar.html , SVHN http://ufldl.stanford.edu/housenumbers , CUB-200-2011 http://www.vision.caltech.edu/visipedia/CUB-200-2011.html , Pascal VOC2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012 , ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K , Cityscapes https://www.cityscapes-dataset.com , and COCO http://cocodataset.org  datasets. All pretrained weights are loaded automatically during use. See examples of such automatic loading of weights in the corresponding sections of the documentation dedicated to a particular package:<|>515<|>537<|>510<|>514<|>924<|>dataset_landing_page
14399<|>https://www.cityscapes-dataset.com<|>Cityscapes<|>Currently, models are mostly implemented on Gluon and then ported to other frameworks. Some models are pretrained on ImageNet-1K http://www.image-net.org , CIFAR-10/100 https://www.cs.toronto.edu/~kriz/cifar.html , SVHN http://ufldl.stanford.edu/housenumbers , CUB-200-2011 http://www.vision.caltech.edu/visipedia/CUB-200-2011.html , Pascal VOC2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012 , ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K , Cityscapes https://www.cityscapes-dataset.com , and COCO http://cocodataset.org  datasets. All pretrained weights are loaded automatically during use. See examples of such automatic loading of weights in the corresponding sections of the documentation dedicated to a particular package:<|>469<|>503<|>458<|>468<|>2741<|>dataset_landing_page
14476<|>https://github.com/harvardnlp/boxscore-data<|>Data-to-Text Datasets<|>This dataset is derived from one of the Data-to-Text Datasets https://github.com/harvardnlp/boxscore-data  (RotoWire) proposed in the paper (Wiseman et al., 2017) Challenges in Data-to-Document Generation https://arxiv.org/abs/1707.08052 , which is for NBA game report generation. The original data can be downloaded from here https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2?raw=true .<|>62<|>105<|>40<|>61<|>5459<|>dataset_landing_page
14478<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>set of scripts<|>ImageNet needs to be downloaded http://image-net.org/download  manually, due to copyright issues. Facebook has created a set of scripts https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to help download and extract the dataset.<|>136<|>232<|>121<|>135<|>3215<|>dataset_landing_page
14608<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
14698<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI vision benchmark suite<|>Otherwise, I started by reading in all the  and  images, around 8000 images in each category. These datasets are comprised of images taken from the GTI vehicle image database http://www.gti.ssr.upm.es/data/Vehicle_database.html  and KITTI vision benchmark suite http://www.cvlibs.net/datasets/kitti/ . Here is an example of one of each of the  and  classes:<|>262<|>299<|>233<|>261<|>2791<|>software
14774<|>http://pandas.pydata.org/<|>Pandas<|>Pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
14850<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>Euroc MAV dataset<|>EuRoC: Euroc MAV dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>25<|>101<|>7<|>24<|>6943<|>dataset_landing_page
14850<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>Kitti odometry<|>Kitti: Kitti odometry http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>22<|>76<|>7<|>21<|>1355<|>other
14851<|>http://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>403<|>453<|>391<|>402<|>1355<|>dataset_landing_page
14851<|>http://vision.in.tum.de/data/datasets/rgbd-dataset/tools<|>associate.py<|>Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:<|>75<|>131<|>62<|>74<|>1355<|>other
14851<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>304<|>358<|>290<|>303<|>1355<|>other
14851<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>503<|>578<|>489<|>502<|>1355<|>dataset_landing_page
15026<|>http://conda.pydata.org/<|>conda<|>If you are new to Python, the easiest way of installing the prerequisites is via conda https://conda.io/docs/index.html . After installing conda http://conda.pydata.org/ , run the following command to create a new environment https://conda.io/docs/user-guide/tasks/manage-environments.html  named  and install all prerequisites:<|>145<|>169<|>139<|>144<|>454<|>software
15090<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>| Name | Channels | Description | | ------- |:------------------:|:-------------:| | KITTI http://www.cvlibs.net/datasets/kitti/  | Stereo,Pose | | | TUMMono https://vision.in.tum.de/data/datasets/mono-dataset  | Monocular | | | TUMRGBD https://vision.in.tum.de/data/datasets/rgbd-dataset  | RGBD,Pose || | EuRoc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  | IMU,Stereo || | NPUDroneMap http://zhaoyong.adv-ci.com/downloads/npu-dronemap-dataset/ | GPS,Monocular || | CVMono | Monocular | Online camera or video dataset using opencv.|<|>91<|>128<|>85<|>90<|>2791<|>software
15090<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUMRGBD<|>| Name | Channels | Description | | ------- |:------------------:|:-------------:| | KITTI http://www.cvlibs.net/datasets/kitti/  | Stereo,Pose | | | TUMMono https://vision.in.tum.de/data/datasets/mono-dataset  | Monocular | | | TUMRGBD https://vision.in.tum.de/data/datasets/rgbd-dataset  | RGBD,Pose || | EuRoc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  | IMU,Stereo || | NPUDroneMap http://zhaoyong.adv-ci.com/downloads/npu-dronemap-dataset/ | GPS,Monocular || | CVMono | Monocular | Online camera or video dataset using opencv.|<|>237<|>288<|>229<|>236<|>4968<|>dataset_landing_page
15090<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoc<|>| Name | Channels | Description | | ------- |:------------------:|:-------------:| | KITTI http://www.cvlibs.net/datasets/kitti/  | Stereo,Pose | | | TUMMono https://vision.in.tum.de/data/datasets/mono-dataset  | Monocular | | | TUMRGBD https://vision.in.tum.de/data/datasets/rgbd-dataset  | RGBD,Pose || | EuRoc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  | IMU,Stereo || | NPUDroneMap http://zhaoyong.adv-ci.com/downloads/npu-dronemap-dataset/ | GPS,Monocular || | CVMono | Monocular | Online camera or video dataset using opencv.|<|>313<|>389<|>307<|>312<|>6943<|>dataset_landing_page
15170<|>http://pandas.pydata.org/<|>Pandas<|>Pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
15171<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>Given that you have already downloaded the KITTI http://www.cvlibs.net/datasets/kitti/  odometry and raw datasets, the provided python script  is able to generate the training data with SIFT feature matches. Yet, the feature and match files are in accord with our internal format, which are not publicly available at this point. Alternatively, we suggest first generating the concatenated image triplets by<|>49<|>86<|>43<|>48<|>2791<|>software
15223<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI raw dataset<|>You need to download KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  first, then the raw data is processed with the following three steps:<|>39<|>88<|>21<|>38<|>2741<|>software
15368<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>KITTI Tracking<|>Download the dataset from KITTI Tracking http://www.cvlibs.net/datasets/kitti/eval_tracking.php .<|>41<|>95<|>26<|>40<|>5125<|>dataset_landing_page
15446<|>https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json<|>here<|>Download the vocabulary file from the original visual dialog codebase ( github https://github.com/batra-mlp-lab/visdial/ ). The vocabulary for the VisDial v0.9 dataset is here https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json .<|>176<|>244<|>171<|>175<|>6794<|>dataset_direct_link
15471<|>http://jmcauley.ucsd.edu/data/amazon/<|>here<|>Besides, the original Amazon datasets (including user-item interaction history and item associations) are provided by Professor Mcauley. You can download them here http://jmcauley.ucsd.edu/data/amazon/ .<|>164<|>201<|>159<|>163<|>4747<|>dataset_landing_page
15478<|>http://www.cvlibs.net/datasets/kitti/eval_object.php<|>here<|>2.2 Download the KITTI Object Detection Dataset here http://www.cvlibs.net/datasets/kitti/eval_object.php .<|>53<|>105<|>48<|>52<|>2895<|>dataset_landing_page
15487<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI 2015<|>Download Scene Flow Datasets https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo , KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>214<|>287<|>203<|>213<|>2741<|>dataset_landing_page
15519<|>https://www.cityscapes-dataset.com/<|>official website<|>Cityscapes dataset can be downloaded from the official website https://www.cityscapes-dataset.com/  (registration required).<|>63<|>98<|>46<|>62<|>3245<|>dataset_landing_page
15614<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
15631<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
15759<|>http://snap.stanford.edu/data/index.html<|>SNAP dataset<|>The following examples are based on the networks provided in the  folder of the project, taken from the SNAP dataset http://snap.stanford.edu/data/index.html .<|>117<|>157<|>104<|>116<|>269<|>dataset_landing_page
15793<|>https://www.cityscapes-dataset.com<|>Cityscapes Dataset<|>Download the Cityscapes Dataset https://www.cityscapes-dataset.com  as the target domain and unzip it to<|>32<|>66<|>13<|>31<|>2741<|>dataset_landing_page
15954<|>https://pandas.pydata.org/<|>pandas<|>In addition to the datasets we are providing the code for evaluating word embeddings. To run it, you should have Python3, and numpy http://www.numpy.org/ , pandas https://pandas.pydata.org/ , scipy https://www.scipy.org/  libraries installed.<|>163<|>189<|>156<|>162<|>3372<|>software
16059<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
16085<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI<|>KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php<|>6<|>55<|>0<|>5<|>2741<|>software
16086<|>https://www.cityscapes-dataset.com/<|>Cityscape<|>: Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:<|>46<|>81<|>36<|>45<|>3245<|>dataset_landing_page
16113<|>http://crcv.ucf.edu/data/UCF101.php<|>here<|>Download videos and train/test splits here http://crcv.ucf.edu/data/UCF101.php .<|>43<|>78<|>38<|>42<|>2024<|>dataset_landing_page
16122<|>http://www.cs.cmu.edu/~glai1/data/race/<|>here<|>RACE: Please submit a data request here http://www.cs.cmu.edu/~glai1/data/race/ . The data will be automatically sent to you. Create a "data" directory alongside "src" directory and download the data.<|>40<|>79<|>35<|>39<|>4376<|>dataset_landing_page
16181<|>http://www.nltk.org/data.html<|>instructions<|>Then install the NLTK data ( instructions http://www.nltk.org/data.html )<|>42<|>71<|>29<|>41<|>1258<|>software
16355<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>Bordes et al., 2013<|>| Baselines | Code | Embedding size | Batch size | |-------------|---------------------------------------------------------------------------|----------------|------------| | TransE ( Bordes et al., 2013 https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data ) | Link https://github.com/jimmywangheng/knowledge_representation_pytorch  | 100, 200 | 1024 | | DistMult ( Yang et al., 2015 http://scottyih.org/files/ICLR2015_updated.pdf ) | Link https://github.com/jimmywangheng/knowledge_representation_pytorch  | 100, 200 | 1024 | | ComplEx ( Trouillon et al., 2016 http://proceedings.mlr.press/v48/trouillon16.pdf ) | Link https://github.com/thunlp/OpenKE  | 50, 100, 200 | 100 | | RGCN ( Schlichtkrull et al., 2018 https://arxiv.org/pdf/1703.06103 ) | Link https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn  | 200 | Default | | ConvE ( Dettmers et al., 2018 https://arxiv.org/pdf/1707.01476.pdf ) | Link https://github.com/TimDettmers/ConvE  | 200 | 128 | | Know-Evolve ( Trivedi et al., 2017 https://arxiv.org/pdf/1705.05742 ) | Link https://github.com/rstriv/Know-Evolve  | Default | Default | | HyTE ( Dasgupta et al., 2018 http://talukdar.net/papers/emnlp2018_HyTE.pdf ) | Link https://github.com/malllabiisc/HyTE  | 128 | Default |<|>204<|>295<|>184<|>203<|>2405<|>other
16372<|>https://visualdialog.org/data<|>VisDialv1.0<|>Evaluation is done on VisDialv1.0 https://visualdialog.org/data .<|>34<|>63<|>22<|>33<|>6794<|>dataset_landing_page
16383<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI<|>translational error on the KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  odometry sequences with  an Inertial Measurement Unit.<|>33<|>87<|>27<|>32<|>1355<|>other
16403<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>We evaluate our model on images rendered from 3D object models ( ShapeNet https://www.shapenet.org/ ) as well as real and synthesized scenes ( KITTI http://www.cvlibs.net/datasets/kitti/  and Synthia http://synthia-dataset.net/ ). We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.<|>149<|>186<|>143<|>148<|>2791<|>software
16415<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>Download a sequence (ASL format) from https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets .<|>115<|>191<|>38<|>114<|>6943<|>dataset_landing_page
16415<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC dataset<|>DSM is a novel approach to monocular SLAM. It is a fully direct system that estimates the camera trajectory and a consistent global map. Is is able to detect and handle map point reobservations when revisiting already mapped areas using the same photometric model and map points. We provide examples to run the SLAM system in the EuRoC dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  and with custom videos. We also provide an optional GUI for 3D visualization of the system results.<|>344<|>420<|>330<|>343<|>6943<|>dataset_landing_page
16448<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>ETHZ EuroC Dataset<|>ETHZ EuroC Dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>19<|>95<|>0<|>18<|>6943<|>dataset_landing_page
16529<|>http://snap.stanford.edu/data/index.html<|>Stanford SNAP<|>The small graphs used in our experiments can be obtained from the Stanford SNAP http://snap.stanford.edu/data/index.html  repository. We recommend using the soc-LiveJournal graph, and have provided a python script to download this graph, symmetrize it, and store it in the the format(s) used by Aspen. This can be done using the SNAPToAdj software in the Ligra https://github.com/jshun/ligra  repository (see ).<|>80<|>120<|>66<|>79<|>269<|>dataset_landing_page
16562<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
16567<|>http://cocodataset.org/#download<|>here<|>Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download<|>64<|>96<|>59<|>63<|>6212<|>dataset_landing_page
16578<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
16582<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI's dataset<|>This version of the app assumes the LiDAR data to be stored in a binary float matrix (.bin extension). Each column is a point, where the rows are in the following order: x, y, z, and intensity (little endian). See the 3D Velodyne point clouds in KITTI's dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  for example.<|>262<|>311<|>246<|>261<|>2741<|>software
16660<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI stereo 2015 leaderboard<|>On the other hand, using the downsample layer provided in DispNet does not affect the performance of the network. In fact, the network definition  (with downsample layer) can produced a  error of 2.67% on the KITTI stereo 2015 leaderboard http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo , exactly the same as our original CRL with the in-house interpolation layer.<|>239<|>312<|>209<|>238<|>2741<|>dataset_landing_page
16675<|>https://www.cityscapes-dataset.com/<|>Cityscapes Dataset<|>Download the Cityscapes Dataset https://www.cityscapes-dataset.com/  as target dataset<|>32<|>67<|>13<|>31<|>3245<|>dataset_landing_page
16895<|>http://conda.pydata.org/miniconda.html<|>Miniconda<|>The simplest way to get started is to use Anaconda https://www.continuum.io/anaconda-overview  Python distribution. If you have limited disk space, the Miniconda http://conda.pydata.org/miniconda.html  installer is recommended. After installing Miniconda and adding the path of folder  to  variable, run the following command:<|>162<|>200<|>152<|>161<|>155<|>software
16961<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM RGB-D Benchmark<|>The provided code is not optimized, nor in an easy-to-read shape. It is provided "as is", as a prototype implementation of our paper. Use it at your own risk. Moreover, compared to the paper, this implementation lacks the features that make it able to deal with invalid measurements. Therefore, it will not produce good models for the TUM RGB-D Benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  scenes. To test it, please use our Bonn RGB-D Dynamic Dataset http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/<|>355<|>406<|>335<|>354<|>4968<|>dataset_landing_page
16961<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM RGB-D Benchmark<|>where  is the path to the directory of a dataset in the format of the TUM RGB-D Benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  (e.g. ). Some example datasets can be found here http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/ .<|>90<|>141<|>70<|>89<|>4968<|>dataset_landing_page
16998<|>http://cocodataset.org/#download<|>COCO dataset<|>If you'd like to train LightTrack, download the COCO dataset http://cocodataset.org/#download  and the PoseTrack dataset https://posetrack.net/users/download.php  first. Note that this script will take a while and dump 21gb of files into . For PoseTrack dataset, you can replicate our ablation experiment results on the validation set. You will need to register at the official website and create entries in order to submit your test results to the server.<|>61<|>93<|>48<|>60<|>6212<|>dataset_landing_page
17206<|>http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools<|>SICK<|>SICK http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools<|>5<|>70<|>0<|>4<|>791<|>dataset_landing_page
17228<|>https://www.audiocontentanalysis.org/data-sets/<|>List of Datasets<|>List of Datasets https://www.audiocontentanalysis.org/data-sets/  by Alexander Lerch<|>17<|>64<|>0<|>16<|>5956<|>dataset_landing_page
17427<|>http://jmcauley.ucsd.edu/data/amazon/<|>Amazon Book Data<|>Amazon Book Data http://jmcauley.ucsd.edu/data/amazon/<|>17<|>54<|>0<|>16<|>4747<|>dataset_landing_page
17429<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI Dataset<|>Download the raw data of KITTI Dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . This dataset is for training and eigen split evaluation.<|>39<|>88<|>25<|>38<|>2741<|>software
17445<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry dataset<|>Download KITTI Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to YOUR_DATASET_FOLDER and set the  and  parameters in  file. Note you also convert KITTI dataset to bag file for easy use by setting proper parameters in .<|>32<|>86<|>9<|>31<|>1355<|>other
17543<|>http://groups.csail.mit.edu/vision/datasets/ADE20K/<|>ADE20K<|>ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K/<|>7<|>58<|>0<|>6<|>6346<|>dataset_landing_page
17543<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Cityscapes https://www.cityscapes-dataset.com/<|>11<|>46<|>0<|>10<|>3245<|>dataset_landing_page
17543<|>http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html<|>NYUDv2<|>NYUDv2 http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html<|>7<|>62<|>0<|>6<|>6073<|>dataset_landing_page
17676<|>https://archive.ics.uci.edu/ml/datasets/HIGGS<|>UCI<|>To run boosting experimets, create the folder  and download there  from UCI https://archive.ics.uci.edu/ml/datasets/HIGGS<|>76<|>121<|>72<|>75<|>5102<|>dataset_landing_page
17791<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>This repository is in large parts based on Multimodallearning's Mask_RCNN https://github.com/multimodallearning/pytorch-mask-rcnn , we also borrow the amodal evaluation code from AmodalMask https://github.com/Wakeupbuddy/amodalAPI  and COCO API https://github.com/cocodataset/cocoapi . The training and evaluation dataset are referenced from COCOA https://arxiv.org/abs/1509.01329  and D2SA https://arxiv.org/abs/1804.08864 . We would like to thank each of them for their kindly work.<|>245<|>283<|>236<|>244<|>5359<|>software
17791<|>https://github.com/cocodataset/cocoapi<|>COCOAPI<|>Configure COCOAPI https://github.com/cocodataset/cocoapi<|>18<|>56<|>10<|>17<|>5359<|>software
17791<|>https://github.com/cocodataset/cocoapi<|>COCOAPI<|>We modify the COCOAPI https://github.com/cocodataset/cocoapi  to meet our need. Here you have to soft link the pycocotool to the root directory for invoking.<|>22<|>60<|>14<|>21<|>5359<|>software
17818<|>http://cocodataset.org/#home<|>COCO dataset<|>To train the SAN-PG, we synthesize a Paired-Image-Texture dataset (PIT dataset), based on SURREAL dataset https://www.di.ens.fr/willow/research/surreal/ , for the purpose of providing the image pairs, i.e., the person image and its texture image. The texture image stores the RGB texture of the full person 3D surface. In particular, we use 929 raster-scanned texture maps provided by the SURREAL dataset to generate the image pairs. On SURREAL, all faces in the texture image are replaced by an average face of either man or woman. We generate 9,290 different meshes of diverse poses/shapes/viewpoints. For each texture map, we assign 10 different meshes and render these 3D meshes with the texture image. Then we obtain in total 9,290 different synthesized (person image, texture image) pairs. To simulate real-world scenes, the background images for rendering are randomly sampled from COCO dataset http://cocodataset.org/#home . Each synthetic person image is centered on a person with resolution 256x128. The resolution of the texture images is 256x256. The PIT dataset can be downloaded from here https://drive.google.com/file/d/1-ndIFhppMG_zjHCRfrnWRvbRQZObw2tT/view?usp=sharing .<|>902<|>930<|>889<|>901<|>6391<|>dataset_landing_page
17831<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/<|>7<|>32<|>0<|>6<|>67<|>software
17886<|>https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>Frequency dictionaries<|>Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>271<|>340<|>248<|>270<|>5620<|>dataset_direct_link
17886<|>http://storage.googleapis.com/books/ngrams/books/datasetsv2.html<|>Google Books Ngram data<|>Google Books Ngram data http://storage.googleapis.com/books/ngrams/books/datasetsv2.html (License) https://creativecommons.org/licenses/by/3.0/  : Provides representative word frequencies<|>24<|>88<|>0<|>23<|>5496<|>dataset_landing_page
17886<|>https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries<|>Frequency dictionaries<|>Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data<|>160<|>247<|>137<|>159<|>5620<|>dataset_direct_link
17886<|>https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/<|>Very fast Data cleaning of product names, company names & street names<|>1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/ Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast-approximate-string-matching/ Very fast Data cleaning of product names, company names & street names https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/ Sub-millisecond compound aware automatic spelling correction https://seekstorm.com/blog/sub-millisecond-compound-aware-automatic.spelling-correction/ SymSpell vs. BK-tree: 100x faster fuzzy string search & spell checking https://seekstorm.com/blog/symspell-vs-bk-tree/ Fast Word Segmentation for noisy text https://seekstorm.com/blog/fast-word-segmentation-noisy-text/ The Pruning Radix Trie — a Radix trie on steroids https://seekstorm.com/blog/pruning-radix-trie/<|>300<|>390<|>229<|>299<|>5620<|>other
18040<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and place it in  (  should contain the folders  and ).<|>24<|>59<|>13<|>23<|>3245<|>dataset_landing_page
18040<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI raw<|>Create  and download the KITTI raw http://www.cvlibs.net/datasets/kitti/raw_data.php  dataset using download_kitti_raw.py https://github.com/fregu856/evaluating_bdl/blob/master/depthCompletion/utils/download_kitti_raw.py .<|>35<|>84<|>25<|>34<|>2741<|>software
18059<|>https://seaborn.pydata.org/<|>seaborn<|>seaborn https://seaborn.pydata.org/  0.9.0<|>8<|>35<|>0<|>7<|>6833<|>software
18175<|>https://github.com/cocodataset/cocoapi<|>COCOAPI<|>Install COCOAPI https://github.com/cocodataset/cocoapi :<|>16<|>54<|>8<|>15<|>5359<|>software
18198<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>The boxscore-data json files can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data .<|>75<|>118<|>56<|>74<|>5459<|>dataset_landing_page
18199<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>models and results reflecting the newly cleaned up data in the boxscore-data repo https://github.com/harvardnlp/boxscore-data  are now given below.<|>82<|>125<|>63<|>81<|>5459<|>dataset_landing_page
18199<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data repo<|>The boxscore-data associated with the above paper can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data , and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar.<|>96<|>139<|>77<|>95<|>5459<|>dataset_landing_page
18205<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC<|>For detailed instructions, please refer to the documentation under . We provide several launch files under  for different datasets, including the EuRoC http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  datasets. These can be used as a reference for customization according to specific needs.<|>152<|>227<|>146<|>151<|>1355<|>dataset_landing_page
18315<|>https://seaborn.pydata.org/index.html<|>seaborn<|>seaborn https://seaborn.pydata.org/index.html<|>8<|>45<|>0<|>7<|>3635<|>software
18367<|>http://cocodataset.org/#download<|>here<|>Download the coco2014 dataset(train and val) from here http://cocodataset.org/#download . You should put the folder  and  to the directory<|>55<|>87<|>50<|>54<|>6212<|>dataset_landing_page
18681<|>[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>On the fly data processing<|>:warning: : On the fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).<|>39<|>145<|>12<|>38<|>6471<|>other
18714<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
18796<|>http://rpg.ifi.uzh.ch/davis_data.html<|>Event-Camera Dataset<|>To get a bag file from the Event-Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html :<|>48<|>85<|>27<|>47<|>2468<|>dataset_landing_page
18797<|>http://rpg.ifi.uzh.ch/davis_data.html<|>Event Camera Dataset<|>Alternatively, you can also play a rosbag file. You can use rosbags from the from the the Event Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html , e.g. here http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.bag .<|>111<|>148<|>90<|>110<|>2468<|>dataset_landing_page
18797<|>http://rpg.ifi.uzh.ch/davis_data.html<|>Event Camera Dataset<|>We provide a minimal example to process events from a plain text file. You can use the event text files from the the Event Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html , e.g. here http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.zip .<|>138<|>175<|>117<|>137<|>2468<|>dataset_landing_page
18804<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Cityscapes https://www.cityscapes-dataset.com/  + scripts https://github.com/mcordts/cityscapesScripts  (if you want to evaluate the model)<|>11<|>46<|>0<|>10<|>3245<|>dataset_landing_page
18846<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>You need to download the Cityscapes https://www.cityscapes-dataset.com/ , LIP http://sysu-hcp.net/lip/  and PASCAL-Context https://cs.stanford.edu/~roozbeh/pascal-context/  datasets.<|>36<|>71<|>25<|>35<|>3245<|>dataset_landing_page
18966<|>https://www.influxdata.com<|>InfluxDB<|>Roaring bitmaps are compressed bitmaps which tend to outperform conventional compressed bitmaps such as WAH, EWAH or Concise. They are used by several major systems such as Apache Lucene https://lucene.apache.org/  and derivative systems such as Solr https://lucene.apache.org/solr/  and Elasticsearch https://www.elastic.co/products/elasticsearch , Metamarkets' Druid http://druid.io/ , LinkedIn Pinot http://github.com/linkedin/pinot/wiki , Netflix Atlas https://github.com/Netflix/atlas , Apache Spark https://spark.apache.org/ , OpenSearchServer http://www.opensearchserver.com , Cloud Torrent https://github.com/jpillora/cloud-torrent , Whoosh https://bitbucket.org/mchaput/whoosh/wiki/Home , InfluxDB https://www.influxdata.com , Pilosa https://www.pilosa.com/ , Bleve http://www.blevesearch.com , Microsoft Visual Studio Team Services (VSTS) https://www.visualstudio.com/team-services/ , and eBay's Apache Kylin http://kylin.apache.org/ . The CRoaring library is used in several systems such as Apache Doris http://doris.incubator.apache.org . The YouTube SQL Engine, Google Procella https://research.google/pubs/pub48388/ , uses Roaring bitmaps for indexing.<|>707<|>733<|>698<|>706<|>1885<|>software
19018<|>http://cocodataset.org/#download<|>coco<|>Download coco http://cocodataset.org/#download  dataset and extract the images to<|>14<|>46<|>9<|>13<|>6212<|>dataset_landing_page
19197<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
19349<|>http://corpus-texmex.irisa.fr/<|>original website<|>| Data set | Download | dimension | nb base vectors | nb query vectors | original website | |-----------|--------------------------|-----------|-----------------|------------------|----------------------------------------------------------------| | SIFT1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 10,000 | original website http://corpus-texmex.irisa.fr/  | | GIST1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 1,000 | original website http://corpus-texmex.irisa.fr/  | | Crawl | crawl.tar.gz http://downloads.zjulearning.org.cn/data/crawl.tar.gz  (1.7GB) | 300 | 1,989,995 | 10,000 | original website http://commoncrawl.org/  | | GloVe-100 | glove-100.tar.gz http://downloads.zjulearning.org.cn/data/glove-100.tar.gz  (424MB) | 100 | 1,183,514 | 10,000 | original website https://nlp.stanford.edu/projects/glove/  | | Deep100M | deep100m.tar.gz* (34GB) | 96 | 100,000,000 | 10,000 | original website http://sites.skoltech.ru/compvision/noimi/  |<|>414<|>444<|>397<|>413<|>1268<|>dataset_landing_page
19349<|>http://corpus-texmex.irisa.fr/<|>original website<|>| Data set | Download | dimension | nb base vectors | nb query vectors | original website | |-----------|--------------------------|-----------|-----------------|------------------|----------------------------------------------------------------| | SIFT1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 10,000 | original website http://corpus-texmex.irisa.fr/  | | GIST1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 1,000 | original website http://corpus-texmex.irisa.fr/  | | Crawl | crawl.tar.gz http://downloads.zjulearning.org.cn/data/crawl.tar.gz  (1.7GB) | 300 | 1,989,995 | 10,000 | original website http://commoncrawl.org/  | | GloVe-100 | glove-100.tar.gz http://downloads.zjulearning.org.cn/data/glove-100.tar.gz  (424MB) | 100 | 1,183,514 | 10,000 | original website https://nlp.stanford.edu/projects/glove/  | | Deep100M | deep100m.tar.gz* (34GB) | 96 | 100,000,000 | 10,000 | original website http://sites.skoltech.ru/compvision/noimi/  |<|>275<|>305<|>258<|>274<|>1268<|>dataset_landing_page
19349<|>http://corpus-texmex.irisa.fr/<|>original website<|>| Data set | Download | dimension | nb base vectors | nb query vectors | original website | |-----------|--------------------------|-----------|-----------------|------------------|----------------------------------------------------------------| | SIFT1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 10,000 | original website http://corpus-texmex.irisa.fr/  | | GIST1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 1,000 | original website http://corpus-texmex.irisa.fr/  | | Crawl | crawl.tar.gz http://downloads.zjulearning.org.cn/data/crawl.tar.gz  (1.7GB) | 300 | 1,989,995 | 10,000 | original website http://commoncrawl.org/  | | GloVe-100 | glove-100.tar.gz http://downloads.zjulearning.org.cn/data/glove-100.tar.gz  (424MB) | 100 | 1,183,514 | 10,000 | original website https://nlp.stanford.edu/projects/glove/  | | Deep100M | deep100m.tar.gz* (34GB) | 96 | 100,000,000 | 10,000 | original website http://sites.skoltech.ru/compvision/noimi/  |<|>352<|>382<|>335<|>351<|>1268<|>dataset_landing_page
19349<|>http://corpus-texmex.irisa.fr/<|>original website<|>| Data set | Download | dimension | nb base vectors | nb query vectors | original website | |-----------|--------------------------|-----------|-----------------|------------------|----------------------------------------------------------------| | SIFT1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 10,000 | original website http://corpus-texmex.irisa.fr/  | | GIST1M | original website http://corpus-texmex.irisa.fr/ | 128 | 1,000,000 | 1,000 | original website http://corpus-texmex.irisa.fr/  | | Crawl | crawl.tar.gz http://downloads.zjulearning.org.cn/data/crawl.tar.gz  (1.7GB) | 300 | 1,989,995 | 10,000 | original website http://commoncrawl.org/  | | GloVe-100 | glove-100.tar.gz http://downloads.zjulearning.org.cn/data/glove-100.tar.gz  (424MB) | 100 | 1,183,514 | 10,000 | original website https://nlp.stanford.edu/projects/glove/  | | Deep100M | deep100m.tar.gz* (34GB) | 96 | 100,000,000 | 10,000 | original website http://sites.skoltech.ru/compvision/noimi/  |<|>490<|>520<|>473<|>489<|>1268<|>dataset_landing_page
19503<|>https://pandas.pydata.org<|>pandas<|>pandas https://pandas.pydata.org  (0.23.0)<|>7<|>32<|>0<|>6<|>4469<|>software
19630<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data<|>You can download the original dataset (Wiseman'2017) from the boxscore-data https://github.com/harvardnlp/boxscore-data  repo, and then, transform it as bellow. The script will create a directory "rotowire-modified", which contains train.json, valid.json, and test.json files.<|>76<|>119<|>62<|>75<|>5459<|>dataset_landing_page
19630<|>https://github.com/harvardnlp/boxscore-data<|>boxscore-data<|>Since this script simply removed duplicate records from the original dataset, the data format is the same as that of the original rotowire dataset. Please refer to boxscore-data https://github.com/harvardnlp/boxscore-data  repo.<|>178<|>221<|>164<|>177<|>5459<|>dataset_landing_page
19632<|>http://cocodataset.org<|>MSCOCO<|>Run the following from the home directory of this repository to install python dependencies, download BAM models, download MSCOCO http://cocodataset.org  and MiniPlaces https://github.com/CSAILVision/miniplaces , and construct BAM dataset.<|>130<|>152<|>123<|>129<|>924<|>dataset_landing_page
19696<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>Oxford flower 102<|>Oxford flower 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>18<|>67<|>0<|>17<|>3400<|>dataset_landing_page
19719<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>grayscale odometry data of KITTI<|>See  and  for further testing which ensures that place recognition performance is consistent between the Matlab and Python implementations. This test requires the grayscale odometry data of KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to be linked in the main folder of the repo.<|>196<|>250<|>163<|>195<|>1355<|>other
19734<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>COCO API https://github.com/cocodataset/cocoapi<|>9<|>47<|>0<|>8<|>5359<|>software
19735<|>https://github.com/cocodataset/cocoapi<|>COCO API<|>COCO API https://github.com/cocodataset/cocoapi<|>9<|>47<|>0<|>8<|>5359<|>software
19817<|>http://conda.pydata.org/miniconda.html<|><|>Install http://conda.pydata.org/miniconda.html  on your computer, by selecting the latest Python version for your operating system. If you already have  or  installed, you should be able to skip this step and move on to step 2.<|>8<|>46<|>0<|>0<|>155<|>software
19817<|>http://conda.pydata.org/miniconda.html<|>miniconda<|>miniconda http://conda.pydata.org/miniconda.html  on your machine. Detailed instructions:<|>10<|>48<|>0<|>9<|>155<|>software
19905<|>http://cocodataset.org/#download<|>COCO<|>Download MS COCO, ImageNet2012, NUS_WIDE in their official website: COCO http://cocodataset.org/#download , ImageNet http://image-net.org/download-images , NUS_WIDE https://lms.comp.nus.edu.sg/research/NUS-WIDE.htm . Unzip all data and put in 'data/dataset_name/'.<|>73<|>105<|>68<|>72<|>6212<|>dataset_landing_page
20129<|>http://jmcauley.ucsd.edu/data/amazon/<|>Amazon Product Data<|>Amazon Product Data http://jmcauley.ucsd.edu/data/amazon/<|>20<|>57<|>0<|>19<|>4747<|>dataset_landing_page
20147<|>http://pandas.pydata.org/<|>Pandas<|>Pandas http://pandas.pydata.org/  Python library v. 20.3+<|>7<|>32<|>0<|>6<|>67<|>software
20201<|>https://www.cityscapes-dataset.com/<|>Link<|>Download the dataset from the Cityscapes dataset server( Link https://www.cityscapes-dataset.com/ ). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in<|>62<|>97<|>57<|>61<|>3245<|>dataset_landing_page
20208<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/  (0.22.0; for DailyDialog dataset)<|>7<|>33<|>0<|>6<|>3372<|>software
20230<|>http://jmcauley.ucsd.edu/data/amazon/<|>link<|>| Dataset | Notes | |---|---| | 20 Newsgroups ( link http://qwone.com/~jason/20Newsgroups/ ) | Processed data available. We used the  version, available at the link provided. | RCV1 ( link https://trec.nist.gov/data/reuters/reuters.html ) | Due to the licensing agreement, we cannot release the raw data. Instead, we provide a list of document IDs and labels. You may request the dataset from the link provided. | | Reuters-21578 ( link https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html ) | Processed data available. | | Amazon reviews ( link http://jmcauley.ucsd.edu/data/amazon/ ) | We used a subset of the product review data. Processed data available. | | HuffPost headlines ( link https://www.kaggle.com/rmisra/news-category-dataset ) | Processed data available. | | FewRel ( link https://thunlp.github.io/fewrel.html ) | Processed data available.<|>558<|>595<|>553<|>557<|>4747<|>dataset_landing_page
20319<|>https://pandas.pydata.org/<|>Pandas<|>NILMTK is a toolkit designed to help  evaluate the accuracy of NILM algorithms. If you are a new Python user, it is recommended to educate yourself on Pandas https://pandas.pydata.org/ , Pytables http://www.pytables.org/  and other tools from the Python ecosystem.<|>158<|>184<|>151<|>157<|>3372<|>software
20330<|>http://cocodataset.org/#home<|>Microsoft COCO<|>Microsoft COCO http://cocodataset.org/#home  - 80 common object categories<|>15<|>43<|>0<|>14<|>6391<|>dataset_landing_page
20335<|>http://cocodataset.org/#home<|>MS COCO<|>We thank all the authors and annotators of vision-and-language datasets (i.e., MS COCO http://cocodataset.org/#home , Visual Genome https://visualgenome.org/ , VQA https://visualqa.org/ , GQA https://cs.stanford.edu/people/dorarad/gqa/ , NLVR2 http://lil.nlp.cornell.edu/nlvr/  ), which allows us to develop a pre-trained model for vision-and-language tasks.<|>87<|>115<|>79<|>86<|>6391<|>dataset_landing_page
20335<|>http://cocodataset.org<|>MS COCO<|>Run on the whole MS COCO http://cocodataset.org  and Visual Genome https://visualgenome.org/  related datasets (i.e., VQA https://visualqa.org/ , GQA https://cs.stanford.edu/people/dorarad/gqa/index.html , COCO caption http://cocodataset.org/#captions-2015 , VG Caption https://visualgenome.org/ , VG QA https://github.com/yukezhu/visual7w-toolkit ). Here, we take a simple single-stage pre-training strategy (20 epochs with all pre-training tasks) rather than the two-stage strategy in our paper (10 epochs without image QA and 10 epochs with image QA). The pre-training finishes in  on . By the way, I hope that my experience experience_in_pretraining.md  in this project would help anyone with limited computational resources.<|>25<|>47<|>17<|>24<|>924<|>dataset_landing_page
20335<|>http://cocodataset.org/#download<|>MS COCO official website<|>Download the MS COCO train2014, val2014, and test2015 images from MS COCO official website http://cocodataset.org/#download .<|>91<|>123<|>66<|>90<|>6212<|>dataset_landing_page
20391<|>http://www.cs.cmu.edu/~glai1/data/race/<|>RACE dataset<|>The following script finetunes the BERT model for evaluation on the RACE dataset http://www.cs.cmu.edu/~glai1/data/race/ . The  and  directory contain the RACE dataset as separate  files. Note that for RACE, the batch size is the number of RACE query's to evaluate. Since each RACE query has four samples, the effective batch size passed through the model will be four times the batch size specified on the command line.<|>81<|>120<|>68<|>80<|>4376<|>dataset_landing_page
20418<|>http://cocodataset.org/#download<|>here<|>Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .<|>61<|>93<|>56<|>60<|>6212<|>dataset_landing_page
20418<|>https://github.com/cocodataset/cocoapi<|>here<|>Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .<|>243<|>281<|>238<|>242<|>5359<|>software
20475<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download Cityscapes https://www.cityscapes-dataset.com/ .<|>20<|>55<|>9<|>19<|>3245<|>dataset_landing_page
20560<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>KITTI Tracking dataset<|>The experiments were done on KITTI Tracking dataset http://www.cvlibs.net/datasets/kitti/eval_tracking.php .<|>52<|>106<|>29<|>51<|>5125<|>dataset_landing_page
20561<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>odometry split<|>The train/test/validation splits are defined in the  folder. By default, the code will train a depth model using Zhou's subset https://github.com/tinghuiz/SfMLearner  of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new benchmark split http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction  or the odometry split http://www.cvlibs.net/datasets/kitti/eval_odometry.php  by setting the  flag.<|>406<|>460<|>391<|>405<|>1355<|>other
20561<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI odometry dataset<|>For this evaluation, the KITTI odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and  zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.<|>48<|>102<|>25<|>47<|>1355<|>other
20561<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>raw KITTI dataset<|>You can download the entire raw KITTI dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  by running:<|>46<|>95<|>28<|>45<|>2741<|>software
20655<|>https://www.fc.up.pt/addi/ph2%20database.html<|>Skin Lesion Segmentation PH2 Dataset<|>March 5, 2020: An extended version of the network has been released(Complete implemenation for SKin Lesion Segmentation on ISIC 217 https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a , Skin Lesion Segmentation PH2 Dataset https://www.fc.up.pt/addi/ph2%20database.html  and cell nuclei   along with the network implementation will be update soon).<|>237<|>282<|>200<|>236<|>1891<|>dataset_landing_page
20681<|>https://visualdialog.org/data<|>VisDial<|>(optional) For the "End-to-end + Visual" baseline, first download images from VisDial https://visualdialog.org/data  to the  folder, then run  to get image features.<|>86<|>115<|>78<|>85<|>6794<|>dataset_landing_page
20741<|>https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption<|>here<|>The scripts use 3 different datasets:  is available from the SNAP dataset collection here http://snap.stanford.edu/data/CollegeMsg.html , while the  dataset can be found here https://www.cs.cornell.edu/~arb/data/temporal-reddit-reply/index.html . Finally, the household power consumption dataset has been downloaded from the UCI ML repository, and is available here https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption .<|>366<|>453<|>361<|>365<|>4042<|>dataset_landing_page
20837<|>https://github.com/ratishsp/data2text-plan-py<|>data2text-plan-py<|>This repo contains code for Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time) https://www.aclweb.org/anthology/D19-1310.pdf  (Gong, H., Feng, X., Qin, B., & Liu, T.; EMNLP 2019); this code is based on data2text-plan-py https://github.com/ratishsp/data2text-plan-py .<|>273<|>318<|>255<|>272<|>5459<|>dataset_landing_page
20867<|>https://github.com/cocodataset/cocoapi<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi<|>12<|>50<|>0<|>11<|>5359<|>software
20895<|>http://cocodataset.org/#download<|>here<|>Download the COCO 2014 dataset from here http://cocodataset.org/#download . In particualr, you'll need the 2014 Training and Validation images.  Then download Karpathy's Train/Val/Test Split. You may download it from here http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip .<|>41<|>73<|>36<|>40<|>6212<|>dataset_landing_page
20895<|>https://github.com/cocodataset/cocoapi<|>here<|>If you want to do evaluation on COCO, download the COCO API from here https://github.com/cocodataset/cocoapi  if your on Linux or from here https://github.com/philferriere/cocoapi  if your on Windows. Then download the COCO caption toolkit from here https://github.com/tylin/coco-caption  and re-name the folder to . Don't forget to download java as well. Simply dowload it from here https://www.java.com/en/download/  if you don't have it.<|>70<|>108<|>65<|>69<|>5359<|>software
20966<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>KITTI Tracking Benchmark<|>We also provide the data split used in our paper in the  directory. You need to download and unzip the data from the KITTI Tracking Benchmark http://www.cvlibs.net/datasets/kitti/eval_tracking.php  and put them in the  directory or any path you like. Do remember to change the path in the configs.<|>142<|>196<|>117<|>141<|>5125<|>dataset_landing_page
20966<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>KITTI Tracking Benchmark<|>The RRC detection are obtained from the link https://drive.google.com/file/d/1ZR1qEf2qjQYA9zALLl-ZXuWhqG9lxzsM/view  provided by MOTBeyondPixels https://github.com/JunaidCS032/MOTBeyondPixels . We use RRC detection for the KITTI Tracking Benchmark http://www.cvlibs.net/datasets/kitti/eval_tracking.php .<|>248<|>302<|>223<|>247<|>5125<|>dataset_landing_page
21020<|>https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet<|>here<|>download  dataset from here https://github.com/dragen1860/LearningToCompare-Pytorch/issues/4 , splitting:  from here https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet .<|>117<|>192<|>112<|>116<|>6649<|>dataset_direct_link
21092<|>https://github.com/cocodataset/cocoapi<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi<|>12<|>50<|>0<|>11<|>5359<|>software
21133<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>Get ImageNet<|>Get ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily.<|>13<|>109<|>0<|>12<|>3215<|>dataset_landing_page
21180<|>https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split<|>Million Song Dataset<|>: indexed files containing the correspondences between audio files and their ground truth. Index files for the MagnaTagATune https://github.com/keunwoochoi/magnatagatune-list  dataset ( ) and the Million Song Dataset https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split  ( ) are already provided.<|>217<|>288<|>196<|>216<|>6350<|>dataset_landing_page
21228<|>https://www.wikidata.org<|>Wikidata<|>Wikidata https://www.wikidata.org : the latest version https://dumps.wikimedia.org/wikidatawiki/entities/20170612/wikidata-20170612-all-BETA.ttl.bz2  of .<|>9<|>33<|>0<|>8<|>6356<|>other
21228<|>https://www.wikidata.org<|>Wikidata<|>If you run the code yourself, you can define (a) what Wikipedia languages to cover, and (b) which specific Wikipedia https://www.wikipedia.org/ , Wikidata https://www.wikidata.org , and Wikimedia Commons https://commons.wikimedia.org  snapshots should be used during the build.<|>155<|>179<|>146<|>154<|>6356<|>other
21245<|>http://www.cvlibs.net/datasets/kitti/<|>KITTI<|>| Annotation format | Import | Export | | ------------------------------------------------------------------------------------------------ | ------ | ------ | | CVAT for images https://opencv.github.io/cvat/docs/manual/advanced/xml_format/#annotation  | ✔️ | ✔️ | | CVAT for a video https://opencv.github.io/cvat/docs/manual/advanced/xml_format/#interpolation  | ✔️ | ✔️ | | Datumaro https://github.com/cvat-ai/datumaro  | ✔️ | ✔️ | | PASCAL VOC http://host.robots.ox.ac.uk/pascal/VOC/  | ✔️ | ✔️ | | Segmentation masks from PASCAL VOC http://host.robots.ox.ac.uk/pascal/VOC/  | ✔️ | ✔️ | | YOLO https://pjreddie.com/darknet/yolo/  | ✔️ | ✔️ | | MS COCO Object Detection http://cocodataset.org/#format-data  | ✔️ | ✔️ | | MS COCO Keypoints Detection http://cocodataset.org/#format-data  | ✔️ | ✔️ | | TFrecord https://www.tensorflow.org/tutorials/load_data/tfrecord  | ✔️ | ✔️ | | MOT https://motchallenge.net/  | ✔️ | ✔️ | | MOTS PNG https://www.vision.rwth-aachen.de/page/mots  | ✔️ | ✔️ | | LabelMe 3.0 http://labelme.csail.mit.edu/Release3.0  | ✔️ | ✔️ | | ImageNet http://www.image-net.org  | ✔️ | ✔️ | | CamVid http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/  | ✔️ | ✔️ | | WIDER Face http://shuoyang1213.me/WIDERFACE/  | ✔️ | ✔️ | | VGGFace2 https://github.com/ox-vgg/vgg_face2  | ✔️ | ✔️ | | Market-1501 https://www.aitribune.com/dataset/2018051063  | ✔️ | ✔️ | | ICDAR13/15 https://rrc.cvc.uab.es/?ch=2  | ✔️ | ✔️ | | Open Images V6 https://storage.googleapis.com/openimages/web/index.html  | ✔️ | ✔️ | | Cityscapes https://www.cityscapes-dataset.com/login/  | ✔️ | ✔️ | | KITTI http://www.cvlibs.net/datasets/kitti/  | ✔️ | ✔️ | | Kitti Raw Format https://www.cvlibs.net/datasets/kitti/raw_data.php  | ✔️ | ✔️ | | LFW http://vis-www.cs.umass.edu/lfw/  | ✔️ | ✔️ | | Supervisely Point Cloud Format https://docs.supervise.ly/data-organization/00_ann_format_navi  | ✔️ | ✔️ |<|>1599<|>1636<|>1593<|>1598<|>2791<|>software
21248<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/  - helper data manipulation library<|>7<|>33<|>0<|>6<|>3372<|>software
21248<|>https://seaborn.pydata.org/<|>seaborn<|>seaborn https://seaborn.pydata.org/  - helper plotting library for some charts<|>8<|>35<|>0<|>7<|>6833<|>software
21260<|>http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df<|>first place<|>KittiSeg performs segmentation of roads by utilizing an FCN based model. The model achieved first place http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df  on the Kitti Road Detection Benchmark at submission time. Check out our paper https://arxiv.org/abs/1612.07695  for a detailed model description.<|>104<|>209<|>92<|>103<|>3492<|>other
21328<|>http://cocodataset.org/#download<|>coco website<|>: Download the coco images and annotations from coco website http://cocodataset.org/#download .<|>61<|>93<|>48<|>60<|>6212<|>dataset_landing_page
21328<|>https://github.com/cocodataset/cocoapi<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi  — for COCO dataset, also available from pip.<|>12<|>50<|>0<|>11<|>5359<|>software
21346<|>http://corpus-tools.org/pepper/<|>Pepper<|>Luke Gessler has written a module for the Pepper http://corpus-tools.org/pepper/  tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See instructions for converting https://github.com/nert-nlp/streusle-pepper-importer .<|>49<|>80<|>42<|>48<|>4294<|>software
21365<|>http://cocodataset.org/#download<|>Coco<|>: We execute correspondence and bidirectional retrieval experiments between the scientific figures and their captions. The corpora used are Scigraph or Semantic Scholar. Also, for the bidirectional retrieval task we support Coco http://cocodataset.org/#download (2014) and Flickr30k http://shannon.cs.illinois.edu/DenotationGraph/ . To use Flickr30k/Coco, download the images from their repositories, resize them to 224x224 resolution, and leave the resulting images in folders "look_read_and_enrich/images/coco/" and "look_read_and_enrich/images/flickr30k/".<|>229<|>261<|>224<|>228<|>6212<|>dataset_landing_page
21556<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for logging to csv<|>7<|>32<|>0<|>6<|>67<|>software
21651<|>https://www.cityscapes-dataset.com/<|><|>Download https://www.cityscapes-dataset.com/ , which contains 5,000 annotated images with 2048 × 1024 resolution taken from real urban street scenes. We resize Cityscapes images to 1024x512 (or 1280x640 which yields sightly better results but costs more time).<|>9<|>44<|>0<|>0<|>3245<|>dataset_landing_page
21708<|>http://conda.pydata.org/miniconda.html<|>http://conda.pydata.org/miniconda.html<|>Install miniconda http://conda.pydata.org/miniconda.html http://conda.pydata.org/miniconda.html<|>57<|>95<|>18<|>56<|>155<|>software
21835<|>https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>Euroc<|>In addition to the real-life tests http://ci-sparklab.mit.edu:8080/job/MIT-SPARK-Kimera/job/master/VIO_20Euroc_20Performance_20Report/  on the Euroc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  dataset, we use a photo-realistic Unity-based simulator to test Kimera. The simulator provides:<|>149<|>225<|>143<|>148<|>6943<|>dataset_landing_page
21848<|>http://cocodataset.org/#download<|>MSCOCO 2014<|>| Dataset | Task(s) | |:--------------:|:------------------------:| | HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#Downloads  | Activity Recognition | | UCF101 https://www.crcv.ucf.edu/data/UCF101.php  | Activity Recognition | | ImageNetVID http://bvisionweb1.cs.unc.edu/ilsvrc2015/download-videos-3j16.php  | Video Object Detection | | MSCOCO 2014 http://cocodataset.org/#download  | Object Detection, Keypoints| | VOC2007 http://host.robots.ox.ac.uk/pascal/VOC/voc2007/  | Object Detection, Classification| | YC2-BB http://youcook2.eecs.umich.edu/download | Video Object Grounding| | DHF1K https://github.com/wenguanwang/DHF1K  | Video Saliency Prediction|<|>388<|>420<|>376<|>387<|>6212<|>dataset_landing_page
21885<|>https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>Pycocotools<|>Pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools<|>12<|>84<|>0<|>11<|>5780<|>software
22047<|>https://www.cityscapes-dataset.com/<|>CityScapes Datasets<|>You can download CityScapes Datasets https://www.cityscapes-dataset.com/ .Put it in data folder.<|>37<|>72<|>17<|>36<|>3245<|>dataset_landing_page
22151<|>http://jmcauley.ucsd.edu/data/amazon/<|>here<|>We use the , which can be downloaded here http://jmcauley.ucsd.edu/data/amazon/ .<|>42<|>79<|>37<|>41<|>4747<|>dataset_landing_page
22155<|>https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/<|>WikiQA<|>Implementation is now only focusing on AS task with WikiQA https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/  corpus. (I originally tried to deal with PI task with MSRP(Microsoft Research Paraphrase) https://www.microsoft.com/en-us/download/details.aspx?id=52398  corpus but it seems that model doesn't work without external features classifier requires.)<|>59<|>174<|>52<|>58<|>1591<|>dataset_landing_page
22160<|>http://corpus-texmex.irisa.fr/<|>TEXMEX's bvecs format<|>Integer sketches should be stored in TEXMEX's bvecs format http://corpus-texmex.irisa.fr/ . That is, the dimension number and feature values for each sketch are interleaved (in little endian), where the number is 4 bytes of size and each feature is 1 byte of size. To convert vectors of real numbers into sketches, you can use consistent_weighted_sampling https://github.com/kampersanda/consistent_weighted_sampling  to generate such a dataset using the GCWS https://doi.org/10.1145/3097983.3098081  algorithm.<|>59<|>89<|>37<|>58<|>1268<|>dataset_landing_page
22183<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Cityscapes https://www.cityscapes-dataset.com/<|>11<|>46<|>0<|>10<|>3245<|>dataset_landing_page
22347<|>https://www.cityscapes-dataset.com/<|>here<|>Follow the instructions here https://www.cityscapes-dataset.com/  to request for the dataset download.<|>29<|>64<|>24<|>28<|>3245<|>dataset_landing_page
22347<|>https://www.cityscapes-dataset.com/<|>here<|>Download the data from here https://www.cityscapes-dataset.com/ .<|>28<|>63<|>23<|>27<|>3245<|>dataset_landing_page
22485<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
22490<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download the GTA5 https://download.visinf.tu-darmstadt.de/data/from_games/  dataset as the source domain, and the Cityscapes https://www.cityscapes-dataset.com/  dataset as the target domain.<|>125<|>160<|>114<|>124<|>3245<|>dataset_landing_page
22514<|>http://cocodataset.org/#download<|>here<|>obtain the train and validation images from the 2014 split here http://cocodataset.org/#download , extract and save them in  and<|>64<|>96<|>59<|>63<|>6212<|>dataset_landing_page
22701<|>http://bokeh.pydata.org<|>bokeh<|>bokeh http://bokeh.pydata.org  for training visualization<|>6<|>29<|>0<|>5<|>4770<|>software
22701<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for logging to csv<|>7<|>32<|>0<|>6<|>67<|>software
22712<|>https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset<|>ImageNet-Compatible images<|>Evaluation on success rate, robustness and transferability on 1000 ImageNet-Compatible images https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset .<|>94<|>194<|>67<|>93<|>6325<|>software
22725<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>: Download website Cityscapes https://www.cityscapes-dataset.com/ , see dataset preparation code in DA-Faster RCNN https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data<|>30<|>65<|>19<|>29<|>3245<|>dataset_landing_page
22725<|>https://fcav.engin.umich.edu/sim-dataset/<|>Sim10k<|>: Website Sim10k https://fcav.engin.umich.edu/sim-dataset/<|>17<|>58<|>10<|>16<|>2895<|>dataset_landing_page
22943<|>http://cocodataset.org/#download<|>the official website<|>Next, download MS COCO 2014 from the official website http://cocodataset.org/#download , extra annotation from here http://datasets.d2.mpi-inf.mpg.de/hosang17cvpr/coco_minival2014.tar.gz  and make symbolic links as follows.<|>54<|>86<|>33<|>53<|>6212<|>dataset_landing_page
22943<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI<|>For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website of KITTI. Placing the dataset on SSD would increase the training speed.<|>10<|>59<|>4<|>9<|>2741<|>software
22990<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for logging to csv<|>7<|>32<|>0<|>6<|>67<|>software
22990<|>http://bokeh.pydata.org<|>bokeh<|>bokeh http://bokeh.pydata.org  for training visualization<|>6<|>29<|>0<|>5<|>4770<|>software
23012<|>https://visualdialog.org/data<|>here<|>Download the VisDial v1.0 dialog json files and images from here https://visualdialog.org/data .<|>65<|>94<|>60<|>64<|>6794<|>dataset_landing_page
23053<|>http://cocodataset.org/#home<|>MSCOCO<|>Download MSCOCO http://cocodataset.org/#home  datasets and transfer the raw images into , according to the python script .<|>16<|>44<|>9<|>15<|>6391<|>dataset_landing_page
23053<|>http://cocodataset.org/#home<|>MSCOCO<|>MSCOCO http://cocodataset.org/#home  dataset is applied for the training of the proposed image reconstruction network.<|>7<|>35<|>0<|>6<|>6391<|>dataset_landing_page
23329<|>http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html<|>ECSSD<|>ECSSD http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html<|>6<|>71<|>0<|>5<|>4612<|>dataset_landing_page
23377<|>https://www.cityscapes-dataset.com/<|>Cityscapes dataset<|>First download the Cityscapes dataset https://www.cityscapes-dataset.com/  and then downsample the images and label maps to 1024x512 using bilinear and nearest-neighbor interpolation, respectively ( augmentation https://github.com/Lextal/pspnet-pytorch/blob/master/augmentation.py ). Next either download the pretrained model https://drive.google.com/file/d/1fh2qSLAAMzX0J27Megisehz9tV62zKtv/view?usp=sharing  (FCN-8s trained to segment Cityscapes) or provide your own pretrained model on Cityscapes. Then run  with suitable arguments. Specify the root folder in which the cityscapes dataset is stored using  and the pretrained model's path with . If you are running a targeted experiment, you need to specify a target label map using .<|>38<|>73<|>19<|>37<|>3245<|>dataset_landing_page
23482<|>https://www.cityscapes-dataset.com<|>link<|>Cityscapes : link https://www.cityscapes-dataset.com<|>18<|>52<|>13<|>17<|>2741<|>dataset_landing_page
23581<|>https://pandas.pydata.org<|>Pandas<|>Pandas https://pandas.pydata.org  library to be able to save the dataset as a dataframe compatible with MatchZoo https://github.com/NTMC-Community/MatchZoo<|>7<|>32<|>0<|>6<|>4469<|>software
23587<|>https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>FlyingChairs dataset<|>Download all the relevant datasets including the FlyingChairs dataset https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html , the FlyingThings3D dataset https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html  (we use  following the practice of FlowNet 2.0), the KITTI dataset http://www.cvlibs.net/datasets/kitti/index.php , and the MPI Sintel dataset http://sintel.is.tue.mpg.de/ .<|>70<|>148<|>49<|>69<|>3263<|>dataset_landing_page
23647<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>here<|>Download the KITTI odometry dataset from here http://www.cvlibs.net/datasets/kitti/eval_odometry.php  - get the color images and ground truth poses. Unzip the data in .<|>46<|>100<|>41<|>45<|>1355<|>other
23714<|>http://cocodataset.org/#download<|>cocodataset<|>Please download the COCO dataset from cocodataset http://cocodataset.org/#download . If you use  format, please specify  in  files or  if you unzip the downloaded dataset.<|>50<|>82<|>38<|>49<|>6212<|>dataset_landing_page
23901<|>https://visualdialog.org/data<|>here<|>Download the VisDial dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.<|>49<|>78<|>44<|>48<|>6794<|>dataset_landing_page
23996<|>https://github.com/harvardnlp/boxscore-data<|>here<|>The dataset used in the paper can be downloaded here https://github.com/harvardnlp/boxscore-data . More specifically, you just need to download the RotoWire dataset https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2 :<|>53<|>96<|>48<|>52<|>5459<|>dataset_landing_page
24048<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI Optical Flow 2015<|>KITTI Optical Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and KITTI Optical Flow 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow<|>24<|>95<|>0<|>23<|>3219<|>dataset_landing_page
24048<|>http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow<|>KITTI Optical Flow 2012<|>KITTI Optical Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and KITTI Optical Flow 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow<|>125<|>197<|>101<|>124<|>3219<|>dataset_landing_page
24073<|>https://seaborn.pydata.org/<|>seaborn<|>The only significant dependency beyond standard packages found in most Python distributions (e.g. numpy, matplotlib etc) is PyTorch https://pytorch.org/  (the experiments were performed with version 1.2.0). Additionally, using the seaborn https://seaborn.pydata.org/  library will make all your plot's nicer!<|>239<|>266<|>231<|>238<|>6833<|>software
24080<|>http://pandas.pydata.org<|>Pandas<|>Pandas http://pandas.pydata.org<|>7<|>31<|>0<|>6<|>6833<|>software
24166<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/  version 0.25.3<|>7<|>33<|>0<|>6<|>3372<|>software
24237<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>304<|>358<|>290<|>303<|>1355<|>other
24237<|>http://vision.in.tum.de/data/datasets/rgbd-dataset<|>TUM dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>403<|>453<|>391<|>402<|>1355<|>dataset_landing_page
24237<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC dataset<|>ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.<|>503<|>578<|>489<|>502<|>1355<|>dataset_landing_page
24237<|>http://vision.in.tum.de/data/datasets/rgbd-dataset/tools<|>associate.py<|>Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:<|>75<|>131<|>62<|>74<|>1355<|>other
24354<|>http://cocodataset.org/#download<|>MS-COCO 2014<|>MS-COCO 2014 http://cocodataset.org/#download  and HPatches http://icvl.ee.ic.ac.uk/vbalnt/hpatches/hpatches-sequences-release.tar.gz  should be downloaded into . The Synthetic Shapes dataset will also be generated there. The folder structure should look like:<|>13<|>45<|>0<|>12<|>6212<|>dataset_landing_page
24356<|>http://cocodataset.org/#download<|>here<|>Download the images (2017 Train and 2017 Val) from here http://cocodataset.org/#download<|>56<|>88<|>51<|>55<|>6212<|>dataset_landing_page
24392<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/ (Installation Help) https://github.com/seemethere/nba_py/wiki/Installing-pandas<|>7<|>32<|>0<|>6<|>67<|>software
24472<|>http://datashare.is.ed.ac.uk/handle/10283/1942<|>Edinburgh DataShare<|>The speech enhancement dataset used in the work can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . :<|>84<|>130<|>64<|>83<|>4200<|>dataset_landing_page
24481<|>https://visualdialog.org/data<|>here<|>We also provide pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VIsDial v1.0 images from here https://visualdialog.org/data  instead. Extracted features for v1.0 train, val and test are available for download at these links.<|>225<|>254<|>220<|>224<|>6794<|>dataset_landing_page
24481<|>https://visualdialog.org/data<|>here<|>Download the VisDial v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.<|>54<|>83<|>49<|>53<|>6794<|>dataset_landing_page
24690<|>https://pandas.pydata.org<|>https://pandas.pydata.org<|>Pandas (see https://pandas.pydata.org https://pandas.pydata.org )<|>38<|>63<|>12<|>37<|>4469<|>software
24739<|>https://pandas.pydata.org/<|>Pandas<|>| Package | Description | License | | ---------------------------------------------- | -------------------------------------------------------------- | --------------------- | | Cython https://cython.org/  | To use C++ code in Python. | Apache License 2.0 | | NumPy https://www.numpy.org/  | Fast linear algebra operations. | BSD 3-Clause | | Matplotlib https://matplotlib.org/  | Visualization and geometrical representation of classifiers. | Matplotlib License https://matplotlib.org/users/license.html  | | PyQt5 https://www.riverbankcomputing.com/software/pyqt/intro  | To create a GUI for using the LIBTwinSVM's features.| GPL | | Scikit-learn https://scikit-learn.org/  | For TwinSVM-based models evaluation and selection. | BSD 3-Clause | | Pandas https://pandas.pydata.org/  | For reading and processing datasets. | BSD 3-Clause | | XlsxWriter https://xlsxwriter.readthedocs.io/  | For saving classification results in an Excel file. | BSD 3-Clause | | Joblib https://joblib.readthedocs.io  | For saving and loading TwinSVM-based models. | BSD 3-Clause | | numpydoc https://numpydoc.readthedocs.io/en/latest/  | API code documentation. | BSD License |<|>755<|>781<|>748<|>754<|>3372<|>software
24821<|>http://vowl.visualdataweb.org/webvowl/<|>WebVOWL<|>Visualize the ontology using WebVOWL http://vowl.visualdataweb.org/webvowl/ . It should be enough to click this link http://visualdataweb.de/webvowl/#iri=http://ethon.consensys.net/EthOn.ttl . WebVOWL is also developed on GitHub: https://github.com/VisualDataWeb/WebVOWL<|>37<|>75<|>29<|>36<|>1616<|>other
24937<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>as described here<|>If the data is not preprocessed as described here https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset , then copy the scripts in  into their corresponding directories. Finally run them each to save each validation files into their category folder. You can specify the dataset path using the system input argument passed to the  file.<|>50<|>146<|>32<|>49<|>3215<|>dataset_landing_page
25212<|>https://pandas.pydata.org/<|>pandas<|>Installation requires numpy https://numpy.org/ , pandas https://pandas.pydata.org/ , and matplotlib https://matplotlib.org/ . Some functions will optionally use scipy https://www.scipy.org/  and/or statsmodels https://www.statsmodels.org/  if they are available.<|>56<|>82<|>49<|>55<|>3372<|>software
25365<|>https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet<|>GitHub link<|>Please note that the split files in  folder are created by Ravi and Larochelle https://openreview.net/pdf?id=rJY0-Kcll  ( GitHub link https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet ). Vinyals et al. http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf  didn't include their split files for mini-ImageNet when they first released their paper, so Ravi and Larochelle https://openreview.net/pdf?id=rJY0-Kcll  created their own splits. Additional split files are provided here https://github.com/yaoyao-liu/mini-imagenet-tools/tree/master/mini_imagenet_split .<|>134<|>209<|>122<|>133<|>6649<|>dataset_direct_link
25396<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>here<|>We support LIBSVM datasets which can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . The downloaded file should be unzipped and put in the following folder<|>56<|>123<|>51<|>55<|>5624<|>dataset_landing_page
25456<|>http://cocodataset.org/#download<|>here<|>Download the COCO train2014 and val2014 data here http://cocodataset.org/#download . Put the COCO train2014 images in the folder , and put the file  in the folder . Similarly, put the COCO val2014 images in the folder , and put the file  in the folder . Furthermore, download the pretrained VGG16 net here https://ucsb.box.com/s/pj4gg3vpei57cf9xewttoqn01qqa3uj4  if you want to use it to initialize the CNN part.<|>50<|>82<|>45<|>49<|>6212<|>dataset_landing_page
25483<|>http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>Oxford Flower 102<|>Oxford Flower 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/<|>18<|>67<|>0<|>17<|>3400<|>dataset_landing_page
25630<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Get dataset from Cityscapes https://www.cityscapes-dataset.com/ , and from Lost and Found http://www.6d-vision.com/lostandfounddataset .<|>28<|>63<|>17<|>27<|>3245<|>dataset_landing_page
25752<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI website<|>The KITTI (raw) dataset used in our experiments can be downloaded from the KITTI website http://www.cvlibs.net/datasets/kitti/raw_data.php . For convenience, we provide the standard splits used for training and evaluation: eigen_zhou https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_zhou_files.txt , eigen_train https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_train_files.txt , eigen_val https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_val_files.txt  and eigen_test https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_test_files.txt , as well as pre-computed ground-truth depth maps: original https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_velodyne.tar.gz  and improved https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_groundtruth.tar.gz . The full KITTI_raw dataset, as used in our experiments, can be directly downloaded here https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar.gz  or with the following command:<|>89<|>138<|>75<|>88<|>2741<|>software
25757<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>here<|>We trained and tested on the KITTI dataset. Download the raw dataset here http://www.cvlibs.net/datasets/kitti/raw_data.php . We provide a dataloader, but we first require that the data be preprocessed. To do so, run  within  (be sure to specify the source and target directory). We preprocessed the data by resizing the images and removing 'static' frames.<|>74<|>123<|>69<|>73<|>2741<|>software
26101<|>https://github.com/cocodataset/cocoapi<|>pycocotools<|>pycocotools https://github.com/cocodataset/cocoapi<|>12<|>50<|>0<|>11<|>5359<|>software
26124<|>https://github.com/cocodataset/cocoapi<|>cocoapi website<|>Install COCOAPI referring to cocoapi website https://github.com/cocodataset/cocoapi , or:<|>45<|>83<|>29<|>44<|>5359<|>software
26124<|>http://cocodataset.org/#download<|>COCO website<|>Download images from COCO website http://cocodataset.org/#download , and put train2014/val2014 splits into  respectively.<|>34<|>66<|>21<|>33<|>6212<|>dataset_landing_page
26217<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>We evaludated HANet on Cityscapes https://www.cityscapes-dataset.com/  and BDD-100K https://bair.berkeley.edu/blog/2018/05/30/bdd/ .<|>34<|>69<|>23<|>33<|>3245<|>dataset_landing_page
26336<|>https://github.com/cocodataset/cocoapi<|>coco python api<|>Additionaly, to use , you will also need coco python api https://github.com/cocodataset/cocoapi . You can get this using<|>57<|>95<|>41<|>56<|>5359<|>software
26459<|>http://cocodataset.org/#download<|>MS-COCO 2014 training set<|>Download the MS-COCO 2014 training set http://cocodataset.org/#download  and unzip it at path .<|>39<|>71<|>13<|>38<|>6212<|>dataset_landing_page
26527<|>https://pandas.pydata.org<|>Pandas<|>StellarGraph is built on TensorFlow 2 https://tensorflow.org/  and its Keras high-level API https://www.tensorflow.org/guide/keras , as well as Pandas https://pandas.pydata.org  and NumPy https://www.numpy.org . It is thus user-friendly, modular and extensible. It interoperates smoothly with code that builds on these, such as the standard Keras layers and scikit-learn http://scikit-learn.github.io/stable , so it is easy to augment the core graph machine learning algorithms provided by StellarGraph. It is thus also easy to install with #installation .<|>151<|>176<|>144<|>150<|>4469<|>software
26549<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI odometry dataset<|>For this evaluation, the KITTI odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and  zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.<|>48<|>102<|>25<|>47<|>1355<|>other
26549<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>raw KITTI dataset<|>You can download the entire raw KITTI dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  by running:<|>46<|>95<|>28<|>45<|>2741<|>software
26549<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>odometry split<|>The train/test/validation splits are defined in the  folder. By default, the code will train a depth model using Zhou's subset https://github.com/tinghuiz/SfMLearner  of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new benchmark split http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction  or the odometry split http://www.cvlibs.net/datasets/kitti/eval_odometry.php  by setting the  flag.<|>406<|>460<|>391<|>405<|>1355<|>other
26603<|>http://cocodataset.org/#download<|>this link<|>For COCO 2017, visit this link http://cocodataset.org/#download  and download the 2017 validation images, which is about 1.1GB. Un-tar this file. It will create a folder called , full of JPEGs. After uncompressing this archive, navigate to  and link (or copy) the dataset folder to . The contents of  should now be all of the  files.<|>31<|>63<|>21<|>30<|>6212<|>dataset_landing_page
26623<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>train<|>: Download the train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json  datasets and move them under<|>21<|>87<|>15<|>20<|>6663<|>dataset_direct_link
26623<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>dev<|>: Download the train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json  datasets and move them under<|>97<|>161<|>93<|>96<|>6663<|>dataset_direct_link
26634<|>https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>FlyingChairs<|>FlyingChairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>13<|>91<|>0<|>12<|>3263<|>dataset_landing_page
26634<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI 2015<|>KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow  & KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>98<|>169<|>87<|>97<|>3219<|>dataset_landing_page
26634<|>http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow<|>KITTI 2012<|>KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow  & KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>11<|>83<|>0<|>10<|>3219<|>dataset_landing_page
26687<|>http://numba.pydata.org/<|>Numba<|>Readable code leveraging NumPy https://numpy.org/ , SciPy https://www.scipy.org/scipylib/index.html  and Numba http://numba.pydata.org/<|>111<|>135<|>105<|>110<|>2124<|>software
26720<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI<|>KITTI http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>6<|>77<|>0<|>5<|>3219<|>dataset_landing_page
26739<|>https://www.cityscapes-dataset.com/<|>cityscapes(C)<|>The training procedure needs gta5(G) https://download.visinf.tu-darmstadt.de/data/from_games/ , synthia_rand_citys(S) https://synthia-dataset.net/downloads/ , idd(I) https://idd.insaan.iiit.ac.in/ , mapillary(M) https://www.mapillary.com/dataset/vistas?pKey=1GyeWFxH_NPIQwgl0onILw  and cityscapes(C) https://www.cityscapes-dataset.com/ . Please download them and put to the same folder which can be specified in . The folder tree structure is as follows:<|>300<|>335<|>286<|>299<|>3245<|>dataset_landing_page
26758<|>https://www.cityscapes-dataset.com/<|>here<|>The Cityscapes dataset can be downloaded at here https://www.cityscapes-dataset.com/<|>49<|>84<|>44<|>48<|>3245<|>dataset_landing_page
26774<|>https://vision.in.tum.de/data/datasets/rgbd-dataset<|>the TUM Vision Groud RGBD datasets<|>the TUM Vision Groud RGBD datasets https://vision.in.tum.de/data/datasets/rgbd-dataset ;<|>35<|>86<|>0<|>34<|>4968<|>dataset_landing_page
26775<|>https://www.cityscapes-dataset.com/<|>Cityscapes & Foggy Cityscapes<|>Cityscapes & Foggy Cityscapes https://www.cityscapes-dataset.com/<|>30<|>65<|>0<|>29<|>3245<|>dataset_landing_page
26870<|>http://pandas.pydata.org/<|>pandas<|>Events and summaries are utilizing pandas http://pandas.pydata.org/  for data structures and analysis. New metrics can reuse already computed values from depending metrics.<|>42<|>67<|>35<|>41<|>67<|>software
26894<|>https://github.com/cocodataset/cocoapi<|>COCOAPI<|>Install COCOAPI https://github.com/cocodataset/cocoapi :<|>16<|>54<|>8<|>15<|>5359<|>software
26931<|>https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type<|>CRDT<|>All databases are implemented https://github.com/orbitdb/orbit-db-store  on top of ipfs-log https://github.com/orbitdb/ipfs-log , an immutable, cryptographically verifiable, operation-based conflict-free replicated data structure ( CRDT https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type ) for distributed systems. ipfs-log is formalized in the paper Merkle-CRDTs https://arxiv.org/abs/2004.00107 . You can also easily extend OrbitDB by implementing and using a custom data model https://github.com/orbitdb/orbit-db/blob/master/GUIDE.md#custom-stores  benefitting from the same properties as the default data models provided by the underlying Merkle-CRDTs.<|>237<|>301<|>232<|>236<|>1185<|>other
26935<|>https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type<|>CRDT<|>is an immutable, operation-based conflict-free replicated data structure ( CRDT https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type ) for distributed systems. It's an append-only log that can be used to model a mutable, shared state between peers in p2p applications.<|>80<|>144<|>75<|>79<|>1185<|>other
26945<|>https://pandas.pydata.org<|>Pandas<|>This repository uses Pandas https://pandas.pydata.org  to process and normalize the data.<|>28<|>53<|>21<|>27<|>4469<|>software
26989<|>http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets<|>EuRoC MAV Dataset<|>Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets . Although it contains stereo cameras, we only use one camera. Before testing, copy the new  file to  path. Change the output frequency of VINS-Mono  to 15 Hz .<|>27<|>102<|>9<|>26<|>1355<|>dataset_landing_page
27122<|>http://numba.pydata.org/<|>numba<|>The notebooks and the pref_voting library is built around a full SciPy stack: MatPlotLib https://matplotlib.org/ , Numpy https://numpy.org/ , Pandas https://pandas.pydata.org/ , numba http://numba.pydata.org/ , networkx https://networkx.org/ , and tabulate https://github.com/astanin/python-tabulate<|>184<|>208<|>178<|>183<|>2124<|>software
27122<|>https://pandas.pydata.org/<|>Pandas<|>The notebooks and the pref_voting library is built around a full SciPy stack: MatPlotLib https://matplotlib.org/ , Numpy https://numpy.org/ , Pandas https://pandas.pydata.org/ , numba http://numba.pydata.org/ , networkx https://networkx.org/ , and tabulate https://github.com/astanin/python-tabulate<|>149<|>175<|>142<|>148<|>3372<|>software
27259<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI Raw Data<|>KITTI Raw Data http://www.cvlibs.net/datasets/kitti/raw_data.php  (synced+rectified data, please refer MonoDepth2 https://github.com/nianticlabs/monodepth2#-kitti-training-data  for downloading all data more easily)<|>15<|>64<|>0<|>14<|>2741<|>software
27259<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>KITTI Scene Flow 2015<|>KITTI Scene Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow<|>22<|>93<|>0<|>21<|>3219<|>dataset_landing_page
27365<|>https://www.cityscapes-dataset.com<|>GoogleDrive<|>Download link: GoogleDrive https://www.cityscapes-dataset.com<|>27<|>61<|>15<|>26<|>2741<|>dataset_landing_page
27431<|>http://cocodataset.org/#download<|>COCO dataset<|>COCO dataset http://cocodataset.org/#download<|>13<|>45<|>0<|>12<|>6212<|>dataset_landing_page
27454<|>https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html<|>LIBSVM Data Repository<|>"covertype.mat": Covertype dataset from the LIBSVM Data Repository https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . It is also used in SVGD by Liu & Wang (2016) http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm .<|>67<|>134<|>44<|>66<|>5624<|>dataset_landing_page
27501<|>http://cocodataset.org/#home<|>COCO<|>We use Lasot https://cis.temple.edu/lasot/ , GOT-10k http://got-10k.aitestunion.com , TrackingNet https://tracking-net.org  and COCO http://cocodataset.org/#home  to train fcot. Before running the training scripts, you should download the datasets and set the correct datasets path in . Also remember to download the pretrained dimp50 https://drive.google.com/file/d/14zFM14cjJY-D_OFsLDlF1fX5XrSXGBQV/view?usp=sharing  model to initialize the backbone and classification-18 branch of fcot. Then switch to your conda environment using . The training scripts can be found at bash bash  folder. We use the two following stategies to train fcot (we report the results of the first training strategy in the paper).<|>133<|>161<|>128<|>132<|>6391<|>dataset_landing_page
27545<|>https://www.cityscapes-dataset.com/<|>Cityscape<|>: Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:<|>46<|>81<|>36<|>45<|>3245<|>dataset_landing_page
27622<|>http://pandas.pydata.org/<|>pandas<|>pandas http://pandas.pydata.org/  for storing data about components and time series<|>7<|>32<|>0<|>6<|>67<|>software
27687<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI 2015<|>Download Scene Flow https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo  and KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  datasets.<|>208<|>281<|>197<|>207<|>2741<|>dataset_landing_page
27812<|>https://pandas.pydata.org<|>pandas<|>Python 3 for code generation and with pandas https://pandas.pydata.org  installed for the evaluation scripts<|>45<|>70<|>38<|>44<|>4469<|>software
27840<|>http://www.cvlibs.net/datasets/kitti/eval_odometry.php<|>KITTI Odometry website<|>Follow the instruction on KITTI Odometry website http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to download the KITTI odometry train set. Then train with<|>49<|>103<|>26<|>48<|>1355<|>other
27970<|>http://graphics.stanford.edu/data/3Dscanrep/<|>here<|>In case you'd like to have fun with some random  meshes, you could download samples from Stanford's 3D Scanning Repository here http://graphics.stanford.edu/data/3Dscanrep/ .<|>128<|>172<|>123<|>127<|>4833<|>dataset_landing_page
28035<|>https://pandas.pydata.org/<|>pandas<|>dlib http://dlib.net/ face_recognition https://github.com/ageitgey/face_recognition/ NumPy http://www.numpy.org/ pandas https://pandas.pydata.org/ scikit-learn http://scikit-learn.org/<|>120<|>146<|>113<|>119<|>3372<|>software
28041<|>https://visualdialog.org/data<|>here<|>Download the VisDial v0.9 and v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  and  directory, respectively.<|>63<|>92<|>58<|>62<|>6794<|>dataset_landing_page
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>| | MRR | Hits@1 | Hits@3 | Hits@10 | Config file | Pretrained model | |-------------------------------------------------------------------------------------------------------|------:|-------:|-------:|--------:|--------------------------------------------------------------------------------------------:|----------------------------------------------------------------------------------------:| | RESCAL http://www.icml-2011.org/papers/438_icmlpaper.pdf  | 0.467 | 0.439 | 0.480 | 0.517 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-rescal.yaml  | KvsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-rescal.pt  | | TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  | 0.228 | 0.053 | 0.368 | 0.520 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-transe.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-transe.pt  | | DistMult https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf  | 0.452 | 0.413 | 0.466 | 0.530 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-distmult.yaml  | KvsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-distmult.pt  | | ComplEx http://proceedings.mlr.press/v48/trouillon16.pdf  | 0.475 | 0.438 | 0.490 | 0.547 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-complex.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-complex.pt  | | ConvE https://arxiv.org/abs/1707.01476  | 0.442 | 0.411 | 0.451 | 0.504 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-conve.yaml  | KvsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/wnrr-conve.pt  | | RotatE https://openreview.net/pdf?id=HkgEQnRqYQ  | 0.478 | 0.439 | 0.494 | 0.553 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wnrr-rotate.yaml  | NegSamp-bce http://web.informatik.uni-mannheim.de/pi1/libkge-models/wnrr-rotate.pt  |<|>676<|>767<|>669<|>675<|>2405<|>other
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>| | MRR | Hits@1 | Hits@3 | Hits@10 | |-------------------------------------------------------------------------------------------------------|------:|-------:|-------:|--------:| | ComplEx http://proceedings.mlr.press/v48/trouillon16.pdf  | 0.594 | 0.511 | 0.667 | 0.726 | | RotatE https://openreview.net/pdf?id=HkgEQnRqYQ  | 0.613 | 0.578 | 0.637 | 0.669 | | TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  | 0.553 | 0.520 | 0.571 | 0.614 |<|>368<|>459<|>361<|>367<|>2405<|>other
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  ( code kge/model/transe.py , config kge/model/transe.yaml )<|>7<|>98<|>0<|>6<|>2405<|>other
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>| | MRR | Hits@1 | Hits@3 | Hits@10 | Config file | Pretrained model | |-------------------------------------------------------------------------------------------------------|------:|-------:|-------:|--------:|--------------------------------------------------------------------------------------------:|----------------------------------------------------------------------------------------:| | RESCAL http://www.icml-2011.org/papers/438_icmlpaper.pdf  | 0.948 | 0.943 | 0.951 | 0.956 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-rescal.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-rescal.pt  | | TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  | 0.553 | 0.315 | 0.764 | 0.924 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-transe.yaml  | NegSamp-bce http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-transe.pt  | | DistMult https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf  | 0.941 | 0.932 | 0.948 | 0.954 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-distmult.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-distmult.pt  | | ComplEx http://proceedings.mlr.press/v48/trouillon16.pdf  | 0.951 | 0.947 | 0.953 | 0.958 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-complex.yaml  | KvsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-complex.pt  | | ConvE https://arxiv.org/abs/1707.01476  | 0.947 | 0.943 | 0.949 | 0.953 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-conve.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-conve.pt  | | RotatE https://openreview.net/pdf?id=HkgEQnRqYQ  | 0.946 | 0.943 | 0.948 | 0.953 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-rotate.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/wn18-rotate.pt  |<|>672<|>763<|>665<|>671<|>2405<|>other
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>| | MRR | Hits@1 | Hits@3 | Hits@10 | Config file | Pretrained model | |-------------------------------------------------------------------------------------------------------|------:|-------:|-------:|--------:|-------------------------------------------------------------------------------------------------:|----------------------------------------------------------------------------------------------:| | RESCAL http://www.icml-2011.org/papers/438_icmlpaper.pdf  | 0.356 | 0.263 | 0.393 | 0.541 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-rescal.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-rescal.pt  | | TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  | 0.313 | 0.221 | 0.347 | 0.497 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-transe.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-transe.pt  | | DistMult https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf  | 0.343 | 0.250 | 0.378 | 0.531 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-distmult.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-distmult.pt  | | ComplEx http://proceedings.mlr.press/v48/trouillon16.pdf  | 0.348 | 0.253 | 0.384 | 0.536 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-complex.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-complex.pt  | | ConvE https://arxiv.org/abs/1707.01476  | 0.339 | 0.248 | 0.369 | 0.521 | config.yaml http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-conve.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/iclr2020-models/fb15k-237-conve.pt  | | RotatE https://openreview.net/pdf?id=HkgEQnRqYQ  | 0.333 | 0.240 | 0.368 | 0.522 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-237-rotate.yaml  | NegSamp-bce http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-237-rotate.pt  |<|>697<|>788<|>690<|>696<|>2405<|>other
28118<|>https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data<|>TransE<|>| | MRR | Hits@1 | Hits@3 | Hits@10 | Config file | Pretrained model | |-------------------------------------------------------------------------------------------------------|------:|-------:|-------:|--------:|-------------------------------------------------------------------------------------------------:|----------------------------------------------------------------------------------------------:| | RESCAL http://www.icml-2011.org/papers/438_icmlpaper.pdf  | 0.644 | 0.544 | 0.708 | 0.824 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-rescal.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-rescal.pt  | | TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  | 0.676 | 0.542 | 0.787 | 0.875 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-transe.yaml  | NegSamp-bce http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-transe.pt  | | DistMult https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf  | 0.841 | 0.806 | 0.863 | 0.903 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-distmult.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-distmult.pt  | | ComplEx http://proceedings.mlr.press/v48/trouillon16.pdf  | 0.838 | 0.807 | 0.856 | 0.893 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-complex.yaml  | 1vsAll-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-complex.pt  | | ConvE https://arxiv.org/abs/1707.01476  | 0.825 | 0.781 | 0.855 | 0.896 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-conve.yaml  | KvsAll-bce http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-conve.pt  | | RotatE https://openreview.net/pdf?id=HkgEQnRqYQ  | 0.783 | 0.727 | 0.820 | 0.877 | config.yaml http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-rotate.yaml  | NegSamp-kl http://web.informatik.uni-mannheim.de/pi1/libkge-models/fb15k-rotate.pt  |<|>686<|>777<|>679<|>685<|>2405<|>other
28202<|>http://www.nltk.org/data.html<|>NLTK.org<|>Data was created by using the Vader dataset from NLTK.org http://www.nltk.org/data.html .<|>58<|>87<|>49<|>57<|>1258<|>software
28227<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>dev-v1.1.json<|>SQuAD v1.1: train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>114<|>178<|>100<|>113<|>6663<|>dataset_direct_link
28227<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>train-v1.1.json<|>SQuAD v1.1: train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>28<|>94<|>12<|>27<|>6663<|>dataset_direct_link
28324<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>train-v1.1.json<|>train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json<|>16<|>82<|>0<|>15<|>6663<|>dataset_direct_link
28324<|>https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>dev-v1.1.json<|>dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json<|>14<|>78<|>0<|>13<|>6663<|>dataset_direct_link
28481<|>http://www.cs.ucr.edu/~eamonn/time_series_data/<|>The UCR Time Series Classification Archive<|>The UCR Time Series Classification Archive http://www.cs.ucr.edu/~eamonn/time_series_data/<|>43<|>90<|>0<|>42<|>5530<|>dataset_landing_page
28481<|>http://www.cs.ucr.edu/~eamonn/time_series_data/<|>[LINK]<|>In our experimental evaluation using a benchmark of time series datasets [LINK] http://www.cs.ucr.edu/~eamonn/time_series_data/ , TEASER is two to three times as early while keeping the same (or even a higher) level of accuracy, when compared to the state of the art.<|>80<|>127<|>73<|>79<|>5530<|>dataset_landing_page
28517<|>https://www.wikidata.org<|>wikidata<|>Person names and location (country, city) names are multilingual, depending on the  language. We got the data notebooks/other/Acquiring%20multilingual%20lexicons%20from%20wikidata.ipynb  from wikidata https://www.wikidata.org , so there is a bias towards names on wikipedia.<|>201<|>225<|>192<|>200<|>6356<|>other
28555<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>Kitti<|>Kitti http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>6<|>60<|>0<|>5<|>5125<|>dataset_landing_page
28690<|>https://pandas.pydata.org/<|>pandas<|>Some functions will optionally use pandas https://pandas.pydata.org/ , bibtexparser https://bibtexparser.readthedocs.io/  and ase https://wiki.fysik.dtu.dk/ase/ .<|>42<|>68<|>35<|>41<|>3372<|>software
28720<|>https://www.cityscapes-dataset.com/<|>Cityscapes Dataset<|>Cityscapes Dataset https://www.cityscapes-dataset.com/<|>19<|>54<|>0<|>18<|>3245<|>dataset_landing_page
28755<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI RAW sequence<|>Download the  data of a KITTI RAW sequence http://www.cvlibs.net/datasets/kitti/raw_data.php . We choose the data  of the  category:<|>43<|>92<|>24<|>42<|>2741<|>software
28839<|>http://crcv.ucf.edu/data/UCF101.php<|>here<|>Please download the UCF101 dataset here http://crcv.ucf.edu/data/UCF101.php .<|>40<|>75<|>35<|>39<|>2024<|>dataset_landing_page
28943<|>https://pandas.pydata.org/<|>Pandas<|>The SINDyBVP.sindy_bvp() method is then called. This method loads the relevant data, randomly selects trial data to use for regression (i.e. finding the model and parameters), and sends that data to a TermBuilder object to construct Pandas https://pandas.pydata.org/ DataFrames https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html  containing symbolic functions. Each trial is sent individually to the TermBuilder, which creates a DataFrame from each individual trial. The DataFrame has (p+1) columns, where (p) is the number of candidate functions to use for SINDy regression. The additional column corresponds to the regression outcome variable. After a DataFrame has been constructed for each of the trials, the data is sent to a grouper object, which takes the DataFrames and reorganizes them for regression, as described in the paper. The organized data is used by the GroupRegressor object to perform the SGTR algorithm. The results are reported, and the coefficients are computed for the operator L (rather than for the algebraically manipulated form learned by SINDy-BVP). The coefficients are returned with a configured plotter object, which can be used to visualize the results.<|>240<|>266<|>233<|>239<|>3372<|>software
29284<|>http://www.cvlibs.net/datasets/kitti/eval_tracking.php<|>KITTI Tracking<|>Download the dataset from KITTI Tracking http://www.cvlibs.net/datasets/kitti/eval_tracking.php .<|>41<|>95<|>26<|>40<|>5125<|>dataset_landing_page
29333<|>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html<|>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html<|>To run the docker image make sure a  is available (either on your notebook or in the cloud you prefer) and your Docker environment supports GPUs. To do so, you might follow the official nvidia guide: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html<|>280<|>359<|>200<|>279<|>5544<|>other
29441<|>http://jmcauley.ucsd.edu/data/amazon/<|>Amazon review corpus<|>We use the , , ,  datasets from from 5-core subsets of the Amazon review corpus http://jmcauley.ucsd.edu/data/amazon/  by Prof. Julian McAuley.<|>80<|>117<|>59<|>79<|>4747<|>dataset_landing_page
29669<|>https://pandas.pydata.org<|>pandas<|>Python3 libraries: h5py https://www.h5py.org , numba https://numba.pydata.org , numpy https://numpy.org , pandas https://pandas.pydata.org<|>113<|>138<|>106<|>112<|>4469<|>software
29738<|>http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset<|>MagnaTagATune Dataset<|>MagnaTagATune Dataset http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset<|>22<|>79<|>0<|>21<|>5956<|>dataset_landing_page
29973<|>https://sites.google.com/view/davis-driving-dataset-2020/home<|>DDD20<|>v2e can convert recordings from DDD20 https://sites.google.com/view/davis-driving-dataset-2020/home  and the original DDD17 https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub  which are the first public end-to-end training datasets of automotive driving using a DAVIS event + frame camera. It lets you compare the real DVS data with the conversion. This dataset is maintained by the Sensors Research Group of Institute of Neuroinformatics.<|>38<|>99<|>32<|>37<|>2468<|>dataset_landing_page
30050<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Some examples of MDEQ segmentation results on the Cityscapes https://www.cityscapes-dataset.com/  dataset.<|>61<|>96<|>50<|>60<|>3245<|>dataset_landing_page
30079<|>http://numba.pydata.org/<|>numba<|>numba http://numba.pydata.org/  - for speeding up calculations of the statistics (available automatically with Anaconda).<|>6<|>30<|>0<|>5<|>2124<|>software
30079<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
30079<|>https://seaborn.pydata.org/<|>seaborn<|>seaborn https://seaborn.pydata.org/  - for the probability density function of the Hotel dataset presented in the article.<|>8<|>35<|>0<|>7<|>6833<|>software
30202<|>http://jmcauley.ucsd.edu/data/amazon/<|>here<|>Amazon datasets are derived from here http://jmcauley.ucsd.edu/data/amazon/ , tradesy dataset is introduced in here http://jmcauley.ucsd.edu/data/tradesy/ . Please cite the corresponding papers if you use the datasets.<|>38<|>75<|>33<|>37<|>4747<|>dataset_landing_page
30209<|>https://github.com/cocodataset/cocoapi<|>COCO<|>The code is a re-write of PythonAPI for COCO https://github.com/cocodataset/cocoapi . The core functionality is the same with LVIS specific changes.<|>45<|>83<|>40<|>44<|>5359<|>software
30260<|>http://numba.pydata.org/<|>numba<|>numba http://numba.pydata.org/<|>6<|>30<|>0<|>5<|>2124<|>software
30260<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
30340<|>https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset<|>Get ImageNet<|>Get ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily.<|>13<|>109<|>0<|>12<|>3215<|>dataset_landing_page
30420<|>http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo<|>KITTI 2015<|>Download Scene Flow Datasets https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo , KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo , ETH3D https://www.eth3d.net/ , Middlebury https://vision.middlebury.edu/stereo/<|>214<|>287<|>203<|>213<|>2741<|>dataset_landing_page
30532<|>https://seaborn.pydata.org/index.html<|>Seaborn<|>The implementation of all tasks and algorithms is based on the qd-branch of the C++ Sferes2 https://github.com/sferes2/sferes2  library presented in Sferesv2: Evolvin' in the multi-core world https://ieeexplore.ieee.org/abstract/document/5586158/?casa_token=EhBJLkircvMAAAAA:ls8I90Y5H2vsJk5RxCYs8X1T9yZHDhDEz5S6g5gatOzETle1LK_ib8zwodx6t5J_-Uwq_YP9 , and the hexapod control task uses the Dart simulator introduced in Dart: Dynamic animation and robotics toolkit https://joss.theoj.org/papers/10.21105/joss.00500.pdf . Furthermore, the analysis of the results is based on Panda https://pandas.pydata.org/ , Matplotlib https://matplotlib.org/  and Seaborn https://seaborn.pydata.org/index.html  libraries.<|>654<|>691<|>646<|>653<|>3635<|>software
30532<|>https://pandas.pydata.org/<|>Panda<|>The implementation of all tasks and algorithms is based on the qd-branch of the C++ Sferes2 https://github.com/sferes2/sferes2  library presented in Sferesv2: Evolvin' in the multi-core world https://ieeexplore.ieee.org/abstract/document/5586158/?casa_token=EhBJLkircvMAAAAA:ls8I90Y5H2vsJk5RxCYs8X1T9yZHDhDEz5S6g5gatOzETle1LK_ib8zwodx6t5J_-Uwq_YP9 , and the hexapod control task uses the Dart simulator introduced in Dart: Dynamic animation and robotics toolkit https://joss.theoj.org/papers/10.21105/joss.00500.pdf . Furthermore, the analysis of the results is based on Panda https://pandas.pydata.org/ , Matplotlib https://matplotlib.org/  and Seaborn https://seaborn.pydata.org/index.html  libraries.<|>577<|>603<|>571<|>576<|>3372<|>software
30533<|>https://www.cityscapes-dataset.com/<|>Cityscape<|>: Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:<|>46<|>81<|>36<|>45<|>3245<|>dataset_landing_page
30632<|>http://cocodataset.org/#download<|>COCO download<|>, please download from COCO download http://cocodataset.org/#download , 2017 Train/Val is needed for COCO keypoints training and validation. Download and extract them under {POSE_ROOT}/data, and make them look like this:<|>37<|>69<|>23<|>36<|>6212<|>dataset_landing_page
30632<|>https://github.com/cocodataset/cocoapi<|>COCOAPI<|>Install COCOAPI https://github.com/cocodataset/cocoapi :<|>16<|>54<|>8<|>15<|>5359<|>software
30657<|>http://crcv.ucf.edu/data/UCF101.php<|>UCF101<|>Download and pre-process UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets as follows.<|>32<|>67<|>25<|>31<|>2024<|>dataset_landing_page
30657<|>http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/<|>HMDB51<|>Download and pre-process UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets as follows.<|>80<|>156<|>73<|>79<|>2024<|>dataset_landing_page
30754<|>https://seaborn.pydata.org/<|>seaborn<|>seaborn https://seaborn.pydata.org/<|>8<|>35<|>0<|>7<|>6833<|>software
30754<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
30775<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Download Cityscapes https://www.cityscapes-dataset.com/  and virtual Kitti https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/<|>20<|>55<|>9<|>19<|>3245<|>dataset_landing_page
30854<|>http://cocodataset.org/#home<|>COCO dataset<|>For the needs of our application we utilized YOLOv3 detector, trained on the COCO dataset http://cocodataset.org/#home . You can download this detector from here https://convcao.hopto.org/index.php/s/mh8WIDpprE70SO3 . After downloading the file, extract the yolo-coco folder inside your local ConvCao_AirSim folder.<|>90<|>118<|>77<|>89<|>6391<|>dataset_landing_page
30887<|>https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html<|>flying chairs<|>Make sure you have downloaded flying chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  and flying things https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , and placed them under the same folder, say /ssd/.<|>44<|>122<|>30<|>43<|>3263<|>dataset_landing_page
30993<|>http://cocodataset.org/#download<|>COCO download<|>, please download from COCO download http://cocodataset.org/#download , 2017 Train/Val is needed for COCO detection/instance segmentation/keypoints training and validation. Download and extract them under {ROOT}/data, and make them look like this:<|>37<|>69<|>23<|>36<|>6212<|>dataset_landing_page
31140<|>http://groups.csail.mit.edu/vision/datasets/ADE20K/<|>ADK20K<|>| Dataset | training set | validation set | testing set | | :----------------------------------------------------------: | :----------: | :------------: | :---------: | | VOC2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar  | 1464 | 1449 | ✘ | | VOCAug http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz  | 11355 | 2857 | ✘ | | ADK20K http://groups.csail.mit.edu/vision/datasets/ADE20K/  | 20210 | 2000 | ✘ | | Cityscapes https://www.cityscapes-dataset.com/downloads/  | 2975 | 500 | ✘ | | COCO http://cocodataset.org/#download  | | | | | SBU-shadow http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip  | 4085 | 638 | ✘ | | LIP(Look into Person) http://sysu-hcp.net/lip/  | 30462 | 10000 | 10000 |<|>413<|>464<|>406<|>412<|>6346<|>dataset_landing_page
31140<|>http://cocodataset.org/#download<|>COCO<|>| Dataset | training set | validation set | testing set | | :----------------------------------------------------------: | :----------: | :------------: | :---------: | | VOC2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar  | 1464 | 1449 | ✘ | | VOCAug http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz  | 11355 | 2857 | ✘ | | ADK20K http://groups.csail.mit.edu/vision/datasets/ADE20K/  | 20210 | 2000 | ✘ | | Cityscapes https://www.cityscapes-dataset.com/downloads/  | 2975 | 500 | ✘ | | COCO http://cocodataset.org/#download  | | | | | SBU-shadow http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip  | 4085 | 638 | ✘ | | LIP(Look into Person) http://sysu-hcp.net/lip/  | 30462 | 10000 | 10000 |<|>573<|>605<|>568<|>572<|>6212<|>dataset_landing_page
31158<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/<|>7<|>33<|>0<|>6<|>3372<|>software
31309<|>http://cocodataset.org/#download<|>official COCO dataset website<|>Please follow the official COCO dataset website http://cocodataset.org/#download  to download the dataset. After downloading the dataset you should have the following directory structure:<|>48<|>80<|>18<|>47<|>6212<|>dataset_landing_page
31442<|>https://www.cityscapes-dataset.com/<|>Link<|>Download the dataset from the Cityscapes dataset server( Link https://www.cityscapes-dataset.com/ ). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in ../data/CityScapes/<|>62<|>97<|>57<|>61<|>3245<|>dataset_landing_page
31457<|>https://pandas.pydata.org/<|>pandas<|>pandas https://pandas.pydata.org/  (tested with version 0.24.2)<|>7<|>33<|>0<|>6<|>3372<|>software
31506<|>https://www.cityscapes-dataset.com<|>Cityscapes<|>This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including PSPNet https://hszhao.github.io/projects/pspnet  and PSANet https://hszhao.github.io/projects/psanet , which ranked 1st places in ImageNet Scene Parsing Challenge 2016 @ECCV16 http://image-net.org/challenges/LSVRC/2016/results , LSUN Semantic Segmentation Challenge 2017 @CVPR17 https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html  and WAD Drivable Area Segmentation Challenge 2018 @CVPR18 https://bdd-data.berkeley.edu/wad-2018.html . Sample experimented datasets are ADE20K http://sceneparsing.csail.mit.edu , PASCAL VOC 2012 http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6  and Cityscapes https://www.cityscapes-dataset.com .<|>951<|>985<|>940<|>950<|>2741<|>dataset_landing_page
31575<|>https://www.cityscapes-dataset.com/<|>The Cityscapes Dataset<|>Download The Cityscapes Dataset https://www.cityscapes-dataset.com/<|>32<|>67<|>9<|>31<|>3245<|>dataset_landing_page
31664<|>https://pandas.pydata.org/<|>Pandas<|>Pandas https://pandas.pydata.org/  (v1.0.1 or later)<|>7<|>33<|>0<|>6<|>3372<|>software
31665<|>http://www.cvlibs.net/datasets/kitti/raw_data.php<|>KITTI Raw Website<|>For color images, KITTI Raw dataset is also needed, which is available at the KITTI Raw Website http://www.cvlibs.net/datasets/kitti/raw_data.php .<|>96<|>145<|>78<|>95<|>2741<|>software
31807<|>https://github.com/cocodataset/cocoapi<|>@cocodataset/cocoapi<|>We provide evaluation tools for COCO-WholeBody dataset. Our evaluation tools is developed based on @cocodataset/cocoapi https://github.com/cocodataset/cocoapi .<|>120<|>158<|>99<|>119<|>5359<|>software
31808<|>http://cocodataset.org/#download<|>here<|>Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .<|>61<|>93<|>56<|>60<|>6212<|>dataset_landing_page
31808<|>https://github.com/cocodataset/cocoapi<|>here<|>Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .<|>243<|>281<|>238<|>242<|>5359<|>software
31827<|>https://www.cityscapes-dataset.com/<|>official website<|>. Please follow the standard download and preparation guidelines on the official website https://www.cityscapes-dataset.com/ . We recommend to symlink its root folder  to  by:<|>89<|>124<|>72<|>88<|>3245<|>dataset_landing_page
31962<|>http://cocodataset.org/#download<|>here<|>Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download<|>64<|>96<|>59<|>63<|>6212<|>dataset_landing_page
32006<|>https://pandas.pydata.org/<|>Pandas<|>: Different machine learning frameworks have different strengths. Flower can be used with any machine learning framework, for example, PyTorch https://pytorch.org , TensorFlow https://tensorflow.org , Hugging Face Transformers https://huggingface.co/ , PyTorch Lightning https://pytorchlightning.ai/ , MXNet https://mxnet.apache.org/ , scikit-learn https://scikit-learn.org/ , JAX https://jax.readthedocs.io/ , TFLite https://tensorflow.org/lite/ , fastai https://www.fast.ai/ , Pandas https://pandas.pydata.org/  for federated analytics, or even raw NumPy https://numpy.org/  for users who enjoy computing gradients by hand.<|>486<|>512<|>479<|>485<|>3372<|>software
32035<|>http://cocodataset.org/#home<|>MS COCO<|>Datasets: MS COCO http://cocodataset.org/#home  for image instance segmentation and YouTube-VIS https://youtube-vos.org/dataset/vis/  for video instance segmentation.<|>18<|>46<|>10<|>17<|>6391<|>dataset_landing_page
32060<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>Segmentation performance measured in IoU/mIoU (%) on Cityscapes https://www.cityscapes-dataset.com/ . For class names: All referred to all classes average, and the rest single class codes are as follows: DeepGlobe: Ro.-road, Sw.-sidewalk, Bu.-building, W.-wall, F.-fence, P.-pole, T.L.-traffic light, R.S.-raffic sign, V.-vegetation, Te.-terrain, Sk.-sky, P.-person, RI.-rider, C.-car, Tru.-truck, Tra.-train, M.-motorcycle, Bi.-bicycle.<|>64<|>99<|>53<|>63<|>3245<|>dataset_landing_page
32060<|>https://www.cityscapes-dataset.com/<|>Cityscapes<|>| Dataset (with Link) | Content | Resolution (pixels) | Number of Classes | | :--: | :--: | :--: | :--: | | Cityscapes https://www.cityscapes-dataset.com/  | urban scenes | 2048x1024 | 19 | | DeepGlobe https://competitions.codalab.org/competitions/18468  | aerial scenes | 2448x2448 | 6 | | Gleason2019 https://gleason2019.grand-challenge.org/  | histopathological | 5000x5000 | 4 |<|>119<|>154<|>108<|>118<|>3245<|>dataset_landing_page
32088<|>https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet<|>here<|>Mini-Imagenet as described here https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet<|>32<|>107<|>27<|>31<|>6649<|>dataset_direct_link
