[
    {
        "text": "The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m  pipeline, with scales selected by MSTEPS.",
        "answer": "  [{\"URL\": \"https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m\", \"label\": \"Software\"}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Boostrap Analysis of Stable Clusters https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m  pipeline, with regular grid of scales.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.",
        "answer": "  [{\"URL\": \"https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m\", \"label\": \"Software\"}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m', 'label': 'dataset_direct_link'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.",
        "answer": "  [{\"URL\": \"https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m\", \"label\": \"Software\"}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m', 'label': 'dataset_direct_link'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m  pipeline, with scales selected by MSTEPS.",
        "answer": "  [{\"URL\": \"https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m\", \"label\": \"Software\"}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Multiscale Statistical Parametric Connectome https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m', 'label': 'dataset_landing_page'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m  pipeline.",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m', 'label': 'dataset_landing_page'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m  pipeline.",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The preprocessing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m  pipeline.",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The region growing https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m  pipeline.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m', 'label': 'Software'}]",
        "repoID": 559,
        "URL_gold_label": [
            {
                "URL": "https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You will find example files for each type of model in the test_data https://github.com/redpony/creg/tree/master/test_data  directory.",
        "answer": "  [{\"URL\": \"https://github.com/redpony/creg/tree/master/test_data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 575,
        "URL_gold_label": [
            {
                "URL": "https://github.com/redpony/creg/tree/master/test_data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "An example for classification of motor imagery in ECoG recordings. For that example the BCI Competition3, Data Set 1 http://www.bbci.de/competition/iii/#data_set_i  was used.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.bbci.de/competition/iii/', 'label': 'dataset_landing_page'}]",
        "repoID": 685,
        "URL_gold_label": [
            {
                "URL": "http://www.bbci.de/competition/iii/#data_set_i",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "An example for classification with a P300 Matrix Speller in EEG recordings. The BCI Competition 3, Data Set 2 http://www.bbci.de/competition/iii/#data_set_ii  was used for that example.",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.bbci.de/competition/iii/#data_set_ii', 'label': 'dataset_landing_page'}]",
        "repoID": 685,
        "URL_gold_label": [
            {
                "URL": "http://www.bbci.de/competition/iii/#data_set_ii",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Novi Quadrianto http://smileclinic.alwaysdata.net/ : letter n followed by dot then quadrianto and the at, and also sussex.ac.uk",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'http://smileclinic.alwaysdata.net/', 'label': 'Other'}]",
        "repoID": 711,
        "URL_gold_label": [
            {
                "URL": "http://smileclinic.alwaysdata.net/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The evaluation dataset is at https://github.com/Spirals-Team/npe-dataset/ https://github.com/Spirals-Team/npe-dataset/",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/Spirals-Team/npe-dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 761,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Spirals-Team/npe-dataset/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The code in this repo makes use of 24 years of data from http://www.ams.org/mr-database , available by request to the director of the American Mathematical Society. The data may require cleaning in order to match the format assumed by  and . The code also uses data included in the https://github.com/corybrunson/bitriad , specifically affiliation networks drawn from the studies http://books.google.com/books?id=Q3b9QTOgLFcC  and http://books.google.com/books?id=fR-LBQAAQBAJ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.ams.org/mr-database\nLabel: dataset_landing_page\n* URL: https://github.com/corybrunson/bitriad\nLabel: dataset_landing_page\n* URL: http://books.google.com/books?id=Q3b9QTOgLFcC\nLabel: Other\n* URL: http://books.google.com/books?id=fR-LBQAAQBAJ\nLabel: Other",
        "repoID": 786,
        "URL_gold_label": [
            {
                "URL": "http://www.ams.org/mr-database",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "SICK dataset http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools  (semantic relatedness task)",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools', 'label': 'dataset_landing_page'}]",
        "repoID": 791,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The goal of this task is to predict similarity ratings for pairs of sentences. We train and evaluate our models on the Sentences Involving Compositional Knowledge (SICK) http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools  dataset.",
        "answer": "  [{\"URL\": \"http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 791,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the data files from Kaggle http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data . Place and extract the files in the following locations:",
        "answer": "  [{'URL': 'http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data', 'label': 'dataset_direct_link'}]",
        "repoID": 831,
        "URL_gold_label": [
            {
                "URL": "http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Data dashboard /data/ , Florence data server http://www.micc.unifi.it/tagsurvey/#data",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n* URL: \"http://www.micc.unifi.it/tagsurvey/#data\"\nLabel: Other",
        "repoID": 843,
        "URL_gold_label": [
            {
                "URL": "http://www.micc.unifi.it/tagsurvey/#data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "contains data from the WHO http://apps.who.int/gho/data/node.ebola-sitrep  that compare sitrep case counts with patient database counts for select cities and countries.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://apps.who.int/gho/data/node.ebola-sitrep', 'label': 'dataset_landing_page'}]",
        "repoID": 858,
        "URL_gold_label": [
            {
                "URL": "http://apps.who.int/gho/data/node.ebola-sitrep",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "W3C RDF data shapes working group charter http://www.w3.org/2014/data-shapes/charter",
        "answer": "  [{\"URL\": \"http://www.w3.org/2014/data-shapes/charter\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 886,
        "URL_gold_label": [
            {
                "URL": "http://www.w3.org/2014/data-shapes/charter",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This project includes the front end user interfaces that are used to annotate COCO dataset. For more details, please visit COCO http://cocodataset.org",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://cocodataset.org', 'label': 'dataset_landing_page'}]",
        "repoID": 924,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Assamese handwriting recognition https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#",
        "answer": "  Sure, here are the annotations for the input URL:\n\n{\n\"URL\": \"https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 941,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Kaggle plankton recognition competition, 2015 https://www.kaggle.com/c/datasciencebowl  Third place. The competition solution is being adapted for research purposes in EcoTaxa http://ecotaxa.obs-vlfr.fr/ .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://www.kaggle.com/c/datasciencebowl', 'label': 'dataset_landing_page'}]",
        "repoID": 941,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/datasciencebowl",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DAWT: Densely Annotated Wikipedia Texts across multiple languages https://github.com/klout/opendata/blob/master/wiki_annotation/README.md  ( arxiv https://arxiv.org/pdf/1703.00948.pdf )",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/klout/opendata/blob/master/wiki_annotation/README.md', 'label': 'dataset_landing_page'}]\n\nThe URL you provided is for a GitHub repository containing the Densely Annotated Wikipedia Texts across multiple languages dataset.",
        "repoID": 1004,
        "URL_gold_label": [
            {
                "URL": "https://github.com/klout/opendata/blob/master/wiki_annotation/README.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For the large-scale experiment, please first check our GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data  for pre-processing the iPinYou data http://data.computational-advertising.org . After downloading the dataset, by simplying  you can generate the standardised data which will be used in the bid optimisation tasks.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/wnzhang/make-ipinyou-data\nLabel: DatasetDirectLink\n* URL: http://data.computational-advertising.org\nLabel: DatasetLandingPage",
        "repoID": 1021,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the LinkedGeoData http://linkedgeodata.org  project.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://linkedgeodata.org', 'label': 'dataset_landing_page'}]",
        "repoID": 1024,
        "URL_gold_label": [
            {
                "URL": "http://linkedgeodata.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "is a java serialization library. There are many https://github.com/eishay/jvm-serializers  like it, but this one is ours. Codec has its own reflection toolset, but we are moving to wards using more and more jackson in the back end. Currently, the most notable features are those that are built on top of jackson-databind https://github.com/FasterXML/jackson-databind  and typesafe-config https://github.com/typesafehub/config .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://github.com/eishay/jvm-serializers\nLabel: Software\n* URL: https://github.com/FasterXML/jackson-databind\nLabel: Software\n* URL: https://github.com/typesafehub/config\nLabel: Software",
        "repoID": 1047,
        "URL_gold_label": [
            {
                "URL": "https://github.com/FasterXML/jackson-databind",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The training and validation of the code was performed using the EDUB-Seg dataset http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/ .",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/', 'label': 'dataset_landing_page'}]",
        "repoID": 1121,
        "URL_gold_label": [
            {
                "URL": "http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Metadata Embeddings for User and Item Cold-start Recommendations http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/', 'label': 'dataset_landing_page'}]",
        "repoID": 1127,
        "URL_gold_label": [
            {
                "URL": "http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "A lexicon of Geoquery http://www.cs.utexas.edu/users/ml/nldata/geoquery.html  is available in lex/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cs.utexas.edu/users/ml/nldata/geoquery.html', 'label': 'dataset_landing_page'}]",
        "repoID": 1139,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.utexas.edu/users/ml/nldata/geoquery.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Swarm.js is a JavaScript client for the Swarm database. Swarm is like \"git for data\" except it's real-time and never has a merge conflict. Swarm is based on Replicated Object Notation http://github.com/gritzko/ron  (RON), a distributed live data format. In turn, RON is based on Conflict-free Replicated Data Types https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type  (CRDTs), a new math apparatus for distributed data.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://github.com/gritzko/ron', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL \"http://github.com/gritzko/ron\" is an introduction or landing page for the Replicated Object Notation (RON) dataset entity.",
        "repoID": 1185,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "[ ] Counter https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(Positive-Negative_Counter) , a positive-negative counter.",
        "answer": "  Sure, here's the annotation for the given input:\n\n[{'URL': 'https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(&Positive-Negative_Counter)', 'label': 'Other'}]",
        "repoID": 1185,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(Positive-Negative_Counter)",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "API Docs https://cljdoc.org/d/datascript/datascript/CURRENT",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://cljdoc.org/d/datascript/datascript/CURRENT\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://cljdoc.org/d/datascript/datascript/CURRENT",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "CatChat, chat demo app: sources https://github.com/tonsky/datascript-chat , code walkthrough http://tonsky.me/blog/datascript-chat/ , live http://tonsky.me/datascript-chat/",
        "answer": "  Sure, here are the annotated URLs for the input:\n\n[{'URL': 'https://github.com/tonsky/datascript-chat', 'label': 'Software'}]\n\nExplanation: The URL \"https://github.com/tonsky/datascript-chat\" is for the source code of a software entity, specifically a chat demo app.",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "http://tonsky.me/blog/datascript-chat/",
                "gold_label": "other"
            },
            {
                "URL": "https://github.com/tonsky/datascript-chat",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "DataScript internals explained http://tonsky.me/blog/datascript-internals/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://tonsky.me/blog/datascript-internals/', 'label': 'dataset_landing_page'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "http://tonsky.me/blog/datascript-internals/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DataScript-Transit https://github.com/tonsky/datascript-transit , transit serialization for database and datoms",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/tonsky/datascript-transit', 'label': 'Software'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tonsky/datascript-transit",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DataScript-mori https://github.com/typeetfunc/datascript-mori , DataScript & Mori wrapper for use from JS",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/typeetfunc/datascript-mori', 'label': 'Software'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/typeetfunc/datascript-mori",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Datamaps https://github.com/djjolicoeur/datamaps , lib designed to leverage datalog queries to query arbitrary maps.",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/djjolicoeur/datamaps', 'label': 'Software'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/djjolicoeur/datamaps",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Getting started https://github.com/tonsky/datascript/wiki/Getting-started",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/tonsky/datascript/wiki/Getting-started', 'label': 'dataset_landing_page'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tonsky/datascript/wiki/Getting-started",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "OmNext TodoMVC https://github.com/madvas/todomvc-omnext-datomic-datascript",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/madvas/todomvc-omnext-datomic-datascript', 'label': 'Software'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/madvas/todomvc-omnext-datomic-datascript",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Quick tutorial into Datalog http://www.learndatalogtoday.org/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.learndatalogtoday.org/', 'label': 'Other'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "http://www.learndatalogtoday.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ToDo, task manager demo app (persistence via localStorage and transit, filtering, undo/redo): sources https://github.com/tonsky/datascript-todo , live http://tonsky.me/datascript-todo/",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/tonsky/datascript-todo', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\n* The URL \"https://github.com/tonsky/datascript-todo\" is the landing page for the dataset entity, as it provides an introduction to the dataset and links to the source code.\n\nI hope this helps! Let me know if you have any questions.",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tonsky/datascript-todo",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Tutorials https://github.com/kristianmandrup/datascript-tutorial",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/kristianmandrup/datascript-tutorial', 'label': 'dataset_landing_page'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/kristianmandrup/datascript-tutorial",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "clj-crud, demo CRUD app: sources https://github.com/thegeez/clj-crud , blog post http://thegeez.net/2014/04/30/datascript_clojure_web_app.html",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/thegeez/clj-crud', 'label': 'Software'}]\n\nExplanation: The URL \"https://github.com/thegeez/clj-crud\" is for a software entity, specifically a GitHub repository containing the source code for a CRUD (Create, Read, Update, Delete) application written in Clojure.",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "http://thegeez.net/2014/04/30/datascript_clojure_web_app.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "or as a CommonJS module ( npm page https://www.npmjs.org/package/datascript ):",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://snap.stanford.edu/data/loc-gowalla.html', 'label': 'dataset_landing_page'}]\n\n[{'URL': 'http://simongog.github.io/assets/data/sdsl-slides/tutorial', 'label': 'Other'}]\n\n[{'URL': 'http://law.di.unimi.it/datasets.php', 'label': 'dataset_landing_page'}]\n\n[{'URL': 'http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable', 'label': 'Software'}]",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://www.npmjs.org/package/datascript",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "\u201cBuilding ToDo list with DataScript\u201d webinar (ClojureScript NYC, Dec 2014): video http://vimeo.com/114688970 , app https://github.com/tonsky/datascript-todo",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://vimeo.com/114688970', 'label': 'Other'}]\n\nExplanation:\n\n* The URL \"http://vimeo.com/114688970\" is not a dataset, landing page, software, or any other category mentioned in the instructions. Therefore, it is classified as \"Other\".",
        "repoID": 1186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tonsky/datascript-todo",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Simon \u0160uster and Gertjan van Noord (2014) From neighborhood to parenthood: the advantages of dependency representation over bigrams in Brown clustering. http://www.let.rug.nl/suster/publications/DepBrown.pdf  COLING. See also induced clusters and experimental details http://github.com/rug-compling/dep-brown-data .",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'http://www.let.rug.nl/suster/publications/DepBrown.pdf', 'label': 'dataset_direct_link'}]\n\nNote: I have only annotated the last URL given in the input.",
        "repoID": 1204,
        "URL_gold_label": [
            {
                "URL": "http://github.com/rug-compling/dep-brown-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We have tested our matrix factorization algorithm on the MovieLens data http://grouplens.org/datasets/movielens/ . In particular, we have used the 1M, 10M, and 20M datasets (after a straightforward preprocessing step to make it compatible with the HAMSI input format).",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://grouplens.org/datasets/movielens/\nLabel: dataset_direct_link",
        "repoID": 1221,
        "URL_gold_label": [
            {
                "URL": "http://grouplens.org/datasets/movielens/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "(*) The stopword corpus is needed. Instructions here http://www.nltk.org/data.html .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.nltk.org/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 1258,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Here we use Amazon dataset (obtained from SNAP http://snap.stanford.edu/data/com-Amazon.html  website) as an illustration. You may switch to other datasets with corresponding file format as well. Note that some parameters might need to be adjusted accordingly based on the properties of network under test.",
        "answer": "  [{\"URL\": \"http://snap.stanford.edu/data/com-Amazon.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1282,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/com-Amazon.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "amazon dataset (available at http://snap.stanford.edu/data/com-Amazon.html http://snap.stanford.edu/data/com-Amazon.html )",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://snap.stanford.edu/data/com-Amazon.html', 'label': 'dataset_landing_page'}]",
        "repoID": 1282,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/com-Amazon.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The images described here are part of a much larger data set (>700 million images) collected by IFCB at the Martha's Vineyard Coastal Observatory ( MVCO http://www.whoi.edu/mvco ) starting in 2006 and continuing to the present. Near real time image data and the complete archive are accessible for browse and download at the IFCB Data Dashboard. http://ifcb-data.whoi.edu/mvco",
        "answer": "  Sure, here are the annotated URLs for the given input:\n\n[{'URL': 'http://ifcb-data.whoi.edu/mvco', 'label': 'dataset_landing_page'}]",
        "repoID": 1301,
        "URL_gold_label": [
            {
                "URL": "http://ifcb-data.whoi.edu/mvco",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can download the GoogleRefexp data directly from this link https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip .",
        "answer": "  [{'URL': 'https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 1313,
        "URL_gold_label": [
            {
                "URL": "https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Gowalla https://snap.stanford.edu/data/loc-gowalla.html : the pre-processed data that we used in the paper can be downloaded here http://dawenl.github.io/data/gowalla_pro.zip .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://snap.stanford.edu/data/loc-gowalla.html', 'label': 'dataset_landing_page'}]\n[{'URL': 'http://dawenl.github.io/data/gowalla_pro.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 1351,
        "URL_gold_label": [
            {
                "URL": "http://dawenl.github.io/data/gowalla_pro.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://snap.stanford.edu/data/loc-gowalla.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:",
        "answer": "  [{\"URL\": \"http://vision.in.tum.de/data/datasets/rgbd-dataset/tools\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1355,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset/tools",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page\n* URL: http://vision.in.tum.de/data/datasets/rgbd-dataset\nLabel: dataset_landing_page\n* URL: http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nLabel: dataset_landing_page\n* URL: http://ros.org\nLabel: Other\n* URL:",
        "repoID": 1355,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            },
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The EDUB-Seg dataset http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/  has been used in this paper. Please cite to this publication https://www.researchgate.net/publication/287901747_SR-Clustering_Semantic_Regularized_Clustering_for_Egocentric_Photo_Streams_Segmentation  if you use it. You might also want to check its project page https://github.com/MarcBS/SR-Clustering .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/\n\t+ Label: dataset_direct_link\n* URL: https://www.researchgate.net/publication/287901747_SR-Clustering_Semantic_Regularized_Clustering_for_Egocentric_Photo_Streams_Segmentation\n\t+ Label: dataset_landing_page\n* URL: https://github.com/MarcBS/SR-Clustering\n\t+ Label: software",
        "repoID": 1374,
        "URL_gold_label": [
            {
                "URL": "http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "OpenMIIR is released under the Open Data Commons Public Domain Dedication and Licence (PDDL) http://opendatacommons.org/licenses/pddl/1-0/ , which means that you can freely use it without any restrictions.",
        "answer": "  [INST]\n    {\n        \"URL\": \"http://opendatacommons.org/licenses/pddl/1-0/\",\n        \"label\": \"dataset_landing_page\"\n    }",
        "repoID": 1435,
        "URL_gold_label": [
            {
                "URL": "http://opendatacommons.org/licenses/pddl/1-0/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This data format can, for instance, also be easily converted into the MAT format used by Matlab, which allows importing into EEGLab. A description on how to do this can be found in the wiki https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab .",
        "answer": "  [{\"URL\": \"https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab\", \"label\": \"Other\"}]",
        "repoID": 1435,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Noise Contrastive Estimate examples/noise-contrastive-estimate.lua  for training multi-layer SeqLSTM #rnn.SeqLSTM  language models on the Google Billion Words dataset https://github.com/Element-Research/dataload#dl.loadGBW . The example uses MaskZero #rnn.MaskZero  to train independent variable length sequences using the NCEModule https://github.com/Element-Research/dpnn#nn.NCEModule  and NCECriterion https://github.com/Element-Research/dpnn#nn.NCECriterion . This script is our fastest yet boasting speeds of 20,000 words/second (on NVIDIA Titan X) with a 2-layer LSTM having 250 hidden units, a batchsize of 128 and sequence length of a 100. Note that you will need to have Torch installed with Lua instead of LuaJIT http://torch.ch/docs/getting-started.html#_ ;",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/Element-Research/dataload', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL \"https://github.com/Element-Research/dataload\" is a landing page for a dataset.",
        "repoID": 1506,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Element-Research/dataload#dl.loadGBW",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "dataload https://github.com/Element-Research/dataload  : a collection of torch dataset loaders;",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/Element-Research/dataload', 'label': 'Software'}]",
        "repoID": 1506,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Element-Research/dataload",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Dataset generated for those papers can be found here https://github.com/v-m/PropagationAnalysis-dataset .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/v-m/PropagationAnalysis-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 1574,
        "URL_gold_label": [
            {
                "URL": "https://github.com/v-m/PropagationAnalysis-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "NLTK Data http://www.nltk.org/data.html : punkt",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.nltk.org/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 1591,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "WikiQA: A Challenge Dataset for Open-Domain Question Answering https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/', 'label': 'dataset_landing_page'}]",
        "repoID": 1591,
        "URL_gold_label": [
            {
                "URL": "https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Data files are in the https://github.com/JJ/splash-volunteer/tree/data  branch, divided by experiment sets.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://github.com/JJ/splash-volunteer/tree/data', 'label': 'dataset_landing_page'}]",
        "repoID": 1608,
        "URL_gold_label": [
            {
                "URL": "https://github.com/JJ/splash-volunteer/tree/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "iPinYou data formalizing repository https://github.com/wnzhang/make-ipinyou-data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/wnzhang/make-ipinyou-data', 'label': 'dataset_landing_page'}]",
        "repoID": 1615,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Evaluation reports of your ontology (using the OOPS! web service https://oops.linkeddata.es/ )",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n[{'URL': 'https://oops.linkeddata.es/', 'label': 'dataset_direct_link'}]",
        "repoID": 1616,
        "URL_gold_label": [
            {
                "URL": "https://oops.linkeddata.es/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Integration with diagram creators ( WebVOWL http://vowl.visualdataweb.org/webvowl/ ).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://vowl.visualdataweb.org/webvowl/', 'label': 'dataset_landing_page'}]",
        "repoID": 1616,
        "URL_gold_label": [
            {
                "URL": "http://vowl.visualdataweb.org/webvowl/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "There are many other projects developing schemas and ontologies for the Web, e.g. Wikidata http://wikidata.org/  or the vocabulary projects in the Linked Data http://lov.okfn.org/  community. Many of these projects go into more expressive detail than is possible for a project like Schema.org. To keep Schema.org manageable, we have a strong bias towards designs that are grounded in large scale usage on the Web, in particular usage https://github.com/schemaorg/schemaorg/issues/652  by data-consuming applications since these in turn motivate data publishers. Other schema initiatives have different priorities and make different tradeoffs.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://wikidata.org/\nLabel: DatasetLandingPage\n* URL: http://lov.okfn.org/\nLabel: DatasetLandingPage\n* URL: https://github.com/schemaorg/schemaorg/issues/652\nLabel: Other",
        "repoID": 1620,
        "URL_gold_label": [
            {
                "URL": "http://wikidata.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "LOV entry: @lov.okfn.org http://lov.okfn.org/dataset/lov/vocabs/mv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': '@lov.okfn.org', 'label': 'dataset_landing_page'}]",
        "repoID": 1624,
        "URL_gold_label": [
            {
                "URL": "http://lov.okfn.org/dataset/lov/vocabs/mv",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Expiring items https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction  automatically based on certain criteria like TTL or last access time",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction', 'label': 'dataset_landing_page'}]",
        "repoID": 1638,
        "URL_gold_label": [
            {
                "URL": "https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For examples in other languages, please refer to the docs https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html .",
        "answer": "  [{\"URL\": \"https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1638,
        "URL_gold_label": [
            {
                "URL": "https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For examples in other languages, please refer to the docs https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html .",
        "answer": "  [{\"URL\": \"https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html\", \"label\": \"DatasetLandingPage\"}]",
        "repoID": 1638,
        "URL_gold_label": [
            {
                "URL": "https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Hazelcast also provides additional data structures such as ReplicatedMap, Set, MultiMap and List. For a full list, refer to the distributed data structures https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html  section of the docs.",
        "answer": "  [{\"URL\": \"https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1638,
        "URL_gold_label": [
            {
                "URL": "https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Read-through and write-through https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data  caching patterns",
        "answer": "  Sure, here are the annotated URLs for the input:\n\n[{'URL': 'https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data', 'label': 'dataset_landing_page'}]",
        "repoID": 1638,
        "URL_gold_label": [
            {
                "URL": "https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "A demo of Visual Event that is automatically loaded is available http://sprymedia.co.uk/VisualEvent/demo . This demo uses DataTables http://datatables.net  to create a test page with a number of events attached to different elements.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://sprymedia.co.uk/VisualEvent/demo', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL \"http://sprymedia.co.uk/VisualEvent/demo\" is a landing page for the Visual Event dataset.",
        "repoID": 1657,
        "URL_gold_label": [
            {
                "URL": "http://datatables.net",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Datasketch https://github.com/ekzhu/datasketch",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/ekzhu/datasketch', 'label': 'Software'}]",
        "repoID": 1688,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ekzhu/datasketch",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the 2D MOT 2015 benchmark dataset https://motchallenge.net/data/MOT15/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://motchallenge.net/data/MOT15/', 'label': 'dataset_direct_link'}]",
        "repoID": 1690,
        "URL_gold_label": [
            {
                "URL": "https://motchallenge.net/data/MOT15/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See Joint Declaration of Data Citation Principle https://www.force11.org/group/joint-declaration-data-citation-principles-final  as a example of a similar deliverable.",
        "answer": "  [{\"URL\": \"https://www.force11.org/group/joint-declaration-data-citation-principles-final\", \"label\": \"Other\"}]",
        "repoID": 1709,
        "URL_gold_label": [
            {
                "URL": "https://www.force11.org/group/joint-declaration-data-citation-principles-final",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "However, spectralDNS depends on two other modules in the spectralDNS https://github.com/spectralDNS  organization: shenfun https://github.com/spectralDNS/shenfun  and mpi4py-fft https://github.com/spectralDNS/mpi4py-fft . And besides that, it requires http://www.h5py.org  built with parallel HDF5, for visualizing the results, and http://cython.org , http://numba.pydata.org  or https://github.com/serge-sans-paille/pythran  are used to optimize a few routines. These dependencies are all available on https://conda-forge.org  and a proper environment would be",
        "answer": "  [{\"URL\": \"https://github.com/spectralDNS\", \"label\": \"dataset_landing_page\"}]\n\nNote: I only annotated the last input given.",
        "repoID": 1734,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Available here https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz', 'label': 'dataset_direct_link'}]",
        "repoID": 1742,
        "URL_gold_label": [
            {
                "URL": "https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "I got them from here https://www.cs.jhu.edu/~mdredze/datasets/sentiment/",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\nLabel: dataset_landing_page",
        "repoID": 1742,
        "URL_gold_label": [
            {
                "URL": "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "All the benchmark datasets are publicly available here http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets . These datasets were first introduced by Auto-WEKA http://www.cs.ubc.ca/labs/beta/Projects/autoweka  and have been widely used to evaluate Bayesian optimization methods.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets\nLabel: dataset_direct_link",
        "repoID": 1762,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Due to the file size limit, we are not able to provide all those datasets in our Github repository. In fact, only the  dataset is provided as an example. To deploy a new benchmark dataset, download the zip file from here http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets  and then uncompress it. You will get a dataset folder including two files  and . Move this folder into the data directory https://github.com/yuyuz/FLASH/tree/master/data , just like the dataset folder  we already put there.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets\nLabel: DatasetDirectLink\n* URL: https://github.com/yuyuz/FLASH/tree/master/data\nLabel: DatasetLandingPage",
        "repoID": 1762,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yuyuz/FLASH/tree/master/data",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets https://travis-ci.org/tdebatty/java-datasets http://www.javadoc.io/doc/info.debatty/java-datasets",
        "answer": "  Sure! Here are the annotations for the given URLs:\n\n[{'URL': 'https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://travis-ci.org/tdebatty/java-datasets', 'label': 'dataset_landing_page'}]\n[{'URL': 'http://www.javadoc.io/doc/info.debatty/java-datasets', 'label': 'Other'}]",
        "repoID": 1766,
        "URL_gold_label": [
            {
                "URL": "https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets",
                "gold_label": "software"
            },
            {
                "URL": "http://www.javadoc.io/doc/info.debatty/java-datasets",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For the other datasets, check the examples ./src/main/java/info/debatty/java/datasets/examples , or the documentation http://www.javadoc.io/doc/info.debatty/java-datasets .",
        "answer": "  [INST]\n    Output:\n    [\n        {\"URL\": \"http://www.javadoc.io/doc/info.debatty/java-datasets\", \"label\": \"dataset_landing_page\"},\n        {\"URL\": \"./src/main/java/info/debatty/java/datasets/examples\", \"label\": \"dataset_direct_link\"}\n    ]",
        "repoID": 1766,
        "URL_gold_label": [
            {
                "URL": "http://www.javadoc.io/doc/info.debatty/java-datasets",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Or check the GitHub releases https://github.com/tdebatty/java-datasets/releases .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/tdebatty/java-datasets/releases', 'label': 'dataset_landing_page'}]",
        "repoID": 1766,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tdebatty/java-datasets/releases",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Bokeh http://bokeh.pydata.org/en/latest/docs/installation.html  0.8.1+",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://bokeh.pydata.org/en/latest/docs/installation.html\",\n\"label\": \"Software\"\n}",
        "repoID": 1786,
        "URL_gold_label": [
            {
                "URL": "http://bokeh.pydata.org/en/latest/docs/installation.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "LZ4 is also compatible with dictionary compression https://github.com/facebook/zstd#the-case-for-small-data-compression , both at API https://github.com/lz4/lz4/blob/v1.8.3/lib/lz4frame.h#L481  and CLI https://github.com/lz4/lz4/blob/v1.8.3/programs/lz4.1.md#operation-modifiers  levels. It can ingest any input file as dictionary, though only the final 64KB are used. This capability can be combined with the Zstandard Dictionary Builder https://github.com/facebook/zstd/blob/v1.3.5/programs/zstd.1.md#dictionary-builder , in order to drastically improve compression performance on small files.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://github.com/lz4/lz4/blob/v1.8.3/lib/lz4frame.h\nLabel: DatasetDirectLink\n\n* URL: https://github.com/lz4/lz4/blob/v1.8.3/programs/lz4.1.md\nLabel: DatasetLandingPage\n\n* URL: https://github.com/facebook/zstd\nLabel: Software\n\n* URL: https://github.com/facebook/zstd/blob/v1.3.5/programs/zstd.1.md\nLabel: DatasetDirectLink\n\n* URL: https://github.com/facebook/zstd/blob/v1.3.5/programs/zstd.1.md",
        "repoID": 1801,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/zstd#the-case-for-small-data-compression",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": " https://travis-ci.org/stanford-futuredata/macrobase https://coveralls.io/github/stanford-futuredata/macrobase?branch=master",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://travis-ci.org/stanford-futuredata/macrobase', 'label': 'dataset_landing_page'}]",
        "repoID": 1815,
        "URL_gold_label": [
            {
                "URL": "https://coveralls.io/github/stanford-futuredata/macrobase?branch=master",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This script will turn an external raw dataset into torch format. The dataset will be split into a training/testing set by using the training ratio. When side inforamtion exist, they are automatically appended to the inputs. The MovieLens http://grouplens.org/datasets/movielens/  and Douban https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban  dataset are supported by default.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"http://grouplens.org/datasets/movielens/\",\n            \"label\": \"dataset_landing_page\"\n        },\n        {\n            \"URL\": \"http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\",\n            \"label\": \"Software\"\n        }\n    ]",
        "repoID": 1821,
        "URL_gold_label": [
            {
                "URL": "http://grouplens.org/datasets/movielens/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MIT300 http://saliency.mit.edu/datasets.html .",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://saliency.mit.edu/datasets.html', 'label': 'dataset_landing_page'}]",
        "repoID": 1822,
        "URL_gold_label": [
            {
                "URL": "http://saliency.mit.edu/datasets.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Please check our GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data . After downloading the dataset, by simplying  you can generate the standardised data which will be used in the bid optimisation tasks.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/wnzhang/make-ipinyou-data>\nLabel: Other",
        "repoID": 1824,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://github.com/hans/adversarial/blob/master/sampler/data_browser.py",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/hans/adversarial/blob/master/sampler/data_browser.py\",\n\"label\": \"Software\"\n}",
        "repoID": 1830,
        "URL_gold_label": [
            {
                "URL": "https://github.com/hans/adversarial/blob/master/sampler/data_browser.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "You can perform your own training runs using these YAML files. The paths in the YAML files reference my own local data; you'll need to download the LFW dataset and change these paths yourself. The \"file-list\" and embedding files referenced in the YAML files are available for LFW in the https://github.com/hans/adversarial/tree/master/data/lfwcrop_color . Once you have the paths in the YAML file, you can start training a model with the simple invocation of Pylearn2's  binary, e.g.",
        "answer": "  [{\"URL\": \"https://github.com/hans/adversarial/tree/master/data/lfwcrop_color\", \"label\": \"DatasetDirectLink\"}]",
        "repoID": 1830,
        "URL_gold_label": [
            {
                "URL": "https://github.com/hans/adversarial/tree/master/data/lfwcrop_color",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Our model's nonparametric prior distribution over skill labels can leverage skill labels provided by a human domain expert. If you want to provide them to our model, create a text file with one line per item. The ith line should be the expert-provided skill ID for the ith item. You can provide the file to WCRP via the command line argument --expertfile. An example file is available here https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt",
        "answer": "  [{\"URL\": \"https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt\", \"label\": \"Other\"}]",
        "repoID": 1857,
        "URL_gold_label": [
            {
                "URL": "https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "WCRP assumes that your student data are in a space-delimited text file with one row per trial. The columns should correspond to a trial's student ID, item ID, and whether the student produced a correct response in the trial. The IDs should be integers beginning at 0, and the trials for each student should be ordered from least to most recent. An example data file is available here https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt",
        "answer": "  [{\"URL\": \"https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 1857,
        "URL_gold_label": [
            {
                "URL": "https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "[ ] SemEval 2014 Cross-level Semantic Similarity Task http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools  (TODO; 500 paragraph-to-sentence training items)",
        "answer": "  [{\"URL\": \"http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1878,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "[ ] TAC tracks RTE-4 to RTE-7 http://www.nist.gov/tac/data/ . Printed user agreement required.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.nist.gov/tac/data/', 'label': 'dataset_landing_page'}]",
        "repoID": 1878,
        "URL_gold_label": [
            {
                "URL": "http://www.nist.gov/tac/data/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "[X] AI2 8th Grade Science Questions http://allenai.org/data.html  are 641 school Science quiz questions (A/B/C/D test format), stemming from The Allen AI Science Challenge https://www.kaggle.com/c/the-allen-ai-science-challenge/  We are going to produce a dataset that merges questions and answers in a single sentence, and pairs each with potential-evidencing sentences from Wikipedia and CK12 textbooks. This will be probably the hardest dataset by far included in this repo for some time. (We may also want to include the Elementary dataset.)",
        "answer": "  Sure, here are the annotations for the input:\n\n[{\"URL\": \"http://allenai.org/data.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1878,
        "URL_gold_label": [
            {
                "URL": "http://allenai.org/data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Apache CarbonData https://carbondata.apache.org ,",
        "answer": "  [{\"URL\": \"https://carbondata.apache.org\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 1885,
        "URL_gold_label": [
            {
                "URL": "https://carbondata.apache.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The YouTube SQL Engine, Google Procella https://research.google/pubs/pub48388/ , uses Roaring bitmaps for indexing. Apache Lucene http://lucene.apache.org/  uses Roaring bitmaps, though they have their own independent implementation https://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.java?view=markup&pathrev=1629606 . Derivatives of Lucene such as Solr and Elastic also use Roaring bitmaps. Other platforms such as Whoosh https://pypi.python.org/pypi/Whoosh/ , Microsoft Visual Studio Team Services (VSTS) https://www.visualstudio.com/team-services/  and Pilosa https://github.com/pilosa/pilosa  also use Roaring bitmaps with their own implementations. You find Roaring bitmaps in InfluxDB https://www.influxdata.com , Bleve http://www.blevesearch.com , Cloud Torrent https://github.com/jpillora/cloud-torrent , and so forth.",
        "answer": "  Here are the annotated URLs in the input text:\n\n1. \"https://research.google.com/pub48388/\" - Label: DatasetLandingPage\n2. \"http://lucene.apache.org/\" - Label: Software\n3. \"https://svn.apache.org/viewvc/lucene/dev/branches/branch_5x/lucene/core/src/java/org/apache/lucene/util/RoaringDocId",
        "repoID": 1885,
        "URL_gold_label": [
            {
                "URL": "https://www.influxdata.com",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "In this example we use the tools provided by the micompr https://github.com/fakenmc/micompr  package to study the PH https://www.fc.up.pt/addi/ph2%20database.html  of dermoscopic images. This image database contains a total of 200 dermoscopic images of melanocytic lesions, including, from benign to more serious, 80 common nevi, 80 atypical nevi, and 40 melanomas. The goal is to verify if images of the three types of lesions form statistically distinguishable samples.",
        "answer": "Sure, here are the annotated URLs:\n\n* URL: https://www.fc.up.pt/addi/ph2 database.html\nLabel: dataset_landing_page",
        "repoID": 1891,
        "URL_gold_label": [
            {
                "URL": "https://www.fc.up.pt/addi/ph2%20database.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you want to work with John Snow's GIS, or the ETEX data (as used in the paper https://arxiv.org/abs/1603.07584 ), you will need to clone another repository, load-data https://github.com/rodrigo-pena/load-data , and follow the installation instructions in the README therein.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/rodrigo-pena/load-data>\nLabel: Other",
        "repoID": 1903,
        "URL_gold_label": [
            {
                "URL": "https://github.com/rodrigo-pena/load-data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Annotation http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 1915,
        "URL_gold_label": [
            {
                "URL": "http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Images http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz', 'label': 'dataset_direct_link'}]",
        "repoID": 1915,
        "URL_gold_label": [
            {
                "URL": "http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "facebook https://snap.stanford.edu/data/egonets-Facebook.html , should find 5",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n* `facebook`: `dataset_landing_page`\n\nThe URL `https://snap.stanford.edu/data/egonets-Facebook.html` is a landing page for a dataset entity, specifically the Facebook egonets dataset.",
        "repoID": 1968,
        "URL_gold_label": [
            {
                "URL": "https://snap.stanford.edu/data/egonets-Facebook.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Hippo index is one of the projects under DataSys Lab http://www.datasyslab.org/  at Arizona State University. The mission of DataSys Lab is designing and developing experimental data management systems (e.g., database systems).",
        "answer": "  Sure, here are the annotated URLs for the given input:\n\n* URL: http://www.datasyslab.org/\nLabel: dataset_landing_page\n\nNote: The URL is for the homepage of DataSys Lab, which is a dataset entity.",
        "repoID": 1971,
        "URL_gold_label": [
            {
                "URL": "http://www.datasyslab.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The data used in this gem is also available as git submodules in YAML https://github.com/countries/countries-data-yaml  and JSON https://github.com/countries/countries-data-json  files.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/countries/countries-data-yaml', 'label': 'dataset_landing_page'}]\n[{'URL': 'https://github.com/countries/countries-data-json', 'label': 'dataset_landing_page'}]",
        "repoID": 1980,
        "URL_gold_label": [
            {
                "URL": "https://github.com/countries/countries-data-yaml",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/countries/countries-data-json",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Use 2486 .lab files https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip  to do even more interesting!",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 2003,
        "URL_gold_label": [
            {
                "URL": "https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The Crossref REST API is one of a variety of tools and APIs https://www.crossref.org/services/metadata-delivery/  that allow anybody to search and reuse our members' metadata in sophisticated ways.",
        "answer": "  [{\"URL\": \"https://www.crossref.org/services/metadata-delivery/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2005,
        "URL_gold_label": [
            {
                "URL": "https://www.crossref.org/services/metadata-delivery/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "What if you want to use our API for a production service that cannot depend on the performance uncertainties of the free and open public API? What if you don't want to be affected by impolite people who do not follow the API Etiquette #api-etiquette  guidelines? Well, if you\u2019re interested in using these tools or APIs for production services, we have a service-level offering https://www.crossref.org/services/metadata-delivery/plus-service/  called \"Plus\". This service provides you with access to all supported APIs and metadata, but with extra service and support guarantees.",
        "answer": "  Sure! Here are the annotated URLs in the input text:\n\n* URL: https://www.crossref.org/services/metadata-delivery/plus-service/\nLabel: Other",
        "repoID": 2005,
        "URL_gold_label": [
            {
                "URL": "https://www.crossref.org/services/metadata-delivery/plus-service/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Finally, the following python utilities can be used to create structured full text corpora of scientific articles. The tool simply takes a list of strong identifiers like DOI or PMID, performing the identification of online Open Access PDF, full text harvesting, metadata aggregation and Grobid processing in one workflow at scale: article-dataset-builder https://github.com/kermitt2/article-dataset-builder",
        "answer": "  [{\"URL\": \"https://github.com/kermitt2/article-dataset-builder\", \"label\": \"Software\"}]",
        "repoID": 2006,
        "URL_gold_label": [
            {
                "URL": "https://github.com/kermitt2/article-dataset-builder",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "GROBID does not use training data derived from existing publisher XML documents, but small, high quality sets https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat  of manually labeled training data.",
        "answer": "  [{\"URL\": \"https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2006,
        "URL_gold_label": [
            {
                "URL": "https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "dataseer-ml https://github.com/dataseer/dataseer-ml : identification of sections and sentences introducing datasets in a scientific article, and classification of the type of these datasets",
        "answer": "  Sure, here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/dataseer/dataseer-ml', 'label': 'dataset_landing_page'}]",
        "repoID": 2006,
        "URL_gold_label": [
            {
                "URL": "https://github.com/dataseer/dataseer-ml",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "datastet https://github.com/kermitt2/datastet : identification of named and implicit research datasets and associated attributes in scientific articles",
        "answer": "  Sure, here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/kermitt2/datastet', 'label': 'dataset_landing_page'}]",
        "repoID": 2006,
        "URL_gold_label": [
            {
                "URL": "https://github.com/kermitt2/datastet",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pre-computed optical flow images and resized rgb frames for the UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets",
        "answer": "  Sure! Here are the annotated URLs:\n\nInput: Pre-computed optical flow images and resized RGB frames for the UCF101 http://crcv.ucf.edu/data/UCF101.php and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ datasets\n\nOutput:\n\n[{\"URL\": \"http://crcv.ucf.edu/data/UCF101.php\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2024,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To run the code one needs first download and unzip the  and  files on Kaggle https://www.kaggle.com/c/rossmann-store-sales/data  and put them in this folder.",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* URL: https://www.kaggle.com/c/rossmann-store-sales/data\nLabel: DatasetDirectLink",
        "repoID": 2031,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/rossmann-store-sales/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "converts a communities network in SNAP format http://snap.stanford.edu/data/index.html  and converts it to symmetric adjacency hypergraph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name.",
        "answer": "  [{'URL': 'http://snap.stanford.edu/data/index.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2046,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "converts a graph in SNAP format http://snap.stanford.edu/data/index.html  and converts it to Ligra's adjacency graph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name. The \"-s\" flag may be used to symmetrize the input file. This converter works for any format that lists the two endpoints of each edge separated by white space per line, with lines starting with '#' ignored.",
        "answer": "  [{\"URL\": \"http://snap.stanford.edu/data/index.html\", \"label\": \"dataset_landing_page\"}]\n\nNote: Since the input URL is a webpage, the label is set to \"dataset_landing_page\".",
        "repoID": 2046,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "3DPeS http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 2047,
        "URL_gold_label": [
            {
                "URL": "http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Datasets http://caffe2.ai/docs/datasets.html",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://caffe2.ai/docs/datasets.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2104,
        "URL_gold_label": [
            {
                "URL": "http://caffe2.ai/docs/datasets.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "TensorFlow: Data and Deployment from Coursera https://www.coursera.org/specializations/tensorflow-data-and-deployment",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.coursera.org/specializations/tensorflow-data-and-deployment', 'label': 'dataset_landing_page'}]",
        "repoID": 2111,
        "URL_gold_label": [
            {
                "URL": "https://www.coursera.org/specializations/tensorflow-data-and-deployment",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Numba http://numba.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://numba.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 2124,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We cannot release our dataset publicly, so the a toy dataset needs to be set up. We use the UCI Adult Data Set https://archive.ics.uci.edu/ml/datasets/Adult  for this purpose. Although it is a regular tabular dataset, and has a Gausian distribution instead of a heavy tailed one over the attributes, it is fine for experimenting. Then you can work on your own.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/Adult', 'label': 'dataset_landing_page'}]",
        "repoID": 2210,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Adult",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "RAPPOR Data Flow http://google.github.io/rappor/doc/data-flow.html",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://google.github.io/rappor/doc/data-flow.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2211,
        "URL_gold_label": [
            {
                "URL": "http://google.github.io/rappor/doc/data-flow.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Some of the data file can be download at here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/', 'label': 'dataset_direct_link'}]",
        "repoID": 2224,
        "URL_gold_label": [
            {
                "URL": "https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The pre-trained model can be download here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/ , if you use the vqa train https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/vqa_model/model_alternating_train_vgg.t7  model, you should use the corresponding json file form here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/\nLabel: dataset_direct_link\n\n* URL: https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json\nLabel: dataset_landing_page",
        "repoID": 2224,
        "URL_gold_label": [
            {
                "URL": "https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "To Evaluate VQA, you need to download the VQA evaluation tool https://github.com/VT-vision-lab/VQA . To evaluate COCO-QA, you can use script  under  folder. If you need to evaluate based on WUPS, download the evaluation script from here http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/VT-vision-lab/VQA\nLabel: DatasetDirectLink\n* URL: http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py\nLabel: Other",
        "repoID": 2224,
        "URL_gold_label": [
            {
                "URL": "http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "if you use the vqa train+val https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/model/vqa_model/model_alternating_train-val_vgg.t7  model, you should use the corresponding json file form here https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json', 'label': 'dataset_landing_page'}]",
        "repoID": 2224,
        "URL_gold_label": [
            {
                "URL": "https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Download the CoNLL training data from here http://conll.cemantix.org/2012/data.html .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://conll.cemantix.org/2012/data.html', 'label': 'dataset_direct_link'}]",
        "repoID": 2252,
        "URL_gold_label": [
            {
                "URL": "http://conll.cemantix.org/2012/data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "There is no formal definition of Big Data \u2013 though many http://www.opentracker.net/article/definitions-big-data  have tried. Most of them agree that it is more than 'large data', more than just its size.",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'http://www.opentracker.net/article/definitions-big-data', 'label': 'Other'}]",
        "repoID": 2264,
        "URL_gold_label": [
            {
                "URL": "http://www.opentracker.net/article/definitions-big-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "dataset_forVD2D3D.spec https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec  defines path for image/label files of your dataset. For example images 1-12 refer to the images from the microscope, 13-24 refer to the output images from the VD2D part of the recursive \"two-stage\" approach of ZNN. The outputs of the VD2D part are provided in the dataset repository https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh , and can be used for re-training of the VD2D3D stage, or if you may you can obtain new VD2D outputs for your dataset if you wish.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec', 'label': 'dataset_landing_page'}]",
        "repoID": 2270,
        "URL_gold_label": [
            {
                "URL": "https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The original datasets can be downloaded from https://github.com/deepmind/rc-data https://github.com/deepmind/rc-data  or http://cs.nyu.edu/~kcho/DMQA/ http://cs.nyu.edu/~kcho/DMQA/ . Our processed ones are just simply concatenation of all data instances and keeping document, question and answer only for our inputs.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/deepmind/rc-data\nLabel: DatasetDirectLink\n* URL: https://github.com/deepmind/rc-data\nLabel: DatasetDirectLink\n\n* URL: http://cs.nyu.edu/~kcho/DMQA/\nLabel: DatasetLandingPage",
        "repoID": 2282,
        "URL_gold_label": [
            {
                "URL": "https://github.com/deepmind/rc-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "First prepare the data as in data_prep/ https://github.com/harvardnlp/BSO/tree/master/data_prep .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/harvardnlp/BSO/tree/master/data_prep', 'label': 'dataset_landing_page'}]",
        "repoID": 2285,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/BSO/tree/master/data_prep",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the official SemEval evaluation materials from here http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip  and put the file  in the  directory.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 2334,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Squeeze the best data loading performance of Python with https://github.com/tensorpack/dataflow .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://github.com/tensorpack/dataflow', 'label': 'Software'}]",
        "repoID": 2346,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorpack/dataflow",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Symbolic programming (e.g. ) does not https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions  offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of  with various autoparallelization strategies.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions', 'label': 'dataset_landing_page'}]",
        "repoID": 2346,
        "URL_gold_label": [
            {
                "URL": "https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The JIGSAWS dataset is available here http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/ . After registering, an automated system will send you a download link. To run this code, you'll need to download the Suturing Kinematics data.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/', 'label': 'dataset_landing_page'}]",
        "repoID": 2354,
        "URL_gold_label": [
            {
                "URL": "http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Buckeye corpus: buckeyecorpus.osu.edu http://buckeyecorpus.osu.edu/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'buckeyecorpus.osu.edu', 'label': 'dataset_landing_page'}]",
        "repoID": 2370,
        "URL_gold_label": [
            {
                "URL": "http://buckeyecorpus.osu.edu/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install http://conda.pydata.org/miniconda.html",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://conda.pydata.org/miniconda.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 2404,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The program also provides the implementation of the embedding model TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data . See an overview of embedding models of entities and relationships for knowledge base completion at HERE https://arxiv.org/pdf/1703.08098.pdf .",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data', 'label': 'dataset_direct_link'}]\n\n[{'URL': 'https://arxiv.org/pdf/1703.08098.pdf', 'label': 'other'}]",
        "repoID": 2405,
        "URL_gold_label": [
            {
                "URL": "https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "All input is completed squeezed of whitespace/newlines so only the output really matters when examining CodeBuff output. You can check out the output https://github.com/antlr/codebuff/tree/master/output  dir for leave-one-out formatting of the various corpora https://github.com/antlr/codebuff/tree/master/corpus . But, here are some sample formatting results.",
        "answer": "  Understood! Here are the annotated URLs:\n\nInput: All input is completed squeezed of whitespace/newlines so only the output really matters when examining CodeBuff output. You can check out the output <https://github.com/antlr/codebuff/tree/master/output> directory for leave-one-out formatting of the various corpora <https://github.com/antlr/codebuff/tree/master/corpus>. But, here are some sample formatting results.\n\nOutput:\n\n1. DatasetDirectLink:\n\t* URL: <https://snap.stanford.edu/data/loc-gowalla.html>\n\t* Label: dataset_direct_link\n2. DatasetLandingPage:\n\t* URL: <http://law.di.unimi.it/datasets.php>\n\t* Label: dataset_landing_page\n3. Software:\n\t* URL: <http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable>\n\t* Label: software\n4. Other:\n\t* URL: <https://github.com/antlr/codebuff/tree/master/output>\n\t* Label: other",
        "repoID": 2414,
        "URL_gold_label": [
            {
                "URL": "https://github.com/antlr/codebuff/tree/master/corpus",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "antlr corpus https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training  and antlr parser grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Parser.g4 , antlr lexer grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Lexer.g4",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: <https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training>\nLabel: DatasetDirectLink\n* URL: <https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Parser.g4>\nLabel: Software\n* URL: <https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/ANTLRv4Lexer.g4>\nLabel: Software",
        "repoID": 2414,
        "URL_gold_label": [
            {
                "URL": "https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava  and java grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java.g4",
        "answer": "  Sure, here are the annotations for the URLs you provided:\n\nInput: guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava\n\nOutput: [{\"URL\": \"https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava\", \"label\": \"dataset_landing_page\"}]\n\nInput: java grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java.g4\n\nOutput: [{\"URL\": \"https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java.g4\", \"label\": \"Software\"}]",
        "repoID": 2414,
        "URL_gold_label": [
            {
                "URL": "https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava  and java8 grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java8.g4",
        "answer": "  Sure, here are the annotations for the URLs you provided:\n\nInput: guava corpus https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava\n\nOutput: [{\"URL\": \"https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava\", \"label\": \"dataset_landing_page\"}]\n\nInput: java8 grammar https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java8.g4\n\nOutput: [{\"URL\": \"https://github.com/antlr/codebuff/blob/master/grammars/org/antlr/codebuff/Java8.g4\", \"label\": \"Software\"}]",
        "repoID": 2414,
        "URL_gold_label": [
            {
                "URL": "https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The preprocessed YFCC100M data https://fasttext.cc/docs/en/dataset.html#content  used in [2].",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://fasttext.cc/docs/en/dataset.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2449,
        "URL_gold_label": [
            {
                "URL": "https://fasttext.cc/docs/en/dataset.html#content",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The majority of the data is real-world from a variety of sources such as AWS server metrics, Twitter volume, advertisement clicking metrics, traffic data, and more. All data is included in the repository, with more details in the data readme https://github.com/numenta/NAB/tree/master/data . Please contact us at nab@numenta.org mailto:nab@numenta.org  if you have similar data (ideally with known anomalies) that you would like to see incorporated into NAB.",
        "answer": "  [{\"URL\": \"https://github.com/numenta/NAB/tree/master/data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2456,
        "URL_gold_label": [
            {
                "URL": "https://github.com/numenta/NAB/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Almatrafi et al. PAMI 2020 #Almatrafi20arxiv : . DVSMOTION20 Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Almatrafi, M., Baldwin, R., Aizawa, K., Hirakawa, K., , IEEE Trans. Pattern Anal. Machine Intell. (TPAMI), 2020. PDF https://arxiv.org/pdf/2003.12680 , Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://arxiv.org/pdf/2003.12680', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://sites.google.com/a/udayton.edu/issl/software/dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Baldwin R.W., Almatrafi M., Asari V., Hirakawa K., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. PDF https://arxiv.org/abs/2003.08282 , Dataset https://sites.google.com/a/udayton.edu/issl/software/dataset",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://arxiv.org/abs/2003.08282', 'label': 'dataset_direct_link'}, {'URL': 'https://sites.google.com/a/udayton.edu/issl/software/dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/a/udayton.edu/issl/software/dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Bardow et al., CVPR2016 #Bardow16cvpr , Four sequences http://wp.doc.ic.ac.uk/pb2114/datasets/",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://wp.doc.ic.ac.uk/pb2114/datasets/', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://wp.doc.ic.ac.uk/pb2114/datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Bardow, P. A., Davison, A. J., Leutenegger, S., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016. YouTube https://youtu.be/1zqJpiheaaI , YouTube 2 https://youtu.be/CASsIFuPxmc , Dataset: 4 sequences http://wp.doc.ic.ac.uk/pb2114/datasets/",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://youtu.be/1zqJpiheaaI', 'label': 'Other'}]\n[{'URL': 'https://youtu.be/CASsIFuPxmc', 'label': 'Other'}]\n[{'URL': 'http://wp.doc.ic.ac.uk/pb2114/datasets/', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://wp.doc.ic.ac.uk/pb2114/datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Binas, J., Neil, D., Liu, S.-C., Delbruck, T.,  Int. Conf. Machine Learning, Workshop on Machine Learning for Autonomous Vehicles, 2017. Dataset http://sensors.ini.uzh.ch/databases.html",
        "answer": "  [{\"URL\": \"http://sensors.ini.uzh.ch/databases.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://sensors.ini.uzh.ch/databases.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Brebion, V., Moreau, J., Davoine, F., , IEEE Trans. Intell. Transp. Syst. (T-ITS), 2021. PDF https://arxiv.org/pdf/2112.10591.pdf , Code https://github.com/vbrebion/rt_of_low_high_res_event_cameras , Dataset https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO , YouTube https://youtube.com/playlist?list=PLLL0eWAd6OXBRXli-tB1NREdhBElAxisD .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://arxiv.org/pdf/2112.10591.pdf', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://github.com/vbrebion/rt_of_low_high_res_event_cameras', 'label':'software'}]\n[{'URL': 'https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO', 'label': 'dataset_landing_page'}]\n[{'URL': 'https://youtube.com/playlist?list=PLLL0eWAd6OXBRXli-tB1NREdhBElAxisD', 'label': 'other'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cannici, M., Plizzari, C., Planamente, M., Ciccone, M., Bottino, A., Caputo, B., Matteucci, M., , IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), 2021. Project page https://n-rod-dataset.github.io/home/ , YouTube https://youtu.be/IOZl8MxrfpQ , Poster https://tub-rip.github.io/eventvision2021/slides/CVPRW21_NROD_poster.pdf .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://n-rod-dataset.github.io/home/>\nLabel: DatasetLandingPage\n* URL: <https://youtu.be/IOZl8MxrfpQ>\nLabel: Other\n* URL: <https://tub-rip.github.io/eventvision2021/slides/CVPRW21_NROD_poster.pdf>\nLabel: Other",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://n-rod-dataset.github.io/home/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "DND21 DeNoising Dynamic vision sensors dataset https://sites.google.com/view/dnd21/datasets?authuser=0  associated to the paper Low Cost and Latency Event Camera Background Activity Denoising #GuoDelbruck22pami",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://sites.google.com/view/dnd21/datasets?authuser=0', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/view/dnd21/datasets?authuser=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "DVSNOISE20 https://sites.google.com/a/udayton.edu/issl/software/dataset  associated to the paper Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras #Baldwin20cvpr .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://sites.google.com/a/udayton.edu/issl/software/dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/a/udayton.edu/issl/software/dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Datasets from the Sensors group at INI http://sensors.ini.uzh.ch/databases.html  (Institute of Neuroinformatics), Zurich:",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://sensors.ini.uzh.ch/databases.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://sensors.ini.uzh.ch/databases.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, D. Scaramuzza,  Int. J. Robotics Research, 36:2, pp. 142-149, 2017. PDF https://arxiv.org/pdf/1610.08336.pdf , PDF IJRR http://dx.doi.org/10.1177/0278364917691115 , YouTube https://youtu.be/bVVBTQ7l36I , Dataset http://rpg.ifi.uzh.ch/davis_data.html .",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: <https://arxiv.org/pdf/1610.08336.pdf>\nLabel: dataset_direct_link\n* URL: <http://dx.doi.org/10.1177/0278364917691115>\nLabel: dataset_direct_link\n* URL: <https://youtu.be/bVVBTQ7l36I>\nLabel: other\n* URL: <http://rpg.ifi.uzh.ch/davis_data.html>\nLabel: dataset_landing_page",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/davis_data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "El Shair, Z., Rawashdeh, S.A., , Journal of Imaging, 2022. PDF https://www.mdpi.com/2313-433X/8/8/210/pdf , dataset http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/ .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.mdpi.com/2313-433X/8/8/210/pdf', 'label': 'dataset_direct_link'}]\n\nThe URL \"https://www.mdpi.com/2313-433X/8/8/210/pdf\" is a dataset direct link.",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Gao, L., Liang, Y., Yang, J., Wu, S., Wang, C., Chen, J., Kneip, L., , IEEE Robotics and Automation Letters (RA-L), 7(3):8217-8224, 2022. PDF https://arxiv.org/abs/2207.01404 , Dataset https://star-datasets.github.io/vector/ , MPL Calibration Toolbox https://github.com/mgaoling/mpl_calibration_toolbox , MPL Dataset Toolbox https://github.com/mgaoling/mpl_dataset_toolbox .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://arxiv.org/abs/2207.01404\nLabel: Other\n* URL: https://star-datasets.github.io/vector/\nLabel: Dataset Landing Page\n* URL: https://github.com/mgaoling/mpl_calibration_toolbox\nLabel: Software\n* URL: https://github.com/mgaoling/mpl_dataset_toolbox\nLabel: Software",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://star-datasets.github.io/vector/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/mgaoling/mpl_dataset_toolbox",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Hu, Y., Binas, J., Neil, D., Liu, S.-C., Delbruck, T., , IEEE Intelligent Transportation Systems Conf. (ITSC), 2020. Dataset https://sites.google.com/view/davis-driving-dataset-2020/home , More datasets http://sensors.ini.uzh.ch/databases.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://sites.google.com/view/davis-driving-dataset-2020/home>\nLabel: dataset_landing_page\n\n* URL: <http://sensors.ini.uzh.ch/databases.html>\nLabel: dataset_landing_page",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/view/davis-driving-dataset-2020/home",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://sensors.ini.uzh.ch/databases.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Lee, A. J., Cho, Y., Shin, Y., Kim, A., Myung, H., ViViD++: Vision for Visibility Dataset https://visibilitydataset.github.io/ , IEEE Robotics and Automation Letters (RA-L), 2022. Dataset https://visibilitydataset.github.io/",
        "answer": "  [{\"URL\": \"https://visibilitydataset.github.io/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://visibilitydataset.github.io/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://visibilitydataset.github.io/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "N-CARS Dataset http://www.prophesee.ai/dataset-n-cars/ : A large real-world event-based dataset for car classification. Sironi et al., CVPR 2018 #Sironi18cvpr .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.prophesee.ai/dataset-n-cars/', 'label': 'dataset_direct_link'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://www.prophesee.ai/dataset-n-cars/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Neuromorphic-MNIST (N-MNIST) dataset http://www.garrickorchard.com/datasets/n-mnist  is a spiking version of the original frame-based MNIST dataset (of handwritten digits). YouTube https://youtu.be/6qK97qM5aB4",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.garrickorchard.com/datasets/n-mnist', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://www.garrickorchard.com/datasets/n-mnist",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Perot, E., de Tournemire, P., Nitti, D., Masci, J., Sironi, A., 1Mpx Detection Dataset https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/ : Learning to Detect Objects with a 1 Megapixel Event Camera. NeurIPS 2020 #Perot20nips .",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Prophesee automotive dataset toolbox, Code https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox",
        "answer": "  [{\"URL\": \"https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox\", \"label\": \"Software\"}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Neuromorphic-Caltech101 (N-Caltech101) dataset http://www.garrickorchard.com/datasets/n-caltech101  is a spiking version of the original frame-based Caltech101 dataset. YouTube https://youtu.be/dxit9Ce5f_E",
        "answer": "  [{\"URL\": \"http://www.garrickorchard.com/datasets/n-caltech101\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "http://www.garrickorchard.com/datasets/n-caltech101",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Wang, L., Kim, T.-K., Yoon, K.-J., , IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. PDF https://arxiv.org/pdf/2003.07640 , YouTube https://youtu.be/OShS_MwHecs , Dataset https://github.com/wl082013/ESIM_dataset",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://arxiv.org/pdf/2003.07640', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://youtu.be/OShS_MwHecs', 'label': 'other'}]\n[{'URL': 'https://github.com/wl082013/ESIM_dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wl082013/ESIM_dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Zhang, J., Yang, X., Fu, Y., Wei, X., Yin, B., Dong, B., , IEEE Int. Conf. Computer Vision (ICCV), 2021. Project https://zhangjiqing.com/publication/iccv21_fe108_tracking/ , PDF https://arxiv.org/abs/2109.09052 , code https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking , dataset https://zhangjiqing.com/dataset/ .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://zhangjiqing.com/publication/iccv21_fe108_tracking/', 'label': 'dataset_landing_page'},\n {'URL': 'https://arxiv.org/abs/2109.09052', 'label': 'dataset_direct_link'},\n {'URL': 'https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking', 'label': 'Software'},\n {'URL': 'https://zhangjiqing.com/dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://zhangjiqing.com/dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "de Tournemire, P., Nitti, D., Perot, E., Migliore, D., Sironi, A., , arXiv, 2020. Code https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox , News https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox', 'label': 'Software'}]\n\nExplanation:\n\n* The URL \"https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox\" is for a software entity, specifically a GitHub repository containing code for an automotive dataset toolbox.\n* The URL \"https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/\" is a news article or blog post, and therefore does not fall into any of the above categories.",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "events_bag2h5 https://github.com/mgaoling/mpl_dataset_toolbox  Python code to convert event data from ROSbags to HDF5.",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/mgaoling/mpl_dataset_toolbox', 'label': 'Software'}]",
        "repoID": 2468,
        "URL_gold_label": [
            {
                "URL": "https://github.com/mgaoling/mpl_dataset_toolbox",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "is the name of a file where the weighted spatial tessellation is stored. The format of the  file must be the following: . Latitude and longitude are float numbers indicating the geographic position of a location and relevance is an integer indicating the importance of the location (e.g., the number of residents in the locations or the number of calls made in that location during a given period). We provide you an example of spatial tessellation in the Italian regione of Trentino, stored in the file . This spatial tessellation is released under a Open Data Commons Open Database License (ODbL) and it is obtained from the publicly available dataset described in this paper http://www.nature.com/articles/sdata201555#data-records http://www.nature.com/articles/sdata201555#data-records .",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: <http://www.nature.com/articles/sdata201555#data-records>\nLabel: Other",
        "repoID": 2485,
        "URL_gold_label": [
            {
                "URL": "http://www.nature.com/articles/sdata201555#data-records",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ProgressiVis relies on well known Python libraries, such as numpy http://www.numpy.org/ , scipy http://www.scipy.org/ , Pandas http://pandas.pydata.org/ , and Scikit-Learn http://scikit-learn.org/ .",
        "answer": "  [{\"URL\": \"http://www.numpy.org/\", \"label\": \"Software\"}, {\"URL\": \"http://www.scipy.org/\", \"label\": \"Software\"}, {\"URL\": \"http://pandas.pydata.org/\", \"label\": \"Software\"}, {\"URL\": \"http://scikit-learn.org/\", \"label\": \"Software\"}]",
        "repoID": 2489,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "https://x-datainitiative.github.io/tick https://x-datainitiative.github.io/tick/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://x-datainitiative.github.io/tick', 'label': 'dataset_landing_page'}]",
        "repoID": 2496,
        "URL_gold_label": [
            {
                "URL": "https://x-datainitiative.github.io/tick/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "https://x-datainitiative.github.io/tick/auto_examples/index.html https://x-datainitiative.github.io/tick/auto_examples/index.html",
        "answer": "  Sure! Here are the annotations for the input URLs:\n\n[{'URL': 'https://x-datainitiative.github.io/tick/auto_examples/index.html', 'label': 'dataset_landing_page'}]",
        "repoID": 2496,
        "URL_gold_label": [
            {
                "URL": "https://x-datainitiative.github.io/tick/auto_examples/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Besides, add \"mscoco\" into the  folder, which can be from mscoco http://mscoco.org/dataset/#overview  COCO's images are used for RefCOCO, RefCOCO+ and refCOCOg. For RefCLEF, please add  into  folder. We extracted the related 19997 images to our cleaned RefCLEF dataset, which is a subset of the original imageCLEF http://imageclef.org/SIAPRdata . Download the subset https://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip  and unzip it to .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://mscoco.org/dataset/#overview\nLabel: DatasetLandingPage\n* URL: https://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip\nLabel: DatasetDirectLink",
        "repoID": 2540,
        "URL_gold_label": [
            {
                "URL": "http://imageclef.org/SIAPRdata",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We experimented on two mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.",
        "answer": "  Sure, here are the annotated URLs:\n\n* UCF-101: [{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]\n* HMDB51: [{'URL': 'http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/', 'label': 'dataset_landing_page'}]",
        "repoID": 2553,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This repository contains annotations in JAMS github.com/marl/jams/  format [1,2] for the UrbanSound8k dataset https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html  [3]. It also contains the extended JAMS files returned by the MUDA audio data augmentation library https://github.com/bmcfee/muda  [4], which contain the deformation parameters utilized to generate the augmented UrbanSound8K training set used in [5].",
        "answer": "  Sure! Here are the annotated URLs:\n\n1. UrbanSound8k dataset: <https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html> (label: dataset_landing_page)\n2. JAMS format: <https://github.com/marl/jams/format> (label: dataset_direct_link)\n3. MUDA audio data augmentation library: <https://github.com/bmcfee/muda> (label: software)\n4. Extended JAMS files: <https://github.com/bmcfee/muda> (label: other)",
        "repoID": 2617,
        "URL_gold_label": [
            {
                "URL": "https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "An example server is available at data.linkeddatafragments.org http://data.linkeddatafragments.org/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://data.linkeddatafragments.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 2678,
        "URL_gold_label": [
            {
                "URL": "http://data.linkeddatafragments.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Instead, this server offers Quad Pattern Fragments https://linkeddatafragments.org/specification/quad-pattern-fragments/  (a.k.a. ). Each Quad Pattern Fragment offers:",
        "answer": "  [{\"URL\": \"https://linkeddatafragments.org/specification/quad-pattern-fragments/\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 2678,
        "URL_gold_label": [
            {
                "URL": "https://linkeddatafragments.org/specification/quad-pattern-fragments/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "On today's Web, Linked Data is published in different ways, which include data dumps http://downloads.dbpedia.org/3.9/en/ , subject pages http://dbpedia.org/page/Linked_data , and results of SPARQL queries http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=CONSTRUCT+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D%0D%0AWHERE+%7B+%3Fp+a+dbpedia-owl%3AArtist+%7D&format=text%2Fturtle . We call each such part a http://linkeddatafragments.org/ .",
        "answer": "ed URLs in the input text:\n\n* URL: http://downloads.dbpedia.org/3.9/en/\nLabel: dataset_direct_link\n* URL: http://dbpedia.org/page/Linked_data\nLabel: dataset_landing_page\n* URL: http://dbpedia.org/sparql?default-graph-uri=http://dbpedia.org&query=CONSTRUCT+{+?p+a+dbpedia-owl:Artist+}\r\nWHERE+{+?p+a+dbpedia-owl:Artist+}&format=text/turtle\nLabel: software",
        "repoID": 2678,
        "URL_gold_label": [
            {
                "URL": "http://dbpedia.org/page/Linked_data",
                "gold_label": "other"
            },
            {
                "URL": "http://linkeddatafragments.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This repository contains modules for Linked Data Fragments (LDF) https://linkeddatafragments.org/  servers.",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://linkeddatafragments.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 2678,
        "URL_gold_label": [
            {
                "URL": "https://linkeddatafragments.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "https://github.com/LinkedDataFragments/Server.js/tree/master/packages/feature-qpf : Feature that enables Quad Pattern Fragments https://linkeddatafragments.org/specification/quad-pattern-fragments/  (a.k.a. Triple Pattern Fragments https://linkeddatafragments.org/specification/triple-pattern-fragments/ ).",
        "answer": "  [INST]\n    Output: [{\"URL\": \"https://github.com/LinkedDataFragments/Server.js/tree/master/packages/feature-qpf\", \"label\": \"Software\"}]",
        "repoID": 2678,
        "URL_gold_label": [
            {
                "URL": "https://linkeddatafragments.org/specification/triple-pattern-fragments/",
                "gold_label": "other"
            },
            {
                "URL": "https://linkeddatafragments.org/specification/quad-pattern-fragments/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": " http://conference.scipy.org/proceedings/scipy2015/sebastian_benthall.html https://codecov.io/gh/datactive/bigbang https://gitter.im/datactive/bigbang?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'http://conference.scipy.org/proceedings/scipy2015/sebastian_benthall.html', 'label': 'dataset_landing_page'},\n {'URL': 'https://codecov.io/gh/datactive/bigbang', 'label': 'dataset_direct_link'},\n {'URL': 'https://gitter.im/datactive/bigbang?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge', 'label': 'Other'}]",
        "repoID": 2683,
        "URL_gold_label": [
            {
                "URL": "https://codecov.io/gh/datactive/bigbang",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Docstrings are preferred, so that auto-generated web-based documentation will be possible ( #412 https://github.com/datactive/bigbang/issues/412 ). You can follow the Google style guide for docstrings https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings .",
        "answer": "  [{\"URL\": \"https://github.com/datactive/bigbang/issues/412\", \"label\": \"Software\"}]",
        "repoID": 2683,
        "URL_gold_label": [
            {
                "URL": "https://github.com/datactive/bigbang/issues/412",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "If you are interested in participating in BigBang development or would like support from the core development team, please subscribe to the bigbang-dev mailing list https://lists.ghserv.net/mailman/listinfo/bigbang-dev  and let us know your suggestions, questions, requests and comments. A development chatroom https://gitter.im/datactive/bigbang  is also available.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://lists.ghserv.net/mailman/listinfo/bigbang-dev\",\n            \"label\": \"Other\"\n        },\n        {\n            \"URL\": \"https://gitter.im/datactive/bigbang\",\n            \"label\": \"Other\"\n        }\n    ]",
        "repoID": 2683,
        "URL_gold_label": [
            {
                "URL": "https://gitter.im/datactive/bigbang",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "MIT, see LICENSE LICENSE  for its text. This license may be changed at any time according to the principles of the project Governance https://github.com/datactive/bigbang/wiki/Governance .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://mit.edu/license/', 'label': 'dataset_landing_page'}]",
        "repoID": 2683,
        "URL_gold_label": [
            {
                "URL": "https://github.com/datactive/bigbang/wiki/Governance",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Development of this software was funded by the Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery  through Grant GBMF4563 http://www.moore.org/grants/list/GBMF4563  to Ethan White and the National Science Foundation http://nsf.gov/  as part of a CAREER award to Ethan White http://nsf.gov/awardsearch/showAward.do?AwardNumber=0953694 .",
        "answer": "  [INST]\n    Input: Development of this software was funded by the Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery through Grant GBMF4563 http://www.moore.org/grants/list/GBMF4563 to Ethan White and the National Science Foundation http://nsf.gov/ as part of a CAREER award to Ethan White http://nsf.gov/awardsearch/showAward.do?AwardNumber=0953694.\n\nOutput:\n[\n{\n\"URL\": \"https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery\",\n\"label\": \"Other\"\n},\n{\n\"URL\": \"http://www.moore.org/grants/list/GBMF4563\",\n\"label\": \"Other\"\n},\n{",
        "repoID": 2689,
        "URL_gold_label": [
            {
                "URL": "https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For more information see the Data Retriever website http://www.data-retriever.org/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.data-retriever.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 2689,
        "URL_gold_label": [
            {
                "URL": "http://www.data-retriever.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "These examples are using the https://en.wikipedia.org/wiki/Iris_flower_data_set . More examples can be found in the Data Retriever documentation.",
        "answer": "  [{\"URL\": \"https://en.wikipedia.org/wiki/Iris_flower_data_set\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2689,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/Iris_flower_data_set",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "corresponds to the 200 official training set pairs from KITTI stereo 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo', 'label': 'dataset_landing_page'}]",
        "repoID": 2741,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "While searching for a list of english words (for an auto-complete tutorial) I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable  (archived).",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: <https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database>\nLabel: Other\n\n* URL: <https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable>\nLabel: DatasetDirectLink\n\n* URL: <https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable>\nLabel: ArchivedDatasetDirectLink",
        "repoID": 2757,
        "URL_gold_label": [
            {
                "URL": "https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Perform a delineation of the selected heartbeats in the  annotation file for the record  from the QT database https://www.physionet.org/physiobank/database/qtdb , and store the result in the  file.",
        "answer": "  Sure, here are the annotated URLs for the input:\n\nInput: Perform a delineation of the selected heartbeats in the annotation file for the record from the QT database <https://www.physionet.org/physiobank/database/qtdb>, and store the result in the file.\n\nOutput:\n[{\"URL\": \"https://www.physionet.org/physiobank/database/qtdb\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2785,
        "URL_gold_label": [
            {
                "URL": "https://www.physionet.org/physiobank/database/qtdb",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Perform a full interpretation of record  from the MIT-BIH Arrhythmia Database https://www.physionet.org/physiobank/database/mitdb  (the output will be stored in the  annotation file):",
        "answer": "  Sure, I can help you annotate the URLs in the input text. Here's the output for the given input:\n\n[{'URL': 'https://www.physionet.org/physiobank/database/mitdb', 'label': 'dataset_landing_page'}]",
        "repoID": 2785,
        "URL_gold_label": [
            {
                "URL": "https://www.physionet.org/physiobank/database/mitdb",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "[1] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \"Vision meets robotics: The KITTI dataset,\" Int. J. Robot. Research (IJRR), vol. 32, no. 11, pp. 1231\u20131237, Sep. 2013. http://www.cvlibs.net/datasets/kitti/ http://www.cvlibs.net/datasets/kitti/",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 2791,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "More data in the paper can be found at HarvardNLP https://github.com/harvardnlp/sent-conv-torch/tree/master/data .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/harvardnlp/sent-conv-torch/tree/master/data', 'label': 'dataset_landing_page'}]",
        "repoID": 2795,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/sent-conv-torch/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Alternatively, see this guide https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/  on running the complete \"phoenix_pipeline\" https://github.com/openeventdata/phoenix_pipeline .",
        "answer": "  [{\"URL\": \"https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/\", \"label\": \"Other\"}]",
        "repoID": 2797,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openeventdata/phoenix_pipeline",
                "gold_label": "software"
            },
            {
                "URL": "https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Because it used the verb dictionaries from TABARI http://eventdata.parusanalytics.com/software.dir/tabari.html , a coder based on shallow parsing, PETRARCH-1 made relatively little use of the CoreNLP constituency parse beyond parts-of-speech markup and noun-phrase markup. PETRARCH-2 makes full use of the deep parse.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://eventdata.parusanalytics.com/software.dir/tabari.html\nLabel: Software",
        "repoID": 2797,
        "URL_gold_label": [
            {
                "URL": "http://eventdata.parusanalytics.com/software.dir/tabari.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Detailed contribution guidlines can be found in the documentation https://petrarch2.readthedocs.io/en/latest/contributing.html . In general, we welcome contributions from anyone and everyone, be it in the form of pull requests, bug reports, or feature requests. If you would like to engage in more real-time conversation with us, please visit our gitter https://gitter.im/openeventdata/petrarch2  channel.",
        "answer": "  [INST]\n    # Annotations\n    Input: https://petrarch2.readthedocs.io/en/latest/contributing.html\n    Output: [{\"URL\": \"https://petrarch2.readthedocs.io/en/latest/contributing.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2797,
        "URL_gold_label": [
            {
                "URL": "https://gitter.im/openeventdata/petrarch2",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For a step-by-step guide to running text through the complete Petrarch2 processing pipeline, see our intro guide https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/ .",
        "answer": "  [{\"URL\": \"https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2797,
        "URL_gold_label": [
            {
                "URL": "https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "It is possible to run PETRARCH-2 as a stand-alone program. Most of our development work has gone into incorporating PETRARCH-2 into a full pipeline of utilities, through, e.g., the Phoenix pipeline https://github.com/openeventdata/phoenix_pipeline . There's also a RESTful wrapper around PETRARCH and CoreNLP named hypnos https://github.com/caerusassociates/hypnos . It's probably worthwhile to explore those options before trying to use PETRARCH as a stand-alone.",
        "answer": "  Sure! Here are the annotated URLs in the input text:\n\n* URL: https://github.com/openeventdata/phoenix_pipeline\nLabel: Other\n\n* URL: https://github.com/caerusassociates/hypnos\nLabel: Software",
        "repoID": 2797,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openeventdata/phoenix_pipeline",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "SIFT1M and GIST1M http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 2816,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "data https://github.com/ZZUTK/Delay_Embedding/tree/master/data",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/ZZUTK/Delay_Embedding/tree/master/data', 'label': 'dataset_landing_page'}]",
        "repoID": 2822,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ZZUTK/Delay_Embedding/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "is the Character Trajectories Data Set https://archive.ics.uci.edu/ml/datasets/Character+Trajectories  from UCI",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/Character+Trajectories', 'label': 'dataset_landing_page'}]",
        "repoID": 2822,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Character+Trajectories",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The tool takes as input a FQDN and an ASN database. Instructions on how to build such a database are online https://github.com/hadiasghari/pyasn#ipasn-data-files .",
        "answer": "  [{'URL': 'https://github.com/hadiasghari/pyasn#ipasn-data-files', 'label': 'dataset_landing_page'}]",
        "repoID": 2832,
        "URL_gold_label": [
            {
                "URL": "https://github.com/hadiasghari/pyasn#ipasn-data-files",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The Small Data Lab http://smalldata.io  ResearchStack Extensions package is the easiest way to include SDL visual surveys ( YADL http://yadl.smalldata.io , MEDL, PAM) into a ResearchStack application.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://smalldata.io', 'label': 'dataset_landing_page'}]",
        "repoID": 2851,
        "URL_gold_label": [
            {
                "URL": "http://smalldata.io",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The Small Data Lab http://smalldata.io  ResearchKit Extensions package is the easiest way to include SDL AVA ( YADL http://yadl.smalldata.io , MEDL, PAM) and Behavioral extensions (Go / No Go, Delayed Discounting, BART) into a ResearchKit application.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://smalldata.io', 'label': 'dataset_landing_page'}]",
        "repoID": 2852,
        "URL_gold_label": [
            {
                "URL": "http://smalldata.io",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Create a directory and download the archive files for 10k images, annotations and image sets from our website https://fcav.engin.umich.edu/sim-dataset/ . Assuming you have downloaded these to a directory named  (driving in the matrix data):",
        "answer": "  [{'URL': 'https://fcav.engin.umich.edu/sim-dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 2895,
        "URL_gold_label": [
            {
                "URL": "https://fcav.engin.umich.edu/sim-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Specifically, we will train MXNet RCNN https://github.com/dmlc/mxnet/tree/master/example/rcnn  on our 10k dataset https://fcav.engin.umich.edu/sim-dataset  and evaluate on KITTI http://www.cvlibs.net/datasets/kitti/eval_object.php .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/dmlc/mxnet/tree/master/example/rcnn>\nLabel: Software\n* URL: <https://fcav.engin.umich.edu/sim-dataset>\nLabel: DatasetDirectLink\n* URL: <http://www.cvlibs.net/datasets/kitti/eval_object.php>\nLabel: DatasetLandingPage",
        "repoID": 2895,
        "URL_gold_label": [
            {
                "URL": "https://fcav.engin.umich.edu/sim-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_object.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Visit KITTI's object detection landing page http://www.cvlibs.net/datasets/kitti/eval_object.php  and follow the links named:",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_object.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 2895,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_object.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Ren, Mengye, Ryan Kiros, and Richard Zemel. \"Exploring models and data for image question answering.\" http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering  In Advances in Neural Information Processing Systems, pp. 2935-2943. 2015. [code] http://gitxiv.com/posts/6pFP3b8gqxWZdBfjf/exploring-models-and-data-for-image-question-answering",
        "answer": "  [INST]\n    URL: http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering\n    Label: dataset_landing_page",
        "repoID": 2925,
        "URL_gold_label": [
            {
                "URL": "http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "You can download the preprocessed data (recommend) from here https://github.com/yoosan/sentpair/releases/download/predata/data.zip . Alternatively you can process them by yourself. The original links are:",
        "answer": "  [{'URL': 'https://github.com/yoosan/sentpair/releases/download/predata/data.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 2927,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yoosan/sentpair/releases/download/predata/data.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": " https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core",
        "answer": "  [{'URL': 'https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core', 'label': 'Software'}]",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "A modern, feature-rich and highly tunable Java client library for Apache Cassandra\u00ae http://cassandra.apache.org/  (2.1+) and DataStax Enterprise https://www.datastax.com/products/datastax-enterprise  (4.7+), and DataStax Astra https://www.datastax.com/products/datastax-astra , using exclusively Cassandra's binary protocol and Cassandra Query Language (CQL) v3.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://cassandra.apache.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "https://www.datastax.com/products/datastax-enterprise",
                "gold_label": "other"
            },
            {
                "URL": "https://www.datastax.com/products/datastax-astra",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "API docs https://docs.datastax.com/en/drivers/java/4.14",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://docs.datastax.com/en/drivers/java/4.14\",\n\"label\": \"Software\"\n}",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "https://docs.datastax.com/en/drivers/java/4.14",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Bug tracking: JIRA https://datastax-oss.atlassian.net/browse/JAVA",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://datastax-oss.atlassian.net/browse/JAVA', 'label': 'dataset_landing_page'}]",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "https://datastax-oss.atlassian.net/browse/JAVA",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Mailing list https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user",
        "answer": "  [{\"URL\": \"https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user\", \"label\": \"Other\"}]",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The driver artifacts are published in Maven central, under the group id com.datastax.oss http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datastax.oss%22 ; there are multiple modules, all prefixed with .",
        "answer": "http://search.maven.org/#search|ga|1|g:\"com.datastax.oss\"\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 2930,
        "URL_gold_label": [
            {
                "URL": "http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datastax.oss%22",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Setup  and download MNIST following these instructions https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst', 'label': 'dataset_landing_page'}]",
        "repoID": 2960,
        "URL_gold_label": [
            {
                "URL": "https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Inpainting images with missing data locations given by a mask image is meant to be used with a dataset in the format created by this script https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py', 'label': 'dataset_landing_page'}]",
        "repoID": 2961,
        "URL_gold_label": [
            {
                "URL": "https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Setup  and download MNIST following these instructions https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst', 'label': 'dataset_landing_page'}]",
        "repoID": 2961,
        "URL_gold_label": [
            {
                "URL": "https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The pretrained ResNet-152 model and related scripts can be found in fb.resnet.torch https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua', 'label': 'dataset_landing_page'}]",
        "repoID": 2967,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The original dataset is available at the Citi Bike website https://www.citibikenyc.com/system-data .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.citibikenyc.com/system-data', 'label': 'dataset_landing_page'}]",
        "repoID": 3014,
        "URL_gold_label": [
            {
                "URL": "https://www.citibikenyc.com/system-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The version of the Taxi dataset that we used in the experiments is not open source, and therefore, we cannot make it available online. However, the Taxi and Limousine Commission has made the trip data available on their website http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml .",
        "answer": "  [{\"URL\": \"http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\", \"label\": \"Other\"}]",
        "repoID": 3014,
        "URL_gold_label": [
            {
                "URL": "http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "CNN/DailyMail https://github.com/deepmind/rc-data/ : This repository contains a script to download CNN and Daily Mail articles from the Wayback Machine.",
        "answer": "  [{\"URL\": \"https://github.com/deepmind/rc-data/\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 3046,
        "URL_gold_label": [
            {
                "URL": "https://github.com/deepmind/rc-data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you're mainly interest in the data, pre and post classified name data is available in the assets directory https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets . If you install the package these will not be included in the install as  files but will be included as compressed binaries (the data are identical).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets>\nLabel: Other",
        "repoID": 3057,
        "URL_gold_label": [
            {
                "URL": "https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Processed data are provided under the Open Government License or the public domain where appropriate. See the LICENSE https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md  for details.",
        "answer": "  [{\"URL\": \"https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md\", \"label\": \"Other\"}]",
        "repoID": 3057,
        "URL_gold_label": [
            {
                "URL": "https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See the LICENSE https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md  file for details.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md', 'label': 'dataset_landing_page'}]",
        "repoID": 3057,
        "URL_gold_label": [
            {
                "URL": "https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We have collected birth record data from the United States and the United Kingdom across a number of years for all births in the two countries and are releasing the collected and cleaned up data here. We have also generated a simple gender classified based on incidence of gender by name. You can use this data for any purpose compatible with the license https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/OpenGenderTracking/globalnamedata>\nLabel: dataset_landing_page",
        "repoID": 3057,
        "URL_gold_label": [
            {
                "URL": "https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can read about some uses of this data along with code examples at the Bocoup blog http://weblog.bocoup.com/global-name-data/ .",
        "answer": "  [{\"URL\": \"http://weblog.bocoup.com/global-name-data/\", \"label\": \"Other\"}]",
        "repoID": 3057,
        "URL_gold_label": [
            {
                "URL": "http://weblog.bocoup.com/global-name-data/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Users can also compare on the multi-class dataset https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist  with the follow command (Note that we only shuffle the training data once in this example, so the standard deviation is zero):",
        "answer": "  [{\"URL\": \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 3060,
        "URL_gold_label": [
            {
                "URL": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Keras https://github.com/fchollet/keras , Kerosene https://github.com/dribnet/kerosene , Blessings https://github.com/erikrose/blessings , and libsvm-compact http://www.di.ens.fr/data/software/ .",
        "answer": "  Sure, here are the annotations for the given URLs:\n\nInput: Keras https://github.com/fchollet/keras, Kerosene https://github.com/dribnet/kerosene, Blessings https://github.com/erikrose/blessings, and libsvm-compact http://www.di.ens.fr/data/software/.\n\nOutput:\n\n[{'URL': 'https://github.com/fchollet/keras', 'label': 'Software'},\n {'URL': 'https://github.com/dribnet/kerosene', 'label': 'Software'},\n {'URL': 'https://github.com/erikrose/blessings', 'label': 'Software'},\n {'URL': 'http://www.di.ens.fr/data/software/', 'label': 'Other'}]",
        "repoID": 3081,
        "URL_gold_label": [
            {
                "URL": "http://www.di.ens.fr/data/software/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "A 10,000-song subset was downloaded from the Million Song Dataset http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 3083,
        "URL_gold_label": [
            {
                "URL": "http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For simplicity, we provide iPinYou dataset at make-ipinyou-data https://github.com/Atomu2014/make-ipinyou-data . Follow the instructions and update the soft link :",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'https://github.com/Atomu2014/make-ipinyou-data', 'label': 'dataset_direct_link'}]",
        "repoID": 3084,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Atomu2014/make-ipinyou-data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Python scrape for MTA current lost property index http://advisory.mtanyct.info/LPUWebServices/CurrentLostProperty.aspx , to revisit Mona's story http://fivethirtyeight.com/datalab/mta-new-york-lost-and-found-subway-most-common/  in a year (or some longer period of time)",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://advisory.mtanyct.info/LPUWebServices/CurrentLostProperty.aspx', 'label': 'Other'}]\n\nExplanation: The URL \"http://advisory.mtanyct.info/LPUWebServices/CurrentLostProperty.aspx\" is not a dataset or software entity, but rather a web page providing information on the current lost property index for the MTA in New York. Therefore, it is classified as \"Other\".",
        "repoID": 3119,
        "URL_gold_label": [
            {
                "URL": "http://fivethirtyeight.com/datalab/mta-new-york-lost-and-found-subway-most-common/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Tesseract 4 adds a new neural net (LSTM) based OCR engine https://en.wikipedia.org/wiki/Optical_character_recognition  which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0). It also needs traineddata https://tesseract-ocr.github.io/tessdoc/Data-Files.html  files which support the legacy engine, for example those from the tessdata https://github.com/tesseract-ocr/tessdata  repository.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://en.wikipedia.org/wiki/Optical_character_recognition\nLabel: Other\n\n* URL: https://tesseract-ocr.github.io/tessdoc/Data-Files.html\nLabel: DatasetLandingPage\n\n* URL: https://github.com/tesseract-ocr/tessdata\nLabel: DatasetLandingPage",
        "repoID": 3134,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tesseract-ocr/tessdata",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The data used to train this model is located here http://crcv.ucf.edu/data/UCF101.php .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]",
        "repoID": 3208,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the ImageNet http://image-net.org/download-images  dataset and move validation images https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to labeled subfolders",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://image-net.org/download-images', 'label': 'dataset_direct_link'}]",
        "repoID": 3215,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The code was developed on Ubuntu 14.04, using Theano+Lasagne+OpenCV. You can see the performance it achieved on the KITTI2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow , KITTI2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and MPI-Sintel http://sintel.is.tue.mpg.de/  optical flow scoreboards.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow\nLabel: DatasetDirectLink\n* URL: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\nLabel: DatasetDirectLink\n* URL: http://sintel.is.tue.mpg.de/\nLabel: Other",
        "repoID": 3219,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Prepare the corresponding datasets (CIFAR-10, CIFAR-100 or ImageNet) before training FBNs. In our experiments, we use RecordIO data format to generate  files for different datasets. Please refer to MXNet Example https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets .",
        "answer": "  [{'URL': 'https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 3220,
        "URL_gold_label": [
            {
                "URL": "https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The Freiburg Groceries Dataset consists of 5000 256x256 RGB images of 25 food classes. Examples for each class can be found below. The paper can be found here http://ais.informatik.uni-freiburg.de/publications/papers/jund16groceries.pdf  and the dataset here http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/ .",
        "answer": "  [{\"URL\": \"http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"http://ais.informatik.uni-freiburg.de/publications/papers/jund16groceries.pdf\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 3223,
        "URL_gold_label": [
            {
                "URL": "http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . [ Citation datasets/bibtex/cityscapes.tex ]",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 3245,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For small norb dataset, please download the raw images in .MAT format from http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/ http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/  and run datasets_norb.convert_orig_to_np() to convert it into numpy format.",
        "answer": "  Sure! Here's the annotated input:\n\nInput: For small norb dataset, please download the raw images in.MAT format from [http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/](http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/) and run datasets_norb.convert_orig_to_np() to convert it into numpy format.\n\nOutput:\n\n[{'URL': 'http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/', 'label': 'dataset_landing_page'}]",
        "repoID": 3248,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We trained our model based on the ImageNet pre-trained ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model and Flying Chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  pre-trained FlowNet https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz  model using a model converter https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter . The converted ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/KaimingHe/deep-residual-networks>\nLabel: DatasetDirectLink\n* URL: <https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html>\nLabel: DatasetLandingPage\n* URL: <https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef8",
        "repoID": 3263,
        "URL_gold_label": [
            {
                "URL": "https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The dataset loader https://github.com/fvisin/dataset_loaders . Thanks a lot to Francesco Visin for its data loader, please cite it if you use it.",
        "answer": "  [{\"URL\": \"https://github.com/fvisin/dataset_loaders\", \"label\": \"Software\"}]",
        "repoID": 3296,
        "URL_gold_label": [
            {
                "URL": "https://github.com/fvisin/dataset_loaders",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For more information, refer to the official leaderboard https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results .",
        "answer": "  [{\"URL\": \"https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 3311,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For detail of the data, see Data https://github.com/NorThanapon/dict-definition/tree/master/data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/NorThanapon/dict-definition/tree/master/data', 'label': 'dataset_landing_page'}]",
        "repoID": 3328,
        "URL_gold_label": [
            {
                "URL": "https://github.com/NorThanapon/dict-definition/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "As mentioned above, EL:DIABLO relies on Vagrant and VirtualBox for most of the heavy lifting. This means that the only things that a user needs to install on their local machine are these two pieces of software. The creators of this software describe the install process better than we can, so a user should look here https://www.vagrantup.com/downloads.html  for Vagrant and here https://www.virtualbox.org/wiki/Downloads  for VirtualBox. Once that software is installed, EL:DIABLO needs to be downloaded from the Github repository https://github.com/openeventdata/eldiablo . For those familiar with , a  should work fine. For those unfamiliar with , it is possible to download the repository as a zip file as shown in the picture below.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.vagrantup.com/downloads.html\nLabel: Software\n* URL: https://www.virtualbox.org/wiki/Downloads\nLabel: Software\n* URL: https://github.com/openeventdata/eldiablo\nLabel: DatasetDirectLink",
        "repoID": 3347,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openeventdata/eldiablo",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The EL:DIABLO event coding platform is comprised of two primary applications: a web scraper and a processing pipeline ( https://github.com/openeventdata/scraper  and https://github.com/openeventdata/phoenix_pipeline  specifically). The scraper is a simple web scraper that makes use of a whitelist of RSS feeds to pull stories from popular news outlets. The pipeline moves the news stories from storage in a database to the event coder, such as TABARI or PETRARCH, and outputs event data. More information about the details of these projects can be found in their respective documentation, linked to above. If you use the standard  script provided with EL:DIABLO, the web scraper will run once an hour, and the pipeline will run once a day at 01:00.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/openeventdata/scraper>\nLabel: Software\n* URL: <https://github.com/openeventdata/phoenix_pipeline>\nLabel: Software\n* URL: <https://github.com/openeventdata/script>\nLabel: Other",
        "repoID": 3347,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openeventdata/scraper",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/openeventdata/phoenix_pipeline",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Although we only mentioned DSTC4 in our paper, users can also locate these sub-dialogs from DSTC5 http://workshop.colips.org/dstc5/data.html , since it provides all the same data in DSTC4, and plus two extra Chinese dialogs (055, 056).",
        "answer": "  [{\"URL\": \"http://workshop.colips.org/dstc5/data.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 3350,
        "URL_gold_label": [
            {
                "URL": "http://workshop.colips.org/dstc5/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "25 Open Datasets for Deep Learning Every Data Scientist Must Work With https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/ , Analytics Vidhya, 2018-03-29.",
        "answer": "  Sure, here are the annotated URLs for the input:\n\n[{'URL': 'https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "All metadata and features for all tracks are distributed in  (342 MiB). The below tables can be used with pandas https://pandas.pydata.org/  or any other data analysis tool. See the paper https://arxiv.org/abs/1612.01840  or the https://nbviewer.jupyter.org/github/mdeff/fma/blob/outputs/usage.ipynb  notebook for a description.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://pandas.pydata.org/\nLabel: Software\n* URL: https://arxiv.org/abs/1612.01840\nLabel: DatasetLandingPage\n* URL: https://nbviewer.jupyter.org/github/mdeff/fma/blob/outputs/usage.ipynb\nLabel: DatasetLandingPage",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Music Genre Classification With TensorFlow https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb , Towards Data Science, 2020-08-11.",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Music Genre Classification: Transformers vs Recurrent Neural Networks https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58 , Towards Data Science, 2020-06-14.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58', 'label': 'Other'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Over 1.5 TB\u2019s of Labeled Audio Datasets https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad , Towards Data Science, 2018-11-13.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Using CNNs and RNNs for Music Genre Recognition https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af , Towards Data Science, 2018-12-13.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af\",\n            \"label\": \"dataset_landing_page\"\n        }\n    ]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "http://www.audiocontentanalysis.org/data-sets http://www.audiocontentanalysis.org/data-sets",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.audiocontentanalysis.org/data-sets', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "http://www.audiocontentanalysis.org/data-sets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "https://data-flair.training/blogs/deep-learning-project-ideas https://data-flair.training/blogs/deep-learning-project-ideas",
        "answer": "  Sure, here are the annotations for the input URL:\n\n[{'URL': 'https://data-flair.training/blogs/deep-learning-project-ideas', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://data-flair.training/blogs/deep-learning-project-ideas",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "https://github.com/caesar0301/awesome-public-datasets https://github.com/caesar0301/awesome-public-datasets",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/caesar0301/awesome-public-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://github.com/caesar0301/awesome-public-datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "https://github.com/ismir/mir-datasets https://github.com/ismir/mir-datasets",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/ismir/mir-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ismir/mir-datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "https://www.datasetlist.com https://www.datasetlist.com",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://www.datasetlist.com\",\n\"label\": \"Other\"\n}",
        "repoID": 3372,
        "URL_gold_label": [
            {
                "URL": "https://www.datasetlist.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the corresponding Vocabulary file for COCO https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/  and Flickr30k https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/', 'label': 'dataset_direct_link'}]\n\nExplanation:\n\n* The URL \"https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/\" is a direct link to a dataset entity (COCO).\n* The label \"dataset_direct_link\" indicates that the URL is directly linking to a dataset file or collection of files, rather than an introduction or landing page.",
        "repoID": 3373,
        "URL_gold_label": [
            {
                "URL": "https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "We won't need the entire distribution. Download http://conda.pydata.org/miniconda.html  a Python 3.7+ & install a minimal version of anaconda.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://conda.pydata.org/miniconda.html', 'label': 'dataset_direct_link'}]",
        "repoID": 3399,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the birds http://www.vision.caltech.edu/visipedia/CUB-200-2011.html  and flowers http://www.robots.ox.ac.uk/~vgg/data/flowers/102/  image data. Extract them to  and , respectively.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.vision.caltech.edu/visipedia/CUB-200-2011.html\nLabel: dataset_landing_page\n* URL: http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\nLabel: dataset_direct_link",
        "repoID": 3400,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "These files are suitable to be further processed with software such as pandas http://pandas.pydata.org/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 3470,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Change directory to IR for experimenting on information Retrieval task. IR Datasets mentioned in the paper can be downloaded from TREC website http://trec.nist.gov/data/docs_eng.html .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://trec.nist.gov/data/docs_eng.html', 'label': 'dataset_landing_page'}]",
        "repoID": 3472,
        "URL_gold_label": [
            {
                "URL": "http://trec.nist.gov/data/docs_eng.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "SIFT1M and GIST1M http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 3482,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "SIFT1M and GIST1M http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 3483,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MultiNet is able to jointly perform road segmentation, car detection and street classification. The model achieves real-time speed and state-of-the-art http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df  performance in segmentation. Check out our paper https://arxiv.org/abs/1612.07695  for a detailed model description.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df\nLabel: dataset_direct_link\n\n* URL: https://arxiv.org/abs/1612.07695\nLabel: software",
        "repoID": 3492,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Level up your serverless game with a GraphQL data-as-a-service layer https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/', 'label': 'dataset_landing_page'}]",
        "repoID": 3531,
        "URL_gold_label": [
            {
                "URL": "https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Using DataLoader to batch GraphQL requests https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433', 'label': 'Other'}]",
        "repoID": 3531,
        "URL_gold_label": [
            {
                "URL": "https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "dataloader-codegen https://github.com/Yelp/dataloader-codegen  - An opinionated JavaScript library for automatically generating predictable, type safe DataLoaders over a set of resources (e.g. HTTP endpoints).",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/Yelp/dataloader-codegen', 'label': 'Software'}]",
        "repoID": 3531,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Yelp/dataloader-codegen",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "graphql-toolkit https://github.com/ardatan/graphql-toolkit  - A set of utils for faster development of GraphQL tools (Schema and documents loading, Schema merging and more).",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/ardatan/graphql-toolkit', 'label': 'Software'}]",
        "repoID": 3531,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ardatan/graphql-toolkit",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "vertx-dataloader https://github.com/engagingspaces/vertx-dataloader  - Port of Facebook DataLoader for efficient, asynchronous batching and caching in clustered GraphQL environments.",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://github.com/engagingspaces/vertx-dataloader', 'label': 'Software'}]",
        "repoID": 3531,
        "URL_gold_label": [
            {
                "URL": "https://github.com/engagingspaces/vertx-dataloader",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "LSHBOX-sample datasets https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data : a dataset for performance tests",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data', 'label': 'dataset_landing_page'}]",
        "repoID": 3552,
        "URL_gold_label": [
            {
                "URL": "https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can get the sample dataset  from http://www.cs.princeton.edu/cass/audio.tar.gz http://www.cs.princeton.edu/cass/audio.tar.gz , if the link is invalid, you can also get it from LSHBOX-sample-data https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://www.cs.princeton.edu/cass/audio.tar.gz', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data', 'label': 'dataset_landing_page'}]",
        "repoID": 3552,
        "URL_gold_label": [
            {
                "URL": "https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Note these results are produced from a subset (the first 350,000 lines of each campaign in iPinYou) under T = 1000 and c0 = 1/32. For the full small-scale evaluation and large-scale evaluation, please first check the GitHub project make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data  for pre-processing the iPinYou data http://data.computational-advertising.org . After downloading the dataset, by \"make all\" you can generate the standardised data.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/wnzhang/make-ipinyou-data\nLabel: Software\n* URL: http://data.computational-advertising.org\nLabel: Other",
        "repoID": 3566,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Freebase Annotations for the TREC KBA 2014 StreamCorpus: FAKBA1 http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html  (~200GB)",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html', 'label': 'dataset_landing_page'}]",
        "repoID": 3591,
        "URL_gold_label": [
            {
                "URL": "http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Provenance Graphs for Datalog https://github.com/IITDBGroup/gprom/wiki/datalog_prov",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/IITDBGroup/gprom/wiki/datalog_prov', 'label': 'dataset_landing_page'}]",
        "repoID": 3613,
        "URL_gold_label": [
            {
                "URL": "https://github.com/IITDBGroup/gprom/wiki/datalog_prov",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/index.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://seaborn.pydata.org/index.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 3635,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/index.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We provide the evaluation script to reproduce our evaluation results on Flickr cropping dataset https://github.com/yiling-chen/flickr-cropping-dataset . For example,",
        "answer": "  [{\"URL\": \"https://github.com/yiling-chen/flickr-cropping-dataset\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 3695,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yiling-chen/flickr-cropping-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You will need to get  and the test images from the Flickr cropping dataset https://github.com/yiling-chen/flickr-cropping-dataset  and specify the path of your model when running . You can also try our pre-trained model, which can be downloaded from here https://drive.google.com/drive/folders/0B0sDVRDPL5zBd3ozNlFmZEZpY1k?resourcekey=0-8LArgbYDQT07L-Dob6Yo9w&usp=sharing .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/yiling-chen/flickr-cropping-dataset>\nLabel: DatasetLandingPage\n* URL: <https://drive.google.com/drive/folders/0B0sDVRDPL5zBd3ozNlFmZEZpY1k?resourcekey=0-8LArgbYDQT07L-Dob6Yo9w&usp=sharing>\nLabel: DatasetDirectLink",
        "repoID": 3695,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yiling-chen/flickr-cropping-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Data set: Ionosphere (UCI Machine Learning Repository) https://archive.ics.uci.edu/ml/datasets/Ionosphere  (#features = 34, #classes = 2)",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/Ionosphere', 'label': 'dataset_landing_page'}]",
        "repoID": 3728,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Ionosphere",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Evaluation scripts for SemEval were adapted & modified from SemEval-2016 official scorer http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools', 'label': 'dataset_landing_page'}]",
        "repoID": 3737,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To initialise image registration, we use six landmarks and perform point-based registration, which is then followed by image-based registration. The landmarks are defined as in the placing the landmarks http://wp.doc.ic.ac.uk/wbai/data  section and they are manually selected using the rview https://www.doc.ic.ac.uk/~dr/software/download.html  software. Alternatively, the landmarks can be automatically detected using stratified decision forests https://www.doc.ic.ac.uk/~oo2113/publication/TMI_stratified/  developed by Ozan Oktay.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://wp.doc.ic.ac.uk/wbai/data\nLabel: dataset_landing_page\n* URL: https://www.doc.ic.ac.uk/~dr/software/download.html\nLabel: software\n* URL: https://www.doc.ic.ac.uk/~oo2113/publication/TMI_stratified/\nLabel: other",
        "repoID": 3766,
        "URL_gold_label": [
            {
                "URL": "http://wp.doc.ic.ac.uk/wbai/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "3DPeS http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 3802,
        "URL_gold_label": [
            {
                "URL": "http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The example application takes input frames and poses from the TUM RGBD datasets http://vision.in.tum.de/data/datasets/rgbd-dataset , and requires that your create an association file http://vision.in.tum.de/data/datasets/rgbd-dataset/tools  to associate the RGB, Depth, and pose information. Instructions for this process can be found here https://github.com/tum-vision/fastfusion .",
        "answer": "  [INST]\n    {\n        \"URL\": \"http://vision.in.tum.de/data/datasets/rgbd-dataset\",\n        \"label\": \"dataset_landing_page\"\n    }\n]",
        "repoID": 3844,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset/tools",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "My Caffe (https://github.com/happynear/caffe-windows/tree/ms). If you don't want to train with class-balance sampling ( image_data_layer.cpp https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/image_data_layer.cpp ) and observing Pearson Correlation during training ( correlation_loss_layer.cpp https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/correlation_loss_layer.cpp ), you may use the official Caffe.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: <https://github.com/happynear/caffe-windows/tree/ms>\nLabel: Software",
        "repoID": 3863,
        "URL_gold_label": [
            {
                "URL": "https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/image_data_layer.cpp",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "In air data: Any RGB-D dataset, e.g. Microsoft 7-Scenes https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/# , NYU Depth http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html , UW RGB-D Object https://www.cs.washington.edu/node/4229 , B3DO http://kinectdata.com/  Note: The current configuration expects 640x480 PNG images for in-air data.",
        "answer": "  Sure, here are the annotated URLs:\n\n[{\"URL\": \"https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/#\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html\", \"label\": \"dataset_direct_link\"},\n {\"URL\": \"https://www.cs.washington.edu/node/4229\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"kinectdata.com/\", \"label\": \"Other\"},\n {\"URL\": \"http://kinectdata.com/\", \"label\": \"Software\"}]",
        "repoID": 3876,
        "URL_gold_label": [
            {
                "URL": "https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/#",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://kinectdata.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "shrinker 0.1 https://code.google.com/p/data-shrinker  - WARNING: it can throw SEGFAULT compiled with gcc 4.9+ -O3",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL':'shrinker 0.1', 'label': 'Software'}]",
        "repoID": 3883,
        "URL_gold_label": [
            {
                "URL": "https://code.google.com/p/data-shrinker",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Zstandard's format is stable and documented in RFC8878 https://datatracker.ietf.org/doc/html/rfc8878 . Multiple independent implementations are already available. This repository represents the reference implementation, provided as an open-source dual BSD LICENSE  and GPLv2 COPYING  licensed  library, and a command line utility producing and decoding , ,  and  files. Should your project require another programming language, a list of known ports and bindings is provided on Zstandard homepage https://facebook.github.io/zstd/#other-languages .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://datatracker.ietf.org/doc/html/rfc8878\nLabel: Other\n* URL: https://facebook.github.io/zstd/\nLabel: Software",
        "repoID": 3884,
        "URL_gold_label": [
            {
                "URL": "https://datatracker.ietf.org/doc/html/rfc8878",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The datasets included originate from the FBMS-59 dataset http://lmb.informatik.uni-freiburg.de/resources/datasets/ . The datasets are provided only for research purposes and without any warranty. When using the BMS-26 or FBMS-59 in your research work, you should cite the appropriate papers in the link above.",
        "answer": "  [{\"URL\": \"http://lmb.informatik.uni-freiburg.de/resources/datasets/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 3915,
        "URL_gold_label": [
            {
                "URL": "http://lmb.informatik.uni-freiburg.de/resources/datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MergeData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp\",\n\"label\": \"Software\"\n}",
        "repoID": 3922,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "RandomCropBoostedData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp\",\n\"label\": \"Software\"\n}",
        "repoID": 3922,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "SplitData https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp\",\n\"label\": \"Software\"\n}",
        "repoID": 3922,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This repository contains the codes and models described in the paper \"Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning\"(https://arxiv.org/abs/1702.08690). These models are those used in Stanford Dogs 120 http://vision.stanford.edu/aditya86/ImageNetDogs/ , Oxford Flowers 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/ , Caltech 256 http://authors.library.caltech.edu/7694/  and MIT Indoor 67 http://web.mit.edu/torralba/www/indoor.html .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://arxiv.org/abs/1702.08690\nLabel: dataset_direct_link\n\n* URL: http://vision.stanford.edu/aditya86/ImageNetDogs/\nLabel: dataset_landing_page\n\n* URL: http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\nLabel: dataset_landing_page\n\n* URL: http://authors.library.caltech.edu/7694/\nLabel: dataset_landing_page\n\n* URL: http://web.mit.edu/torralba/www/indoor.html\nLabel: dataset_landing_page",
        "repoID": 3922,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4 . , page 36.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4', 'label': 'dataset_landing_page'}]",
        "repoID": 3942,
        "URL_gold_label": [
            {
                "URL": "https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The ECO dataset http://www.vs.inf.ethz.ch/res/show.html?what=eco-data : Together with , a Swiss energy provider, we collected the ECO data set (Electricity Consumption and Occupancy). Using NILM-Eval, we evaluated the performance of four NILM algorithms on the ECO data set.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://www.vs.inf.ethz.ch/res/show.html?what=eco-data', 'label': 'dataset_landing_page'}]",
        "repoID": 3955,
        "URL_gold_label": [
            {
                "URL": "http://www.vs.inf.ethz.ch/res/show.html?what=eco-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The first thing you need to do is to download the data. You have to register here http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database  and download these two files:",
        "answer": "  [{'URL': 'http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database', 'label': 'dataset_landing_page'}]",
        "repoID": 3956,
        "URL_gold_label": [
            {
                "URL": "http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can download my Makefile.config http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config  for reference. 2. Python packages you might not have: , ,",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config\nLabel: DatasetDirectLink",
        "repoID": 3962,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This repository contains the code for the optimization step in the paper. The inference code is here https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax .",
        "answer": "  [{'URL': 'https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax', 'label': 'Software'}]",
        "repoID": 3997,
        "URL_gold_label": [
            {
                "URL": "https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The data corpus used in the research is publicly available and can be requested at dataminingtutorial.com http://dataminingtutorial.com",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://dataminingtutorial.com', 'label': 'dataset_landing_page'}]",
        "repoID": 4003,
        "URL_gold_label": [
            {
                "URL": "http://dataminingtutorial.com",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Ucinet IV Datasets http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm .",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm', 'label': 'dataset_landing_page'}]",
        "repoID": 4027,
        "URL_gold_label": [
            {
                "URL": "http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the Lip Reading in the Wild https://www.robots.ox.ac.uk/~vgg/data/lip_reading/  dataset",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.robots.ox.ac.uk/~vgg/data/lip_reading/', 'label': 'dataset_direct_link'}]",
        "repoID": 4040,
        "URL_gold_label": [
            {
                "URL": "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The repository supports optimization of the above models on artifical multivariate noisy AR time series and household electricity conspumption dataset https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption  The dataset has to be specified alongside the paremeters in each of the files listed above.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption>\nLabel: DatasetDirectLink",
        "repoID": 4042,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install a RAM disk like IMDisk http://www.ltr-data.se/opencode.html/#ImDisk  and set  to use it.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.ltr-data.se/opencode.html/#ImDisk', 'label': 'Software'}]",
        "repoID": 4044,
        "URL_gold_label": [
            {
                "URL": "http://www.ltr-data.se/opencode.html/#ImDisk",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Provided data https://www.kaggle.com/c/ultrasound-nerve-segmentation/data  is processed by  script. This script just loads the images and saves them into NumPy binary format files  for faster loading later.",
        "answer": "  [{\"URL\": \"https://www.kaggle.com/c/ultrasound-nerve-segmentation/data\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 4064,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/ultrasound-nerve-segmentation/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The PH2 Dataset http://www.fc.up.pt/addi/ph2%20database.html  has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.",
        "answer": "Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.fc.up.pt/addi/ph2 database.html', 'label': 'dataset_landing_page'}]",
        "repoID": 4066,
        "URL_gold_label": [
            {
                "URL": "http://www.fc.up.pt/addi/ph2%20database.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See the https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir  for an example user directory.",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir', 'label': 'dataset_landing_page'}]",
        "repoID": 4072,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example. Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .",
        "answer": "  [INST]\n    # Annotate\n    Input: https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py : the pre-processed data that we used in the paper can be downloaded here http://dawenl.github.io/data/gowalla_pro.zip.\n    Output: [{\"URL\": \"https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4072,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "consist of features such as inputs and targets, and metadata such as each feature's modality (e.g. symbol, image, audio) and vocabularies. Problem features are given by a dataset, which is stored as a  file with  protocol buffers. All problems are imported in https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py  or are registered with . Run https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen  to see the list of available problems and download them.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py\nLabel: DatasetLandingPage",
        "repoID": 4072,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Take a look at the alpine example chart https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine  for reference when you're writing your first few charts.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine', 'label': 'dataset_landing_page'}]",
        "repoID": 4075,
        "URL_gold_label": [
            {
                "URL": "https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Frequently Occurring Surnames from the Census 2000 http://www.census.gov/topics/population/genealogy/data/2000_surnames.html . Surnames occurring >= 100 more times in the 2000 census. Details here: http://www2.census.gov/topics/genealogy/2000surnames/surnames.pdf http://www2.census.gov/topics/genealogy/2000surnames/surnames.pdf",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://www.census.gov/topics/population/genealogy/data/2000_surnames.html', 'label': 'dataset_landing_page'}]\n\nThe URL you provided is for a dataset landing page, which is a page that provides an introduction or overview of a dataset entity.",
        "repoID": 4095,
        "URL_gold_label": [
            {
                "URL": "http://www.census.gov/topics/population/genealogy/data/2000_surnames.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download winequality dataset https://archive.ics.uci.edu/ml/datasets/Wine+Quality , and other datasets and change utils.py to add new datasets to test out the pruning algorithm.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: <https://archive.ics.uci.edu/ml/datasets/Wine+Quality>\nLabel: DatasetDirectLink\n\n* URL: <utils.py>\nLabel: Other",
        "repoID": 4100,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Wine+Quality",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Amsterdam Museum ( http://datahub.io/dataset/amsterdam-museum-as-edm-lod http://datahub.io/dataset/amsterdam-museum-as-edm-lod )",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://datahub.io/dataset/amsterdam-museum-as-edm-lod', 'label': 'dataset_direct_link'}]",
        "repoID": 4113,
        "URL_gold_label": [
            {
                "URL": "http://datahub.io/dataset/amsterdam-museum-as-edm-lod",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "(Fraxtil) Fraxtil's Arrow Arrangements https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's%20Arrow%20Arrangements%20[SM5].zip",
        "answer": "! Here's the annotation for the input:\n\n[{'URL': 'https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's Arrow Arrangements [SM5].zip', 'label': 'dataset_direct_link'}]",
        "repoID": 4135,
        "URL_gold_label": [
            {
                "URL": "https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's%20Arrow%20Arrangements%20[SM5].zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "(Fraxtil) Fraxtil's Beast Beats https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's%20Beast%20Beats%20[SM5].zip",
        "answer": "! Here's the annotation for the input:\n\n[{'URL': 'https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's Beast Beats [SM5].zip', 'label': 'dataset_direct_link'}]",
        "repoID": 4135,
        "URL_gold_label": [
            {
                "URL": "https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's%20Beast%20Beats%20[SM5].zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": " https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/', 'label': 'Other'}]",
        "repoID": 4139,
        "URL_gold_label": [
            {
                "URL": "https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "If you find it useful, feel welcome to leave a comment on the blog. https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/",
        "answer": "  [{\"URL\": \"https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/\", \"label\": \"Other\"}]",
        "repoID": 4139,
        "URL_gold_label": [
            {
                "URL": "https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "SMILES enumeration is the process of writing out all possible SMILES forms of a molecule. It's a useful technique for data augmentation before sequence based modeling of molecules. You can read more about the background in this blog post https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/  or this preprint on arxiv.org https://arxiv.org/abs/1703.07076",
        "answer": "  [{\"URL\": \"https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/\", \"label\": \"Other\"}]",
        "repoID": 4139,
        "URL_gold_label": [
            {
                "URL": "https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The following example starts the tracker on one of the MOT16 benchmark https://motchallenge.net/data/MOT16/  sequences. We assume resources have been extracted to the repository root directory and the MOT16 benchmark data is in :",
        "answer": "  [{\"URL\": \"https://motchallenge.net/data/MOT16/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4148,
        "URL_gold_label": [
            {
                "URL": "https://motchallenge.net/data/MOT16/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database ( paper http://www.nature.com/articles/sdata201635 , website http://mimic.physionet.org ). Our four clinical prediction tasks are critical care variants of four opportunities to transform health care using in \"big clinical data\" as described in Bates, et al, 2014 http://content.healthaffairs.org/content/33/7/1123.abstract :",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://mimic.physionet.org\nLabel: DatasetLandingPage\n* URL: http://www.nature.com/articles/sdata201635\nLabel: DatasetDirectLink\n* URL: http://content.healthaffairs.org/content/33/7/1123.abstract\nLabel: Other",
        "repoID": 4158,
        "URL_gold_label": [
            {
                "URL": "http://www.nature.com/articles/sdata201635",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database ( paper http://www.nature.com/articles/sdata201635 , website http://mimic.physionet.org ). Our four clinical prediction tasks are critical care variants of four opportunities to transform health care using in \"big clinical data\" as described in Bates, et al, 2014 http://content.healthaffairs.org/content/33/7/1123.abstract :",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://mimic.physionet.org\nLabel: DatasetLandingPage\n* URL: http://www.nature.com/articles/sdata201635\nLabel: DatasetDirectLink\n* URL: http://content.healthaffairs.org/content/33/7/1123.abstract\nLabel: Other",
        "repoID": 4159,
        "URL_gold_label": [
            {
                "URL": "http://www.nature.com/articles/sdata201635",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "source https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java cache examples/awips2-upc_14.4.1.zip",
        "answer": "  Sure, here are the annotations for the input source:\n\n[{'URL': 'https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java', 'label': 'Software'}]",
        "repoID": 4180,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Urbansound8K https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 4189,
        "URL_gold_label": [
            {
                "URL": "https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "emoMusic http://cvml.unige.ch/databases/emoMusic/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://cvml.unige.ch/databases/emoMusic/\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 4189,
        "URL_gold_label": [
            {
                "URL": "http://cvml.unige.ch/databases/emoMusic/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The speech enhancement dataset used in this work (Valentini et al. 2016) http://ssw9.net/papers/ssw9_PS2-4_Valentini-Botinhao.pdf  can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . However, :",
        "answer": "  [{\"URL\": \"http://ssw9.net/papers/ssw9_PS2-4_Valentini-Botinhao.pdf\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 4200,
        "URL_gold_label": [
            {
                "URL": "http://datashare.is.ed.ac.uk/handle/10283/1942",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download our model https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz', 'label': 'dataset_direct_link'}]",
        "repoID": 4207,
        "URL_gold_label": [
            {
                "URL": "https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Download the C3D representation of a couple of videos from here https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5 .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5', 'label': 'dataset_direct_link'}]",
        "repoID": 4207,
        "URL_gold_label": [
            {
                "URL": "https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Ensure that you have gcc https://gcc.gnu.org/ , conda ihttp://conda.pydata.org/docs/index.html , CUDA and CUDNN https://developer.nvidia.com/cuda-downloads  (optional).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://gcc.gnu.org/\nLabel: Software\n* URL: http://conda.pydata.org/\nLabel: Software\n* URL: https://developer.nvidia.com/cuda-downloads\nLabel: Software (optional)",
        "repoID": 4207,
        "URL_gold_label": [
            {
                "URL": "ihttp://conda.pydata.org/docs/index.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pre-trained models https://github.com/escorciav/daps/tree/master/data/models . Our generalization experiment suggests that you may expect decent results for other kind of action classes with similar lengths. Check out the models trained on the validation set of THUMOS14.",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://github.com/escorciav/daps/tree/master/data/models', 'label': 'dataset_direct_link'}]",
        "repoID": 4207,
        "URL_gold_label": [
            {
                "URL": "https://github.com/escorciav/daps/tree/master/data/models",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Python 3 (and python 2 only for get_geographic_data.ipynb https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb  )",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* \"Python 3\" - label: \"Software\"\n* \"https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb\" - label: \"DatasetDirectLink\"",
        "repoID": 4209,
        "URL_gold_label": [
            {
                "URL": "https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We trained our model based on the ImageNet pre-trained ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model and Flying Chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  pre-trained FlowNet https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz  model using a model converter https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter . The converted ResNet-v1-101 https://github.com/KaimingHe/deep-residual-networks  model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/KaimingHe/deep-residual-networks>\nLabel: DatasetDirectLink\n* URL: <https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html>\nLabel: DatasetLandingPage\n* URL: <https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef8",
        "repoID": 4212,
        "URL_gold_label": [
            {
                "URL": "https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Alluxio is used in production to manage Petabytes of data in many leading companies, with the largest deployment exceeding 3,000 nodes. You can find more use cases at Powered by Alluxio https://www.alluxio.io/powered-by-alluxio  or visit our first community conference ( Data Orchestration Summit https://www.alluxio.io/data-orchestration-summit-2019/ ) to learn from other community members!",
        "answer": "  [INST]\n    Input: Alluxio is used in production to manage Petabytes of data in many leading companies, with the largest deployment exceeding 3,000 nodes. You can find more use cases at Powered by Alluxio https://www.alluxio.io/powered-by-alluxio or visit our first community conference (Data Orchestration Summit https://www.alluxio.io/data-orchestration-summit-2019) to learn from other community members!\n\nOutput:\n[\n{\n\"URL\": \"https://www.alluxio.io/powered-by-alluxio\",\n\"label\": \"Other\"\n},\n{\n\"URL\": \"https://www.alluxio.io/data-orchestration-summit-2019\",\n\"label\": \"Other\"\n}\n]",
        "repoID": 4223,
        "URL_gold_label": [
            {
                "URL": "https://www.alluxio.io/data-orchestration-summit-2019/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy https://www.numpy.org/  / SciPy https://www.scipy.org/  / scikit-learn https://scikit-learn.org  etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython https://cython.org/  and Numba http://numba.pydata.org/ . Our goal is to not reinvent the wheel where appropriate.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://www.numpy.org/\nLabel: DatasetLandingPage\n* URL: https://www.scipy.org/\nLabel: DatasetLandingPage\n* URL: https://scikit-learn.org/\nLabel: DatasetLandingPage\n* URL: https://cython.org/\nLabel: Software\n* URL: http://numba.pydata.org/\nLabel: Software",
        "repoID": 4233,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The  and  datasets can be downloaded on Github https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2  or on Google Drive https://drive.google.com/open?id=0B6-YKFW-MnbOYWxUMTBEZ1FBam8 .",
        "answer": "  [{\"URL\": \"https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2\", \"label\": \"dataset_direct_link\"}, {\"URL\": \"https://drive.google.com/open?id=0B6-YKFW-MnbOYWxUMTBEZ1FBam8\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4235,
        "URL_gold_label": [
            {
                "URL": "https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Document identifier data set http://lemire.me/data/integercompression2014.html",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://lemire.me/data/integercompression2014.html', 'label': 'dataset_landing_page'}]",
        "repoID": 4254,
        "URL_gold_label": [
            {
                "URL": "http://lemire.me/data/integercompression2014.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Raw 32 bits binary data file Test data https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data",
        "answer": "  Sure, here are the annotations for the input URL:\n\n{\n\"URL\": \"https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 4254,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Test file Timestamps: ts.txt(sorted) https://github.com/zhenjl/encoding/tree/master/benchmark/data",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/zhenjl/encoding/tree/master/benchmark/data', 'label': 'dataset_landing_page'}]",
        "repoID": 4254,
        "URL_gold_label": [
            {
                "URL": "https://github.com/zhenjl/encoding/tree/master/benchmark/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Text file: 1 entry per line. Test data: ts.txt(sorted) and lat.txt(unsorted) https://github.com/zhenjl/encoding/tree/master/benchmark/data )",
        "answer": "  Sure, here are the annotated URLs for the input text:\n\nInput: Text file: 1 entry per line.\n\nOutput:\n[{'URL': 'https://github.com/zhenjl/encoding/tree/master/benchmark/data', 'label': 'dataset_landing_page'}]",
        "repoID": 4254,
        "URL_gold_label": [
            {
                "URL": "https://github.com/zhenjl/encoding/tree/master/benchmark/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "However, if you're using a  Node build, you may see Lighthouse log messages about your locale not being available. To remedy this, you can manually install ICU data by using the https://www.npmjs.com/package/full-icu  module and the https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime  at launch.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.npmjs.com/package/full-icu\nLabel: Software\n\n* URL: https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime\nLabel: Software",
        "repoID": 4270,
        "URL_gold_label": [
            {
                "URL": "https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Purge or recreate a Ruby on Rails database http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database",
        "answer": "  Sure! Here's the annotated input:\n\nInput: Purge or recreate a Ruby on Rails database http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database\n\nOutput:\n\n[{\"URL\": \"http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database\", \"label\": \"Other\"}]",
        "repoID": 4279,
        "URL_gold_label": [
            {
                "URL": "http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "To run experiments for SIGMORPHON 2016 http://ryancotterell.github.io/sigmorphon2016/ , first download training, validatation and test data here https://github.com/ryancotterell/sigmorphon2016/tree/master/data/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://github.com/ryancotterell/sigmorphon2016/tree/master/data/\nLabel: dataset_direct_link",
        "repoID": 4284,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ryancotterell/sigmorphon2016/tree/master/data/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Alternatively, you could run . Currently, this only supports files of the  and  forms. These are explained here https://gist.github.com/sampollard/f9169c4eb04669390a834884682c080d . It should accept any graph file you can find from SNAP Database https://snap.stanford.edu/data/index.html  or the KONECT Database http://konect.uni-koblenz.de/networks/ .",
        "answer": "  [INST]\n    # Annotation\n    Input: Alternative...\n    Output: [{\"URL\": \"https://gist.github.com/sampollard/f9169c4eb04669390a834884682c080d\", \"label\": \"Other\"}]",
        "repoID": 4292,
        "URL_gold_label": [
            {
                "URL": "https://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Luke Gessler has written a module for the Pepper http://corpus-tools.org/pepper/  tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See instructions for converting https://github.com/nert-nlp/streusle-pepper-importer .",
        "answer": "  [{'URL': 'https://github.com/nert-nlp/streusle-pepper-importer', 'label': 'dataset_landing_page'}]",
        "repoID": 4294,
        "URL_gold_label": [
            {
                "URL": "http://corpus-tools.org/pepper/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Annotation http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 4304,
        "URL_gold_label": [
            {
                "URL": "http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Images http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz', 'label': 'dataset_direct_link'}]",
        "repoID": 4304,
        "URL_gold_label": [
            {
                "URL": "http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Download DPED dataset http://people.ee.ethz.ch/~ihnatova/#dataset  (patches for CNN training) and extract it into  folder. This folder should contain three subolders: ,  and",
        "answer": "  Sure, here's the annotated input:\n\n[{'URL': 'http://people.ee.ethz.ch/~ihnatova/#dataset', 'label': 'dataset_direct_link'}]",
        "repoID": 4309,
        "URL_gold_label": [
            {
                "URL": "http://people.ee.ethz.ch/~ihnatova/#dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For numba instructions, you can find a tutorial and installation guideline here http://numba.pydata.org/numba-doc/dev/user/installing.html .",
        "answer": "  [{\"URL\": \"http://numba.pydata.org/numba-doc/dev/user/installing.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4328,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/numba-doc/dev/user/installing.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "[Optional] If you want to use COCO, please see the notes here https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md .",
        "answer": "  [{\"URL\": \"https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md\", \"label\": \"Other\"}]",
        "repoID": 4336,
        "URL_gold_label": [
            {
                "URL": "https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The converted (i.e. pre-processed) NYUV2, SBD and SUNRGBD datasets are now available in the data repository https://github.com/davidstutz/superpixel-benchmark-data .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/davidstutz/superpixel-benchmark-data', 'label': 'dataset_landing_page'}]",
        "repoID": 4353,
        "URL_gold_label": [
            {
                "URL": "https://github.com/davidstutz/superpixel-benchmark-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Additional dataset configuration values passed as command line arguments (  with  being a string or in JSON format, put in single quotes  if necessary, see dataset arguments https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets  for details)",
        "answer": "  Sure, here are the annotations for the input:\n\nInput: Additional dataset configuration values passed as command line arguments ( with being a string or in JSON format, put in single quotes if necessary, see dataset arguments for details)\n\nOutput:\n[{\"URL\": \"https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets\", \"label\": \"Other\"}]",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Existential: [One shape] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html  \u00b7 [Collision-free] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html  \u00b7 [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html  \u00b7 [Chinese] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html\nLabel: dataset_landing_page\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html\nLabel: dataset_direct_link\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html\nLabel: dataset_direct_link\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html\nLabel: dataset_landing_page",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Logical: [Existential] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html  \u00b7 [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\n* The URL \"https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html\" is a landing page for a dataset entity, as it provides an introduction to the dataset and its contents.\n\nI hope this helps! Let me know if you have any questions or need further assistance.",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Quantification: [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html  \u00b7 [Ratio] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html  \u00b7 [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html  \u00b7 [Complex] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html\nLabel: dataset_direct_link\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html\nLabel: dataset_direct_link\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html\nLabel: dataset_direct_link\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html\nLabel: dataset_direct_link",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Relational: [Spatial two shapes] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html  \u00b7 [Spatial] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html  \u00b7 [Attribute] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html  \u00b7 [Comparative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html  \u00b7 [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html\nLabel: DatasetDirectLink\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html\nLabel: DatasetDirectLink\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html\nLabel: DatasetDirectLink\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html\nLabel: Dat",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Selection: [Positive] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html  \u00b7 [Superlative] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html  \u00b7 [Full] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html\nLabel: [DatasetDirectLink]\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html\nLabel: [DatasetDirectLink]\n\n* [URL]: https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html\nLabel: [DatasetDirectLink]",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Shape: [Single] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html  \u00b7 [Multi] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html  \u00b7 [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html",
        "answer": "  Sure, here are the annotated URLs:\n\n* [Single] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html: label = \"dataset_direct_link\"\n* [Multi] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html: label = \"dataset_landing_page\"\n* [Count] https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html: label = \"Other\"",
        "repoID": 4366,
        "URL_gold_label": [
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html",
                "gold_label": "other"
            },
            {
                "URL": "https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "RACE: Please submit a data request here http://www.cs.cmu.edu/~glai1/data/race/ . The data will be automatically sent to you. Create a \"data\" directory alongside \"src\" directory and download the data.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://www.cs.cmu.edu/~glai1/data/race/', 'label': 'dataset_direct_link'}]",
        "repoID": 4376,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cmu.edu/~glai1/data/race/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We provide the following pre-processed files in https://github.com/ppuliu/GloRE/tree/master/data :",
        "answer": "  [{'URL': 'https://github.com/ppuliu/GloRE/tree/master/data', 'label': 'dataset_landing_page'}]",
        "repoID": 4403,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ppuliu/GloRE/tree/master/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "This script will download datasets from here http://www.cs.columbia.edu/~andrews/mil/datasets.html  and make necessary changes.",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.cs.columbia.edu/~andrews/mil/datasets.html', 'label': 'dataset_landing_page'}]",
        "repoID": 4427,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.columbia.edu/~andrews/mil/datasets.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "English Wiktionary: total http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser , semantic relations http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Semantic_relations , translations http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Translations , part of speech http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:POS",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser\nLabel: DatasetDirectLink\n* URL: http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Semantic_relations\nLabel: DatasetLandingPage\n* URL: http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Translations\nLabel: DatasetDirectLink\n* URL: http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:POS\nLabel: Software",
        "repoID": 4438,
        "URL_gold_label": [
            {
                "URL": "http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The goal of this project is to extract semi-structured information http://en.wikipedia.org/wiki/Semi-structured_data  from Wiktionary and construct machine-readable dictionary http://en.wikipedia.org/wiki/Machine-readable_dictionary  (database + API http://en.wikipedia.org/wiki/API  + GUI http://en.wikipedia.org/wiki/GUI ).",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://en.wikipedia.org/wiki/Semi-structured_data', 'label': 'Other'}]\n[{'URL': 'http://en.wikipedia.org/wiki/Machine-readable_dictionary', 'label': 'dataset_landing_page'}]",
        "repoID": 4438,
        "URL_gold_label": [
            {
                "URL": "http://en.wikipedia.org/wiki/Semi-structured_data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "start from the paper describing the database (tables and relations) of machine-readable Wiktionary: Transformation of Wiktionary entry structure into tables and relations in a relational database schema http://arxiv.org/abs/1011.1368 . 2010. But there are new tables (absent in the publication) related to  and , see Machine-readable database schema https://github.com/componavt/wikokit/blob/wiki/File_wikt_parsed_empty_sql.md#machine-readable-database-schema ;",
        "answer": "  Sure, here are the annotated URLs for the input:\n\n[{'URL': 'http://arxiv.org/abs/1011.1368', 'label': 'dataset_landing_page'}]\n\nThe URL is for a paper that describes the database (tables and relations) of machine-readable Wiktionary.",
        "repoID": 4438,
        "URL_gold_label": [
            {
                "URL": "https://github.com/componavt/wikokit/blob/wiki/File_wikt_parsed_empty_sql.md#machine-readable-database-schema",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Credit scoring https://www.kaggle.com/c/GiveMeSomeCredit/data/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'https://www.kaggle.com/c/GiveMeSomeCredit/data/', 'label': 'dataset_direct_link'}]",
        "repoID": 4457,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/GiveMeSomeCredit/data/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "For questions, bug reports, or feature requests, please open an issue on GitHub https://github.com/databricks/spark-perf/issues .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://github.com/databricks/spark-perf/issues\nLabel: Other",
        "repoID": 4458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/databricks/spark-perf/issues",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . Then run the following command",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 4461,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]",
        "repoID": 4461,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For the pose experiments, we used the KITTI odometry split, which can be downloaded here http://www.cvlibs.net/datasets/kitti/eval_odometry.php . Then you can change  option to  when preparing the data.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_direct_link",
        "repoID": 4461,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Notice that all the predictions and ground-truth are 5-frame snippets with the format of  consistent with the TUM evaluation toolkit https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation . Then you could run",
        "answer": "  Understood! Here are the annotated URLs for the given input:\n\nInput: Notice that all the predictions and ground-truth are 5-frame snippets with the format of consistent with the TUM evaluation toolkit https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation. Then you could run\n\nOutput:\n\n[{\"URL\": \"https://snap.stanford.edu/data/loc-gowalla.html\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"http://dawenl.github.io/data/gowalla_pro.zip\", \"label\": \"dataset_direct_link\"},\n {\"URL\": \"http://simongog.github.io/assets/data/sdsl-slides/tutorial\", \"label\": \"Other\"},\n {\"URL\": \"http://law.di.unimi.it/datasets.php\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\", \"label\": \"Software\"}]",
        "repoID": 4461,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org\",\n\"label\": \"Software\"\n}",
        "repoID": 4469,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Test images are from Blur Detection Dataset http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html  [2].",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html', 'label': 'dataset_landing_page'}]",
        "repoID": 4495,
        "URL_gold_label": [
            {
                "URL": "http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "CrisisLexT26 Dataset https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26', 'label': 'dataset_landing_page'}]",
        "repoID": 4549,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "CrowdFlower10K Dataset http://www.crowdflower.com/data-for-everyone",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.crowdflower.com/data-for-everyone', 'label': 'dataset_landing_page'}]",
        "repoID": 4549,
        "URL_gold_label": [
            {
                "URL": "http://www.crowdflower.com/data-for-everyone",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "SentEval also includes a series of https://github.com/facebookresearch/SentEval/tree/master/data/probing  to evaluate what linguistic properties are encoded in your sentence embeddings:",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/facebookresearch/SentEval/tree/master/data/probing', 'label': 'dataset_direct_link'}]",
        "repoID": 4567,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebookresearch/SentEval/tree/master/data/probing",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ECSSD results (link1) https://www.dropbox.com/s/zets1xsne570bgl/ECSSD_ELD.zip?dl=1 (link2) http://pan.baidu.com/s/1i4QslAP  (ECSSD dataset site http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html )",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* [{'URL': 'https://www.dropbox.com/s/zets1xsne570bgl/ECSSD_ELD.zip?dl=1', 'label': 'dataset_direct_link'}]\n* [{'URL': 'http://pan.baidu.com/s/1i4QslAP', 'label': 'other'}]\n* [{'URL': 'http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html', 'label': 'dataset_landing_page'}]",
        "repoID": 4612,
        "URL_gold_label": [
            {
                "URL": "http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Note that you will be asked to select a JPEG image from available CK data sets. We have added standard demo images ( , , , ) to the 'ctuning-datasets-min' repository https://github.com/ctuning/ctuning-datasets-min .",
        "answer": "  [{'URL': 'https://github.com/ctuning/ctuning-datasets-min', 'label': 'dataset_landing_page'}]",
        "repoID": 4633,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ctuning/ctuning-datasets-min",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Final raw bounding box results can be found here https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/', 'label': 'dataset_direct_link'}]",
        "repoID": 4644,
        "URL_gold_label": [
            {
                "URL": "https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Directory: https://github.com/Lab41/Magnolia/tree/master/data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/Lab41/Magnolia/tree/master/data', 'label': 'dataset_direct_link'}]",
        "repoID": 4647,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Lab41/Magnolia/tree/master/data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "H. Yin, C. Berger: \"When to use what data set for your self-driving car algorithm: An overview of publicly available driving datasets\" https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 4661,
        "URL_gold_label": [
            {
                "URL": "https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": ": Embeddings of UMLS https://www.nlm.nih.gov/research/umls/  concept unique identifiers (CUIs), derived from 20 million clinical notes spanning 19 years of data from Stanford Hospital and Clinics, using a data set http://datadryad.org/resource/doi:10.5061/dryad.jp917  released in a paper http://www.nature.com/articles/sdata201432  by Finlayson, LePendu & Shah.",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: <https://www.nlm.nih.gov/research/umls/>\nLabel: dataset_landing_page\n* URL: <http://datadryad.org/resource/doi:10.5061/dryad.jp917>\nLabel: dataset_direct_link\n* URL: <http://www.nature.com/articles/sdata201432>\nLabel: Other",
        "repoID": 4705,
        "URL_gold_label": [
            {
                "URL": "http://www.nature.com/articles/sdata201432",
                "gold_label": "other"
            },
            {
                "URL": "http://datadryad.org/resource/doi:10.5061/dryad.jp917",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Histopathology Gray: the dataset could be downloaded from link https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray ;",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray', 'label': 'dataset_direct_link'}]",
        "repoID": 4709,
        "URL_gold_label": [
            {
                "URL": "https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "static MNIST: links to the datasets can found at link https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST ;",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST', 'label': 'dataset_landing_page'}]",
        "repoID": 4709,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Activeloop Hub https://docs.activeloop.ai/datasets/fashion-mnist-dataset",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://docs.activeloop.ai/datasets/fashion-mnist-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://docs.activeloop.ai/datasets/fashion-mnist-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Apache MXNet Gluon https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST",
        "answer": "  [{\"URL\": \"https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Chainer https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "HuggingFace Datasets https://huggingface.co/datasets/fashion_mnist",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://huggingface.co/datasets/fashion_mnist', 'label': 'dataset_direct_link'}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://huggingface.co/datasets/fashion_mnist",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Keras https://keras.io/api/datasets/fashion_mnist/",
        "answer": "  [{'URL': 'https://keras.io/api/datasets/fashion_mnist/', 'label': 'dataset_direct_link'}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://keras.io/api/datasets/fashion_mnist/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pytorch https://pytorch.org/vision/stable/datasets.html#fashion-mnist",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pytorch.org/vision/stable/datasets.html#fashion-mnist\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://pytorch.org/vision/stable/datasets.html#fashion-mnist",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "TensorFlow Datasets https://www.tensorflow.org/datasets/catalog/fashion_mnist",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://www.tensorflow.org/datasets/catalog/fashion_mnist', 'label': 'dataset_landing_page'}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://www.tensorflow.org/datasets/catalog/fashion_mnist",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "TensorFlow.js https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js', 'label': 'Software'}]",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Tensorflow https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 4711,
        "URL_gold_label": [
            {
                "URL": "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Amazon dataset http://jmcauley.ucsd.edu/data/amazon/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://jmcauley.ucsd.edu/data/amazon/', 'label': 'dataset_landing_page'}]",
        "repoID": 4747,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Data Skeptic https://dataskeptic.com/blog/episodes/2017/pix2code  (podcast)",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://dataskeptic.com/blog/episodes/2017/pix2code', 'label': 'Other'}]",
        "repoID": 4748,
        "URL_gold_label": [
            {
                "URL": "https://dataskeptic.com/blog/episodes/2017/pix2code",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The paper presents a thorough off-line evaluation conducted on two playlist datasets: the publicly available AotM-2011 https://bmcfee.github.io/data/aotm2011.html  dataset (derived from the Art of the Mix http://www.artofthemix.org  platform), and a private collection that 8tracks https://8tracks.com/  shared with us for research purposes. The playlist collections are enriched with song features derived from the publicly available Million Song Dataset https://labrosa.ee.columbia.edu/millionsong/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"https://bmcfee.github.io/data/aotm2011.html\" - label: \"dataset_landing_page\"\n* \"https://8tracks.com/\" - label: \"other\"\n* \"https://labrosa.ee.columbia.edu/millionsong/\" - label: \"dataset_landing_page\"",
        "repoID": 4752,
        "URL_gold_label": [
            {
                "URL": "https://bmcfee.github.io/data/aotm2011.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "bokeh http://bokeh.pydata.org  for training visualization",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://bokeh.pydata.org', 'label': 'Software'}]",
        "repoID": 4770,
        "URL_gold_label": [
            {
                "URL": "http://bokeh.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for logging to csv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 4770,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download Hopkings dataset http://vision.jhu.edu/data/  into  folder",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://vision.jhu.edu/data/', 'label': 'dataset_direct_link'}]",
        "repoID": 4789,
        "URL_gold_label": [
            {
                "URL": "http://vision.jhu.edu/data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the AdelaideRMF dataset https://cs.adelaide.edu.au/users/hwong/doku.php?id=data  into  folder",
        "answer": "  [{'URL': 'https://cs.adelaide.edu.au/users/hwong/doku.php?id=data', 'label': 'dataset_direct_link'}]",
        "repoID": 4789,
        "URL_gold_label": [
            {
                "URL": "https://cs.adelaide.edu.au/users/hwong/doku.php?id=data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "To train the model on a custom dataset you need to define a class with a specific interface. Suppose we want to train the model on the digits http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html  dataset. This datasets consists of 8x8 images of digits. Let's suppose that the data is stored in , ,  and  files. We will assume that  and  have shapes of the form . We can then define the class corresponding to this dataset in  as follows.",
        "answer": "  [{\"URL\": \"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4795,
        "URL_gold_label": [
            {
                "URL": "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The silver data that I used in the experiments for the CLIN paper can be downloaded here http://www.let.rug.nl/rikvannoord/AMR/silver_data/ . The silver data was obtained by parsing all sentences in the Groningen Meaning Bank http://gmb.let.rug.nl/  with the parsers CAMR https://github.com/c-amr/camr  and JAMR https://github.com/jflanigan/jamr . The data folder contains seven files: all CAMR and JAMR parses (1.25 million, aligned with each other) and sets of AMRs (20k, 50k, 75k, 100k, 500k) that were used in our experiments (CAMR only). For more details please see our CLIN paper https://clinjournal.org/clinj/article/view/72/64 .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://www.let.rug.nl/rikvannoord/AMR/silver_data/\nLabel: dataset_direct_link\n* URL: http://gmb.let.rug.nl/\nLabel: dataset_landing_page\n* URL: https://github.com/c-amr/camr\nLabel: Software\n* URL: https://github.com/jflanigan/jamr\nLabel: Software\n* URL: https://clinjournal.org/clinj/article/view/72/64\nLabel: Other",
        "repoID": 4807,
        "URL_gold_label": [
            {
                "URL": "http://www.let.rug.nl/rikvannoord/AMR/silver_data/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The values associated to the keys can be either one value or a list of values. In the case of a list of values, the PIMC generator will consider all the possible combinations of values between all the keys. Some examples of configuration files for PIMC generation can be found in data/generator/config https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config . Some examples of PRISM files can be found in data/generator/inputs https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs  (from PRISM benchmarks http://www.prismmodelchecker.org/benchmarks/models.php#dtmcs ).",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: The values associated to the keys can be either one value or a list of values. In the case of a list of values, the PIMC generator will consider all the possible combinations of values between all the keys. Some examples of configuration files for PIMC generation can be found in data/generator/config https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config. Some examples of PRISM files can be found in data/generator/inputs https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs (from PRISM benchmarks http://www.prismmodelchecker.org/benchmarks/models.php#dtmcs).\n\nOutput:\n\n[{'URL': 'https://github.com/anicet-bart/pimc_pylib/tree/master/data/",
        "repoID": 4847,
        "URL_gold_label": [
            {
                "URL": "https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config",
                "gold_label": "other"
            },
            {
                "URL": "https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This recipe and collection of scripts enables you to train large vocabulary German acoustic models for speaker-independent automatic speech recognition (ASR) with Kaldi http://kaldi.sourceforge.net/ . The scripts currently use three freely available German speech corpora: The Tuda-De corpus is recorded with a Microsoft Kinect and two other microphones in parallel at Technische Universit\u00e4t Darmstadt and has been released under a permissive license (CC-BY 4.0) http://creativecommons.org/licenses/by/4.0/ . This corpus compromises ~31h of training data per microphone and ~5h separated into development and test partitions. We also make use of the German subset from the Spoken Wikipedia Corpora (SWC) https://nats.gitlab.io/swc/ , containing about 285h of additional data and the German subset of m-ailabs read speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/  (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://kaldi.sourceforge.net/\nLabel: DatasetDirectLink\n* URL: http://creativecommons.org/licenses/by/4.0/\nLabel: Other\n* URL: http://nats.gitlab.",
        "repoID": 4851,
        "URL_gold_label": [
            {
                "URL": "https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We added more aligned speech data (630h total now), thanks to the m-ailabs speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ . We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (label: dataset_landing_page)\n* URL: https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ (label: dataset_landing_page)\n* URL: https://www.kaldi-project.org/ (label: Software)",
        "repoID": 4851,
        "URL_gold_label": [
            {
                "URL": "https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "F1 scores of models against secret blind data in the STUART and CRAWFORD wells. The logs for those wells are available in the repo https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv , but contestants do not have access to the facies.",
        "answer": "  [{\"URL\": \"https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 4869,
        "URL_gold_label": [
            {
                "URL": "https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Check this repo https://github.com/amarasovic/abstract-anaphora-data .",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'https://github.com/amarasovic/abstract-anaphora-data', 'label': 'dataset_landing_page'}]",
        "repoID": 4904,
        "URL_gold_label": [
            {
                "URL": "https://github.com/amarasovic/abstract-anaphora-data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Two short RGB-D sequences are included as examples. To add more sequences, download from: TUM_RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset  or ICL-NUIM https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html  and place them inside the directory  the same way it has been done for the examples. Then add the respective camera parameters in the same format as the examples as",
        "answer": "  [INST]\n    # Annotation\n    Input: Two short RGB-D sequences are included as examples. To add more sequences, download from: TUM_RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset or ICL-NUIM https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html and place them inside the directory the same way it has been done for the examples. Then add the respective camera parameters in the same format as the examples as:\n    Output: [{\"URL\": \"https://vision.in.tum.de/data/datasets/rgbd-dataset\", \"label\": \"dataset_direct_link\"}, {\"URL\": \"https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 4968,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "A series of positions of the Helsinki Regional Transport https://www.hsl.fi/en  fleet obtained by sampling http://dev.hsl.fi/siriaccess/vm/json every 30 seconds, restricting the timeperiod to the time of the trial and geoboxing the area around the coordinates https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/transit-data-bounding-box.png  sampled from the test participants.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://www.hsl.fi/en>\nLabel: Other\n\n* URL: <http://dev.hsl.fi/siriaccess/vm/json>\nLabel: Software\n\n* URL: <https://github.com/aalto-trafficsense/public-transport-dataset>\nLabel: DatasetLandingPage",
        "repoID": 4969,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/transit-data-bounding-box.png",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "CSV-versions of the data are available in the csv folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv . The psql folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql  contains instructions and scripts to import the data into a PostgreSQL database and produce reports and comparison tables between the manual log and recognised trips. Output from three sample methods is included for testing.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv\nLabel: dataset_direct_link\n* URL: https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql\nLabel: dataset_landing_page",
        "repoID": 4969,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql",
                "gold_label": "other"
            },
            {
                "URL": "https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "JSON-format information in the trains-json folder https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json  is fetched from: http://rata.digitraffic.fi/api/v1/history?departure_date=2016-08-26",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json', 'label': 'dataset_landing_page'}]",
        "repoID": 4969,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The mobile client uses the Android fused location provider https://developers.google.com/android/reference/com/google/android/gms/location/FusedLocationProviderApi , which combines data from satellite, WLAN and cellular positioning. Despite that, sometimes there can be problematic positioning fixes, which should be taken care of by filtering. One example is shown here https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/example-positioning-problem.png .",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://developers.google.com/android/reference/com/google/android/gms/location/FusedLocationProviderApi\",\n            \"label\": \"Software\"\n        }\n    ]",
        "repoID": 4969,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/example-positioning-problem.png",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The train data is licensed under the Creative Commons BY 4.0 licence http://creativecommons.org/licenses/by/4.0/  from Digitraffic http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic#.V9BlOxB96Ho  offered by the Finnish Traffic Agency http://www.liikennevirasto.fi/web/en .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://creativecommons.org/licenses/by/4.0/\nLabel: DatasetDirectLink\n* URL: http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic\nLabel: DatasetLandingPage\n* URL: http://www.liikennevirasto.fi/web/en\nLabel: Other",
        "repoID": 4969,
        "URL_gold_label": [
            {
                "URL": "http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic#.V9BlOxB96Ho",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "as_relationship : CAIDAs AS relationship files found here http://data.caida.org/datasets/as-relationships/serial-1/",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://data.caida.org/datasets/as-relationships/serial-1/', 'label': 'dataset_direct_link'}]",
        "repoID": 4976,
        "URL_gold_label": [
            {
                "URL": "http://data.caida.org/datasets/as-relationships/serial-1/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Dataloaders for common audio datasets http://pytorch.org/audio/main/datasets.html",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://pytorch.org/audio/main/datasets.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5016,
        "URL_gold_label": [
            {
                "URL": "http://pytorch.org/audio/main/datasets.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download here http://datashare.is.ed.ac.uk/handle/10283/1942",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://datashare.is.ed.ac.uk/handle/10283/1942', 'label': 'dataset_direct_link'}]",
        "repoID": 5053,
        "URL_gold_label": [
            {
                "URL": "http://datashare.is.ed.ac.uk/handle/10283/1942",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Personalized Dialog dataset can be downloaded using . Alternatively, it is accessable using this link https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1  or through the ParlAI framework http://parl.ai/  for dialog AI research.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1', 'label': 'dataset_direct_link'}]\n\n[{'URL': 'http://parl.ai/', 'label': 'dataset_landing_page'}]",
        "repoID": 5063,
        "URL_gold_label": [
            {
                "URL": "https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "AudioContentAnalysis nearly exhaustive list of music-related datasets http://www.audiocontentanalysis.org/data-sets/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://www.audiocontentanalysis.org/data-sets/', 'label': 'dataset_landing_page'}]",
        "repoID": 5074,
        "URL_gold_label": [
            {
                "URL": "http://www.audiocontentanalysis.org/data-sets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Awesome public datasets https://github.com/caesar0301/awesome-public-datasets",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/caesar0301/awesome-public-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 5074,
        "URL_gold_label": [
            {
                "URL": "https://github.com/caesar0301/awesome-public-datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Battle of the Deep Learning frameworks https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750  - DL frameworks comparison and evolution",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750', 'label': 'dataset_landing_page'}]",
        "repoID": 5074,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Estimating Optimal Learning Rate https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0  - Blog post on the learning rate optimisation",
        "answer": "  [{\"URL\": \"https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\", \"label\": \"Other\"}]",
        "repoID": 5074,
        "URL_gold_label": [
            {
                "URL": "https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Wikipedia's list of datasets for machine learning research https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research', 'label': 'dataset_landing_page'}]",
        "repoID": 5074,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We also tested our implementation with the  dataset, available at LibSVM Datasets http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5102,
        "URL_gold_label": [
            {
                "URL": "http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "dataset_class.ipynb https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb  examples for using the data generator",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 5113,
        "URL_gold_label": [
            {
                "URL": "https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For image denoise, launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_test_demo.m  to see the denoise result by pre-train models. For training, you need to generate the data yourself since the data used in the training is large. Please do the following steps to generate data: a) download MIT saliency dataset from here http://saliency.mit.edu/BenchmarkIMAGES.zip  and put all the image files here https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency ; b) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m  to generate training data; c) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m  to generate validation data; d) launch this script https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_train_demo.m  to start the training.",
        "answer": "  Here are the annotated URLs:\n\n* URL: <https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/denoise_test_demo.m>\nLabel: dataset_direct_link\n\n* URL",
        "repoID": 5116,
        "URL_gold_label": [
            {
                "URL": "https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download KITTI dataset from here http://www.cvlibs.net/datasets/kitti/eval_tracking.php . We need left color images http://www.cvlibs.net/download.php?file=data_tracking_image_2.zip  and tracking labels http://www.cvlibs.net/download.php?file=data_tracking_label_2.zip .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_tracking.php', 'label': 'dataset_landing_page'}]\n\nPlease note that I only annotated the last URL in the input, which is the URL of the dataset landing page.",
        "repoID": 5125,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This code implements the state-of-the-art Knowledge Graph Embedding algorithms http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf  such as RESCAL http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf , TransE http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data , HOLE http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12484/11828 , and S-RESCAL with added Type Regularizer for Freebase. Computational Graphs are implemented using Theano http://deeplearning.net/software/theano/ . Available SGD Algorithms : ADAM, Adagrad. Algorithms are hand coded and implementing SGD variations should be straightforward.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf\nLabel: dataset_direct_link\n* URL: http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf\nLabel: dataset_direct_link\n* URL: http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\nLabel: dataset_direct_link\n* URL: http://www.aaai.",
        "repoID": 5127,
        "URL_gold_label": [
            {
                "URL": "http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The valid TLDs file (get if from IANA http://data.iana.org/TLD/tlds-alpha-by-domain.txt )",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://data.iana.org/TLD/tlds-alpha-by-domain.txt', 'label': 'dataset_direct_link'}]",
        "repoID": 5131,
        "URL_gold_label": [
            {
                "URL": "http://data.iana.org/TLD/tlds-alpha-by-domain.txt",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Second, this project uses pretrained GloVe word embeddings https://nlp.stanford.edu/projects/glove/  of 100 dimensions, trained on 6B tokens. Download http://nlp.stanford.edu/data/glove.6B.100d.zip  and extract under .",
        "answer": "  [{\"URL\": \"https://nlp.stanford.edu/data/glove.6B.100d.zip\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 5140,
        "URL_gold_label": [
            {
                "URL": "http://nlp.stanford.edu/data/glove.6B.100d.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The output, in a CoNLL 2009-like format will be written to  and in the frame-elements file format https://github.com/Noahs-ARK/semafor/tree/master/training/data  to  for frame and argument identification.",
        "answer": "  Sure, here are the annotated URLs in CoNLL 2009 format:\n\nInput: The output, in a CoNLL 2009-like format will be written to <https://github.com/Noahs-ARK/semafor/tree/master/training/data> for frame and argument identification.\n\nOutput:\n\n[\n{\n\"URL\": \"https://github.com/Noahs-ARK/semafor/tree/master/training/data\",\n\"label\": \"Other\"\n}\n]",
        "repoID": 5140,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Noahs-ARK/semafor/tree/master/training/data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "SIFT1M and GIST1M http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 5166,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "SIFT1M and GIST1M http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 5167,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To run inference with your model in MediaPipe inference demo https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset , you need to export your checkpoint to a SavedModel.",
        "answer": "  [{\"URL\": \"https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset\", \"label\": \"Software\"}]",
        "repoID": 5189,
        "URL_gold_label": [
            {
                "URL": "https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the dataset http://spatialrelations.cs.uni-freiburg.de/#dataset",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://spatialrelations.cs.uni-freiburg.de/#dataset', 'label': 'dataset_direct_link'}]",
        "repoID": 5194,
        "URL_gold_label": [
            {
                "URL": "http://spatialrelations.cs.uni-freiburg.de/#dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This repo is useful as a corpus for typing training programs. According to analysis of the Oxford English Corpus https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/ , the 7,000 most common English lemmas account for approximately 90% of usage, so a 10,000 word training corpus is more than sufficient for practical training applications.",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* URL: <https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/>\nLabel: Other",
        "repoID": 5222,
        "URL_gold_label": [
            {
                "URL": "https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Besides, the locations of sensors in Los Angeles, i.e., METR-LA, are available at data/sensor_graph/graph_sensor_locations.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv , and the locations of sensors in PEMS-BAY are available at data/sensor_graph/graph_sensor_locations_bay.csv https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv\nLabel: dataset_direct_link\n\n* URL: https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv\nLabel: dataset_direct_link",
        "repoID": 5237,
        "URL_gold_label": [
            {
                "URL": "https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org/  ( )",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas http://pandas.pydata.org/\":\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 5264,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To reproduce our experimental results, you need dictionaries ( LexNet https://github.com/vered1986/LexNET/tree/v2/datasets  for English, Parsed Wiktionary http://ustalov.imm.uran.ru/pub/projlearn-ruwikt.tar.gz  for Russian) and word embeddings ( Google News https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing  for English, Russian Distributional Thesaurus http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz500-w10-cb0-it3-min5.w2v  for Russian). Since our implementation uses Python 3 and TensorFlow 0.12, please install them, too.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/vered1986/LexNET/tree/v2/datasets\nLabel: DatasetDirectLink\n* URL: http://ustalov.imm.uran.ru/pub/projlearn-ruwikt.tar.gz\nLabel: DatasetDirectLink\n* URL: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\nLabel: DatasetDirectLink\n* URL: http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz500-w10-cb0-it",
        "repoID": 5293,
        "URL_gold_label": [
            {
                "URL": "https://github.com/vered1986/LexNET/tree/v2/datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Both Quasar-S and Quasar-T are available for download here http://curtis.ml.cmu.edu/datasets/quasar/ . See the accompanying  for a description of the included files. The release includes:",
        "answer": "  [{\"URL\": \"http://curtis.ml.cmu.edu/datasets/quasar/\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 5295,
        "URL_gold_label": [
            {
                "URL": "http://curtis.ml.cmu.edu/datasets/quasar/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The code for handling the meshes is a prelimary version of what became the DiSk++ https://github.com/datafl4sh/diskpp  library by Matteo Cicuttin. The code for assembling the high order operators is based on code written by Danielle Di Pietro for similar HHO schemes.",
        "answer": "  [{\"URL\": \"https://github.com/datafl4sh/diskpp\", \"label\": \"Software\"}]",
        "repoID": 5306,
        "URL_gold_label": [
            {
                "URL": "https://github.com/datafl4sh/diskpp",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "incremental subsamples of 150K, 700K, 1M and 1.7M (~all news2016) parallel back-translation corpora used in the  where the target (TR) side samples are from monolingual Turkish data http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz . The sentences are translated into EN with a single TR->EN NMT system (~14 BLEU on newstest2016):",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* [{'URL': 'http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL is for a dataset landing page, specifically the page for the News2016 parallel corpora used in the paper.",
        "repoID": 5322,
        "URL_gold_label": [
            {
                "URL": "http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'Software'}]",
        "repoID": 5359,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "derive beat annotations by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py",
        "answer": "  Sure! Here are the annotations for the input URL:\n\n[{'URL': 'https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py', 'label': 'Software'}]",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "derive singing voice note annotations by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py  Since MIDI standard does not define an instrument for singing voice, the singing voice track is given a different program # in a random channel in each MIDI. Thus one needs to manually identify the MIDI channel # that corresponds to the melody of the singing voice track Optionally, doing in advance an annotation of segments with active vocal is helpful.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py', 'label': 'Software'}]",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "find recording MSD_TRACK_id from this list https://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_tracks.txt . Then match by this script https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/match.py",
        "answer": "  Sure, I can help you with that! Here's the annotated output for the given input:\n\n[{'URL': 'https://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_tracks.txt', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\nThe input contains a list of URLs, and the first URL is \"https://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/unique_tracks.txt\". Based on the provided examples, this URL falls under the label \"dataset_landing_page\".",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/match.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "get the matched MIDI from lakh-matched MIDI fetch_midi https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py  (if more than one match, pick the MIDI for the best match)",
        "answer": "  Sure, I can help you with that! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\nThe input URL is \"get the matched MIDI from lakh-matched MIDI fetch_midi <https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py>\". Since the URL is not directly pointing to a dataset or software, I have classified it as a \"dataset landing page\".",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "if systematic delay/advance of timestamps, measure the difference to onsets with SV's measure tool and run shift time of annotation https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py",
        "answer": "  [{\"URL\": \"https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py\", \"label\": \"Software\"}]",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "scripts: python scripts for loading data, more scripts are in the similar repository https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts', 'label': 'dataset_landing_page'}]",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "verify annotations of note onsets and beats. Correct manually some imprecise vocal annotations. Open as note layer in Sonic Visualiser http://www.sonicvisualiser.org/  by script 'sh open_in_sv.sh https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/open_in_sv.sh '",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'http://www.sonicvisualiser.org/', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\n* The input is a URL pointing to a dataset, specifically the Lakh Vocal Segments dataset.\n* The URL is a landing page for the dataset, providing information on how to access and use the data.\n* Therefore, the label is set to 'dataset_landing_page'.",
        "repoID": 5374,
        "URL_gold_label": [
            {
                "URL": "https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/open_in_sv.sh",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip",
        "answer": "  Sure! Here are the annotated URLs:\n\n* https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.",
        "repoID": 5385,
        "URL_gold_label": [
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": " https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip",
        "answer": ".",
        "repoID": 5385,
        "URL_gold_label": [
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The non-English data (Czech, French, German, Russian, and Spanish) can be downloaded from Jan Botha's website https://bothameister.github.io . For ease of use you can use the Yoon Kim's script https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh , which downloads these data and saves them into the relevant folders.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://bothameister.github.io\nLabel: DatasetLandingPage\n* URL: https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh\nLabel: Software",
        "repoID": 5399,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "http://cs.stanford.edu/~danqi/data/cnn.tar.gz http://cs.stanford.edu/~danqi/data/cnn.tar.gz  already preprocessed, and the original one from https://github.com/deepmind/rc-data https://github.com/deepmind/rc-data  for the CNN news article",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://cs.stanford.edu/~danqi/data/cnn.tar.gz', 'label': 'dataset_direct_link'}]\n\nExplanation:\n\n* The URL \"http://cs.stanford.edu/~danqi/data/cnn.tar.gz\" is a direct link to a dataset file.\n* The URL \"https://github.com/deepmind/rc-data\" is an introduction or landing page for a dataset entity.",
        "repoID": 5457,
        "URL_gold_label": [
            {
                "URL": "https://github.com/deepmind/rc-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For an improved implementation of the extractive evaluation metrics (and improved models), please see the data2text-plan-py https://github.com/ratishsp/data2text-plan-py  repo associated with the Puduppully et al. (AAAI 2019) paper https://arxiv.org/abs/1809.00582 .",
        "answer": "  [{\"URL\": \"https://github.com/ratishsp/data2text-plan-py\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5459,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ratishsp/data2text-plan-py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The boxscore-data associated with the above paper can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data , and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: <https://github.com/harvardnlp/boxscore-data>\nLabel: DatasetDirectLink",
        "repoID": 5459,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "models and results reflecting the newly cleaned up data in the boxscore-data repo https://github.com/harvardnlp/boxscore-data  are now given below.",
        "answer": "  [{\"URL\": \"https://github.com/harvardnlp/boxscore-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5459,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Data is from the GenX corpus https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus  produced by FitzGerald http://nfitz.net/  et al.",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus', 'label': 'dataset_landing_page'}]",
        "repoID": 5464,
        "URL_gold_label": [
            {
                "URL": "https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Following our text recognition experiments might be a little difficult, because we can not offer the entire dataset used by us. But it is possible to perform the experiments based on the Synth-90k dataset provided by Jaderberg et al. here https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth . After downloading and extracting this file you'll need to adapt the groundtruth file provided with this dataset to fit to the format used by our code. Our format is quite easy. You need to create a  file with tabular separated values. The first column is the absolute path to the image and the rest of the line are the labels corresponding to this image.",
        "answer": "  [INST]\n    URL: https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth\n    Label: dataset_direct_link\n    [/INST]  Got it! Here's the annotated output for the given input:\n\n[{'URL': 'https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth', 'label': 'dataset_direct_link'}]",
        "repoID": 5484,
        "URL_gold_label": [
            {
                "URL": "https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "\u5173\u4e8e\u6807\u6ce8\u96c6\u542b\u4e49\uff0c\u8bf7\u53c2\u8003 \u300a\u8bed\u8a00\u5b66\u6807\u6ce8\u89c4\u8303\u300b https://hanlp.hankcs.com/docs/annotations/index.html \u53ca \u300a\u683c\u5f0f\u89c4\u8303\u300b https://hanlp.hankcs.com/docs/data_format.html \u3002\u6211\u4eec\u8d2d\u4e70\u3001\u6807\u6ce8\u6216\u91c7\u7528\u4e86\u4e16\u754c\u4e0a\u91cf\u7ea7\u6700\u5927\u3001\u79cd\u7c7b\u6700\u591a\u7684\u8bed\u6599\u5e93\u7528\u4e8e\u8054\u5408\u591a\u8bed\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6240\u4ee5HanLP\u7684\u6807\u6ce8\u96c6\u4e5f\u662f\u8986\u76d6\u9762\u6700\u5e7f\u7684\u3002",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://hanlp.hankcs.com/docs/annotations/index.html', 'label': 'dataset_landing_page'}]\n[{'URL': 'https://hanlp.hankcs.com/docs/data_format.html', 'label': 'other'}]",
        "repoID": 5495,
        "URL_gold_label": [
            {
                "URL": "https://hanlp.hankcs.com/docs/data_format.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Create a dictionary with inverse document frequency (idf) values from the Google NGrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html  dataset.",
        "answer": "  Sure, here's the annotated output for the given input:\n\n[{'URL': 'http://storage.googleapis.com/books/ngrams/books/datasetsv2.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5496,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the 1-gram files and the  files for your language from the Google NGrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html  site into a common folder.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://storage.googleapis.com/books/ngrams/books/datasetsv2.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5496,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Analysis of Network Data http://bactra.org/notebooks/network-data-analysis.html .",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'http://bactra.org/notebooks/network-data-analysis.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://bactra.org/notebooks/network-data-analysis.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Eric D. Kolaczyk\u2019s Network Datasets http://math.bu.edu/people/kolaczyk/datasets.html .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://math.bu.edu/people/kolaczyk/datasets.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://math.bu.edu/people/kolaczyk/datasets.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "International Currencies 1890-1910 http://eh.net/database/international-currencies-1890-1910/  - Historical data on the international connections between 45 currencies.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://eh.net/database/international-currencies-1890-1910/', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://eh.net/database/international-currencies-1890-1910/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Introducing tidygraph https://www.data-imaginist.com/2017/introducing-tidygraph/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.data-imaginist.com/2017/introducing-tidygraph/', 'label': 'Other'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://www.data-imaginist.com/2017/introducing-tidygraph/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Mark E.J. Newman\u2019s Network Data http://www-personal.umich.edu/~mejn/netdata/  ( example visualizations http://www-personal.umich.edu/~mejn/networks/ ).",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://www-personal.umich.edu/~mejn/netdata/', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://www-personal.umich.edu/~mejn/netdata/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Methods of Social Network Visualization http://moreno.ss.uci.edu/90.pdf  ( , 2009; poster version http://www.pfeffer.at/data/visposter/ ).",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://moreno.ss.uci.edu/90.pdf', 'label': 'dataset_landing_page'},\n {'URL': 'http://www.pfeffer.at/data/visposter/', 'label': 'dataset_direct_link'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://www.pfeffer.at/data/visposter/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Network Science Book - Network Datasets http://networksciencebook.com/translations/en/resources/data.html  - Network data sets from Albert-L\u00e1szl\u00f3 Barab\u00e1si\u2019s  book. Includes data on IMDB actors, arXiv scientific collaboration, network of routers, the US power grid, protein-protein interactions, cell phone users, citation networks, metabolic reactions, e-mail networks, and nd.edu Web pages.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://networksciencebook.com/translations/en/resources/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://networksciencebook.com/translations/en/resources/data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pajek Datasets http://vlado.fmf.uni-lj.si/pub/networks/data/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://vlado.fmf.uni-lj.si/pub/networks/data/', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://vlado.fmf.uni-lj.si/pub/networks/data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Siena Datasets http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "SocioPatterns Datasets http://www.sociopatterns.org/datasets/  - Network data obtained through the SocioPatterns http://www.sociopatterns.org/  sensing platform.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.sociopatterns.org/datasets/', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://www.sociopatterns.org/datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Stanford Large Network Dataset Collection http://snap.stanford.edu/data/index.html .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://snap.stanford.edu/data/index.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Ties that Divide: A Network Analysis of the International Monetary System, 1890\u20131910 http://www.stats.ox.ac.uk/~snijders/FlandreauJobst2005.pdf  ( , 2005) and The Empirics of International Currencies: Network Externalities, History and Persistence http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02219.x/abstract  ( , 2009), both by Marc Flandreau and Clemens Jobst - Network analysis of the foreign exchange system in the late 19th century ( data http://eh.net/database/international-currencies-1890-1910/ ).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.stats.ox.ac.uk/~snijders/FlandreauJobst2005.pdf\nLabel: dataset_landing_page\n* URL: http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02219.x/abstract\nLabel: Other\n* URL: http://eh.net/database/international-currencies-1890-1910/\nLabel: dataset_direct_link",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "http://eh.net/database/international-currencies-1890-1910/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "UCINET Datasets https://sites.google.com/site/ucinetsoftware/datasets  - Network data in UCINET format.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://sites.google.com/site/ucinetsoftware/datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/site/ucinetsoftware/datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Using Metadata to Find Paul Revere https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/  and The Other Ride of Paul Revere: The Brokerage Role in the Making of the American Revolution http://www.sscnet.ucla.edu/polisci/faculty/chwe/ps269/han.pdf  - Network analysis applied to American revolutionaries.",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: Using Metadata to Find Paul Revere https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/\n\nOutput: [{\"URL\": \"https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/\", \"label\": \"dataset_landing_page\"}]\n\nInput: The Other Ride of Paul Revere: The Brokerage Role in the Making of the American Revolution http://www.sscnet.ucla.edu/polisci/faculty/chwe/ps269/han.pdf\n\nOutput: [{\"URL\": \"http://www.sscnet.ucla.edu/polisci/faculty/chwe/ps269/han.pdf\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Working with Bipartite/Affiliation Network Data in R https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/  (2012).",
        "answer": "  [{\"URL\": \"https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "igraphdata https://CRAN.R-project.org/package=igraphdata  - R data-centric package.",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://CRAN.R-project.org/package=igraphdata\",\n\"label\": \"Software\"\n}",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://CRAN.R-project.org/package=igraphdata",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "networkdata https://github.com/schochastics/networkdata  - Includes 979 network datasets containing 2135 networks.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/schochastics/networkdata', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://github.com/schochastics/networkdata",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "tnet Datasets https://toreopsahl.com/datasets/  - Weighted network data.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://toreopsahl.com/datasets/', 'label': 'dataset_landing_page'}]",
        "repoID": 5507,
        "URL_gold_label": [
            {
                "URL": "https://toreopsahl.com/datasets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We have provided a DROP demo script ( ) and sample dataset from the UCR Time Series Repository http://www.cs.ucr.edu/~eamonn/time_series_data/  ( ).",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: <http://www.cs.ucr.edu/~eamonn/time_series_data/>\nLabel: Other\n\n* URL: <(>)\nLabel: None (no URL provided)",
        "repoID": 5530,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ucr.edu/~eamonn/time_series_data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the VOC 2007 http://host.robots.ox.ac.uk/pascal/VOC/voc2007/  dataset and Koen van de Sande's selective search windows http://koen.me/research/selectivesearch/  for VOC 2007 and the VGG-F https://gist.github.com/ksimonyan/a32c9063ec8e1118221a  model by running the first command. Optionally download the VOC 2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/  and Ross Girshick's selective search windows https://github.com/rbgirshick/fast-rcnn/blob/master/data/scripts/fetch_fast_rcnn_models.sh  by manually downloading the VOC 2012 test data tarball http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar  to  and then running the second command:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\nLabel: dataset_direct_link\n\n* URL: http://koen.me/research/selectivesearch/\nLabel: software\n\n* URL: https://gist.github.com/ksimonyan/a32c9063ec8e1118221a\nLabel: software\n\n* URL: http://host.robots.ox.ac.uk/",
        "repoID": 5539,
        "URL_gold_label": [
            {
                "URL": "https://github.com/rbgirshick/fast-rcnn/blob/master/data/scripts/fetch_fast_rcnn_models.sh",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For instructions on getting started with the NVIDIA Container Toolkit, refer to the installation guide https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker .",
        "answer": "  [{\"URL\": \"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5544,
        "URL_gold_label": [
            {
                "URL": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Product documentation including an architecture overview, platform support, installation and usage guides can be found in the documentation repository https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html .",
        "answer": "  [{'URL': 'https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html', 'label': 'dataset_landing_page'}]",
        "repoID": 5544,
        "URL_gold_label": [
            {
                "URL": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The user guide https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html  provides information on the configuration and command line options available when running GPU containers with Docker.",
        "answer": "  [{\"URL\": \"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5544,
        "URL_gold_label": [
            {
                "URL": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The input specification format mimics that of Haskell algebraic data types https://wiki.haskell.org/Algebraic_data_type  where in addition each type constructor may be annotated with an additional  parameter. For instance:",
        "answer": "  Sure, I'm ready to annotate the URLs in the input. Please provide the input text.",
        "repoID": 5564,
        "URL_gold_label": [
            {
                "URL": "https://wiki.haskell.org/Algebraic_data_type",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Standardized train https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list , valid https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list , and test https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list  sets are also available. Here https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list  is a list of all the files.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list>\nLabel: <dataset_direct_link>\n\n* URL: <https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list>\nLabel: <dataset_direct_link>\n\n* URL: <https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list>\nLabel: <dataset_direct_link>\n\n* URL: <https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list>\nLabel: <dataset_direct_link>",
        "repoID": 5587,
        "URL_gold_label": [
            {
                "URL": "https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "ConvoKit ships with several datasets ready for use \"out-of-the-box\". These datasets can be downloaded using the helper function https://github.com/CornellNLP/ConvoKit/blob/master/convokit/util.py . Alternatively you can access them directly here http://zissou.infosci.cornell.edu/convokit/datasets/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/CornellNLP/ConvoKit/blob/master/convokit/util.py\nLabel: dataset_direct_link\n\n* URL: http://zissou.infosci.cornell.edu/convokit/datasets/\nLabel: dataset_landing_page",
        "repoID": 5592,
        "URL_gold_label": [
            {
                "URL": "http://zissou.infosci.cornell.edu/convokit/datasets/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "In addition to the provided datasets, you may also use ConvoKit with your own custom datasets by loading them into a  object. This example script https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb  shows how to construct a Corpus from custom data.",
        "answer": "  [{\"URL\": \"https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb\", \"label\": \"Other\"}]",
        "repoID": 5592,
        "URL_gold_label": [
            {
                "URL": "https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The full corpus of Wikipedia talk page conversations, based on the reconstruction described in this paper http://www.cs.cornell.edu/~cristian/index_files/wikiconv-conversation-corpus.pdf . Note that due to the large size of the data, it is split up by year. We separately provide block data retrieved directly from the Wikipedia block log https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json , for reproducing the Trajectories of Blocked Community Members http://www.cs.cornell.edu/~cristian/Recidivism_online_files/recidivism_online.pdf  paper.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cs.cornell.edu/~cristian/index_files/wikiconv-conversation-corpus.pdf\nLabel: dataset_landing_page\n* URL: https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json\nLabel: dataset_direct_link",
        "repoID": 5592,
        "URL_gold_label": [
            {
                "URL": "https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "This toolkit contains tools to extract conversational features and analyze social phenomena in conversations, using a single unified interface https://convokit.cornell.edu/documentation/architecture.html  inspired by (and compatible with) scikit-learn. Several large conversational datasets https://github.com/CornellNLP/ConvoKit#datasets  are included together with scripts exemplifying the use of the toolkit on these datasets. The latest version is 2.5.3 https://github.com/CornellNLP/ConvoKit/releases/tag/v2.5.2  (released 16 Jan 2022); follow the project on GitHub https://github.com/CornellNLP/ConvoKit  to keep track of updates.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: <https://convokit.cornell.edu/documentation/architecture.html>\nLabel: dataset_landing_page\n* URL: <https://github.com/CornellNLP/ConvoKit>\nLabel: dataset_direct_link\n* URL: <https://github.com/CornellNLP/ConvoKit/releases/tag/v2.5.2>\nLabel: dataset_direct_link\n* URL: <https://github.com/CornellNLP/ConvoKit>\nLabel: software",
        "repoID": 5592,
        "URL_gold_label": [
            {
                "URL": "https://github.com/CornellNLP/ConvoKit#datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 5619,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/ Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast-approximate-string-matching/ Very fast Data cleaning of product names, company names & street names https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/ Sub-millisecond compound aware automatic spelling correction https://seekstorm.com/blog/sub-millisecond-compound-aware-automatic.spelling-correction/ SymSpell vs. BK-tree: 100x faster fuzzy string search & spell checking https://seekstorm.com/blog/symspell-vs-bk-tree/ Fast Word Segmentation for noisy text https://seekstorm.com/blog/fast-word-segmentation-noisy-text/ The Pruning Radix Trie \u2014 a Radix trie on steroids https://seekstorm.com/blog/pruning-radix-trie/",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: 1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/\n\nOutput: [{'URL': 'https://seekstorm.com/blog/1000x-spelling-correction/', 'label': 'Software'}]\n\nInput: Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast",
        "repoID": 5620,
        "URL_gold_label": [
            {
                "URL": "https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data",
        "answer": "  Sure, here are the annotated URLs:\n\n* [{'URL': 'https://github.com/hermitdave/FrequencyWords', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/hermitdave/FrequencyWords', 'label': 'Software'}]",
        "repoID": 5620,
        "URL_gold_label": [
            {
                "URL": "https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Google Books Ngram data http://storage.googleapis.com/books/ngrams/books/datasetsv2.html (License) https://creativecommons.org/licenses/by/3.0/  : Provides representative word frequencies",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'http://storage.googleapis.com/books/ngrams/books/datasetsv2.html', 'label': 'dataset_direct_link'}]",
        "repoID": 5620,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For this example, we'll use the \"a1a\" dataset, acquired from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . Currently the Photon ML dataset converter supports only the LibSVM format.",
        "answer": "  [{\"URL\": \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5624,
        "URL_gold_label": [
            {
                "URL": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can downlooad pre-trained models for English, German, and Russian http://ltdata1.informatik.uni-hamburg.de/sensegram/ . Note that to run examples from the QuickStart you only need files with extensions , , and . Other files are supplementary.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://ltdata1.informatik.uni-hamburg.de/sensegram/\nLabel: DatasetDirectLink",
        "repoID": 5628,
        "URL_gold_label": [
            {
                "URL": "http://ltdata1.informatik.uni-hamburg.de/sensegram/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets . Although it contains stereo cameras, we only use one camera. The system also works with ETH-asl cla dataset http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/ . We take EuRoC as the example.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets', 'label': 'dataset_landing_page'}]\n\nNote: I have only annotated the last URL given in the input, which is \"Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\".",
        "repoID": 5643,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "This example uses the famous Pima diabetes study at the UCI data repository. The following table shows the 9 variables in , followed by their descriptions from this link https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes :",
        "answer": "  [{\"URL\": \"https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5679,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Data/Resources used for our SISA (Syntactic Iberian Polarity classification) model can be found here http://grupolys.org/software/UUUSA/sisa-data.zip",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://grupolys.org/software/UUUSA/sisa-data.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 5686,
        "URL_gold_label": [
            {
                "URL": "http://grupolys.org/software/UUUSA/sisa-data.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The sample data and processing results of detected corners can be downloaded from here https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0  (181M) for panoramic image and here https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0  (29M) for perspective image.  These data are acquired with the chessboard file readme_files/chessboard_A0_0.75_6_8.pdf  which contains 6*8 patterns and the length of one grid is 7.5cm if it is printed by A0 size.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0 (label: dataset_direct_link)\n* URL: https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0 (label: dataset_direct_link)",
        "repoID": 5693,
        "URL_gold_label": [
            {
                "URL": "https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "All models http://lxcenter.di.fc.ul.pt/datasets/models/",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'http://lxcenter.di.fc.ul.pt/datasets/models/', 'label': 'dataset_landing_page'}]",
        "repoID": 5714,
        "URL_gold_label": [
            {
                "URL": "http://lxcenter.di.fc.ul.pt/datasets/models/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "LX-DSemVectors 2.2b http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/', 'label': 'dataset_direct_link'}]",
        "repoID": 5714,
        "URL_gold_label": [
            {
                "URL": "http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Cornell Movie Review http://www.cs.cornell.edu/people/pabo/movie-review-data/  -- MovieReview.ipynb emoint/examples/MovieReview.ipynb",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n* URL: http://www.cs.cornell.edu/people/pabo/movie-review-data/\nLabel: dataset_landing_page\n\nNote: This URL is the landing page for a dataset of movie reviews, which can be accessed by clicking on the \"MovieReview.ipynb\" file.",
        "repoID": 5717,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cornell.edu/people/pabo/movie-review-data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This code implements the state-of-the-art Knowledge Graph Embedding algorithms http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf  such as RESCAL http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf , TransE http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data , COMPLEX http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12484/11828 , Computational Graphs are implemented using PyTorch https://pytorch.org/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cs.technion.ac.il/~gabr/publications/papers/Nickel2016RRM.pdf\nLabel: dataset_direct_link\n* URL: http://www.dbs.ifi.lmu.de/~tresp/papers/p271.pdf\nLabel: dataset_direct_link\n* URL: http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data\nLabel: dataset_direct_link\n* URL: http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12484/11828\nLabel: dataset_direct_link\n* URL: https://pytorch.org/",
        "repoID": 5734,
        "URL_gold_label": [
            {
                "URL": "http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DBpedia 2016-10 knowledge graph represented by three files: nodes.txt, relations.txt, and abbrev_cup_dbpedia_filtered.nt, which have been derived from raw data on the DBpedia downloads page: DBpedia 2016-10 http://wiki.dbpedia.org/downloads-2016-10 . Additionally, it contains a directory named  which contains datastructures in binary format as required by the code. If you are interested in applying methods in this repository on your own knowledge graph, you may use the following script to generate the required graph files (.npy): KG generation script https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* \"DBpedia 2016-10 knowledge graph represented by three files: nodes.txt, relations.txt, and abbrev_cup_dbpedia_filtered.nt, which have been derived from raw data on the DBpedia downloads page: DBpedia 2016-10 http://wiki.dbpedia.org/downloads-2016-10\" - label: \"dataset_landing_page\"\n* \"KG generation script https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py\" - label: \"Software\"",
        "repoID": 5753,
        "URL_gold_label": [
            {
                "URL": "https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The executable (named \"gmmreg_demo\") takes a configuration file (in INI format) and a tag string from command line. For more on usage, please check this file https://github.com/bing-jian/gmmreg/blob/master/C%2B%2B/gmmreg_api.cpp . For examples of config INI file, please check this folder https://github.com/bing-jian/gmmreg/tree/master/data .",
        "answer": "re, here are the annotations for the input:\n\n[{'URL': 'https://github.com/bing-jian/gmmreg/blob/master/C++/gmmreg_api.cpp', 'label': 'Software'}]\n\nExplanation:\n\n* The URL \"https://github.com/bing-jian/gmmreg/blob/master/C++/gmmreg_api.cpp\" points to a file containing the source code for a software entity, specifically the \"gmmreg_demo\" executable.\n* The label \"Software\" is assigned because the URL is for a software file or documentation.",
        "repoID": 5772,
        "URL_gold_label": [
            {
                "URL": "https://github.com/bing-jian/gmmreg/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This Python script https://github.com/bing-jian/gmmreg/blob/master/expts/dragon_expts.py  can be used to reproduce the results described in Section 6.1 of Jian&Vemuri PAMI'11 paper https://github.com/bing-jian/gmmreg/blob/master/gmmreg_PAMI_preprint.pdf  using the Stanford dragon_stand dataset http://graphics.stanford.edu/data/3Dscanrep/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/bing-jian/gmmreg/blob/master/expts/dragon_expts.py\nLabel: Software\n\n* URL: https://github.com/bing-jian/gmmreg/blob/master/gmmreg_PAMI_preprint.pdf\nLabel: Other\n\n* URL: http://graphics.stanford.edu/data/3Dscanrep/\nLabel: DatasetLandingPage",
        "repoID": 5772,
        "URL_gold_label": [
            {
                "URL": "http://graphics.stanford.edu/data/3Dscanrep/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This Python script https://github.com/bing-jian/gmmreg/blob/master/expts/lounge_expts.py  can be used to register depth frames in the Stanford lounge dataset http://qianyi.info/scenedata.html . Please see section below for results.",
        "answer": "  [{\"URL\": \"http://qianyi.info/scenedata.html\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"https://github.com/bing-jian/gmmreg/blob/master/expts/lounge_expts.py\", \"label\": \"Software\"}]",
        "repoID": 5772,
        "URL_gold_label": [
            {
                "URL": "http://qianyi.info/scenedata.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools\",\n\"label\": \"Software\"\n}",
        "repoID": 5780,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We experimented on three mainstream action recognition datasets: UCF-101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and Hollywood2 http://www.di.ens.fr/~laptev/actions/hollywood2/ . Videos can be downloaded directly from their websites. After download, please extract the videos from the  archives.",
        "answer": "  Sure, here are the annotated URLs:\n\n* UCF-101: [{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]\n* HMDB51: [{'URL': 'http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/', 'label': 'dataset_landing_page'}]\n* Hollywood2: [{'URL': 'http://www.di.ens.fr/~laptev/actions/hollywood2/', 'label': 'dataset_landing_page'}]",
        "repoID": 5808,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We organized a data challenge about recommender systems with Kyoto University. See the results. https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/",
        "answer": "  Sure, here are the annotated URLs for the given input:\n\n[{'URL': 'https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/', 'label': 'dataset_landing_page'}]",
        "repoID": 5840,
        "URL_gold_label": [
            {
                "URL": "https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For instructions to get annotated sketch dataset, navigate to exp-src/data/sketch-dataset https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset . Pose dataset is present in https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt . Find instruction regarding pose dataset here https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset>\nLabel: DatasetLandingPage\n\n* URL: <https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt>\nLabel: DatasetDirectLink\n\n* URL: <https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists>\nLabel: Other",
        "repoID": 5855,
        "URL_gold_label": [
            {
                "URL": "https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "scvi-tools https://scvi-tools.org/  (single-cell variational inference tools) is a package for probabilistic modeling and analysis of single-cell omics data, built on top of PyTorch https://pytorch.org  and AnnData https://anndata.readthedocs.io/en/latest/ .",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://scvi-tools.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 5879,
        "URL_gold_label": [
            {
                "URL": "https://anndata.readthedocs.io/en/latest/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This modification allows Leviathan to divide work between multiple independant jobs that can be run in parallel. This was be presented http://staffhome.ecm.uwa.edu.au/~00061811/GandALF2017a  at GandALF 2017 http://eptcs.web.cse.unsw.edu.au/paper.cgi?GANDALF2017:10.pdf . The raw benchmark data (705MB) http://staffhome.ecm.uwa.edu.au/~00061811/parallel_benchdata.tar.gz , used in this paper is available online, including the comparison with PolSAT (26MB) http://staffhome.ecm.uwa.edu.au/~00061811/polsat_benchdata.tar.gz . There is some Documentation https://github.com/gmatht/leviathan/blob/master/samples/benchmark_data_DOC.txt  for the benchmark data.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://staffhome.ecm.uwa.edu.au/~00061811/GandALF2017a\nLabel: dataset_landing_page\n* URL: http://eptcs.web.cse.unsw.edu.au/paper.cgi?GANDALF2017:10.pdf\nLabel: dataset_direct_link\n* URL: http://staffhome.ecm.uwa.edu.au/~00061811/parallel_benchdata.tar.gz\nLabel: dataset_",
        "repoID": 5881,
        "URL_gold_label": [
            {
                "URL": "https://github.com/gmatht/leviathan/blob/master/samples/benchmark_data_DOC.txt",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Conditional Random Field http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/  which parses \"123 Main Street New York New York\" into {\"house_number\": 123, \"road\": \"Main Street\", \"city\": \"New York\", \"state\": \"New York\"}. The parser works for a wide variety of countries and languages, not just US/English. The model is trained on over 1 billion addresses and address-like strings, using the templates in the OpenCage address formatting repo https://github.com/OpenCageData/address-formatting  to construct formatted, tagged traning examples for every inhabited country in the world. Many types of normalizations https://github.com/openvenues/libpostal/blob/master/scripts/geodata/addresses/components.py  are performed to make the training data resemble real messy geocoder input as closely as possible.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/>\n\t* Label: DatasetLandingPage",
        "repoID": 5893,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openvenues/libpostal/blob/master/scripts/geodata/addresses/components.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Libpostal is a bit different because it's trained on open data that's available to everyone, so we've released the entire training pipeline (the geodata https://github.com/openvenues/libpostal/tree/master/scripts/geodata  package in this repo), as well as the resulting training data itself on the Internet Archive. It's over 100GB unzipped.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://github.com/openvenues/libpostal/tree/master/scripts/geodata\",\n            \"label\": \"dataset_direct_link\"\n        }\n    ]",
        "repoID": 5893,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openvenues/libpostal/tree/master/scripts/geodata",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The geodata https://github.com/openvenues/libpostal/tree/master/scripts/geodata  Python package in the libpostal repo contains the pipeline for preprocessing the various geo data sets and building training data for the C models to use. This package shouldn't be needed for most users, but for those interested in generating new types of addresses or improving libpostal's training data, this is where to look.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://github.com/openvenues/libpostal/tree/master/scripts/geodata\",\n            \"label\": \"dataset_landing_page\"\n        }\n    ]",
        "repoID": 5893,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openvenues/libpostal/tree/master/scripts/geodata",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We provide demo attack data and classifier on Dropbox https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0  and \u767e\u5ea6\u7f51\u76d8 https://pan.baidu.com/s/1gfpcB5p  (\u5bc6\u7801: yzt4). Please download and put the unzipped files in . You may also use your own data for test.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0\nLabel: dataset_direct_link\n* URL: https://pan.baidu.com/s/1gfpcB5p (password: yzt4)\nLabel: dataset_direct_link",
        "repoID": 5898,
        "URL_gold_label": [
            {
                "URL": "https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The data is hosted on Dataverse https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT  and Google Drive https://drive.google.com/drive/folders/1wZwDIR18IHPPTiH1C0dyBbGPR-3MktI7?usp=sharing .",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT\",\n            \"label\": \"dataset_landing_page\"\n        },\n        {\n            \"URL\": \"https://drive.google.com/drive/folders/1wZwDIR18IHPPTiH1C0dyBbGPR-3MktI7?usp=sharing\",\n            \"label\": \"dataset_direct_link\"\n        }\n    ]",
        "repoID": 5899,
        "URL_gold_label": [
            {
                "URL": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To use the KITTI odometry benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to train DPC-Net, you can use the scripts  and  as starting points. If you use our framework, you'll need to save your estimator's poses in a  object.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_odometry.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 5916,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Then, manually download download MultiNLI 0.9 matched https://www.kaggle.com/c/multinli-matched-open-evaluation/data  and mismatched https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data  test set under data/multinli_0.9 folder",
        "answer": "  [{\"URL\": \"https://www.kaggle.com/c/multinli-matched-open-evaluation/data\", \"label\": \"dataset_direct_link\"}, {\"URL\": \"https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 5954,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/multinli-matched-open-evaluation/data",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "MIR datasets https://www.audiocontentanalysis.org/data-sets/ : An awesome list of MIR datasets",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.audiocontentanalysis.org/data-sets/', 'label': 'dataset_landing_page'}]",
        "repoID": 5956,
        "URL_gold_label": [
            {
                "URL": "https://www.audiocontentanalysis.org/data-sets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MagnaTagATune http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset : (29s, 188 tags, 25,880 mp3) for tagging and triplet similarity",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 5956,
        "URL_gold_label": [
            {
                "URL": "http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Our best model has an accuracy greater than 99.0% with mean squared error of ~6.00 watts. For correlation analysis, we applied a metric called Maximal Information Coefficient (MIC) http://exploredata.net  that has the equitability and generality properties.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://exploredata.net\nLabel: Other",
        "repoID": 5988,
        "URL_gold_label": [
            {
                "URL": "http://exploredata.net",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ", such as those of The Event Camera Dataset and Simulator http://rpg.ifi.uzh.ch/davis_data.html . :",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n[{'URL': 'http://rpg.ifi.uzh.ch/davis_data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 6028,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/davis_data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download a squence of the dataset, such as slider_depth.bag http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag",
        "answer": "  Sure! Here's the annotation for the input:\n\n{\n\"URL\": \"http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 6028,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "As introduced in our paper, our model is trained by our newly-established eye-tracking database, LEDOV https://github.com/remega/LEDOV-eye-tracking-database , which is also available at Dropbox https://www.dropbox.com/s/pc8symd9i3cky1q/LEDOV.zip?dl=0  and BaiduYun http://pan.baidu.com/s/1pLmfjCZ",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/remega/LEDOV-eye-tracking-database>\nLabel: DatasetDirectLink\n* URL: <https://www.dropbox.com/s/pc8symd9i3cky1q/LEDOV.zip?dl=0>\nLabel: DatasetDirectLink\n* URL: <http://pan.baidu.com/s/1pLmfjCZ>\nLabel: Other",
        "repoID": 6029,
        "URL_gold_label": [
            {
                "URL": "https://github.com/remega/LEDOV-eye-tracking-database",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://travis-ci.org/Wikidata-lib/PropertySuggester https://coveralls.io/r/Wikidata-lib/PropertySuggester?branch=master",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://travis-ci.org/Wikidata-lib/PropertySuggester', 'label': 'Software'}]",
        "repoID": 6046,
        "URL_gold_label": [
            {
                "URL": "https://coveralls.io/r/Wikidata-lib/PropertySuggester?branch=master",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This extension adds a new table \"wbs_propertypairs\" that contains the information that is needed to generate suggestions. You can use PropertySuggester-Python https://github.com/Wikidata-lib/PropertySuggester-Python  to generate this data from a wikidata dump.",
        "answer": "  [{\"URL\": \"https://github.com/Wikidata-lib/PropertySuggester-Python\", \"label\": \"Software\"}]",
        "repoID": 6046,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Wikidata-lib/PropertySuggester-Python",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download pretrained weights ( Google Drive https://drive.google.com/file/d/0Bx3H_TbKFPCjNlMtSGJlQ0dxVzQ/view?usp=sharing , One Drive https://studenthcmusedu-my.sharepoint.com/:u:/g/personal/nqanh_mso_hcmus_edu_vn/ETD6q64-L1lCgtNEryA42NwBNM9vNoyE8QyxAYzgt8NqnA?e=uRCxPg ). This weight is trained on the training set of the IIT-AFF dataset https://sites.google.com/site/iitaffdataset/ :",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: <https://drive.google.com/file/d/0Bx3H_TbKFPCjNlMtSGJlQ0dxVzQ/view?usp=sharing>\nLabel: dataset_direct_link\n* URL: <https://studenthcmusedu-my.sharepoint.com/:u:/g/personal/nqanh_mso_hcmus_edu_vn/ETD6q64-L1lCgtNEryA42NwBNM9vNoyE8QyxAYzgt8NqnA?e=uRCxPg>\nLabel: dataset_direct_link\n* URL: <https://sites.google.com/site/iitaffdataset/>\nLabel: dataset_landing_page",
        "repoID": 6065,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/site/iitaffdataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you use IIT-AFF dataset https://sites.google.com/site/iitaffdataset/ , please consider citing:",
        "answer": "  [{\"URL\": \"https://sites.google.com/site/iitaffdataset/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6065,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/site/iitaffdataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We train AffordanceNet on IIT-AFF dataset https://sites.google.com/site/iitaffdataset/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://sites.google.com/site/iitaffdataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 6065,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/site/iitaffdataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the preprocessed NYU Depth V2 http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html  and/or KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  datasets in HDF5 formats and place them under the  folder. The downloading process might take an hour or so. The NYU dataset requires 32G of storage space, and KITTI requires 81G.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\nLabel: dataset_direct_link\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page",
        "repoID": 6073,
        "URL_gold_label": [
            {
                "URL": "http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Probably the easiest way to build RHIPE is to provision a Vagrant https://github.com/tesseradata/install-vagrant  machine that has all the prerequisites configured. Another option is to set up a local pseudo-distributed Hadoop cluster, for example see here https://github.com/hafen/RHIPE/blob/master/cdh5-on-mac.md .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/tesseradata/install-vagrant>\nLabel: Software\n* URL: <https://github.com/hafen/RHIPE/blob/master/cdh5-on-mac.md>\nLabel: Other",
        "repoID": 6075,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tesseradata/install-vagrant",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The Macroscopic Internet Topology Data Kit https://www.caida.org/data/internet-topology-data-kit/  from Caida including measured real world internet topologies",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'https://www.caida.org/data/internet-topology-data-kit/', 'label': 'dataset_landing_page'}]",
        "repoID": 6079,
        "URL_gold_label": [
            {
                "URL": "https://www.caida.org/data/internet-topology-data-kit/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you want to find a language data set to run Tesseract, then look at our tessdata repository https://github.com/tesseract-ocr/tessdata  instead.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/tesseract-ocr/tessdata', 'label': 'dataset_landing_page'}]",
        "repoID": 6091,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tesseract-ocr/tessdata",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster",
        "answer": "Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/datastax/spark-cassandra-connector/actions?query=branch:master\",\n\"label\": \"Software\"\n}",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Create a SPARKC JIRA https://datastax-oss.atlassian.net/projects/SPARKC/issues",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://datastax-oss.atlassian.net/projects/SPARKC/issues', 'label': 'Other'}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax-oss.atlassian.net/projects/SPARKC/issues",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Currently, the following branches are actively supported: 3.3.x ( master https://github.com/datastax/spark-cassandra-connector/tree/master ), 3.2.x ( b3.2 https://github.com/datastax/spark-cassandra-connector/tree/b3.2 ), 3.1.x ( b3.1 https://github.com/datastax/spark-cassandra-connector/tree/b3.1 ), 3.0.x ( b3.0 https://github.com/datastax/spark-cassandra-connector/tree/b3.0 ) and 2.5.x ( b2.5 https://github.com/datastax/spark-cassandra-connector/tree/b2.5 ).",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* `master`: https://github.com/datastax/spark-cassandra-connector/tree/master (label: `dataset_landing_page`)\n* `b3.2`: https://github.com/datastax/spark-cassandra-connector/tree/b3.2 (label: `dataset_landing_page`)\n* `b3.1`: https://github.com/datastax/spark-cassandra-connector/tree/b3.1 (label: `dataset_landing_page`)\n* `b3.0`: https://github.com/datastax/spark-cassandra-connector/tree/b3.0 (label: `dataset_landing_page`)\n* `b2.5`: https://github.com/datastax/spark",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/tree/b3.0",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/tree/b2.5",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/tree/master",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/tree/b3.1",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/datastax/spark-cassandra-connector/tree/b3.2",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Embedded-Cassandra http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded', 'label': 'dataset_landing_page'}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "New issues may be reported using JIRA https://datastax-oss.atlassian.net/browse/SPARKC/ . Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal.",
        "answer": "  [{\"URL\": \"https://datastax-oss.atlassian.net/browse/SPARKC/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax-oss.atlassian.net/browse/SPARKC/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Questions and requests for help may be submitted to the user mailing list https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user .",
        "answer": "  [{\"URL\": \"https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user\", \"label\": \"Other\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/",
        "answer": "  [{\"URL\": \"http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/\", \"label\": \"Software\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package",
        "answer": "  [{\"URL\": \"https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html",
        "answer": "  [{\"URL\": \"https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html\", \"label\": \"Software\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html",
        "answer": "  [{\"URL\": \"https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html\", \"label\": \"Software\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html",
        "answer": "  [{\"URL\": \"https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html\", \"label\": \"Software\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Spark-Cassandra-Connector https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html",
        "answer": "  [{\"URL\": \"https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html\", \"label\": \"Software\"}]",
        "repoID": 6113,
        "URL_gold_label": [
            {
                "URL": "https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Collection data https://github.com/rasvaan/accurator/wiki/2.-Collection-data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/rasvaan/accurator/wiki/2.-Collection-data', 'label': 'dataset_landing_page'}]",
        "repoID": 6122,
        "URL_gold_label": [
            {
                "URL": "https://github.com/rasvaan/accurator/wiki/2.-Collection-data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Collection data https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data', 'label': 'dataset_landing_page'}]",
        "repoID": 6123,
        "URL_gold_label": [
            {
                "URL": "https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "GTFS-PLUS https://github.com/osplanning-data-standards/GTFS-PLUS  - GTFS-based data transit network data standard suitable for dynamic transit modeling",
        "answer": "  [{\"URL\": \"https://github.com/osplanning-data-standards/GTFS-PLUS\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6129,
        "URL_gold_label": [
            {
                "URL": "https://github.com/osplanning-data-standards/GTFS-PLUS",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "General Modeling Network Specification https://github.com/zephyr-data-specs/GMNS  - GMNS defines a common human and machine readable format for sharing routable road network files. It is designed to be used in multi-modal static and dynamic transportation planning and operations models.",
        "answer": "  [{\"URL\": \"https://github.com/zephyr-data-specs/GMNS\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6129,
        "URL_gold_label": [
            {
                "URL": "https://github.com/zephyr-data-specs/GMNS",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "M. Rudolph, F. Ruiz, S. Athey, D. Blei, https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data",
        "answer": "  [{\"URL\": \"https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6143,
        "URL_gold_label": [
            {
                "URL": "https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Authors: Markus Kroetzsch http://korrekt.org , Julian Mendez https://julianmendez.github.io/ , Fredo Erxleben https://github.com/fer-rum , Michael Guenther https://github.com/guenthermi , Markus Damm https://github.com/mardam , Antonin Delpeuch http://antonin.delpeuch.eu/ , Thomas Pellissier Tanon https://thomas.pellissier-tanon.fr/  and other contributors https://github.com/Wikidata/Wikidata-Toolkit/graphs/contributors",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://korrekt.org\nLabel: Software\n* URL: https://julianmendez.github.io/\nLabel: Software\n* URL: https://github.com/fer-rum\nLabel: Software\n* URL: https://github.com/guenthermi\nLabel: Software\n* URL: https://github.com/mardam\nLabel: Software\n* URL: http://antonin.delpeuch.eu/\nLabel: Other\n* URL: https://thomas.pellissier-tanon.fr/\nLabel: Other",
        "repoID": 6165,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Wikidata/Wikidata-Toolkit/graphs/contributors",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The development of Wikidata Toolkit has been partially funded by the Wikimedia Foundation under the Wikibase Toolkit Individual Engagement Grant https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit , and by the German Research Foundation (DFG) under Emmy Noether grant KR 4381/1-1 \"DIAMOND\" https://ddll.inf.tu-dresden.de/web/DIAMOND/en .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit>\nLabel: <dataset_direct_link>\n* URL: <https://ddll.inf.tu-dresden.de/web/DIAMOND/en>\nLabel: <dataset_landing_page>",
        "repoID": 6165,
        "URL_gold_label": [
            {
                "URL": "https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Wikidata Toolkit Javadocs http://wikidata.github.io/Wikidata-Toolkit/ : API documentation",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://wikidata.github.io/Wikidata-Toolkit/', 'label': 'dataset_landing_page'}]",
        "repoID": 6165,
        "URL_gold_label": [
            {
                "URL": "http://wikidata.github.io/Wikidata-Toolkit/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Wikidata Toolkit examples https://github.com/Wikidata/Wikidata-Toolkit-Examples : stand-alone Java project that shows how to use Wikidata Toolkit as a library for your own code.",
        "answer": "  [{\"URL\": \"https://github.com/Wikidata/Wikidata-Toolkit-Examples\", \"label\": \"Software\"}]",
        "repoID": 6165,
        "URL_gold_label": [
            {
                "URL": "https://github.com/Wikidata/Wikidata-Toolkit-Examples",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Wikidata Toolkit homepage https://www.mediawiki.org/wiki/Wikidata_Toolkit : project homepage with basic user documentation, including guidelines on how to setup your Java IDE for using Maven and git.",
        "answer": "  [{\"URL\": \"https://www.mediawiki.org/wiki/Wikidata_Toolkit\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6165,
        "URL_gold_label": [
            {
                "URL": "https://www.mediawiki.org/wiki/Wikidata_Toolkit",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Available here https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design', 'label': 'dataset_landing_page'}]",
        "repoID": 6185,
        "URL_gold_label": [
            {
                "URL": "https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For uploading your datasets, you can follow the instructions explained here https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets .",
        "answer": "  [{'URL': 'https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 6186,
        "URL_gold_label": [
            {
                "URL": "https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "MSCOCO http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://cocodataset.org/#download\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 6212,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Can parse cueing data https://www.webmproject.org/docs/container/#cueing-data  elements for DASH's SegmentBase@indexRange and SegmentTemplate@index",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.webmproject.org/docs/container/#cueing-data', 'label': 'dataset_landing_page'}]",
        "repoID": 6230,
        "URL_gold_label": [
            {
                "URL": "https://www.webmproject.org/docs/container/#cueing-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The code in this repository is compatible with Python 2 and Python 3. Its only other external dependency is NLTK http://www.nltk.org/install.html , with the data installed http://www.nltk.org/data.html  so that WordNet is available.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.nltk.org/install.html\nLabel: Software\n\n* URL: http://www.nltk.org/data.html\nLabel: Other",
        "repoID": 6264,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Our annotations https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt  and categories https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt  are available in the  directory of this repository.",
        "answer": "  [{\"URL\": \"https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6287,
        "URL_gold_label": [
            {
                "URL": "https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Datasets https://github.com/haozhenWu/lightchem/tree/master/datasets",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/haozhenWu/lightchem/tree/master/datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 6315,
        "URL_gold_label": [
            {
                "URL": "https://github.com/haozhenWu/lightchem/tree/master/datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  version = 0.18.1",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 6315,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To save some time, we've also published the data set generated by this code in a separate (very large!) repository, which can be found here https://github.com/vs-uulm/vnc2017-CACC-data .",
        "answer": "  [{\"URL\": \"https://github.com/vs-uulm/vnc2017-CACC-data\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 6322,
        "URL_gold_label": [
            {
                "URL": "https://github.com/vs-uulm/vnc2017-CACC-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We use a subset of ImageNet validation set containing 1000 images, most of which are correctly classified by those models. Our dataset can be downloaded at http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip . You can alternatively use the NIPS 2017 competition official dataset https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip\nLabel: dataset_direct_link\n\n* URL: https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset\nLabel: dataset_landing_page",
        "repoID": 6325,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset",
                "gold_label": "software"
            },
            {
                "URL": "http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools\",\n\"label\": \"Software\"\n}",
        "repoID": 6336,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download Caltech Lanes Dataset http://www.mohamedaly.info/datasets/caltech-lanes .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.mohamedaly.info/datasets/caltech-lanes', 'label': 'dataset_direct_link'}]",
        "repoID": 6338,
        "URL_gold_label": [
            {
                "URL": "http://www.mohamedaly.info/datasets/caltech-lanes",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the ADE20k http://groups.csail.mit.edu/vision/datasets/ADE20K/  dataset and put it in .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://groups.csail.mit.edu/vision/datasets/ADE20K/', 'label': 'dataset_direct_link'}]",
        "repoID": 6346,
        "URL_gold_label": [
            {
                "URL": "http://groups.csail.mit.edu/vision/datasets/ADE20K/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Honk is a PyTorch reimplementation of Google's TensorFlow convolutional neural networks for keyword spotting, which accompanies the recent release of their Speech Commands Dataset https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html . For more details, please consult our writeup:",
        "answer": "  [{\"URL\": \"https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6347,
        "URL_gold_label": [
            {
                "URL": "https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Codes and files are available under \"skoda\" folder: RAE_on_Skoda_dataset.ipynb https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb', 'label': 'dataset_landing_page'}]",
        "repoID": 6348,
        "URL_gold_label": [
            {
                "URL": "https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Here are examples of how to run the code. (To run 1. and 2., you need MSD audio files and its related metadata from msd-artist-split https://github.com/jiyoungpark527/msd-artist-split , MSD_split https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split )",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/jiyoungpark527/msd-artist-split', 'label': 'dataset_landing_page'},\n {'URL': 'https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split', 'label': 'dataset_landing_page'}]",
        "repoID": 6350,
        "URL_gold_label": [
            {
                "URL": "https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The aim of the SLING project is to learn to read and understand Wikipedia articles in many languages for the purpose of knowledge base completion, e.g. adding facts mentioned in Wikipedia (and other sources) to the Wikidata https://www.wikidata.org  knowledge base. We use frame semantics doc/guide/frames.md  as a common representation for both knowledge representation and document annotation. The SLING parser can be trained to produce frame semantic representations of text directly without any explicit intervening linguistic representation.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://www.wikidata.org\nLabel: DatasetDirectLink",
        "repoID": 6356,
        "URL_gold_label": [
            {
                "URL": "https://www.wikidata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": " http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration",
        "answer": "  [{'URL': 'http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration', 'label': 'dataset_landing_page'}]",
        "repoID": 6362,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In this example we create a cblox map using the lidar data and \"ground-truth\" pose estimates from the kitti dataset http://www.cvlibs.net/datasets/kitti/ . This simple example demonstrates the  and  of submaps using c-blox - because we use drift-free pose estimates (rather than a SLAM system), no submap correction is required/used.",
        "answer": "  Sure! Here's the annotated input:\n\nInput: In this example, we create a cblox map using the lidar data and \"ground-truth\" pose estimates from the kitti dataset <http://www.cvlibs.net/datasets/kitti/>. This simple example demonstrates the and of submaps using c-blox - because we use drift-free pose estimates (rather than a SLAM system), no submap correction is required/used.\n\nOutput:\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6365,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To run the example download a kitti raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . To produce the map above, we ran the \"2011_09_30_drive_0018\" dataset under the catagory \"residential\". Convert the data to a rosbag using kitti_to_rosbag https://github.com/ethz-asl/kitti_to_rosbag .",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* URL: http://www.cvlibs.net/datasets/kitti/raw_data.php\nLabel: dataset_direct_link\n* URL: https://github.com/ethz-asl/kitti_to_rosbag\nLabel: Software",
        "repoID": 6365,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Code to generate the FigureQA dataset. The dataset is available for download here https://www.microsoft.com/en-us/research/project/figureqa-dataset/ .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://www.microsoft.com/en-us/research/project/figureqa-dataset/', 'label': 'dataset_direct_link'}]",
        "repoID": 6367,
        "URL_gold_label": [
            {
                "URL": "https://www.microsoft.com/en-us/research/project/figureqa-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We can see that the matrix has a block structure: the first larger block on the diagonal are the similarities between MTS of class 1, the second smaller block is relative to the elements of class 2. Results are saved in /Data/TCK_data.mat https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat  and they are used in the next section to train the dkAE.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat\nLabel: dataset_direct_link",
        "repoID": 6378,
        "URL_gold_label": [
            {
                "URL": "https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Multi-label Image Annotation(COCO) Please download COCO http://cocodataset.org/#home  dataset first. Specifically, 2014 train, val and test splits are used in the paper. To prepare a well trained multi-label model for feedback-prop, you can either train a new model by running train.py https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel/train.py  in coco_multilabel https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel  folder or directly download the pretrained model http://www.cs.virginia.edu/~tw8cb/files/model_best.pth.tar . We recommend to train a multi-label model by first fixing all CNN layers for a few epochs and then finetuning the model end-to-end. To apply feedback-prop, please go through COCO-Feedback-prop.ipynb https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel/COCO-Feedback-prop.ipynb .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#home\nLabel: dataset_landing_page\n* URL: https://github.com/uvavision/feedbackprop/blob/master/coco_multilabel/train.py\nLabel: software\n* URL: http://www.cs.virginia.edu/~tw8cb/files/model_best.pth.tar\nLabel: dataset_direct_link\n* URL: https://github.com/uvavision/feedbackprop/blob/master/c",
        "repoID": 6391,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Components and Templates https://angular.io/guide/displaying-data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://angular.io/guide/displaying-data', 'label': 'dataset_landing_page'}]",
        "repoID": 6401,
        "URL_gold_label": [
            {
                "URL": "https://angular.io/guide/displaying-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Download the surface forms file which is available at https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 6409,
        "URL_gold_label": [
            {
                "URL": "https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "dataGeneration.m https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m : Generating data for numerical simulations",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m', 'label': 'Software'}]",
        "repoID": 6414,
        "URL_gold_label": [
            {
                "URL": "https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The notebooks use https://pandas.pydata.org/  for data analysis. We used v0.20.3 but anything above and some below should do as well. For plotting,  and  were used. Finally, you need  in version  or above.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://pandas.pydata.org/\nLabel: Software\n* URL: https://pandas.pydata.org/ (again)\nLabel: Software",
        "repoID": 6424,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "(See VOWL http://vowl.visualdataweb.org/  for good online tools for visualizing OWL and data from endpoints as node-edge diagrams, but no textual visualization.)",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://vowl.visualdataweb.org/', 'label': 'Other'}]",
        "repoID": 6433,
        "URL_gold_label": [
            {
                "URL": "http://vowl.visualdataweb.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "WebVOWL http://visualdataweb.de/webvowl/#  for visualizing OWL ontologies online. It is very good. The VOWL http://vowl.visualdataweb.org/  project includes online querying an endpoint for visualizing the structure of data and other tools.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://visualdataweb.de/webvowl/#', 'label': 'dataset_landing_page'},\n {'URL': 'http://vowl.visualdataweb.org/', 'label': 'dataset_direct_link'}]",
        "repoID": 6433,
        "URL_gold_label": [
            {
                "URL": "http://vowl.visualdataweb.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "What are the most useful analysis and visualizations for biological and medical data? MOOC https://www.coursera.org/specializations/bioinformatics , MOOC specialization https://www.coursera.org/learn/bioinformatics-pku/ . This other MOOC specialization https://www.coursera.org/specializations/genomic-data-science  has a Python for BioInformatics course. A MOOC for big data and bioinformatics https://www.coursera.org/learn/data-genes-medicine .",
        "answer": "  Here are the annotated URLs:\n\n* [{'URL': 'https://www.coursera.org/specializations/bioinformatics', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://www.coursera.org/learn/bioinformatics-pku/', 'label': 'dataset_direct_link'}]\n* [{'URL': 'https://www.coursera.org/specializations/genomic-data-science', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://www.coursera.org/learn/data-genes-medicine', 'label': 'other'}]",
        "repoID": 6433,
        "URL_gold_label": [
            {
                "URL": "https://www.coursera.org/specializations/genomic-data-science",
                "gold_label": "other"
            },
            {
                "URL": "https://www.coursera.org/learn/data-genes-medicine",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "bokeh http://bokeh.pydata.org  for training visualization",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://bokeh.pydata.org', 'label': 'Software'}]",
        "repoID": 6444,
        "URL_gold_label": [
            {
                "URL": "http://bokeh.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for logging to csv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 6444,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "CIFAR100: Download https://www.cs.toronto.edu/~kriz/cifar.html  CIFAR10: Download https://www.cs.toronto.edu/~kriz/cifar.html  MNIST: Download http://yann.lecun.com/exdb/mnist/  LCSTS: Download http://icrc.hitsz.edu.cn/Article/show/139.html  IWSLT2015: Download https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.cs.toronto.edu/~kriz/cifar.html', 'label': 'dataset_landing_page'},\n {'URL': 'https://www.cs.toronto.edu/~kriz/cifar.html', 'label': 'dataset_direct_link'},\n {'URL': 'http://yann.lecun.com/exdb/mnist/', 'label': 'dataset_landing_page'},\n {'URL': 'http://icrc.hitsz.edu.cn/Article/show/139.html', 'label': 'dataset_landing_page'},\n {'URL': 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/', 'label': 'dataset_direct_link'}]",
        "repoID": 6447,
        "URL_gold_label": [
            {
                "URL": "https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Download audio data and tag annotations from here http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset . Then you should see 3  files and 1  file:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset\nLabel: dataset_direct_link",
        "repoID": 6450,
        "URL_gold_label": [
            {
                "URL": "http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 6450,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Alon (1999) https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Alon-(1999)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Borovecki (2005) https://github.com/ramey/datamicroarray/wiki/Borovecki-%282005%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Borovecki-(2005)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Borovecki-%282005%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Burczynski (2006) https://github.com/ramey/datamicroarray/wiki/Burczynski-%282006%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Burczynski-(2006)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Burczynski-%282006%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Chiaretti (2004) https://github.com/ramey/datamicroarray/wiki/Chiaretti-%282004%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Chiaretti-(2004)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Chiaretti-%282004%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Chin (2006) https://github.com/ramey/datamicroarray/wiki/Chin-%282006%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Chin-(2006)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Chin-%282006%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Chowdary (2006) https://github.com/ramey/datamicroarray/wiki/Chowdary-%282006%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Chowdary-(2006)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Chowdary-%282006%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Christensen (2009) https://github.com/ramey/datamicroarray/wiki/Christensen-%282009%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Christensen-(2009)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Christensen-%282009%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Golub (1999) https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Golub-(1999)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Gordon (2002) https://github.com/ramey/datamicroarray/wiki/Gordon-%282002%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Gordon-(2002)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Gordon-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Gravier (2010) https://github.com/ramey/datamicroarray/wiki/Gravier-%282010%29",
        "answer": "re! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Gravier-(2010)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Gravier-%282010%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Here is a summary for the Alon et al. (1999) Colon Cancer data set https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29 .",
        "answer": "re! Here are the annotations for the URLs in the input:\n\n* URL: https://github.com/ramey/datamicroarray/wiki/Alon-(1999)\nLabel: dataset_landing_page",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Khan (2001) https://github.com/ramey/datamicroarray/wiki/Khan-%282001%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Khan-(2001)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Khan-%282001%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Nakayama (2007) https://github.com/ramey/datamicroarray/wiki/Nakayama-%282007%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Nakayama-(2007)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Nakayama-%282007%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Once you have installed and loaded the  package, you can load a data set with the  command. For example, to load the well-known Alon et al. (1999) Colon Cancer data set https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29 , type the following at the R console:",
        "answer": "'URL': 'https://github.com/ramey/datamicroarray/wiki/Alon-(1999)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pomeroy (2002) https://github.com/ramey/datamicroarray/wiki/Pomeroy-%282002%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Pomeroy-(2002)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Pomeroy-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Shipp (2002) https://github.com/ramey/datamicroarray/wiki/Shipp-%282002%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Shipp-(2002)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Shipp-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Singh (2002) https://github.com/ramey/datamicroarray/wiki/Singh-%282002%29",
        "answer": "'URL': 'https://github.com/ramey/datamicroarray/wiki/Singh-(2002)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Singh-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Sorlie (2001) https://github.com/ramey/datamicroarray/wiki/Sorlie-%282001%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Sorlie-(2001)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Sorlie-%282001%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Su (2002) https://github.com/ramey/datamicroarray/wiki/Su-%282002%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Su-(2002)\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Su-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Subramanian (2005) https://github.com/ramey/datamicroarray/wiki/Subramanian-%282005%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Subramanian-(2005)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Subramanian-%282005%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Sun (2006) https://github.com/ramey/datamicroarray/wiki/Sun-%282006%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/Sun-\uff082006\uff09\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Sun-%282006%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Tian (2003) https://github.com/ramey/datamicroarray/wiki/Tian-%282003%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Tian-(2003)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Tian-%282003%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "West (2001) https://github.com/ramey/datamicroarray/wiki/West-%282001%29",
        "answer": "re! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/ramey/datamicroarray/wiki/West-\uff082001\uff09\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/West-%282001%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Yeoh (2002) https://github.com/ramey/datamicroarray/wiki/Yeoh-%282002%29",
        "answer": "re! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/ramey/datamicroarray/wiki/Yeoh-(2002)', 'label': 'dataset_landing_page'}]",
        "repoID": 6458,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ramey/datamicroarray/wiki/Yeoh-%282002%29",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "End-to-end training with on-the-fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://opennmt.net/OpenNMT-py/FAQ.html\",\n            \"label\": \"dataset_landing_page\"\n        }\n    ]",
        "repoID": 6471,
        "URL_gold_label": [
            {
                "URL": "[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "We use the answer sentence selection dataset https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data  from TREC QA as our source of indirect supervision. We ran Stanford NER to extract entity mentions on both question and answer sentences and process the dataset into JSON format containing QA-pairs. Details of how we construct QA-pairs can be found in our paper.",
        "answer": "  [{\"URL\": \"https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6475,
        "URL_gold_label": [
            {
                "URL": "https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ConcatDataset from pytorch/tnt https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py .",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py', 'label': 'dataset_landing_page'}]",
        "repoID": 6488,
        "URL_gold_label": [
            {
                "URL": "https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "https://rose1.ntu.edu.sg/dataset/actionRecognition/ https://rose1.ntu.edu.sg/dataset/actionRecognition/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://rose1.ntu.edu.sg/dataset/actionRecognition/', 'label': 'dataset_landing_page'}]",
        "repoID": 6503,
        "URL_gold_label": [
            {
                "URL": "https://rose1.ntu.edu.sg/dataset/actionRecognition/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "More info on the contents of the proj-data package can be found at the PROJ-data GitHub repository https://github.com/OSGeo/PROJ-data .",
        "answer": "  [{\"URL\": \"https://github.com/OSGeo/PROJ-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6504,
        "URL_gold_label": [
            {
                "URL": "https://github.com/OSGeo/PROJ-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Please, help us to improve this code, by submitting an issue https://github.com/learningtitans/data-depth-design/issues  if you find any problems.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/learningtitans/data-depth-design/issues', 'label': 'Other'}]",
        "repoID": 6516,
        "URL_gold_label": [
            {
                "URL": "https://github.com/learningtitans/data-depth-design/issues",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The PH2 Dataset http://www.fc.up.pt/addi/ph2%20database.html  has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.",
        "answer": "Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.fc.up.pt/addi/ph2 database.html', 'label': 'dataset_landing_page'}]",
        "repoID": 6516,
        "URL_gold_label": [
            {
                "URL": "http://www.fc.up.pt/addi/ph2%20database.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download data https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip', 'label': 'dataset_direct_link'}]",
        "repoID": 6546,
        "URL_gold_label": [
            {
                "URL": "https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Open Data Commons Attribution License (ODC-By) v1.0 http://opendatacommons.org/licenses/by/1.0/",
        "answer": "  [{\"URL\": \"http://opendatacommons.org/licenses/by/1.0/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6551,
        "URL_gold_label": [
            {
                "URL": "http://opendatacommons.org/licenses/by/1.0/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py ). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb', 'label': 'dataset_landing_page'}]\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py', 'label': 'Software'}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb ). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb', 'label': 'dataset_landing_page'}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb ) ( code https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py ). Introducing TensorFlow Dataset API for optimizing the input data pipeline.",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n* URL: <https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb>\nLabel: dataset_landing_page",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "( notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb ). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb', 'label': 'dataset_landing_page'}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Introduction to MNIST Dataset https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb', 'label': 'dataset_landing_page'}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Introduction to MNIST Dataset https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb', 'label': 'dataset_landing_page'}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Some examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples. MNIST is a database of handwritten digits, for a quick description of that dataset, you can check this notebook https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb .",
        "answer": "  [{\"URL\": \"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6568,
        "URL_gold_label": [
            {
                "URL": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The present document describes how Apache Mesos is used by the INDIGO-DataCloud https://www.indigo-datacloud.eu/  PaaS layer.  INDIGO-DataCloud (start date: 01/04/2015, end date: 30/09/2017) is a project funded under the Horizon2020 framework program of the European Union and led by the National Institute for Nuclear Physics (INFN). It developed a data and computing platform targeting scientific communities, deployable on multiple hardware and provisioned over hybrid (private or public) e-infrastructures. The INDIGO solutions are being evolved in the context of other European projects like DEEP Hybrid-DataCloud https://deep-hybrid-datacloud.eu , eXtreme-DataCloud http://www.extreme-datacloud.eu/  and EOSC-Hub https://www.eosc-hub.eu/",
        "answer": "  Here are the annotated URLs in the input:\n\n* URL: https://www.indigo-datacloud.eu/\nLabel: DatasetLandingPage\n* URL: https://deep-hybrid-datacloud.eu/\nLabel: Other\n* URL: http://www.extreme-datacloud.eu/\nLabel: Other\n* URL: https://www.eosc-hub.eu/\nLabel: Other",
        "repoID": 6572,
        "URL_gold_label": [
            {
                "URL": "http://www.extreme-datacloud.eu/",
                "gold_label": "other"
            },
            {
                "URL": "https://deep-hybrid-datacloud.eu",
                "gold_label": "other"
            },
            {
                "URL": "https://www.indigo-datacloud.eu/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": " https://travis-ci.org/indigo-dc/orchestrator https://codecov.io/gh/indigo-dc/orchestrator https://sonarcloud.io/dashboard?id=it.reply%3Aorchestrator https://snyk.io/test/github/indigo-dc/orchestrator https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/",
        "answer": "Sure, here are the annotated URLs:\n\n* https://travis-ci.org/indigo-dc/orchestrator: Label = \"Software\"\n* https://codecov.io/gh/indigo-dc/orchestrator: Label = \"Software\"\n* https://sonarcloud.io/dashboard?id=it.reply:orchestrator: Label = \"Software\"\n* https://snyk.io/test/github/indigo-dc/orchestrator: Label = \"Software\"\n* https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/: Label = \"Other\"",
        "repoID": 6573,
        "URL_gold_label": [
            {
                "URL": "https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DEEP-HybridDataCloud project https://deep-hybrid-datacloud.eu/  (Horizon 2020) under Grant number 777435.",
        "answer": "  [{\"URL\": \"https://deep-hybrid-datacloud.eu/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6573,
        "URL_gold_label": [
            {
                "URL": "https://deep-hybrid-datacloud.eu/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "INDIGO-DataCloud project https://www.indigo-datacloud.eu/  (Horizon 2020) under Grant number 653549.",
        "answer": "  [{\"URL\": \"https://www.indigo-datacloud.eu/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6573,
        "URL_gold_label": [
            {
                "URL": "https://www.indigo-datacloud.eu/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "eXtreme-DataCloud project http://www.extreme-datacloud.eu/  (Horizon 2020) under Grant number 777367.",
        "answer": "  [{\"URL\": \"http://www.extreme-datacloud.eu/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6573,
        "URL_gold_label": [
            {
                "URL": "http://www.extreme-datacloud.eu/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": " http://jenkins.i3m.upv.es/job/indigo/job/im-unit/ https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/ LICENSE https://imdocs.readthedocs.io/en/latest/",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://jenkins.i3m.upv.es/job/indigo/job/im-unit/\nLabel: dataset_landing_page\n* URL: https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/\nLabel: dataset_direct_link\n* URL: LICENSE\nLabel: other",
        "repoID": 6581,
        "URL_gold_label": [
            {
                "URL": "https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "In order to try deploying Onedata, or specific components we have prepared a set of example configurations and scenarios https://github.com/onedata/getting-started .",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'https://github.com/onedata/getting-started', 'label': 'dataset_landing_page'}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://github.com/onedata/getting-started",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "LUMA https://onedata.org/docs/doc/administering_onedata/luma.html  - service which allows mapping of between Onedata user accounts and local storage ID's, here we provide an example implementation of this service.",
        "answer": "  [{\"URL\": \"https://onedata.org/docs/doc/administering_onedata/luma.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/doc/administering_onedata/luma.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "More information about support can be found here https://onedata.org/support .",
        "answer": "  [{\"URL\": \"https://onedata.org/support\", \"label\": \"Other\"}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/support",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Oneclient https://onedata.org/docs/doc/using_onedata/oneclient.html  - command line tool which enables transparent access to users data spaces through Fuse https://github.com/libfuse/libfuse  virtual filesystem,",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://onedata.org/docs/doc/using_onedata/oneclient.html', 'label': 'dataset_landing_page'}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/doc/using_onedata/oneclient.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Onepanel https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html  - administration and configuration interface for  and  components,",
        "answer": "  Sure, here are the annotations for the input URL:\n\n[{'URL': 'https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html', 'label': 'dataset_landing_page'}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Oneprovider https://onedata.org/docs/doc/administering_onedata/provider_overview.html  - the main data management component of Onedata, deployed at each storage provider site, responsible for unifying and controlling access to data over low level storage resources of the provider,",
        "answer": "  [{\"URL\": \"https://onedata.org/docs/doc/administering_onedata/provider_overview.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/doc/administering_onedata/provider_overview.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Onezone https://onedata.org/docs/doc/administering_onedata/onezone_overview.html  - allows to connect multiple storage providers into a larger distributed domain and provides users with Graphical User Interface for typical data management tasks,",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://onedata.org/docs/doc/administering_onedata/onezone_overview.html', 'label': 'dataset_landing_page'}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/doc/administering_onedata/onezone_overview.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Please use GitHub issues https://github.com/onedata/onedata/issues  mechanism as the main channel for reporting bugs and requesting support or new features.",
        "answer": "  [{\"URL\": \"https://github.com/onedata/onedata/issues\", \"label\": \"Other\"}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://github.com/onedata/onedata/issues",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The best way to use Onedata is to use our Docker images available at Docker Hub https://hub.docker.com/u/onedata/  or the binary packages available here https://get.onedata.org/ . Currently the binary packages are only available for  component.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: Docker Hub <https://hub.docker.com/u/onedata/>\nLabel: Software\n\n* URL: <https://get.onedata.org/>\nLabel: Other",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://hub.docker.com/u/onedata/",
                "gold_label": "other"
            },
            {
                "URL": "https://get.onedata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The easiest way to get started with using or deploying Onedata is to start with our official documentation https://onedata.org/docs/index.html .",
        "answer": "  [{\"URL\": \"https://onedata.org/docs/index.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "https://onedata.org/docs/index.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This is the main code repository of Onedata http://onedata.org  - a global data management system, providing easy access to distributed storage resources, supporting wide range of use cases from personal data management to data-intensive scientific computations.",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'http://onedata.org', 'label': 'dataset_landing_page'}]",
        "repoID": 6582,
        "URL_gold_label": [
            {
                "URL": "http://onedata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "DEEP-Hybrid-DataCloud https://deep-hybrid-datacloud.eu https://deep-hybrid-datacloud.eu",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://deep-hybrid-datacloud.eu\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6585,
        "URL_gold_label": [
            {
                "URL": "https://deep-hybrid-datacloud.eu",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "INDIGO DataCloud https://www.indigo-datacloud.eu https://www.indigo-datacloud.eu",
        "answer": "  [{\"URL\": \"https://www.indigo-datacloud.eu\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6585,
        "URL_gold_label": [
            {
                "URL": "https://www.indigo-datacloud.eu",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Katrin Wisniewski et al.  2013 corpus1 https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets corpus2 https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets\" - label: \"dataset_landing_page\"\n* \"https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/\" - label: \"dataset_landing_page\"",
        "repoID": 6591,
        "URL_gold_label": [
            {
                "URL": "https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "A tutorial presentation http://simongog.github.io/assets/data/sdsl-slides/tutorial  with the example code tutorial/  using in the sides demonstrating all features of the library in a step-by-step walk-through.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://simongog.github.io/assets/data/sdsl-slides/tutorial', 'label': 'Other'}]",
        "repoID": 6605,
        "URL_gold_label": [
            {
                "URL": "http://simongog.github.io/assets/data/sdsl-slides/tutorial",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Next we suggest you look at the comprehensive tutorial http://simongog.github.io/assets/data/sdsl-slides/tutorial  which describes all major features of the library or look at some of the provided examples examples .",
        "answer": "  [{\"URL\": \"http://simongog.github.io/assets/data/sdsl-slides/tutorial\", \"label\": \"Other\"}]",
        "repoID": 6605,
        "URL_gold_label": [
            {
                "URL": "http://simongog.github.io/assets/data/sdsl-slides/tutorial",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "We provide a large collection of supporting documentation consisting of examples, cheat sheet http://simongog.github.io/assets/data/sdsl-cheatsheet.pdf , tutorial slides and walk-through http://simongog.github.io/assets/data/sdsl-slides/tutorial .",
        "answer": "  [{\"URL\": \"http://simongog.github.io/assets/data/sdsl-cheatsheet.pdf\", \"label\": \"Other\"}, {\"URL\": \"http://simongog.github.io/assets/data/sdsl-slides/tutorial\", \"label\": \"Other\"}]",
        "repoID": 6605,
        "URL_gold_label": [
            {
                "URL": "http://simongog.github.io/assets/data/sdsl-slides/tutorial",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Prophet is open source software https://code.facebook.com/projects/  released by Facebook's Core Data Science team https://research.fb.com/category/data-science/ . It is available for download on CRAN https://cran.r-project.org/package=prophet  and PyPI https://pypi.python.org/pypi/prophet/ .",
        "answer": "  [{\"URL\": \"https://code.facebook.com/projects/\", \"label\": \"Software\"}, {\"URL\": \"https://research.fb.com/category/data-science/\", \"label\": \"Software\"}, {\"URL\": \"https://cran.r-project.org/package=prophet\", \"label\": \"Software\"}, {\"URL\": \"https://pypi.python.org/pypi/prophet/\", \"label\": \"Software\"}]",
        "repoID": 6611,
        "URL_gold_label": [
            {
                "URL": "https://research.fb.com/category/data-science/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "JATE 2.0 Beta.11 released. The main changes include: 1) migration to Solr 7.2.1. : the index files created by this version of Solr is not compatible with the previous versions; 2) fixing a couple of minor bugs documented in the Issues page; 3) added two more example configrations for the TTC https://github.com/ziqizhang/data#ate  corpora; 4) added two new algorithms, Basic https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/Basic.java  and ComboBasic https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/ComboBasic.java ; 5) improved introduction page.",
        "answer": "  Sure, here are the annotated URLs in the input:\n\n* URL: https://github.com/ziqizhang/data\nLabel: Other\n\n* URL: https://github.com/ziqizhang/jate\nLabel: Software\n\n* URL: https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/Basic.java\nLabel: Software\n\n* URL: https://github.com/ziqizhang/jate/blob/master/src/main/java/uk/ac/shef/dcs/jate/algorithm/ComboBasic.java\nLabel: Software\n\n* URL: https://issues.github.com/ziqizhang/jate\nLabel: Other\n\n*",
        "repoID": 6621,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data#ate",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Ziqi Zhang's research data page https://github.com/ziqizhang/data#ate  contains 4 datasets used for ATE research.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/ziqizhang/data#ate', 'label': 'dataset_landing_page'}]",
        "repoID": 6621,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data#ate",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": dataset containing annotated instructions that describe procedures (e.g., how to cook a recipe, how to mount snow chain on wheels etc. : procedure, instruction, annotation, classification : Procedural knowledge https://en.wikipedia.org/wiki/Procedural_knowledge : /procedural knowledge https://github.com/ziqizhang/data/tree/master/procedural%20knowledge",
        "answer": "Sure, I can help you annotate the URLs in the input dataset. Here are the annotations for each URL:\n\nInput: [https://snap.stanford.edu/data/loc-gowalla.html](https://snap.stanford.edu/data/loc-gowalla.html), [http://dawenl.github.io/data/gowalla_pro.zip](http://dawenl.github.io/data/gowalla_pro.zip)\n\nOutput:\n\n[{'URL': 'https://snap.stanford.edu/data/loc-gowalla.html', 'label': 'dataset_landing_page'},\n {'URL': 'http://dawenl.github.io/data/gowalla_pro.zip', 'label': 'dataset_direct_link'}]\n\nExplanation:\n\n* The first URL is for a landing page that provides an introduction to the Gowalla dataset, which is a collection of location-based data.\n* The second URL is for a direct download link of the pre-processed Gowalla data that was used in a paper.\n\nInput: [http://simongog.github.io/assets/data/sdsl-slides/tutorial](http://simongog.github.io/assets/data/sdsl-slides/tutorial)\n\nOutput:\n\n[",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/procedural%20knowledge",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": ": dataset used for evaluating author name and organisation linking in scholarly data : author name disambiguation, link discovery, entity linking, entity disambiguation : Author name disambiguation https://en.wikipedia.org/wiki/Author_Name_Disambiguation : scholarlydata http://www.scholarlydata.org/ : /scholarly data linking https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking",
        "answer": "re, here are the annotated URLs:\n\n* URL: https://en.wikipedia.org/wiki/Author_Name_Disambiguation\nLabel: Other\n* URL: http://www.scholarlydata.org/\nLabel: DatasetLandingPage\n* URL: https://github.com/ziqizhang/data/tree/master/scholarly data linking\nLabel: Other",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.scholarlydata.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": ": dataset used for evaluating automatic term extraction/recognition. : automatic term extraction or recognition, ATE, ATR, text mining, terminology, thesaurus, glossary, ontology engineering : Terminology extraction https://en.wikipedia.org/wiki/Terminology_extraction : SemRe-Rank https://github.com/ziqizhang/semrerank : /terminology extraction https://github.com/ziqizhang/data/tree/master/terminology%20extraction",
        "answer": "Sure, here are the annotated URLs:\n\n* URL: <https://en.wikipedia.org/wiki/Terminology_extraction>\nLabel: Other\n* URL: <https://github.com/ziqizhang/semrerank>\nLabel: DatasetDirectLink\n* URL: <https://github.com/ziqizhang/data/tree/master/terminology extraction>\nLabel: DatasetLandingPage",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/terminology%20extraction",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": ": dataset used for evaluating entity linking in webtables, and also table header classification and relation annotation; contains 16,000+ annotated relational tables that can be used for many studies related to webtables. : Entity linking https://en.wikipedia.org/wiki/Entity_linking : webtable, web table, entity linking, classification, relation extraction : sti https://github.com/ziqizhang/sti : /webtable entity linking https://github.com/ziqizhang/data/tree/master/webtable%20entity%20linking",
        "answer": "re, I can help you annotate the input text. Here's the output:\n\n[{'URL': 'https://en.wikipedia.org/wiki/Entity_linking', 'label': 'Other'}]\n\nExplanation:\n\n* The input text contains a URL to a Wikipedia page on entity linking, which is a technique used in natural language processing to identify and link entities in unstructured text.\n* The label \"Other\" is assigned because the URL is not directly related to any of the other labels (dataset, landing page, software, or other).",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/webtable%20entity%20linking",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": dataset used for evaluating hate speech on Twitter. : hate speech, Twitter, social media, abusive language, classification : chase https://github.com/ziqizhang/chase : /hate speech https://github.com/ziqizhang/data/tree/master/hate%20speech",
        "answer": "Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/ziqizhang/chase', 'label': 'Software'}]\n\nExplanation:\n\n* The URL \"https://github.com/ziqizhang/chase\" is for a software entity, specifically a tool for hate speech detection on Twitter.\n* The URL \"https://github.com/ziqizhang/data/tree/master/hate speech\" is a dataset used for evaluating hate speech on Twitter.",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/hate%20speech",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": ": dataset used for evaluating mapping relations collected from DBpedia : ontology mapping, ontology alignment, DBpedia : Ontology alignment https://en.wikipedia.org/wiki/Ontology_alignment : LODIE http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/J019488/1 : /ontology mapping https://github.com/ziqizhang/data/tree/master/ontology%20mapping",
        "answer": "Sure, here are the annotated URLs:\n\n* URL: https://en.wikipedia.org/wiki/Ontology_alignment\nLabel: Other\n* URL: http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/J019488/1\nLabel: DatasetLandingPage\n* URL: https://github.com/ziqizhang/data/tree/master/ontology mapping\nLabel: DatasetDirectLink",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ziqizhang/data/tree/master/ontology%20mapping",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Z. Zhang, A. N. Nuzzolese, and A. L. Gentile. Entity Deduplication on ScholarlyData http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html . In Proceedings of ESWC 2017, pp 85-100, Lecture Notes in Computer Science. Springer, 2017.",
        "answer": "  [{\"URL\": \"http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6623,
        "URL_gold_label": [
            {
                "URL": "http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The splits  can be downloaded from Ravi and Larochelle - splits https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet . For more information on how to obtain the images check the original source Ravi and Larochelle - github https://github.com/twitter/meta-learning-lstm",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet\nLabel: dataset_direct_link",
        "repoID": 6649,
        "URL_gold_label": [
            {
                "URL": "https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "SQuAD: train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json , dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json', 'label': 'dataset_direct_link'}]",
        "repoID": 6663,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "WebQuestions: train http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2 , test http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2 , entities https://dl.fbaipublicfiles.com/drqa/freebase-entities.txt.gz",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2', 'label': 'dataset_direct_link'}]\n\n[{'URL': 'http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2', 'label': 'dataset_direct_link'}]\n\n[{'URL': 'https://dl.fbaipublicfiles.com/drqa/freebase-entities.txt.gz', 'label': 'other'}]",
        "repoID": 6663,
        "URL_gold_label": [
            {
                "URL": "http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": ": 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 6706,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Prepocess the dataset. Follow the instruction in . You can choose one dataset to run. Three datasets need different prepocessing. I write the instruction for Flickr30k https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare , MSCOCO https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare  and CUHK-PEDES https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare .",
        "answer": "  Sure, I'm ready to annotate the URLs in the input. Please provide the input text containing one or more URLs, and I will output the corresponding labels for each URL span.",
        "repoID": 6719,
        "URL_gold_label": [
            {
                "URL": "https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For additional details on preprocessing, check this https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py , refer to our paper or contact the authors.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py', 'label': 'dataset_landing_page'}]",
        "repoID": 6754,
        "URL_gold_label": [
            {
                "URL": "https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The release of the DeepMind Kinetics dataset www.deepmind.com/kinetics  only included the YouTube IDs and the start and end times of the clips. For the sample data here, we use a video from the UCF101 dataset, for which all the videos are provided in full. The video used is  which can be downloaded from the UCF101 website http://crcv.ucf.edu/data/UCF101.php .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'www.deepmind.com/kinetics', 'label': 'dataset_landing_page'}]\n\n[{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_direct_link'}]",
        "repoID": 6754,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Change the \u2018self._data_path\u2019 in https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py  to yours.",
        "answer": "  [{'URL': 'https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py', 'label': 'dataset_direct_link'}]",
        "repoID": 6774,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Change the \u2018self._devkit_path\u2019 in https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py  to yours.",
        "answer": "  [{\"URL\": \"https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py\", \"label\": \"Software\"}]",
        "repoID": 6774,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Follow the data/VOC0712/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md  to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2007 training and testing.",
        "answer": "  [{\"URL\": \"https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6774,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Follow the data/VOC0712Plus/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md  to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2012 training and testing.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md', 'label': 'dataset_landing_page'}]",
        "repoID": 6774,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Follow the data/coco/README.md https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md  to download MS COCO dataset and create the LMDB file for the COCO training and testing.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md', 'label': 'dataset_landing_page'}]",
        "repoID": 6774,
        "URL_gold_label": [
            {
                "URL": "https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download MSCOCO images from http://cocodataset.org/#download http://cocodataset.org/#download . We train in COCO trainvalminusminival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset and validate in minival https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing  dataset. Then put the data and evaluation PythonAPI https://github.com/cocodataset/cocoapi/tree/master/PythonAPI  in $CPN_ROOT/data/COCO/MSCOCO. All paths are defined in config.py and you can modify them as you wish.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n\n* URL: https://drive.google.com/drive/folders/15loPFQCMQnJqLK1viSMeIwTFT-KbNzdG?usp=sharing\nLabel: DatasetLandingPage\n\n* URL: https://github.com/cocodataset/cocoapi/tree/master/PythonAPI\nLabel: Software",
        "repoID": 6786,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/cocodataset/cocoapi/tree/master/PythonAPI",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To download and preprocess Visdial v0.9 https://visualdialog.org/data  dataset, provide an extra  argument while execution.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://visualdialog.org/data', 'label': 'dataset_direct_link'}]",
        "repoID": 6794,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "VisDial v1.0 https://visualdialog.org/data  dataset can be downloaded and preprocessed as specified below. The path provided as  must have four subdirectories - http://images.cocodataset.org/zips/train2014.zip  and http://images.cocodataset.org/zips/val2014.zip  as per COCO dataset,  and  which can be downloaded from here https://visualdialog.org/data .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://visualdialog.org/data', 'label': 'dataset_landing_page'}]",
        "repoID": 6794,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "An additional method using Conda http://conda.pydata.org/  is also possible:",
        "answer": "  [{\"URL\": \"http://conda.pydata.org/\", \"label\": \"Software\"}]",
        "repoID": 6800,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "NOTE: this script requires Lua and luaTorch. As an alternative, you can download all necessary files from this repo https://github.com/pcyin/pytorch_nmt/tree/master/data",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/pcyin/pytorch_nmt/tree/master/data', 'label': 'dataset_direct_link'}]",
        "repoID": 6801,
        "URL_gold_label": [
            {
                "URL": "https://github.com/pcyin/pytorch_nmt/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Run the script (borrowed from Harvard NLP repo https://github.com/harvardnlp/BSO/tree/master/data_prep/MT ) to download and preprocess IWSLT'14 dataset:",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/harvardnlp/BSO/tree/master/data_prep/MT', 'label': 'dataset_landing_page'}]",
        "repoID": 6801,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/BSO/tree/master/data_prep/MT",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Install the MS COCO dataset at /path/to/coco from official website http://mscoco.org/ , default is ~/data/COCO. Following the instructions https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md  to prepare  and  annotations. All label files (.json) should be under the COCO/annotations/ folder. It should have this basic structure",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://mscoco.org/', 'label': 'dataset_direct_link'}]",
        "repoID": 6803,
        "URL_gold_label": [
            {
                "URL": "https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download COCO2017 image from COCO Dataset http://cocodataset.org/#home",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 6816,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Seaborn https://seaborn.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 6833,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org\",\n\"label\": \"Software\"\n}",
        "repoID": 6833,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We provide demo attack data and classifier on Dropbox https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0  and \u767e\u5ea6\u7f51\u76d8 https://pan.baidu.com/s/1gfpcB5p  (\u5bc6\u7801: yzt4). Please download and put the unzipped files in . You may also use your own data for test.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0\nLabel: dataset_direct_link\n* URL: https://pan.baidu.com/s/1gfpcB5p (password: yzt4)\nLabel: dataset_direct_link",
        "repoID": 6836,
        "URL_gold_label": [
            {
                "URL": "https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "HitL-SLAM can be used on other datasets as well, as long as they are 2D, and based on depth scans or depth images. Many well-known datasets of this nature can be found here http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php . After downloading a dataset or generating some data yourself, it needs to be put into the right format.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php\nLabel: dataset_landing_page",
        "repoID": 6838,
        "URL_gold_label": [
            {
                "URL": "http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph. For this, we load the Cora https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html  dataset, and create a simple 2-layer GCN model using the pre-defined https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html :",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html\nLabel: dataset_direct_link",
        "repoID": 6846,
        "URL_gold_label": [
            {
                "URL": "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "It consists of various methods for deep learning on graphs and other irregular structures, also known as , from a variety of published papers. In addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu , https://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html  support, https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py  support, a large number of common benchmark datasets (based on simple interfaces to create your own), the GraphGym https://pytorch-geometric.readthedocs.io/en/latest/advanced/graphgym.html  experiment manager, and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.",
        "answer": "  Here are the annotated URLs in the input:\n\n1. \"https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu\" - Label: \"Software\"\n2. \"https://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html\" - Label: \"Software\"\n3. \"https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py\" - Label: \"Software\"\n4. \"https://pytorch-geometric.readthedocs.io/en/latest/advanced/graphgym.html\" - Label: \"Software\"",
        "repoID": 6846,
        "URL_gold_label": [
            {
                "URL": "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download train2014, val2014 images and their annotations from the MSCOCO http://cocodataset.org/#download  webpage and put them in ./data/coco",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 6854,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "notMNIST ./notebooks/notMNIST.ipynb  does the same accuracy comparisons, but for the notMNIST dataset http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html . We omit the textual explanations since it would be redundant with what's in the MNIST notebook.",
        "answer": "  [{\"URL\": \"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6871,
        "URL_gold_label": [
            {
                "URL": "http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download videos and train/test splits here http://crcv.ucf.edu/data/UCF101.php .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]",
        "repoID": 6884,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download videos and train/test splits here http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ .",
        "answer": "  Sure! Here are the annotated URLs for the input:\n\n[{'URL': 'http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/', 'label': 'dataset_direct_link'}]",
        "repoID": 6884,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The SPOT dataset used in the above paper is available on the EdinburghNLP github page: link https://github.com/EdinburghNLP/spot-data",
        "answer": "  [{\"URL\": \"https://github.com/EdinburghNLP/spot-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 6887,
        "URL_gold_label": [
            {
                "URL": "https://github.com/EdinburghNLP/spot-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The dataset for evaluation is WIKIBIO https://github.com/DavidGrangier/wikipedia-biography-dataset  from Lebret et al. 2016 https://arxiv.org/abs/1603.07771 . We preprocess the dataset in a easy-to-use way.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/DavidGrangier/wikipedia-biography-dataset', 'label': 'dataset_direct_link'}]",
        "repoID": 6888,
        "URL_gold_label": [
            {
                "URL": "https://github.com/DavidGrangier/wikipedia-biography-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Assamese handwriting recognition https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#",
        "answer": "  Sure, here are the annotations for the input URL:\n\n{\n\"URL\": \"https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 6909,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Kaggle plankton recognition competition, 2015 https://www.kaggle.com/c/datasciencebowl  Third place. The competition solution is being adapted for research purposes in EcoTaxa http://ecotaxa.obs-vlfr.fr/ .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://www.kaggle.com/c/datasciencebowl', 'label': 'dataset_landing_page'}]",
        "repoID": 6909,
        "URL_gold_label": [
            {
                "URL": "https://www.kaggle.com/c/datasciencebowl",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Prepare datasets following this page https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen', 'label': 'dataset_landing_page'}]",
        "repoID": 6910,
        "URL_gold_label": [
            {
                "URL": "https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In the interest of facilitating research reproducibility and thereby increasing the value of your MATPOWER-related research publications, we strongly encourage you to also publish, whenever possible, all of the code and data required to generate the results you are publishing. Zenodo/GitHub https://guides.github.com/activities/citable-code/  and IEEE DataPort https://ieee-dataport.org  are two of many available options https://www.re3data.org .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://guides.github.com/activities/citable-code/\nLabel: Software\n* URL: https://ieee-dataport.org\nLabel: DatasetLandingPage",
        "repoID": 6929,
        "URL_gold_label": [
            {
                "URL": "https://www.re3data.org",
                "gold_label": "other"
            },
            {
                "URL": "https://ieee-dataport.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "First obtain either the EuRoC https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  or the UPenn fast flight https://github.com/KumarRobotics/msckf_vio/wiki/Dataset  dataset.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\",\n            \"label\": \"dataset_landing_page\"\n        }\n    ]",
        "repoID": 6943,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Once the  is built and sourced (via ), there are two launch files prepared for the EuRoC https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  and UPenn fast flight https://github.com/KumarRobotics/msckf_vio/wiki/Dataset  dataset named  and  respectively. Each launch files instantiates two ROS nodes:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets>\nLabel: DatasetLandingPage\n* URL: <https://github.com/KumarRobotics/msckf_vio/wiki/Dataset>\nLabel: DatasetLandingPage",
        "repoID": 6943,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Vicon Room 1 01 http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 6943,
        "URL_gold_label": [
            {
                "URL": "http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Vicon Room 1 02 http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag', 'label': 'dataset_direct_link'}]",
        "repoID": 6943,
        "URL_gold_label": [
            {
                "URL": "http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "In all MS-COCO experiments, we use  for training, and  (a.k.a. ) for validation. Follow MS-COCO website http://cocodataset.org/#download  to download images/annotations, and set-up the COCO API.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink",
        "repoID": 6962,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This project is based on asyncCall https://github.com/dielc/asyncCall.js  and uses parts of datastore https://github.com/bredele/datastore . The two projects are combined such that changes two types of data (observable and replicated data) can be used. Changes to these data types are propagated to server or clients via remote procedure calls. This way, custom failure handling can be added when for example no network is available. Proxies are used to ensure that every assignment on these objects are propagated to the server and every connected client.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/dielc/asyncCall.js\nLabel: Software\n* URL: https://github.com/bredele/datastore\nLabel: Software\n* URL: remote procedure calls\nLabel: Other",
        "repoID": 6980,
        "URL_gold_label": [
            {
                "URL": "https://github.com/bredele/datastore",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "GBFS is an open source project developed under a consensus-based governance model. Contributors come from across the shared mobility industry, public sector, civic technology and elsewhere. Comments or questions can be addressed to the community by opening an issue https://github.com/NABSA/gbfs/issues . Proposals for changes or additions to the specification can be made through pull requests https://github.com/NABSA/gbfs/pulls .  Questions can also be addressed to the community via the public GBFS Slack channel https://bit.ly/mobilitydata-slack  or to the shared mobility staff at MobilityData: sharedmobility@mobilitydata.org mailto:sharedmobility@mobilitydata.org",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: <https://github.com/NABSA/gbfs>\nLabel: Other\n\nExplanation: This URL is a link to a GitHub repository for a project called GBFS, which is an open source project developed under a consensus-based governance model. The URL does not fall into any of the predefined categories, so it is classified as \"Other\".",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://bit.ly/mobilitydata-slack",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "GBFS was created in 2014 by Mitch Vars https://github.com/mplsmitch  with collaboration from public, private sector and non-profit shared mobility system owners and operators, application developers, and technology vendors. Michael Frumin https://github.com/fruminator , Jesse Chan-Norris https://github.com/jcn  and others made significant contributions of time and expertise toward the development of v1.0 on behalf of Motivate International LLC (now Lyft). The North American Bikeshare Association\u2019s http://www.nabsa.net  endorsement, support, and hosting was key to its success starting in 2015. In 2019, NABSA chose MobilityData to govern and facilitate the improvement of GBFS. MobilityData hosts a GBFS Resource Center https://gbfs.mobilitydata.org/  and a public GBFS Slack channel https://bit.ly/mobilitydata-slack  - you are welcome to contact us there or at sharedmobility@mobilitydata.org mailto:sharedmobility@mobilitydata.org  with questions.",
        "answer": "  Here are the annotated URLs in the input:\n\n1. \"https://github.com/mplsmitch\" - Label: Software\n2. \"https://github.com/fruminator\" - Label: Software\n3. \"https://github.com/jcn\" - Label: Software\n4. \"http://www.nabsa.net\" - Label: DatasetLandingPage\n5. \"https://gbfs.mobilitydata.org\" - Label: DatasetDirectLink\n6. \"https://bit.ly/mobilitydata-s",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://gbfs.mobilitydata.org/",
                "gold_label": "other"
            },
            {
                "URL": "https://bit.ly/mobilitydata-slack",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Including APIs, datasets, validators, research, and software can be found here https://gbfs.mobilitydata.org/toolbox/resources/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://gbfs.mobilitydata.org/toolbox/resources/', 'label': 'Other'}]",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://gbfs.mobilitydata.org/toolbox/resources/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The copyright for GBFS is held by the MobilityData https://mobilitydata.org/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://mobilitydata.org/', 'label': 'dataset_landing_page'}]",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://mobilitydata.org/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The person calling for the vote should announce the vote in the GBFS Slack channel https://mobilitydata-io.slack.com  with a link to the PR. The message should conform to this template:",
        "answer": "  [INST]\n    Output: [{\"URL\": \"https://mobilitydata-io.slack.com\", \"label\": \"Other\"}]",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://mobilitydata-io.slack.com",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "There are many similarities between GBFS and MDS https://github.com/openmobilityfoundation/mobility-data-specification  (Mobility Data Specification), however, their intended use cases are different. GBFS is a real-time or near real-time specification for public data primarily intended to provide transit advice through consumer-facing applications. MDS is not public data and is intended for use only by mobility regulators. Publishing a public GBFS feed is a requirement https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement  of all MDS compatible  APIs.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/openmobilityfoundation/mobility-data-specification>\nLabel: DatasetLandingPage",
        "repoID": 6989,
        "URL_gold_label": [
            {
                "URL": "https://github.com/openmobilityfoundation/mobility-data-specification",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "get a dataset with backgrounds, e.g. the Describable Textures Dataset http://www.robots.ox.ac.uk/~vgg/data/dtd/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.robots.ox.ac.uk/~vgg/data/dtd/', 'label': 'dataset_landing_page'}]",
        "repoID": 6999,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/dtd/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The data set is available on Dataverse https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME  (361 MB). This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in our experiments.",
        "answer": "  [{\"URL\": \"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 7008,
        "URL_gold_label": [
            {
                "URL": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Go to this page https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to prepare ImageNet 1K data.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset', 'label': 'dataset_direct_link'}]",
        "repoID": 7039,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Parses the UrbanSound8K https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html  data set. This parser requires the following directory structure below the data set base directory.",
        "answer": "  [{\"URL\": \"https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 7106,
        "URL_gold_label": [
            {
                "URL": "https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Additionally, the NLTK data package  needs to be downloaded. For installing packages, see the official guide Installing NLTK Data http://www.nltk.org/data.html .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://www.nltk.org/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 7166,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The dataset for query-based abstractive summarization is created by converting an existing dataset for question answering, released by DeepMind https://github.com/deepmind/rc-data . Archives containing the processed DeepMind dataset can be downloaded at http://cs.nyu.edu/~kcho/DMQA/ http://cs.nyu.edu/~kcho/DMQA/ , which we used. Both the  and  archives are required for the conversion, from either news organization, or both. To use both, merge the extracted directories, for  and  separately.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/deepmind/rc-data\nLabel: dataset_direct_link\n\n* URL: http://cs.nyu.edu/~kcho/DMQA/\nLabel: dataset_landing_page",
        "repoID": 7166,
        "URL_gold_label": [
            {
                "URL": "https://github.com/deepmind/rc-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "download data from HIGGS https://archive.ics.uci.edu/ml/datasets/HIGGS  and uncompress gz file.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/HIGGS', 'label': 'dataset_direct_link'}]",
        "repoID": 7191,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/HIGGS",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": We use  to name COCO's API as inheritance. Download the annotations and images http://cocodataset.org/#download  into . Note the valminusminival and minival can be downloaded here https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md .",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: http://cocodataset.org/#download\nLabel: dataset_direct_link\n\n* URL: https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md\nLabel: other",
        "repoID": 7528,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We highly recommended to use conda http://conda.pydata.org/miniconda.html  as your Python distribution. Once downloading and installing conda http://conda.pydata.org/miniconda.html , this project can be installed by using the  file as follows:",
        "answer": "  [{\"URL\": \"http://conda.pydata.org/miniconda.html\", \"label\": \"Software\"}]",
        "repoID": 7641,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            },
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download MS-COCO 2017 dataset from here http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 7642,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "dataset, Datasets for approximate nearest neighbor search http://corpus-texmex.irisa.fr/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 7670,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "adult.csv link https://archive.ics.uci.edu/ml/datasets/Adult",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/Adult', 'label': 'dataset_direct_link'}]",
        "repoID": 7782,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Adult",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "A special effort was made to make a , which stresses on the ease of use of the framework. For example, that's how you build a pure SVD recommender on top of the Movielens 1M http://grouplens.org/datasets/movielens/  dataset:",
        "answer": "  [{\"URL\": \"http://grouplens.org/datasets/movielens/\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 7908,
        "URL_gold_label": [
            {
                "URL": "http://grouplens.org/datasets/movielens/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The original translation corpora can be downloaded from ( IWLST'16 En-De https://wit3.fbk.eu/ , WMT'16 En-Ro http://www.statmt.org/wmt16/translation-task.html , WMT'15 En-De http://www.statmt.org/wmt15/translation-task.html , MS COCO http://cocodataset.org/#home ). For the preprocessed corpora and pre-trained models, see below.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://wit3.fbk.eu/\nLabel: DatasetDirectLink\n* URL: http://www.statmt.org/wmt16/translation-task.html\nLabel: DatasetLandingPage\n* URL: http://www.statmt.org/wmt15/translation-task.html\nLabel: DatasetLandingPage\n* URL: http://cocodataset.org/#home\nLabel: Other",
        "repoID": 7931,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The non-English data (Czech, French, German, Russian, and Spanish) can be downloaded from Jan Botha's website https://bothameister.github.io . For ease of use you can use the Yoon Kim's script https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh , which downloads these data and saves them into the relevant folders.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://bothameister.github.io\nLabel: DatasetLandingPage\n* URL: https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh\nLabel: Software",
        "repoID": 8004,
        "URL_gold_label": [
            {
                "URL": "https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes: Link https://www.cityscapes-dataset.com",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com', 'label': 'dataset_landing_page'}]",
        "repoID": 8045,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Coco: Link http://cocodataset.org/#home",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 8045,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Each .ipynb file corresponds to the particular set of experiments with given dimension of the problem (for Nesterov's function) or given dataset. All datasets are taken from LIBSVM library https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html .",
        "answer": "  Sure, here are the annotated URLs for the given input:\n\n[{'URL': 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html', 'label': 'dataset_landing_page'}]",
        "repoID": 8050,
        "URL_gold_label": [
            {
                "URL": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Higgs https://archive.ics.uci.edu/ml/datasets/HIGGS",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://archive.ics.uci.edu/ml/datasets/HIGGS\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 8054,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/HIGGS",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Power https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 8054,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "An additional method using Conda http://conda.pydata.org/  is also possible:",
        "answer": "  [{\"URL\": \"http://conda.pydata.org/\", \"label\": \"Software\"}]",
        "repoID": 8084,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the Cityscapes Dataset https://www.cityscapes-dataset.com/  as the target domain, and put it in the  folder",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 8096,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pretrain our model with RACE dataset http://www.cs.cmu.edu/~glai1/data/race/  for 10 epochs.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cs.cmu.edu/~glai1/data/race/', 'label': 'dataset_direct_link'}]",
        "repoID": 8119,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cmu.edu/~glai1/data/race/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In this work, two speech enhancement datasets are used. The first one is device recorded version of VCTK corpus which can be found in Edinburgh DataShare https://doi.org/10.7488/ds/2316 . The second dataset is (Valentini et al. 2016) http://ssw9.net/papers/ssw9_PS2-4_Valentini-Botinhao.pdf  which also can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . However, the following script downloads and prepares the second dataset for TensorFlow format:",
        "answer": "  [INST]\n\n* URL: https://doi.org/10.7488/ds/2316\nLabel: dataset_direct_link\n\n* URL: http://datashare.is.ed.ac.uk/handle/10283/1942\nLabel: dataset_direct_link",
        "repoID": 8141,
        "URL_gold_label": [
            {
                "URL": "http://datashare.is.ed.ac.uk/handle/10283/1942",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For  and  tasks, the training data is KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  and you can download them by the official script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip ;",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/raw_data.php\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"http://www.cvlibs.net/download.php?file=raw_data_downloader.zip\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 8196,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Please go to directory , and run  to download Kitti Stereo 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  and Kitti Raw http://www.cvlibs.net/datasets/kitti/raw_data.php  datasets.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo\nLabel: dataset_direct_link\n\n* URL: http://www.cvlibs.net/datasets/kitti/raw_data.php\nLabel: dataset_direct_link",
        "repoID": 8209,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In RTSeg, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The code and the experimental results are presented on the CityScapes dataset for urban scenes https://www.cityscapes-dataset.com/ .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://www.cityscapes-dataset.com/\nLabel: dataset_landing_page",
        "repoID": 8214,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "KITTI Odometry benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  contains 22 stereo sequences, in which 11 sequences are provided with ground truth. The 11 sequences are used for evaluation or training of visual odometry.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_odometry.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 8259,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The main dataset used in this project is KITTI Driving Dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . Please follow the instruction in  to prepare the required dataset.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_direct_link'}]",
        "repoID": 8259,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 8273,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Leaderboard Link http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo', 'label': 'dataset_landing_page'}]",
        "repoID": 8465,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the coco http://cocodataset.org/#download  image data. Extract them to .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 8646,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from http://www.cvlibs.net/datasets/kitti/eval_odometry.php http://www.cvlibs.net/datasets/kitti/eval_odometry.php  using:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/JakobEngel/dso>\nLabel: Software\n* URL: <http://www.cvlibs.net/datasets/kitti/eval_odometry.php>\nLabel: DatasetLandingPage",
        "repoID": 8650,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 8911,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json', 'label': 'dataset_direct_link'}]",
        "repoID": 8911,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "SentEval also includes a series of https://github.com/facebookresearch/SentEval/tree/master/data/probing  to evaluate what linguistic properties are encoded in your sentence embeddings:",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/facebookresearch/SentEval/tree/master/data/probing', 'label': 'dataset_direct_link'}]",
        "repoID": 8933,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebookresearch/SentEval/tree/master/data/probing",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": download the data (link) https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html . Partitions were already defined by the dataset authors, and we have some code to get those! Just list all the audios in a file, and set the config file: , , , .",
        "answer": "  [{\"URL\": \"https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 9134,
        "URL_gold_label": [
            {
                "URL": "https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Also see the data generators README https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md', 'label': 'dataset_landing_page'}]",
        "repoID": 9148,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To add a new dataset, subclass https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py  and register it with . See https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py  for an example.",
        "answer": "  [{\"URL\": \"https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py\", \"label\": \"Software\"}]",
        "repoID": 9148,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py",
                "gold_label": "software"
            },
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "are all standardized on  files with  protocol buffers. All datasets are registered and generated with the data generator https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen  and many common sequence datasets are already available for generation and use.",
        "answer": "  [{\"URL\": \"https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen\", \"label\": \"Software\"}]",
        "repoID": 9148,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The code in this repository is compatible with Python 2 and Python 3. Its only other external dependency is NLTK http://www.nltk.org/install.html , with the data installed http://www.nltk.org/data.html  so that WordNet is available.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.nltk.org/install.html\nLabel: Software\n\n* URL: http://www.nltk.org/data.html\nLabel: Other",
        "repoID": 9285,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "HIGGS https://archive.ics.uci.edu/ml/datasets/HIGGS",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://archive.ics.uci.edu/ml/datasets/HIGGS\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 9294,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/HIGGS",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Other datasets http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html', 'label': 'dataset_landing_page'}]",
        "repoID": 9294,
        "URL_gold_label": [
            {
                "URL": "http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We use the Cityscapes dataset. To train a model on the full dataset, please download it from the official website https://www.cityscapes-dataset.com/  (registration required). After downloading, please put it under the  folder in the same way the example images are provided.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://www.cityscapes-dataset.com/\nLabel: dataset_direct_link",
        "repoID": 9329,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Graphs from the SNAP dataset collection https://snap.stanford.edu/data/index.html  are commonly used for graph algorithm benchmarks. We provide a tool that converts the most common SNAP graph format to the adjacency graph format that GBBS accepts. Usage example:",
        "answer": "  [{\"URL\": \"https://snap.stanford.edu/data/index.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 9392,
        "URL_gold_label": [
            {
                "URL": "https://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/  - helper data manipulation library",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas https://pandas.pydata.org/\":\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 9473,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/  - helper plotting library for some charts",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 9473,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Get the N-MNIST dataset by the link http://www.garrickorchard.com/datasets/n-mnist . Then unzip the ''Test.zip'' and ''Train.zip''.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.garrickorchard.com/datasets/n-mnist', 'label': 'dataset_direct_link'}]",
        "repoID": 9503,
        "URL_gold_label": [
            {
                "URL": "http://www.garrickorchard.com/datasets/n-mnist",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ImageNet dataset should be stored in  path and set up in the usual way (separate  and  folders with 1000 subfolders each). See this repo https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  for detailed instructions how to download and set up the dataset.",
        "answer": "  [{\"URL\": \"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 9526,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Please follow the official COCO dataset website http://cocodataset.org/#download  to download the dataset. After downloading the dataset you should have the following directory structure:",
        "answer": "  [{\"URL\": \"http://cocodataset.org/#download\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 9588,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . You will probably need to contact the administrators to be able to get it.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://www.cityscapes-dataset.com/>\nLabel: Other",
        "repoID": 9610,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command.",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]",
        "repoID": 9610,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For pose evaluation, you need to download KITTI Odometry http://www.cvlibs.net/datasets/kitti/eval_odometry.php  dataset.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 9610,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "For testing optical flow ground truths on KITTI, download KITTI2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  dataset. You need to download 1)  data set (2 GB), 2)  (14 GB), and 3)  (1 MB) . In addition, download semantic labels from here https://keeper.mpdl.mpg.de/f/239c2dda94e54c449401/?dl=1 . You should have the following directory structure:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\nLabel: dataset_direct_link\n\n* URL: https://keeper.mpdl.mpg.de/f/239c2dda94e54c449401/?dl=1\nLabel: dataset_landing_page",
        "repoID": 9610,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To benchmark our method, we use the two standard benchmark datasets BigANN http://corpus-texmex.irisa.fr/  and Deep1b https://yadi.sk/d/11eDCm7Dsn9GA , see here https://github.com/facebookresearch/faiss/tree/master/benchs#getting-bigann  for more info on how to download. You need to indicate the path to these in lib/data.py:",
        "answer": "  [{\"URL\": \"http://corpus-texmex.irisa.fr/\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"https://github.com/facebookresearch/faiss/tree/master/benchs\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 9929,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MS COCO Dataset http://cocodataset.org/#home",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"http://cocodataset.org/#home\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 9943,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "RACE: Large-scale ReAding Comprehension Dataset From Examinations http://www.cs.cmu.edu/~glai1/data/race/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.cs.cmu.edu/~glai1/data/race/', 'label': 'dataset_landing_page'}]",
        "repoID": 9967,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cmu.edu/~glai1/data/race/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 10079,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  -- for data structuring and manipulation,",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 10208,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The -gram counts files follow the Google format http://storage.googleapis.com/books/ngrams/books/datasetsv2.html , i.e., one separate file for each distinct value of  (order) listing one gram per row. We enrich this format with a file header indicating the total number of -grams in the file (rows):",
        "answer": "  [{\"URL\": \"http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 10262,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This project is forked from make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data , slightly changing the data format and feature alignment for future use.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/wnzhang/make-ipinyou-data', 'label': 'dataset_landing_page'}]",
        "repoID": 10358,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The feature engineering is contributed by @weinan zhang. On his benchmark make-ipinyou-data https://github.com/wnzhang/make-ipinyou-data , we re-organized the feature alignment and removed the  feature considering leaky problems make-ipinyou-data-refined https://github.com/Atomu2014/make-ipinyou-data .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/wnzhang/make-ipinyou-data\nLabel: DatasetLandingPage\n* URL: https://github.com/Atomu2014/make-ipinyou-data\nLabel: Other",
        "repoID": 10359,
        "URL_gold_label": [
            {
                "URL": "https://github.com/wnzhang/make-ipinyou-data",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/Atomu2014/make-ipinyou-data",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "You must have downloaded the UCF101 http://crcv.ucf.edu/data/UCF101.php  (Action Recognition Data Set)",
        "answer": "  [{\"URL\": \"http://crcv.ucf.edu/data/UCF101.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 10368,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "COCO image http://cocodataset.org :",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://cocodataset.org', 'label': 'dataset_landing_page'}]",
        "repoID": 10381,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For training on KITTI http://www.cvlibs.net/datasets/kitti/eval_object.php , run:",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_object.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 10408,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_object.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For training on MS COCO http://cocodataset.org/#home , run:",
        "answer": "  [{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 10408,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To aggregate the data and convert them to various formats we use a pandas https://pandas.pydata.org/  data frame.",
        "answer": "  [{\"URL\": \"https://pandas.pydata.org/\", \"label\": \"Software\"}]",
        "repoID": 10437,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Flowers 102: http://www.robots.ox.ac.uk/~vgg/data/flowers/102/ http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/', 'label': 'dataset_direct_link'}]",
        "repoID": 10546,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We have tested our method on COCO Dataset http://cocodataset.org",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org', 'label': 'dataset_landing_page'}]",
        "repoID": 10550,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas\":\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 10642,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This repository holds the code used for our recent work on domain randomization. We try to study how we can use this technique in the creation of large domain-specific synthetic datasets. We conclude that this approach can be preferable to fine-tuning  object detection CNNs on small real image datasets, after being pre-trained on huge and available datasets such as COCO http://cocodataset.org/#home .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://cocodataset.org/#home\nLabel: dataset_direct_link",
        "repoID": 10642,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  >= 0.18",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 10672,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This recipe and collection of scripts enables you to train large vocabulary German acoustic models for speaker-independent automatic speech recognition (ASR) with Kaldi http://kaldi.sourceforge.net/ . The scripts currently use three freely available German speech corpora: The Tuda-De corpus is recorded with a Microsoft Kinect and two other microphones in parallel at Technische Universit\u00e4t Darmstadt and has been released under a permissive license (CC-BY 4.0) http://creativecommons.org/licenses/by/4.0/ . This corpus compromises ~31h of training data per microphone and ~5h separated into development and test partitions. We also make use of the German subset from the Spoken Wikipedia Corpora (SWC) https://nats.gitlab.io/swc/ , containing about 285h of additional data and the German subset of m-ailabs read speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/  (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://kaldi.sourceforge.net/\nLabel: DatasetDirectLink\n* URL: http://creativecommons.org/licenses/by/4.0/\nLabel: Other\n* URL: http://nats.gitlab.",
        "repoID": 10817,
        "URL_gold_label": [
            {
                "URL": "https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We added more aligned speech data (630h total now), thanks to the m-ailabs speech data corpus http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (mirror) https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ . We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/ (label: dataset_landing_page)\n* URL: https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/ (label: dataset_landing_page)\n* URL: https://www.kaldi-project.org/ (label: Software)",
        "repoID": 10817,
        "URL_gold_label": [
            {
                "URL": "https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Google Books Ngrams http://storage.googleapis.com/books/ngrams/books/datasetsv2.html : available also in hadoop format on amazon s3 (2.2 TB)",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'http://storage.googleapis.com/books/ngrams/books/datasetsv2.html', 'label': 'dataset_landing_page'}]",
        "repoID": 10851,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Python 3 https://www.python.org/ , Pandas https://pandas.pydata.org/ , Matplotlib https://matplotlib.org/  and NumPy http://www.numpy.org/  for producing the plots and analyse the data.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://www.python.org/', 'label': 'Software'}]\n[{'URL': 'https://pandas.pydata.org/', 'label': 'Software'}]\n[{'URL': 'https://matplotlib.org/', 'label': 'Software'}]\n[{'URL': 'http://www.numpy.org/', 'label': 'Software'}]",
        "repoID": 10889,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "COCO API https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 10916,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 10924,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For example, a Transformer https://arxiv.org/abs/1706.03762  language model for unsupervised modeling of large text datasets, such as the amazon-review dataset http://jmcauley.ucsd.edu/data/amazon/ , is implemented in PyTorch. We also support other tokenization methods, such as character or sentencepiece tokenization, and language models using various recurrent architectures.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: <https://arxiv.org/abs/1706.03762>\nLabel: Software\n* URL: <http://jmcauley.ucsd.edu/data/amazon/>\nLabel: DatasetLandingPage",
        "repoID": 10928,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In the  folder we've provided processed copies of the Binary Stanford Sentiment Treebank (Binary SST) https://nlp.stanford.edu/sentiment/index.html , IMDB Movie Review http://ai.stanford.edu/~amaas/data/sentiment/ , and the SemEval2018 Tweet Emotion https://competitions.codalab.org/competitions/17751  datasets as part of this repository. In order to train on the amazon dataset please download the \"aggressively deduplicated data\" version from Julian McAuley's original site http://jmcauley.ucsd.edu/data/amazon/ . Access requests to the dataset should be approved instantly. While using the dataset make sure to load it with the  flag.",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"https://nlp.stanford.edu/sentiment/index.html\" - DatasetLandingPage\n* \"http://ai.stanford.edu/~amaas/data/sentiment/\" - DatasetDirectLink\n* \"http://jmcauley.ucsd.edu/data/amazon/\" - DatasetDirectLink\n* \"http://competitions.codalab.org/competitions/17751\" - DatasetDirectLink\n* \"http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\" - Software",
        "repoID": 10928,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This project uses the amazon review dataset http://jmcauley.ucsd.edu/data/amazon/  collected by J. McAuley",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://jmcauley.ucsd.edu/data/amazon/', 'label': 'dataset_direct_link'}]",
        "repoID": 10928,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The indoor Synthetic Dataset renders from SUNCG http://suncg.cs.princeton.edu/  and indoor Realistic Dataset comes from NYUv2 https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html . The outdooe Synthetic Dataset is vKITTI http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds  and outdoor Realistic dataset is KITTI http://www.cvlibs.net/datasets/kitti/",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"http://suncg.cs.princeton.edu/\" - Label: DatasetLandingPage\n* \"https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\" - Label: DatasetLandingPage\n* \"http://www.europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds\" - Label: Software\n* \"http://www.cvlibs.net/datasets/kitti/\" - Label: DatasetLandingPage",
        "repoID": 10929,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The data used in this project comes from the UCR archive http://www.cs.ucr.edu/~eamonn/time_series_data/ , which contains the 85 univariate time series datasets we used in our experiements.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://www.cs.ucr.edu/~eamonn/time_series_data/', 'label': 'dataset_landing_page'}]",
        "repoID": 10965,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ucr.edu/~eamonn/time_series_data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 10965,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Provenance Graphs for Datalog https://github.com/IITDBGroup/gprom/wiki/datalog_prov",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/IITDBGroup/gprom/wiki/datalog_prov', 'label': 'dataset_landing_page'}]",
        "repoID": 11101,
        "URL_gold_label": [
            {
                "URL": "https://github.com/IITDBGroup/gprom/wiki/datalog_prov",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Awesome Public Datasets https://github.com/caesar0301/awesome-public-datasets  - An awesome list of public datasets.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/caesar0301/awesome-public-datasets', 'label': 'dataset_landing_page'}]",
        "repoID": 11112,
        "URL_gold_label": [
            {
                "URL": "https://github.com/caesar0301/awesome-public-datasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We follow the Facebook process of ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset . Two subfolders (\"train\" and \"val\") are included in the \"/path/to/ImageNet2012\". The correspding code is here https://github.com/he-y/soft-filter-pruning/blob/master/pruning_train.py#L129-L130 .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL \"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md\" is the landing page for the ImageNet dataset, which is a popular dataset for image classification tasks.",
        "repoID": 11171,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can downlooad pre-trained models for English, German, and Russian http://ltdata1.informatik.uni-hamburg.de/sensegram/ . Note that to run examples from the QuickStart you only need files with extensions , , and . Other files are supplementary.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://ltdata1.informatik.uni-hamburg.de/sensegram/\nLabel: DatasetDirectLink",
        "repoID": 11296,
        "URL_gold_label": [
            {
                "URL": "http://ltdata1.informatik.uni-hamburg.de/sensegram/",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "The WikiBio data is available here https://github.com/DavidGrangier/wikipedia-biography-dataset , and the preprocessed version of the target-side data used for training is at data/wb_aligned.tar.gz https://github.com/harvardnlp/neural-template-gen/blob/master/data/wb_aligned.tar.gz . This target-side data is again preprocessed to annotate spans appearing in the corresponding database. Code for this annotation is at data/make_wikibio_labedata.py https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_wikibio_labedata.py . The source-side data can be downloaded directly from the WikiBio repo https://github.com/DavidGrangier/wikipedia-biography-dataset , and we used it unchanged; in particular the  files become our  files mentioned below.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/DavidGrangier/wikipedia-biography-dataset\nLabel: dataset_landing_page\n* URL: https://github.com/harvardnlp/neural-template-gen/blob/master/data/wb_aligned.tar.gz\nLabel: dataset_direct_link\n* URL: https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_wikibio_labedata.py\nLabel: Software\n* URL: https://github.com/DavidGrangier/wikipedia-biography-dataset\nLabel: Other",
        "repoID": 11368,
        "URL_gold_label": [
            {
                "URL": "https://github.com/DavidGrangier/wikipedia-biography-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/DavidGrangier/wikipedia-biography-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "End-to-end training with on-the-fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://opennmt.net/OpenNMT-py/FAQ.html', 'label': 'dataset_landing_page'}]",
        "repoID": 11422,
        "URL_gold_label": [
            {
                "URL": "[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The boxscore-data json files can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/harvardnlp/boxscore-data', 'label': 'dataset_direct_link'}]",
        "repoID": 11462,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and convert the dataset to 19 categories https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 11599,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 11694,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The work of Dr. Eamonn Keogh at University of California Riverside http://www.cs.ucr.edu/~eamonn/time_series_data/  has shown that a good way to classify time series is with a k-NN algorithm using a dynamic time warping similarity measure.",
        "answer": "  [{\"URL\": \"http://www.cs.ucr.edu/~eamonn/time_series_data/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 11733,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ucr.edu/~eamonn/time_series_data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For better adaptation to real world images, we have used the Cityscapes dataset https://www.cityscapes-dataset.com/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com', 'label': 'dataset_landing_page'}]",
        "repoID": 11848,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "These images are from the Cityscapes https://www.cityscapes-dataset.com/  and the SVS https://svsdataset.github.io/  datasets.",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]\n[{'URL': 'https://svsdataset.github.io/', 'label': 'dataset_landing_page'}]",
        "repoID": 11848,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download The Cityscapes Dataset https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 11948,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from http://www.cvlibs.net/datasets/kitti/eval_odometry.php http://www.cvlibs.net/datasets/kitti/eval_odometry.php  using:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/JakobEngel/dso>\nLabel: Software\n* URL: <http://www.cvlibs.net/datasets/kitti/eval_odometry.php>\nLabel: DatasetLandingPage",
        "repoID": 11962,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The program supports datasets in the format of the TUM RGB-D benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  with two small additions:",
        "answer": "  [{\"URL\": \"https://vision.in.tum.de/data/datasets/rgbd-dataset\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 12049,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We use KITTI Odometry sequences http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and Oxford Robotcar dataset http://robotcar-dataset.robots.ox.ac.uk/datasets/  to evaluate the robustness of our method.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page\n* URL: http://robotcar-dataset.robots.ox.ac.uk/datasets/\nLabel: dataset_landing_page",
        "repoID": 12056,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "We trained our FutureGAN on three different datasets, MovingMNIST https://github.com/emansim/unsupervised-videos  (64\u00d764 px), KTH Action http://www.nada.kth.se/cvap/actions/  (bicubically resized to 128\u00d7128 px), and Cityscapes https://www.cityscapes-dataset.com/  (bicubically resized to 128\u00d7128 px). For MovingMNIST and KTH Action the networks were trained to predict 6 frames conditioned on 6 input frames. For Cityscapes they were trained to predict 5 frames based on 5 input frames. All pre-trained models are available upon request, please contact sandra.aigner@tum.de.",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"https://github.com/emansim/unsupervised-videos\" (label: \"dataset_landing_page\")\n* \"www.nada.kth.se/cvap/actions/\" (label: \"dataset_landing_page\")\n* \"www.cityscapes-dataset.com/\" (label: \"dataset_landing_page\")\n* \"http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\" (label: \"Software\")",
        "repoID": 12076,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "we introduce  a visual complexity dataset that compromises of more than 1,400 images from seven image categories namely http://places2.csail.mit.edu/ , http://people.cs.pitt.edu/~kovashka/ads/ , http://massvis.mit.edu/ , http://cocodataset.org/#home , , https://github.com/BathVisArtData/PeopleArt , and https://github.com/BathVisArtData/PeopleArt/tree/master/JPEGImages/Suprematism . The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. The ground truth for SAVOIAS is obtained by crowdsourcing more than 37,000 pairwise comparisons of images using the forced-choice methodology and with more than 1,600 contributors using the Figure-Eight http://figure-eight.com/  crowdplatform. The resulting relative scores are then converted to absolute visual complexity scores using the Bradley-Terry method and matrix completion. When applying five state-of-the-art algorithms to analyze the visual complexity of the images in the SAVOIAS dataset, we found that the scores obtained from these baseline tools only correlate well with crowd-sourced labels for abstract patterns in the Suprematism category (Pearson correlation r=0.84). For the other categories, in particular, the objects and advertisement categories, low correlation coefficients were revealed (r=0.3 and 0.56, respectively). These findings suggest that (1) state-of-the-art approaches are mostly insufficient and (2) SAVOIAS enables category-specific method development, which is likely to improve the impact of visual complexity analysis on specific application areas, including computer vision.",
        "answer": "ologies",
        "repoID": 12102,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": 2975 images from the Cityscapes training set https://www.cityscapes-dataset.com/ . [ Citation datasets/bibtex/cityscapes.tex ]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to  for more detail.",
        "answer": "  [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 12125,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For Cityscapes https://www.cityscapes-dataset.com/ , download the following packages: 1) , 2) . You will probably need to contact the administrators to be able to get it. Then run the following command",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 12185,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website, and then run the following command. The  option will save resized copies of groundtruth to help you setting hyper parameters. The  will dump the sequence pose in the same format as Odometry dataset (see pose evaluation)",
        "answer": "  Sure! Here's the annotated input:\n\nInput: For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php, first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip provided on the official website, and then run the following command. The option will save resized copies of groundtruth to help you setting hyper parameters. The will dump the sequence pose in the same format as Odometry dataset (see pose evaluation.\n\nOutput:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]\n\nNote: I have only annotated the first URL in the input as it is the only URL present in the input.",
        "repoID": 12185,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pose evaluation is also available on Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php . Be sure to download both color images and pose !",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 12185,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "You would need to download all of the KITTI raw data http://www.cvlibs.net/datasets/kitti/raw_data.php  and calibration files to train the model. You would also need the training files of KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo  and KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  for validating the models.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/raw_data.php\", \"label\": \"dataset_direct_link\"}, {\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 12208,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Sequeval requires numpy http://www.numpy.org/ , pandas http://pandas.pydata.org/ , pytimeparse https://github.com/wroberts/pytimeparse , and scipy http://www.scipy.org/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://www.numpy.org/', 'label': 'Software'}]\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]\n[{'URL': 'https://github.com/wroberts/pytimeparse', 'label': 'Software'}]\n[{'URL': 'http://www.scipy.org/', 'label': 'Software'}]",
        "repoID": 12273,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "In Mayo, we decouple the description of each neural network application into three separate components: the dataset, model and trainer, each written in YAML http://yaml.org . The reason for decoupling is to encourage reuse, for instance a ResNet-50 model can not only be used for ImageNet http://www.image-net.org  classification, but also object detection with COCO http://cocodataset.org , or even customized tasks.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://yaml.org\nLabel: Other\n\n* URL: http://www.image-net.org\nLabel: DatasetDirectLink\n\n* URL: http://cocodataset.org\nLabel: DatasetDirectLink",
        "repoID": 12293,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To run StructVIO on the Euroc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  datasets, download the configuration file euroc_data.yaml https://github.com/danping/structvio/blob/master/euroc_data.yaml  and type",
        "answer": "  [{\"URL\": \"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 12361,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Code is included for training the PredNet on the raw KITTI http://www.cvlibs.net/datasets/kitti/  dataset. We include code for downloading and processing the data, as well as training and evaluating the model. The preprocessed data and can also be downloaded directly using  and the  by running . The model download will include the original weights trained for t+1 prediction, the fine-tuned weights trained to extrapolate predictions for multiple timesteps, and the \"L all \" weights trained with an 0.1 loss weight on upper layers (see paper for details).",
        "answer": "  Sure! Here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/\nLabel: dataset_landing_page\n* URL: http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\nLabel: Software\n* URL: None\nLabel: Other",
        "repoID": 12383,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset http://www.cvlibs.net/datasets/kitti/eval_object.php . A detailed description of Fastbox can be found in our MultiNet paper https://arxiv.org/abs/1612.07695 .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_object.php\nLabel: dataset_landing_page\n* URL: https://arxiv.org/abs/1612.07695\nLabel: paper\n\nNote: I've only annotated the two URLs provided in the input.",
        "repoID": 12453,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_object.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The preprocessed YFCC100M data https://fasttext.cc/docs/en/dataset.html#content  used in [2].",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://fasttext.cc/docs/en/dataset.html', 'label': 'dataset_landing_page'}]",
        "repoID": 12585,
        "URL_gold_label": [
            {
                "URL": "https://fasttext.cc/docs/en/dataset.html#content",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Our method can be evaluated on MOT17 https://motchallenge.net/data/MOT17/ , MOT15 https://motchallenge.net/data/MOT15/  and UA-DETRAC https://detrac-db.rit.albany.edu/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://motchallenge.net/data/MOT17/\nLabel: dataset_direct_link\n* URL: https://motchallenge.net/data/MOT15/\nLabel: dataset_direct_link\n* URL: https://detrac-db.rit.albany.edu/\nLabel: dataset_landing_page",
        "repoID": 12590,
        "URL_gold_label": [
            {
                "URL": "https://motchallenge.net/data/MOT15/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We apply our method to implicit surface reconstruction. We reconstruct the Stanford bunny http://graphics.stanford.edu/data/3Dscanrep/  from noisy surface normals (i.e. gradients), which is difficult to do using traditional spline methods. Note that, as we are using roughly 40 thousand points and gradients, reconstruction will take a bit of time. Run  located in  to recreate the following plot (figure 8 in the appendix).",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://graphics.stanford.edu/data/3Dscanrep/\nLabel: DatasetDirectLink",
        "repoID": 12619,
        "URL_gold_label": [
            {
                "URL": "http://graphics.stanford.edu/data/3Dscanrep/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the MSCOCO2014 dataset here http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 12633,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": We provide ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K/  as an example.",
        "answer": "  [{\"URL\": \"http://groups.csail.mit.edu/vision/datasets/ADE20K/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 12650,
        "URL_gold_label": [
            {
                "URL": "http://groups.csail.mit.edu/vision/datasets/ADE20K/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For detail of the data, see Data https://github.com/NorThanapon/dict-definition/tree/master/data",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/NorThanapon/dict-definition/tree/master/data', 'label': 'dataset_landing_page'}]",
        "repoID": 12705,
        "URL_gold_label": [
            {
                "URL": "https://github.com/NorThanapon/dict-definition/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": " https://pandas.pydata.org",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org\",\n\"label\": \"Software\"\n}",
        "repoID": 12738,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For most dataset, it could be downloaded from https://github.com/harvardnlp/sent-conv-torch/tree/master/data https://github.com/harvardnlp/sent-conv-torch/tree/master/data",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/harvardnlp/sent-conv-torch/tree/master/data', 'label': 'dataset_direct_link'}]",
        "repoID": 12776,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/sent-conv-torch/tree/master/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 12793,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "spyn is build upon numpy http://www.numpy.org/ , sklearn http://scikit-learn.org/stable/ , scipy http://www.scipy.org/ , numba http://numba.pydata.org/ , matplotlib http://matplotlib.org/  and theano http://deeplearning.net/software/theano/ .",
        "answer": "  [{\"URL\": \"http://www.numpy.org/\", \"label\": \"Software\"}, {\"URL\": \"http://scikit-learn.org/stable/\", \"label\": \"Software\"}, {\"URL\": \"http://www.scipy.org/\", \"label\": \"Software\"}, {\"URL\": \"http://numba.pydata.org/\", \"label\": \"Software\"}, {\"URL\": \"http://matplotlib.org/\", \"label\": \"Software\"}, {\"URL\": \"http://deeplearning.net/software/theano/\", \"label\": \"Software\"}]",
        "repoID": 12900,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "If you are new to Python, the easiest way of installing the prerequisites is via conda https://conda.io/docs/index.html . After installing conda http://conda.pydata.org/ , run the following command to create a new environment https://conda.io/docs/user-guide/tasks/manage-environments.html  named  and install all prerequisites:",
        "answer": "  [{\"URL\": \"https://conda.io/docs/index.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 13006,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the datasets of Pascal VOC 2007 & 2012 http://host.robots.ox.ac.uk/pascal/VOC/ , MS COCO 2017 http://cocodataset.org/#download  and COCO-Text http://rrc.cvc.uab.es/?ch=5&com=introduction .",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'http://host.robots.ox.ac.uk/pascal/VOC/', 'label': 'dataset_direct_link'}]\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]\n[{'URL': 'http://rrc.cvc.uab.es/?ch=5&com=introduction', 'label': 'dataset_landing_page'}]",
        "repoID": 13050,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and convert the dataset to 19 categories https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py . It should have this basic structure.",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 13128,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We have successfully trained on Kinetics https://deepmind.com/research/open-source/open-source-datasets/kinetics/ , UCF101 http://crcv.ucf.edu/data/UCF101.php , HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/ , Something-Something-V1 https://20bn.com/datasets/something-something/v1  and V2 https://20bn.com/datasets/something-something/v2 , Jester https://20bn.com/datasets/jester  datasets with this codebase. Basically, the processing of video data can be summarized into 3 steps:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://deepmind.com/research/open-source/open-source-datasets/kinetics/>\nLabel: DatasetDirectLink\n* URL: <http://crcv.ucf.edu/data/UCF101.php>\nLabel: DatasetLandingPage\n* URL: <http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/>\nLabel: DatasetLandingPage\n* URL: <https://20bn.com/datasets/something-something/v1>\nLabel: DatasetDirectLink\n* URL: <https://20bn.com/datasets/something-something/v2>\nLabel: DatasetDirectLink\n* URL: <https://20bn.com/datasets/jester>\nLabel: DatasetLandingPage\n",
        "repoID": 13147,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Extract the dataset  and  inside the folder Dataset. These files are downloadable from Link http://cocodataset.org/#download . Make sure the pre-trained model is present inside the Model folder ('Model/resnet50_csv_50_focal_seen_w2v.h5'). This pre-trained model is trained by focal loss on 65 seen classes without considering any vocabulary metric. This model is available to download from ( Link to pre-trained model for training (h5 format) https://www.dropbox.com/s/dc0vit1dj83rd56/resnet50_csv_50_focal_seen_w2v.h5?dl=0 ). Also, make sure the  folder is already created to store intermediate models of each epoch. Then, run the following commands for training and testing.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://cocodataset.org/#download\nLabel: dataset_direct_link\n* URL: https://www.dropbox.com/s/dc0vit1dj83rd56/resnet50_csv_50_focal_seen_w2v.h5?dl=0\nLabel: dataset_direct_link\n* URL: None (no other URLs were provided in the input text)",
        "repoID": 13170,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The resources required to reproduce results are kept in the directory . For training and testing, we used MSCOCO-2014 train images from  and validation images from . These zipped archives are downloadable from MSCOCO website ( Link http://cocodataset.org/#download ). Please find the exact list of images (with annotations) used for \"training\" in . The lists of images used for \"testing\" different ZSL settings are:",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n* URL: /\nLabel: Other\n* URL: /train\nLabel: DatasetLandingPage\n* URL: /validation\nLabel: DatasetLandingPage",
        "repoID": 13170,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "1.3 Download COCO2017 http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 13221,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  and organize the image files (from the videos) as follows:",
        "answer": "  Sure, here are the annotated URLs:\n\n* UCF101: [{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]\n* HMDB: [{'URL': 'http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/', 'label': 'dataset_landing_page'}]",
        "repoID": 13225,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The model requires MS COCO http://cocodataset.org/#home  and the CocoAPI https://github.com/waleedka/coco  to be added to .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'},\n {'URL': 'https://github.com/waleedka/coco', 'label': 'dataset_direct_link'}]",
        "repoID": 13329,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the dataset (colour images) from here http://www.cvlibs.net/datasets/kitti/eval_odometry.php .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 13349,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "SIVO's localization functionality builds upon ORB_SLAM2. ORB_SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see here #slam-and-localization-modes .",
        "answer": "  Here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page\n* URL: http://vision.in.tum.de/data/datasets/rgbd-dataset\nLabel: dataset_landing_page\n* URL: http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatas",
        "repoID": 13349,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "There are 2 separate weights files. There are files for both  and  Bayesian SegNet trained on the KITTI Semantic Dataset http://www.cvlibs.net/datasets/kitti/eval_semantics.php ; these weights were first trained using the Cityscapes Dataset https://www.cityscapes-dataset.com , and were then fine tuned. All weights have the batch normalization layer merged with the preceding convolutional layer in order to speed up inference.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* \"http://www.cvlibs.net/datasets/kitti/eval_semantics.php\" - label: \"dataset_landing_page\"\n* \"https://www.cityscapes-dataset.com\" - label: \"dataset_landing_page\"\n* \"http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable\" - label: \"Software\"",
        "repoID": 13349,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes is a dataset that can be used to train SegNet/Bayesian SegNet, but a few steps must be done first. You can download the dataset here https://www.cityscapes-dataset.com/  and the Cityscape scripts repo here https://github.com/mcordts/cityscapesScripts . Once downloaded, follow these steps:",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 13350,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "DensePoseData https://drive.google.com/open?id=1WiTLYVIgMyCDENXHPVEWW7qbZ-3EBjbt (using original MSCOCO2017 http://cocodataset.org/#download  images)",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://drive.google.com/open?id=1WiTLYVIgMyCDENXHPVEWW7qbZ-3EBjbt', 'label': 'dataset_direct_link'}]",
        "repoID": 13382,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the VisDial v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://visualdialog.org/data\nLabel: DatasetDirectLink",
        "repoID": 13541,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Prepare the MSCOCO http://cocodataset.org/#download  and Flickr https://visualdialog.org/data  images.",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://visualdialog.org/data', 'label': 'dataset_landing_page'}]",
        "repoID": 13541,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "batra-mlp-lab https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch  provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VisDial v1.0 images from here https://visualdialog.org/data  instead. Extracted features for v1.0 train, val and test are available for download at these links. Note that these files do not contain the bounding box information.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch\nLabel: Software\n* URL: https://visualdialog.org/data\nLabel: Other",
        "repoID": 13541,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Squeeze the best data loading performance of Python with https://github.com/tensorpack/dataflow .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://github.com/tensorpack/dataflow', 'label': 'Software'}]",
        "repoID": 13584,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorpack/dataflow",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Symbolic programming (e.g. ) does not https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions  offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of  with various autoparallelization strategies.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions', 'label': 'dataset_landing_page'}]",
        "repoID": 13584,
        "URL_gold_label": [
            {
                "URL": "https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use cocoapi https://github.com/cocodataset/cocoapi  or poseval https://github.com/leonid-pishchulin/poseval  to evaluate result on the MS COCO or PoseTrack dataset. You have to convert the produced  file to MPII  format to evaluate on MPII dataset following this http://human-pose.mpi-inf.mpg.de/#evaluation .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://github.com/cocodataset/cocoapi\nLabel: DatasetDirectLink\n\n* URL: https://github.com/leonid-pishchulin/poseval\nLabel: DatasetDirectLink\n\n* URL: http://human-pose.mpi-inf.mpg.de/\nLabel: Other",
        "repoID": 13597,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "COCO API https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 13597,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use cocoapi https://github.com/cocodataset/cocoapi  or poseval https://github.com/leonid-pishchulin/poseval  to evaluate result on the MS COCO or PoseTrack dataset. You have to convert the produced  file to MPII  format to evaluate on MPII dataset following this http://human-pose.mpi-inf.mpg.de/#evaluation .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://github.com/cocodataset/cocoapi\nLabel: DatasetDirectLink\n\n* URL: https://github.com/leonid-pishchulin/poseval\nLabel: DatasetDirectLink\n\n* URL: http://human-pose.mpi-inf.mpg.de/\nLabel: Other",
        "repoID": 13598,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "COCO API https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 13598,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The script takes the meshes generated in the previous step and evaluates them using a standardized protocol. The output will be written to /  files in the corresponding generation folder which can be processed using pandas https://pandas.pydata.org/ .",
        "answer": "  [{'URL': '/', 'label': 'dataset_landing_page'}]",
        "repoID": 13604,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 13632,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We first downloaded from Wikidata https://www.wikidata.org  the URIs of all the cities and the associated countries available on GeoNames https://www.geonames.org , as well as their population, if available. To this purpose, we exploited the  script.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.wikidata.org\nLabel: DatasetDirectLink\n* URL: https://www.geonames.org\nLabel: DatasetLandingPage",
        "repoID": 13635,
        "URL_gold_label": [
            {
                "URL": "https://www.wikidata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "KITTI http://www.cvlibs.net/datasets/kitti/ :",
        "answer": "  Sure! Here's the annotation for the URL \"KITTI http://www.cvlibs.net/datasets/kitti/\":\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 13654,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Try it out on NYU-Depth https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html , ScanNet http://www.scan-net.org/ , TUM-RGBD https://vision.in.tum.de/data/datasets/rgbd-dataset , or KITTI http://www.cvlibs.net/datasets/kitti/ . Using more keyframes  reduces drift but results in slower tracking.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html>\nLabel: DatasetLandingPage\n* URL: <http://www.scan-net.org/>\nLabel: DatasetLandingPage\n* URL: <https://vision.in.tum.de/data/datasets/rgbd-dataset>\nLabel: DatasetLandingPage\n* URL: <http://www.cvlibs.net/datasets/kitti/>\nLabel: DatasetLandingPage",
        "repoID": 13654,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": ": Download website Cityscape https://www.cityscapes-dataset.com/ , see dataset preparation code in DA-Faster RCNN https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 13661,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Website Sim10k https://fcav.engin.umich.edu/sim-dataset/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://fcav.engin.umich.edu/sim-dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 13661,
        "URL_gold_label": [
            {
                "URL": "https://fcav.engin.umich.edu/sim-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "All codes are written to fit for the format of PASCAL_VOC. For example, the dataset Sim10k https://fcav.engin.umich.edu/sim-dataset/  is stored as follows.",
        "answer": "  [{\"URL\": \"https://fcav.engin.umich.edu/sim-dataset/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 13661,
        "URL_gold_label": [
            {
                "URL": "https://fcav.engin.umich.edu/sim-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Dataset generated for those papers can be found here https://github.com/v-m/PropagationAnalysis-dataset .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/v-m/PropagationAnalysis-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 13746,
        "URL_gold_label": [
            {
                "URL": "https://github.com/v-m/PropagationAnalysis-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "denotes mAP@0.5:0.95 metric measured on the 5000-image COCO val2017 http://cocodataset.org  dataset over various inference sizes from 256 to 1536.",
        "answer": "  [{\"URL\": \"http://cocodataset.org\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 13818,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "measures average inference time per image on COCO val2017 http://cocodataset.org  dataset using a AWS p3.2xlarge https://aws.amazon.com/ec2/instance-types/p3/  V100 instance at batch-size 32.",
        "answer": "  [{\"URL\": \"http://cocodataset.org\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"https://aws.amazon.com/ec2/instance-types/p3/\", \"label\": \"Other\"}]",
        "repoID": 13818,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "values are for single-model single-scale on COCO val2017 http://cocodataset.org  dataset. Reproduce by",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n* URL: <http://cocodataset.org/>\nLabel: 4. Other (not a dataset or software)",
        "repoID": 13818,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Quicker ADC achieves excellent performance outperforming polysemous codes in numerous configurations. We evaluated its performance for exhaustive search in the SIFT1M http://corpus-texmex.irisa.fr/  dataset with both 64-bit and 128-bit codes.",
        "answer": "  Sure! Here are the annotated URLs for the given input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]",
        "repoID": 13863,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We also evaluated its performance for index-based (i.e., non-exhaustive) search on the SIFT1000M http://corpus-texmex.irisa.fr/  dataset and Deep1B http://sites.skoltech.ru/compvision/noimi/  dataset. For this evaluation, we used three different type of indexes ( standard IVF https://doi.org/10.1109/TPAMI.2010.57 , Inverted Multi-Index https://doi.org/10.1109/TPAMI.2014.2361319 , and HNSW-based IVFs https://arxiv.org/abs/1603.09320 ). For each combination of a product quantizer implementation with an index, we plot the curve that gives the achievable recall (R@1 and R@100) given a query time budget. Quicker ADC is particularly efficient for low query times, R@100 and Deep1B dataset.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://corpus-texmex.irisa.fr/\nLabel: DatasetDirectLink\n* URL: http://sites.skoltech.ru/compvision/noimi/\nLabel: DatasetLandingPage\n* URL: https://doi.org/10.1109/TPAMI.2010.57\nLabel: Software\n* URL: https://doi.org/10.1109/TPAMI.2014.2361319\nLabel: Software\n* URL: https://arxiv.org/abs/1",
        "repoID": 13863,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download DPED dataset http://people.ee.ethz.ch/~ihnatova/#dataset  (patches for CNN training) and extract it into  folder. This folder should contain three subolders: ,  and , but only  is needed.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://people.ee.ethz.ch/~ihnatova/#dataset', 'label': 'dataset_direct_link'}]",
        "repoID": 13969,
        "URL_gold_label": [
            {
                "URL": "http://people.ee.ethz.ch/~ihnatova/#dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "KITTI http://www.cvlibs.net/datasets/kitti/ : copy the raw data to a folder with the path '../kitti'. Our method expects dense input depth maps, therefore, you need to run a depth inpainting method https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html  on the Lidar data. For our experiments, we used our Python re-implmentaiton https://gist.github.com/ialhashim/be6235489a9c43c6d240e8331836586a  of the Matlab code provided with NYU Depth V2 toolbox. The entire 80K images took 2 hours on an 80 nodes cluster for inpainting. For our training, we used the subset defined here https://s3-eu-west-1.amazonaws.com/densedepth/kitti_train.csv .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.cvlibs.net/datasets/kitti/\nLabel: dataset_landing_page\n* URL: https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\nLabel: dataset_direct_link\n* URL: https://gist.github.com/ialhashim/be6235489a9c43c6d240e8331836586a\nLabel: Software\n* URL: https://s3-eu-west-1.amazonaws.com/densedepth/kitti_train.csv\nLabel: Other",
        "repoID": 13972,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download images from COCO website http://cocodataset.org/#download , and put train2014/val2014 splits into  respectively.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 13985,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install COCOAPI referring to cocoapi website https://github.com/cocodataset/cocoapi , or:",
        "answer": "  [{\"URL\": \"https://github.com/cocodataset/cocoapi\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 13985,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "obtain the train and validation images from the 2014 split here http://cocodataset.org/#download , extract and save them in  and",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 14016,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You need to download the notMNIST http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html  dataset from here http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz .",
        "answer": "  [{\"URL\": \"http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 14101,
        "URL_gold_label": [
            {
                "URL": "http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  to YOUR_DATASET_FOLDER. Take MH_01 for example, you can run VINS-Fusion with three sensor types (monocular camera + IMU, stereo cameras + IMU and stereo cameras). Open four terminals, run vins odometry, visual loop closure(optional), rviz and play the bag file respectively. Green path is VIO odometry; red path is odometry under visual loop closure.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nLabel: dataset_landing_page\n\nExplanation: The URL is for a landing page that provides access to a dataset called EuRoC MAV Dataset.",
        "repoID": 14163,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download KITTI Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to YOUR_DATASET_FOLDER. Take sequences 00 for example, Open two terminals, run vins and rviz respectively. (We evaluated odometry on KITTI benchmark without loop closure funtion)",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_direct_link'}]",
        "repoID": 14163,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Download KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  to YOUR_DATASET_FOLDER. Take 2011_10_03_drive_0027_synced https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_sync.zip  for example. Open three terminals, run vins, global fusion and rviz respectively. Green path is VIO odometry; blue path is odometry under GPS global fusion.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: http://www.cvlibs.net/datasets/kitti/raw_data.php\nLabel: dataset_direct_link\n\n* URL: https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_sync.zip\nLabel: dataset_direct_link",
        "repoID": 14163,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We are the  open-sourced stereo algorithm on KITTI Odometry Benchmark http://www.cvlibs.net/datasets/kitti/eval_odometry.php  (12.Jan.2019).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 14163,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Onedata http://onedata.org",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://onedata.org\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 14168,
        "URL_gold_label": [
            {
                "URL": "http://onedata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Download the COCO train2014 and val2014 data here http://cocodataset.org/#download . Put the COCO train2014 images in the folder , and put the file  in the folder . Similarly, put the COCO val2014 images in the folder , and put the file  in the folder . Furthermore, download the pretrained VGG16 net here https://app.box.com/s/idt5khauxsamcg3y69jz13w6sc6122ph  or ResNet50 net here https://app.box.com/s/17vthb1zl0zeh340m4gaw0luuf2vscne  if you want to use it to initialize the CNN part.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n* URL: https://app.box.com/s/idt5khauxsamcg3y69jz13w6sc6122ph\nLabel: DatasetDirectLink\n* URL: https://app.box.com/s/17vthb1zl0zeh340m4gaw0luuf2vscne\nLabel: Software",
        "repoID": 14270,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 14300,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Seaborn https://seaborn.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 14300,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "In our experiments, we used a random subset of the HIGGS dataset from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/HIGGS . This subset can be downloaded in HDF5 format from here https://www.dropbox.com/s/x7qdf9bmsvfezl9/HIGGSsubset.zip?dl=0 . HDF5 format is convenient for sequential tests since it allows constant time sampling.",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* URL: <https://archive.ics.uci.edu/ml/datasets/HIGGS>\nLabel: <dataset_landing_page>\n\n* URL: <https://www.dropbox.com/s/x7qdf9bmsvfezl9/HIGGSsubset.zip?dl=0>\nLabel: <dataset_direct_link>",
        "repoID": 14340,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/HIGGS",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Facebook https://snap.stanford.edu/data/egonets-Facebook.html  (combined network)",
        "answer": "  Sure! Here's the annotation for the URL \"Facebook\":\n\n{\n\"URL\": \"https://snap.stanford.edu/data/egonets-Facebook.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 14353,
        "URL_gold_label": [
            {
                "URL": "https://snap.stanford.edu/data/egonets-Facebook.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download annotation files (2017 train/val and test image info) from coco website http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 14380,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the images (2017 Train, 2017 Val, 2017 Test) from coco website http://cocodataset.org/#download .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 14380,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Currently, models are mostly implemented on Gluon and then ported to other frameworks. Some models are pretrained on ImageNet-1K http://www.image-net.org , CIFAR-10/100 https://www.cs.toronto.edu/~kriz/cifar.html , SVHN http://ufldl.stanford.edu/housenumbers , CUB-200-2011 http://www.vision.caltech.edu/visipedia/CUB-200-2011.html , Pascal VOC2012 http://host.robots.ox.ac.uk/pascal/VOC/voc2012 , ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K , Cityscapes https://www.cityscapes-dataset.com , and COCO http://cocodataset.org  datasets. All pretrained weights are loaded automatically during use. See examples of such automatic loading of weights in the corresponding sections of the documentation dedicated to a particular package:",
        "answer": "  Here are the annotated URLs:\n\n* URL: http://www.image-net.org\nLabel: DatasetDirectLink\n* URL: https://www.cs.toronto.edu/~kriz/cifar.html\nLabel: DatasetDirectLink\n* URL: http://ufldl.stanford.edu/housenumbers\nLabel: DatasetDirectLink\n* URL: http://www.vision.caltech.edu/visipedia/CUB-200-2011.html\nLabel: DatasetDirectLink\n* URL: http://host.robots.ox.ac.uk/pascal/",
        "repoID": 14399,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This dataset is derived from one of the Data-to-Text Datasets https://github.com/harvardnlp/boxscore-data  (RotoWire) proposed in the paper (Wiseman et al., 2017) Challenges in Data-to-Document Generation https://arxiv.org/abs/1707.08052 , which is for NBA game report generation. The original data can be downloaded from here https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2?raw=true .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/harvardnlp/boxscore-data', 'label': 'dataset_landing_page'}]\n\nExplanation: The input URL is a link to a GitHub repository containing a dataset used in a research paper. The label \"dataset_landing_page\" is assigned because the URL is an introduction or a landing page for a dataset entity.",
        "repoID": 14476,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ImageNet needs to be downloaded http://image-net.org/download  manually, due to copyright issues. Facebook has created a set of scripts https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  to help download and extract the dataset.",
        "answer": "  [{\"URL\": \"http://image-net.org/download\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 14478,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 14608,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Otherwise, I started by reading in all the  and  images, around 8000 images in each category. These datasets are comprised of images taken from the GTI vehicle image database http://www.gti.ssr.upm.es/data/Vehicle_database.html  and KITTI vision benchmark suite http://www.cvlibs.net/datasets/kitti/ . Here is an example of one of each of the  and  classes:",
        "answer": "  [{\"URL\": \"http://www.gti.ssr.upm.es/data/Vehicle_database.html\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"http://www.cvlibs.net/datasets/kitti/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 14698,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas\":\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 14774,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "EuRoC: Euroc MAV dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets', 'label': 'dataset_landing_page'}]",
        "repoID": 14850,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Kitti: Kitti odometry http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 14850,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:",
        "answer": "  [{\"URL\": \"http://vision.in.tum.de/data/datasets/rgbd-dataset/tools\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 14851,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset/tools",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page\n* URL: http://vision.in.tum.de/data/datasets/rgbd-dataset\nLabel: dataset_landing_page\n* URL: http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nLabel: dataset_landing_page\n* URL: http://ros.org\nLabel: Other\n* URL:",
        "repoID": 14851,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            },
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you are new to Python, the easiest way of installing the prerequisites is via conda https://conda.io/docs/index.html . After installing conda http://conda.pydata.org/ , run the following command to create a new environment https://conda.io/docs/user-guide/tasks/manage-environments.html  named  and install all prerequisites:",
        "answer": "  [{\"URL\": \"https://conda.io/docs/index.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 15026,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas\":\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 15170,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Given that you have already downloaded the KITTI http://www.cvlibs.net/datasets/kitti/  odometry and raw datasets, the provided python script  is able to generate the training data with SIFT feature matches. Yet, the feature and match files are in accord with our internal format, which are not publicly available at this point. Alternatively, we suggest first generating the concatenated image triplets by",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/', 'label': 'dataset_landing_page'}]",
        "repoID": 15171,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "You need to download KITTI raw dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  first, then the raw data is processed with the following three steps:",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/raw_data.php\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 15223,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the dataset from KITTI Tracking http://www.cvlibs.net/datasets/kitti/eval_tracking.php .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_tracking.php', 'label': 'dataset_direct_link'}]",
        "repoID": 15368,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the vocabulary file from the original visual dialog codebase ( github https://github.com/batra-mlp-lab/visdial/ ). The vocabulary for the VisDial v0.9 dataset is here https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/batra-mlp-lab/visdial/>\nLabel: Software\n\n* URL: <https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json>\nLabel: DatasetDirectLink",
        "repoID": 15446,
        "URL_gold_label": [
            {
                "URL": "https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Besides, the original Amazon datasets (including user-item interaction history and item associations) are provided by Professor Mcauley. You can download them here http://jmcauley.ucsd.edu/data/amazon/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://jmcauley.ucsd.edu/data/amazon/\nLabel: dataset_landing_page",
        "repoID": 15471,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "2.2 Download the KITTI Object Detection Dataset here http://www.cvlibs.net/datasets/kitti/eval_object.php .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_object.php', 'label': 'dataset_direct_link'}]",
        "repoID": 15478,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_object.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download Scene Flow Datasets https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo , KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"Download Scene Flow Datasets\": \"dataset_landing_page\"\n* \"KITTI 2012\": \"dataset_direct_link\"\n* \"KITTI 2015\": \"dataset_direct_link\"",
        "repoID": 15487,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes dataset can be downloaded from the official website https://www.cityscapes-dataset.com/  (registration required).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 15519,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 15614,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 15631,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The following examples are based on the networks provided in the  folder of the project, taken from the SNAP dataset http://snap.stanford.edu/data/index.html .",
        "answer": "  [{\"URL\": \"http://snap.stanford.edu/data/index.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 15759,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the Cityscapes Dataset https://www.cityscapes-dataset.com  as the target domain and unzip it to",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com', 'label': 'dataset_direct_link'}]",
        "repoID": 15793,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "In addition to the datasets we are providing the code for evaluating word embeddings. To run it, you should have Python3, and numpy http://www.numpy.org/ , pandas https://pandas.pydata.org/ , scipy https://www.scipy.org/  libraries installed.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.numpy.org/\nLabel: Software\n* URL: https://pandas.pydata.org/\nLabel: Software\n* URL: https://www.scipy.org/\nLabel: Software",
        "repoID": 15954,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 16059,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/raw_data.php\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 16085,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": ": Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:",
        "answer": "  [INST]\n    Output: [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 16086,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download videos and train/test splits here http://crcv.ucf.edu/data/UCF101.php .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]",
        "repoID": 16113,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "RACE: Please submit a data request here http://www.cs.cmu.edu/~glai1/data/race/ . The data will be automatically sent to you. Create a \"data\" directory alongside \"src\" directory and download the data.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://www.cs.cmu.edu/~glai1/data/race/', 'label': 'dataset_direct_link'}]",
        "repoID": 16122,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cmu.edu/~glai1/data/race/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Then install the NLTK data ( instructions http://www.nltk.org/data.html )",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.nltk.org/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 16181,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Evaluation is done on VisDialv1.0 https://visualdialog.org/data .",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://visualdialog.org/data', 'label': 'dataset_landing_page'}]",
        "repoID": 16372,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "translational error on the KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  odometry sequences with  an Inertial Measurement Unit.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 16383,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "We evaluate our model on images rendered from 3D object models ( ShapeNet https://www.shapenet.org/ ) as well as real and synthesized scenes ( KITTI http://www.cvlibs.net/datasets/kitti/  and Synthia http://synthia-dataset.net/ ). We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.",
        "answer": "  Sure, here are the annotated URLs:\n\n* ShapeNet: <https://www.shapenet.org/> (label: dataset_landing_page)\n* KITTI: <http://www.cvlibs.net/datasets/kitti/> (label: dataset_direct_link)\n* Synthia: <http://synthia-dataset.net/> (label: dataset_landing_page)",
        "repoID": 16403,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "DSM is a novel approach to monocular SLAM. It is a fully direct system that estimates the camera trajectory and a consistent global map. Is is able to detect and handle map point reobservations when revisiting already mapped areas using the same photometric model and map points. We provide examples to run the SLAM system in the EuRoC dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  and with custom videos. We also provide an optional GUI for 3D visualization of the system results.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets>\nLabel: DatasetLandingPage",
        "repoID": 16415,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download a sequence (ASL format) from https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets', 'label': 'dataset_landing_page'}]",
        "repoID": 16415,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ETHZ EuroC Dataset https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
        "answer": "  [{\"URL\": \"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 16448,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The small graphs used in our experiments can be obtained from the Stanford SNAP http://snap.stanford.edu/data/index.html  repository. We recommend using the soc-LiveJournal graph, and have provided a python script to download this graph, symmetrize it, and store it in the the format(s) used by Aspen. This can be done using the SNAPToAdj software in the Ligra https://github.com/jshun/ligra  repository (see ).",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://snap.stanford.edu/data/index.html\nLabel: dataset_landing_page\n\n* URL: http://github.com/jshun/ligra\nLabel: Software\n\nNote: The URL \"http://snap.stanford.edu/data/index.html\" is a dataset landing page, while \"http://github.com/jshun/ligra\" is a software repository.",
        "repoID": 16529,
        "URL_gold_label": [
            {
                "URL": "http://snap.stanford.edu/data/index.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 16562,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 16567,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 16578,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This version of the app assumes the LiDAR data to be stored in a binary float matrix (.bin extension). Each column is a point, where the rows are in the following order: x, y, z, and intensity (little endian). See the 3D Velodyne point clouds in KITTI's dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  for example.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/raw_data.php\nLabel: dataset_landing_page",
        "repoID": 16582,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "On the other hand, using the downsample layer provided in DispNet does not affect the performance of the network. In fact, the network definition  (with downsample layer) can produced a  error of 2.67% on the KITTI stereo 2015 leaderboard http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo , exactly the same as our original CRL with the in-house interpolation layer.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo\nLabel: Other",
        "repoID": 16660,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the Cityscapes Dataset https://www.cityscapes-dataset.com/  as target dataset",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 16675,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The simplest way to get started is to use Anaconda https://www.continuum.io/anaconda-overview  Python distribution. If you have limited disk space, the Miniconda http://conda.pydata.org/miniconda.html  installer is recommended. After installing Miniconda and adding the path of folder  to  variable, run the following command:",
        "answer": "  [{'URL': 'https://www.continuum.io/anaconda-overview', 'label': 'dataset_landing_page'}]",
        "repoID": 16895,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The provided code is not optimized, nor in an easy-to-read shape. It is provided \"as is\", as a prototype implementation of our paper. Use it at your own risk. Moreover, compared to the paper, this implementation lacks the features that make it able to deal with invalid measurements. Therefore, it will not produce good models for the TUM RGB-D Benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  scenes. To test it, please use our Bonn RGB-D Dynamic Dataset http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: \"https://vision.in.tum.de/data/datasets/rgbd-dataset\"\nLabel: \"dataset_landing_page\"\n\n* URL: \"http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/\"\nLabel: \"dataset_landing_page\"",
        "repoID": 16961,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "where  is the path to the directory of a dataset in the format of the TUM RGB-D Benchmark https://vision.in.tum.de/data/datasets/rgbd-dataset  (e.g. ). Some example datasets can be found here http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/ .",
        "answer": "  [{\"URL\": \"http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 16961,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you'd like to train LightTrack, download the COCO dataset http://cocodataset.org/#download  and the PoseTrack dataset https://posetrack.net/users/download.php  first. Note that this script will take a while and dump 21gb of files into . For PoseTrack dataset, you can replicate our ablation experiment results on the validation set. You will need to register at the official website and create entries in order to submit your test results to the server.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n* URL: https://posetrack.net/users/download.php\nLabel: DatasetDirectLink",
        "repoID": 16998,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "SICK http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools', 'label': 'dataset_landing_page'}]",
        "repoID": 17206,
        "URL_gold_label": [
            {
                "URL": "http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "List of Datasets https://www.audiocontentanalysis.org/data-sets/  by Alexander Lerch",
        "answer": "  Sure, here are the annotations for the URLs in the input:\n\n[{'URL': 'https://www.audiocontentanalysis.org/data-sets/', 'label': 'dataset_landing_page'}]",
        "repoID": 17228,
        "URL_gold_label": [
            {
                "URL": "https://www.audiocontentanalysis.org/data-sets/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Amazon Book Data http://jmcauley.ucsd.edu/data/amazon/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://jmcauley.ucsd.edu/data/amazon/', 'label': 'dataset_landing_page'}]",
        "repoID": 17427,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the raw data of KITTI Dataset http://www.cvlibs.net/datasets/kitti/raw_data.php . This dataset is for training and eigen split evaluation.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_direct_link'}]",
        "repoID": 17429,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download KITTI Odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to YOUR_DATASET_FOLDER and set the  and  parameters in  file. Note you also convert KITTI dataset to bag file for easy use by setting proper parameters in .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_direct_link'}]",
        "repoID": 17445,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ADE20K http://groups.csail.mit.edu/vision/datasets/ADE20K/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://groups.csail.mit.edu/vision/datasets/ADE20K/\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 17543,
        "URL_gold_label": [
            {
                "URL": "http://groups.csail.mit.edu/vision/datasets/ADE20K/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 17543,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "NYUDv2 http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 17543,
        "URL_gold_label": [
            {
                "URL": "http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To run boosting experimets, create the folder  and download there  from UCI https://archive.ics.uci.edu/ml/datasets/HIGGS",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'https://archive.ics.uci.edu/ml/datasets/HIGGS', 'label': 'dataset_direct_link'}]",
        "repoID": 17676,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/HIGGS",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Configure COCOAPI https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 17791,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This repository is in large parts based on Multimodallearning's Mask_RCNN https://github.com/multimodallearning/pytorch-mask-rcnn , we also borrow the amodal evaluation code from AmodalMask https://github.com/Wakeupbuddy/amodalAPI  and COCO API https://github.com/cocodataset/cocoapi . The training and evaluation dataset are referenced from COCOA https://arxiv.org/abs/1509.01329  and D2SA https://arxiv.org/abs/1804.08864 . We would like to thank each of them for their kindly work.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: https://github.com/multimodallearning/pytorch-mask-rcnn\nLabel: DatasetDirectLink\n* URL: https://github.com/Wakeupbuddy/amodalAPI\nLabel: Other\n* URL: https://github.com/cocodataset/cocoapi\nLabel: Software\n* URL: https://arxiv.org/abs/1509.01329\nLabel: DatasetLandingPage\n* URL: https://arxiv.org/abs/1804.08864\nLabel: DatasetLandingPage",
        "repoID": 17791,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We modify the COCOAPI https://github.com/cocodataset/cocoapi  to meet our need. Here you have to soft link the pycocotool to the root directory for invoking.",
        "answer": "  [{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 17791,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To train the SAN-PG, we synthesize a Paired-Image-Texture dataset (PIT dataset), based on SURREAL dataset https://www.di.ens.fr/willow/research/surreal/ , for the purpose of providing the image pairs, i.e., the person image and its texture image. The texture image stores the RGB texture of the full person 3D surface. In particular, we use 929 raster-scanned texture maps provided by the SURREAL dataset to generate the image pairs. On SURREAL, all faces in the texture image are replaced by an average face of either man or woman. We generate 9,290 different meshes of diverse poses/shapes/viewpoints. For each texture map, we assign 10 different meshes and render these 3D meshes with the texture image. Then we obtain in total 9,290 different synthesized (person image, texture image) pairs. To simulate real-world scenes, the background images for rendering are randomly sampled from COCO dataset http://cocodataset.org/#home . Each synthetic person image is centered on a person with resolution 256x128. The resolution of the texture images is 256x256. The PIT dataset can be downloaded from here https://drive.google.com/file/d/1-ndIFhppMG_zjHCRfrnWRvbRQZObw2tT/view?usp=sharing .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.di.ens.fr/willow/research/surreal/\nLabel: DatasetLandingPage\n* URL: http://cocodataset.org",
        "repoID": 17818,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 17831,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/ Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast-approximate-string-matching/ Very fast Data cleaning of product names, company names & street names https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/ Sub-millisecond compound aware automatic spelling correction https://seekstorm.com/blog/sub-millisecond-compound-aware-automatic.spelling-correction/ SymSpell vs. BK-tree: 100x faster fuzzy string search & spell checking https://seekstorm.com/blog/symspell-vs-bk-tree/ Fast Word Segmentation for noisy text https://seekstorm.com/blog/fast-word-segmentation-noisy-text/ The Pruning Radix Trie \u2014 a Radix trie on steroids https://seekstorm.com/blog/pruning-radix-trie/",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: 1000x Faster Spelling Correction algorithm https://seekstorm.com/blog/1000x-spelling-correction/\n\nOutput: [{'URL': 'https://seekstorm.com/blog/1000x-spelling-correction/', 'label': 'Software'}]\n\nInput: Fast approximate string matching with large edit distances in Big Data https://seekstorm.com/blog/fast",
        "repoID": 17886,
        "URL_gold_label": [
            {
                "URL": "https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Frequency dictionaries in many other languages can be found here: FrequencyWords repository https://github.com/hermitdave/FrequencyWords Frequency dictionaries https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries Frequency dictionaries https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data",
        "answer": "  Sure, here are the annotated URLs:\n\n* [{'URL': 'https://github.com/hermitdave/FrequencyWords', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://github.com/hermitdave/FrequencyWords', 'label': 'Software'}]",
        "repoID": 17886,
        "URL_gold_label": [
            {
                "URL": "https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "Google Books Ngram data http://storage.googleapis.com/books/ngrams/books/datasetsv2.html (License) https://creativecommons.org/licenses/by/3.0/  : Provides representative word frequencies",
        "answer": "  Sure, here's the annotation for the input:\n\n[{'URL': 'http://storage.googleapis.com/books/ngrams/books/datasetsv2.html', 'label': 'dataset_direct_link'}]",
        "repoID": 17886,
        "URL_gold_label": [
            {
                "URL": "http://storage.googleapis.com/books/ngrams/books/datasetsv2.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Create  and download the KITTI raw http://www.cvlibs.net/datasets/kitti/raw_data.php  dataset using download_kitti_raw.py https://github.com/fregu856/evaluating_bdl/blob/master/depthCompletion/utils/download_kitti_raw.py .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]",
        "repoID": 18040,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the Cityscapes https://www.cityscapes-dataset.com/  dataset and place it in  (  should contain the folders  and ).",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 18040,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/  0.9.0",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://seaborn.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 18059,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Install COCOAPI https://github.com/cocodataset/cocoapi :",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_direct_link'}]",
        "repoID": 18175,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The boxscore-data json files can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://github.com/harvardnlp/boxscore-data', 'label': 'dataset_direct_link'}]",
        "repoID": 18198,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The boxscore-data associated with the above paper can be downloaded from the boxscore-data repo https://github.com/harvardnlp/boxscore-data , and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: <https://github.com/harvardnlp/boxscore-data>\nLabel: DatasetDirectLink",
        "repoID": 18199,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "models and results reflecting the newly cleaned up data in the boxscore-data repo https://github.com/harvardnlp/boxscore-data  are now given below.",
        "answer": "  [{\"URL\": \"https://github.com/harvardnlp/boxscore-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 18199,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For detailed instructions, please refer to the documentation under . We provide several launch files under  for different datasets, including the EuRoC http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  datasets. These can be used as a reference for customization according to specific needs.",
        "answer": "  [INST]\n    Output:\n    [\n        {\n            \"URL\": \"http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\",\n            \"label\": \"dataset_landing_page\"\n        }\n    ]",
        "repoID": 18205,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/index.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://seaborn.pydata.org/index.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 18315,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/index.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the coco2014 dataset(train and val) from here http://cocodataset.org/#download . You should put the folder  and  to the directory",
        "answer": "  [{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 18367,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ":warning: : On the fly data processing [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
        "answer": "  [{\"URL\": \"https://opennmt.net/OpenNMT-py/FAQ.html\", \"label\": \"Other\"}]",
        "repoID": 18681,
        "URL_gold_label": [
            {
                "URL": "[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 18714,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "To get a bag file from the Event-Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html :",
        "answer": "  [{\"URL\": \"http://rpg.ifi.uzh.ch/davis_data.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 18796,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/davis_data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Alternatively, you can also play a rosbag file. You can use rosbags from the from the the Event Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html , e.g. here http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.bag .",
        "answer": "  [{'URL': 'http://rpg.ifi.uzh.ch/davis_data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 18797,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/davis_data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We provide a minimal example to process events from a plain text file. You can use the event text files from the the Event Camera Dataset http://rpg.ifi.uzh.ch/davis_data.html , e.g. here http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.zip .",
        "answer": "  [{'URL': 'http://rpg.ifi.uzh.ch/davis_data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 18797,
        "URL_gold_label": [
            {
                "URL": "http://rpg.ifi.uzh.ch/davis_data.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes https://www.cityscapes-dataset.com/  + scripts https://github.com/mcordts/cityscapesScripts  (if you want to evaluate the model)",
        "answer": "  Sure, here are the annotations for the URLs you provided:\n\n* URL: <https://www.cityscapes-dataset.com/>\nLabel: dataset_landing_page\n\n* URL: <https://github.com/mcordts/cityscapesScripts>\nLabel: software",
        "repoID": 18804,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You need to download the Cityscapes https://www.cityscapes-dataset.com/ , LIP http://sysu-hcp.net/lip/  and PASCAL-Context https://cs.stanford.edu/~roozbeh/pascal-context/  datasets.",
        "answer": "  Sure, here are the annotations for the URLs you provided:\n\n* URL: <https://www.cityscapes-dataset.com/>\nLabel: DatasetDirectLink\n* URL: <http://sysu-hcp.net/>\nLabel: DatasetLandingPage\n* URL: <https://cs.stanford.edu/~roozbeh/pascal-context/>\nLabel: DatasetLandingPage",
        "repoID": 18846,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Roaring bitmaps are compressed bitmaps which tend to outperform conventional compressed bitmaps such as WAH, EWAH or Concise. They are used by several major systems such as Apache Lucene https://lucene.apache.org/  and derivative systems such as Solr https://lucene.apache.org/solr/  and Elasticsearch https://www.elastic.co/products/elasticsearch , Metamarkets' Druid http://druid.io/ , LinkedIn Pinot http://github.com/linkedin/pinot/wiki , Netflix Atlas https://github.com/Netflix/atlas , Apache Spark https://spark.apache.org/ , OpenSearchServer http://www.opensearchserver.com , Cloud Torrent https://github.com/jpillora/cloud-torrent , Whoosh https://bitbucket.org/mchaput/whoosh/wiki/Home , InfluxDB https://www.influxdata.com , Pilosa https://www.pilosa.com/ , Bleve http://www.blevesearch.com , Microsoft Visual Studio Team Services (VSTS) https://www.visualstudio.com/team-services/ , and eBay's Apache Kylin http://kylin.apache.org/ . The CRoaring library is used in several systems such as Apache Doris http://doris.incubator.apache.org . The YouTube SQL Engine, Google Procella https://research.google/pubs/pub48388/ , uses Roaring bitmaps for indexing.",
        "answer": "  Here are the annotated URLs in the input text:\n\n1. \"Apache Lucene\": [{\"URL\": \"https://lucene.apache.org/\", \"label\":",
        "repoID": 18966,
        "URL_gold_label": [
            {
                "URL": "https://www.influxdata.com",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download coco http://cocodataset.org/#download  dataset and extract the images to",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 19018,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 19197,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org  (0.23.0)",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 19503,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Since this script simply removed duplicate records from the original dataset, the data format is the same as that of the original rotowire dataset. Please refer to boxscore-data https://github.com/harvardnlp/boxscore-data  repo.",
        "answer": "  [{\"URL\": \"https://github.com/harvardnlp/boxscore-data\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 19630,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "You can download the original dataset (Wiseman'2017) from the boxscore-data https://github.com/harvardnlp/boxscore-data  repo, and then, transform it as bellow. The script will create a directory \"rotowire-modified\", which contains train.json, valid.json, and test.json files.",
        "answer": "  [{'URL': 'https://github.com/harvardnlp/boxscore-data', 'label': 'dataset_direct_link'}]",
        "repoID": 19630,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Run the following from the home directory of this repository to install python dependencies, download BAM models, download MSCOCO http://cocodataset.org  and MiniPlaces https://github.com/CSAILVision/miniplaces , and construct BAM dataset.",
        "answer": "  [{'URL': 'http://cocodataset.org', 'label': 'dataset_direct_link'}]",
        "repoID": 19632,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Oxford flower 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 19696,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "See  and  for further testing which ensures that place recognition performance is consistent between the Matlab and Python implementations. This test requires the grayscale odometry data of KITTI http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to be linked in the main folder of the repo.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_direct_link",
        "repoID": 19719,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "COCO API https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 19734,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "COCO API https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 19735,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Install http://conda.pydata.org/miniconda.html  on your computer, by selecting the latest Python version for your operating system. If you already have  or  installed, you should be able to skip this step and move on to step 2.",
        "answer": "  [{\"URL\": \"http://conda.pydata.org/miniconda.html\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 19817,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "miniconda http://conda.pydata.org/miniconda.html  on your machine. Detailed instructions:",
        "answer": "  [{\"URL\": \"http://conda.pydata.org/miniconda.html\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 19817,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download MS COCO, ImageNet2012, NUS_WIDE in their official website: COCO http://cocodataset.org/#download , ImageNet http://image-net.org/download-images , NUS_WIDE https://lms.comp.nus.edu.sg/research/NUS-WIDE.htm . Unzip all data and put in 'data/dataset_name/'.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n\n* URL: http://image-net.org/download-images\nLabel: DatasetDirectLink\n\n* URL: https://lms.comp.nus.edu.sg/research/NUS-WIDE.htm\nLabel: DatasetLandingPage",
        "repoID": 19905,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Amazon Product Data http://jmcauley.ucsd.edu/data/amazon/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://jmcauley.ucsd.edu/data/amazon/', 'label': 'dataset_landing_page'}]",
        "repoID": 20129,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org/  Python library v. 20.3+",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 20147,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the dataset from the Cityscapes dataset server( Link https://www.cityscapes-dataset.com/ ). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 20201,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/  (0.22.0; for DailyDialog dataset)",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 20208,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "NILMTK is a toolkit designed to help  evaluate the accuracy of NILM algorithms. If you are a new Python user, it is recommended to educate yourself on Pandas https://pandas.pydata.org/ , Pytables http://www.pytables.org/  and other tools from the Python ecosystem.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://nilmtk.readthedocs.io/', 'label': 'dataset_landing_page'}]",
        "repoID": 20319,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Microsoft COCO http://cocodataset.org/#home  - 80 common object categories",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 20330,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the MS COCO train2014, val2014, and test2015 images from MS COCO official website http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 20335,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Run on the whole MS COCO http://cocodataset.org  and Visual Genome https://visualgenome.org/  related datasets (i.e., VQA https://visualqa.org/ , GQA https://cs.stanford.edu/people/dorarad/gqa/index.html , COCO caption http://cocodataset.org/#captions-2015 , VG Caption https://visualgenome.org/ , VG QA https://github.com/yukezhu/visual7w-toolkit ). Here, we take a simple single-stage pre-training strategy (20 epochs with all pre-training tasks) rather than the two-stage strategy in our paper (10 epochs without image QA and 10 epochs with image QA). The pre-training finishes in  on . By the way, I hope that my experience experience_in_pretraining.md  in this project would help anyone with limited computational resources.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org\nLabel: dataset_landing_page\n* URL: https://visualgenome.org\nLabel: dataset_landing_page\n* URL: https://visualqa.org\nLabel: dataset_direct_link\n* URL: https://cs.stanford.edu/people/dorarad/gqa/index.html\nLabel: dataset_landing_page\n* URL: http://cocodataset.org/captions-2015\nLabel: dataset_direct_link\n* URL: https://visualgenome.org/\nLabel: dataset_direct_link\n* URL: https://github.com/yukezhu",
        "repoID": 20335,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We thank all the authors and annotators of vision-and-language datasets (i.e., MS COCO http://cocodataset.org/#home , Visual Genome https://visualgenome.org/ , VQA https://visualqa.org/ , GQA https://cs.stanford.edu/people/dorarad/gqa/ , NLVR2 http://lil.nlp.cornell.edu/nlvr/  ), which allows us to develop a pre-trained model for vision-and-language tasks.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* MS COCO: <http://cocodataset.org/#home> (label: 'dataset_landing_page')\n* Visual Genome: <https://visualgenome.org/> (label: 'dataset_landing_page')\n* VQA: <https://visualqa.org/> (label: 'dataset_landing_page')\n* GQA: <https://cs.stanford.edu/people/dorarad/gqa/> (label: 'dataset_landing_page')\n* NLVR2: <http://lil.nlp.cornell.edu/nlvr/> (label: 'dataset_landing_page')\n\nNote that I only annotated the URLs in the input text, and did not provide any additional information or context.",
        "repoID": 20335,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The following script finetunes the BERT model for evaluation on the RACE dataset http://www.cs.cmu.edu/~glai1/data/race/ . The  and  directory contain the RACE dataset as separate  files. Note that for RACE, the batch size is the number of RACE query's to evaluate. Since each RACE query has four samples, the effective batch size passed through the model will be four times the batch size specified on the command line.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://www.cs.cmu.edu/~glai1/data/race/\nLabel: dataset_landing_page",
        "repoID": 20391,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.cmu.edu/~glai1/data/race/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .",
        "answer": "  [{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 20418,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download Cityscapes https://www.cityscapes-dataset.com/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 20475,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The experiments were done on KITTI Tracking dataset http://www.cvlibs.net/datasets/kitti/eval_tracking.php .",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_tracking.php', 'label': 'dataset_landing_page'}]",
        "repoID": 20560,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For this evaluation, the KITTI odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and  zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_odometry.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 20561,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The train/test/validation splits are defined in the  folder. By default, the code will train a depth model using Zhou's subset https://github.com/tinghuiz/SfMLearner  of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new benchmark split http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction  or the odometry split http://www.cvlibs.net/datasets/kitti/eval_odometry.php  by setting the  flag.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/tinghuiz/SfMLearner\nLabel: DatasetDirectLink\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction\nLabel: DatasetLandingPage\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: Other",
        "repoID": 20561,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "You can download the entire raw KITTI dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  by running:",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_direct_link'}]",
        "repoID": 20561,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "March 5, 2020: An extended version of the network has been released(Complete implemenation for SKin Lesion Segmentation on ISIC 217 https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a , Skin Lesion Segmentation PH2 Dataset https://www.fc.up.pt/addi/ph2%20database.html  and cell nuclei   along with the network implementation will be update soon).",
        "answer": "Sure, here are the annotated URLs in the input text:\n\n* URL: https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a\nLabel: dataset_direct_link\n* URL: https://www.fc.up.pt/addi/ph2 database.html\nLabel: dataset_landing_page",
        "repoID": 20655,
        "URL_gold_label": [
            {
                "URL": "https://www.fc.up.pt/addi/ph2%20database.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "(optional) For the \"End-to-end + Visual\" baseline, first download images from VisDial https://visualdialog.org/data  to the  folder, then run  to get image features.",
        "answer": "  [{'URL': 'https://visualdialog.org/data', 'label': 'dataset_direct_link'}]",
        "repoID": 20681,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The scripts use 3 different datasets:  is available from the SNAP dataset collection here http://snap.stanford.edu/data/CollegeMsg.html , while the  dataset can be found here https://www.cs.cornell.edu/~arb/data/temporal-reddit-reply/index.html . Finally, the household power consumption dataset has been downloaded from the UCI ML repository, and is available here https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption .",
        "answer": "  Sure, here are the annotated URLs:\n\n* [{'URL': 'http://snap.stanford.edu/data/CollegeMsg.html', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://www.cs.cornell.edu/~arb/data/temporal-reddit-reply/index.html', 'label': 'dataset_landing_page'}]\n* [{'URL': 'https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption', 'label': 'dataset_landing_page'}]",
        "repoID": 20741,
        "URL_gold_label": [
            {
                "URL": "https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "This repo contains code for Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time) https://www.aclweb.org/anthology/D19-1310.pdf  (Gong, H., Feng, X., Qin, B., & Liu, T.; EMNLP 2019); this code is based on data2text-plan-py https://github.com/ratishsp/data2text-plan-py .",
        "answer": "  Sure! Here are the annotated URLs in the input:\n\n* URL: https://www.aclweb.org/anthology/D19-1310.pdf\nLabel: dataset_landing_page\n* URL: https://github.com/ratishsp/data2text-plan-py\nLabel: Software",
        "repoID": 20837,
        "URL_gold_label": [
            {
                "URL": "https://github.com/ratishsp/data2text-plan-py",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'Software'}]",
        "repoID": 20867,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the COCO 2014 dataset from here http://cocodataset.org/#download . In particualr, you'll need the 2014 Training and Validation images.  Then download Karpathy's Train/Val/Test Split. You may download it from here http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n* URL: http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\nLabel: DatasetDirectLink",
        "repoID": 20895,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you want to do evaluation on COCO, download the COCO API from here https://github.com/cocodataset/cocoapi  if your on Linux or from here https://github.com/philferriere/cocoapi  if your on Windows. Then download the COCO caption toolkit from here https://github.com/tylin/coco-caption  and re-name the folder to . Don't forget to download java as well. Simply dowload it from here https://www.java.com/en/download/  if you don't have it.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/cocodataset/cocoapi\nLabel: dataset_direct_link\n* URL: https://github.com/philferriere/cocoapi\nLabel: dataset_direct_link\n* URL: https://github.com/tylin/coco-caption\nLabel: dataset_landing_page\n* URL: https://www.java.com/en/download/\nLabel: Other",
        "repoID": 20895,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The RRC detection are obtained from the link https://drive.google.com/file/d/1ZR1qEf2qjQYA9zALLl-ZXuWhqG9lxzsM/view  provided by MOTBeyondPixels https://github.com/JunaidCS032/MOTBeyondPixels . We use RRC detection for the KITTI Tracking Benchmark http://www.cvlibs.net/datasets/kitti/eval_tracking.php .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://drive.google.com/file/d/1ZR1qEf2qjQYA9zALLl-ZXuWhqG9lxzsM/view\nLabel: dataset_direct_link\n\nNote: The URL is for downloading a dataset file (RRC detection) provided by MOTBeyondPixels.",
        "repoID": 20966,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We also provide the data split used in our paper in the  directory. You need to download and unzip the data from the KITTI Tracking Benchmark http://www.cvlibs.net/datasets/kitti/eval_tracking.php  and put them in the  directory or any path you like. Do remember to change the path in the configs.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_tracking.php\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 20966,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "download  dataset from here https://github.com/dragen1860/LearningToCompare-Pytorch/issues/4 , splitting:  from here https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://github.com/dragen1860/LearningToCompare-Pytorch/issues/4', 'label': 'dataset_direct_link'}]\n\nNote: The URL \"https://github.com/dragen1860/LearningToCompare-Pytorch/issues/4\" is a direct link to a dataset.",
        "repoID": 21020,
        "URL_gold_label": [
            {
                "URL": "https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'Software'}]",
        "repoID": 21092,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Get ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily.",
        "answer": "  [{\"URL\": \"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md\", \"label\": \"Software\"}]",
        "repoID": 21133,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": indexed files containing the correspondences between audio files and their ground truth. Index files for the MagnaTagATune https://github.com/keunwoochoi/magnatagatune-list  dataset ( ) and the Million Song Dataset https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split  ( ) are already provided.",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: indexed files containing the correspondences between audio files and their ground truth.\n\nOutput:\n\n[{\"URL\": \"https://github.com/keunwoochoi/magnatagatune-list\", \"label\": \"dataset_landing_page\"},\n {\"URL\": \"https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 21180,
        "URL_gold_label": [
            {
                "URL": "https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "If you run the code yourself, you can define (a) what Wikipedia languages to cover, and (b) which specific Wikipedia https://www.wikipedia.org/ , Wikidata https://www.wikidata.org , and Wikimedia Commons https://commons.wikimedia.org  snapshots should be used during the build.",
        "answer": "  [INST]\n    Output:\n    [\n        {\"URL\": \"https://www.wikipedia.org/\", \"label\": \"Other\"},\n        {\"URL\": \"https://www.wikidata.org/\", \"label\": \"Other\"},\n        {\"URL\": \"https://commons.wikimedia.org/\", \"label\": \"Other\"}\n    ]",
        "repoID": 21228,
        "URL_gold_label": [
            {
                "URL": "https://www.wikidata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Wikidata https://www.wikidata.org : the latest version https://dumps.wikimedia.org/wikidatawiki/entities/20170612/wikidata-20170612-all-BETA.ttl.bz2  of .",
        "answer": "  [{\"URL\": \"https://www.wikidata.org\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 21228,
        "URL_gold_label": [
            {
                "URL": "https://www.wikidata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/  - helper data manipulation library",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas https://pandas.pydata.org/\":\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 21248,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/  - helper plotting library for some charts",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 21248,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KittiSeg performs segmentation of roads by utilizing an FCN based model. The model achieved first place http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df  on the Kitti Road Detection Benchmark at submission time. Check out our paper https://arxiv.org/abs/1612.07695  for a detailed model description.",
        "answer": "  Sure! Here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df\nLabel: dataset_landing_page\n* URL: https://arxiv.org/abs/1612.07695\nLabel: Software",
        "repoID": 21260,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": ": Download the coco images and annotations from coco website http://cocodataset.org/#download .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 21328,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi  \u2014 for COCO dataset, also available from pip.",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://github.com/cocodataset/cocoapi\",\n\"label\": \"Software\"\n}",
        "repoID": 21328,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Luke Gessler has written a module for the Pepper http://corpus-tools.org/pepper/  tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See instructions for converting https://github.com/nert-nlp/streusle-pepper-importer .",
        "answer": "  [{'URL': 'https://github.com/nert-nlp/streusle-pepper-importer', 'label': 'dataset_landing_page'}]",
        "repoID": 21346,
        "URL_gold_label": [
            {
                "URL": "http://corpus-tools.org/pepper/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": ": We execute correspondence and bidirectional retrieval experiments between the scientific figures and their captions. The corpora used are Scigraph or Semantic Scholar. Also, for the bidirectional retrieval task we support Coco http://cocodataset.org/#download (2014) and Flickr30k http://shannon.cs.illinois.edu/DenotationGraph/ . To use Flickr30k/Coco, download the images from their repositories, resize them to 224x224 resolution, and leave the resulting images in folders \"look_read_and_enrich/images/coco/\" and \"look_read_and_enrich/images/flickr30k/\".",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#download\nLabel: dataset_direct_link\n* URL: http://shannon.cs.illinois.edu/DenotationGraph/\nLabel: dataset_landing_page",
        "repoID": 21365,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for logging to csv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 21556,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download https://www.cityscapes-dataset.com/ , which contains 5,000 annotated images with 2048 \u00d7 1024 resolution taken from real urban street scenes. We resize Cityscapes images to 1024x512 (or 1280x640 which yields sightly better results but costs more time).",
        "answer": "  [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 21651,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install miniconda http://conda.pydata.org/miniconda.html http://conda.pydata.org/miniconda.html",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://conda.pydata.org/miniconda.html', 'label': 'dataset_landing_page'}]",
        "repoID": 21708,
        "URL_gold_label": [
            {
                "URL": "http://conda.pydata.org/miniconda.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "In addition to the real-life tests http://ci-sparklab.mit.edu:8080/job/MIT-SPARK-Kimera/job/master/VIO_20Euroc_20Performance_20Report/  on the Euroc https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  dataset, we use a photo-realistic Unity-based simulator to test Kimera. The simulator provides:",
        "answer": "  [{\"URL\": \"http://ci-sparklab.mit.edu:8080/job/MIT-SPARK-Kimera/job/master/VIO_20Euroc_20Performance_20Report/\", \"label\": \"Software\"}]",
        "repoID": 21835,
        "URL_gold_label": [
            {
                "URL": "https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pycocotools https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools\",\n\"label\": \"Software\"\n}",
        "repoID": 21885,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "You can download CityScapes Datasets https://www.cityscapes-dataset.com/ .Put it in data folder.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 22047,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We use the , which can be downloaded here http://jmcauley.ucsd.edu/data/amazon/ .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://jmcauley.ucsd.edu/data/amazon/', 'label': 'dataset_direct_link'}]",
        "repoID": 22151,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Implementation is now only focusing on AS task with WikiQA https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/  corpus. (I originally tried to deal with PI task with MSRP(Microsoft Research Paraphrase) https://www.microsoft.com/en-us/download/details.aspx?id=52398  corpus but it seems that model doesn't work without external features classifier requires.)",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/', 'label': 'dataset_landing_page'}]",
        "repoID": 22155,
        "URL_gold_label": [
            {
                "URL": "https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Integer sketches should be stored in TEXMEX's bvecs format http://corpus-texmex.irisa.fr/ . That is, the dimension number and feature values for each sketch are interleaved (in little endian), where the number is 4 bytes of size and each feature is 1 byte of size. To convert vectors of real numbers into sketches, you can use consistent_weighted_sampling https://github.com/kampersanda/consistent_weighted_sampling  to generate such a dataset using the GCWS https://doi.org/10.1145/3097983.3098081  algorithm.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://corpus-texmex.irisa.fr/', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL provided is for a dataset landing page, specifically for the TEXMEX dataset.",
        "repoID": 22160,
        "URL_gold_label": [
            {
                "URL": "http://corpus-texmex.irisa.fr/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 22183,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the data from here https://www.cityscapes-dataset.com/ .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 22347,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Follow the instructions here https://www.cityscapes-dataset.com/  to request for the dataset download.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 22347,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 22485,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the GTA5 https://download.visinf.tu-darmstadt.de/data/from_games/  dataset as the source domain, and the Cityscapes https://www.cityscapes-dataset.com/  dataset as the target domain.",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://download.visinf.tu-darmstadt.de/data/from_games/', 'label': 'dataset_direct_link'}]\n\nNote: The URL \"https://download.visinf.tu-darmstadt.de/data/from_games/\" is a direct link to a dataset.",
        "repoID": 22490,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "obtain the train and validation images from the 2014 split here http://cocodataset.org/#download , extract and save them in  and",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 22514,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "bokeh http://bokeh.pydata.org  for training visualization",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://bokeh.pydata.org', 'label': 'Software'}]",
        "repoID": 22701,
        "URL_gold_label": [
            {
                "URL": "http://bokeh.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for logging to csv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 22701,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Evaluation on success rate, robustness and transferability on 1000 ImageNet-Compatible images https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 22712,
        "URL_gold_label": [
            {
                "URL": "https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": ": Download website Cityscapes https://www.cityscapes-dataset.com/ , see dataset preparation code in DA-Faster RCNN https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 22725,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Website Sim10k https://fcav.engin.umich.edu/sim-dataset/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://fcav.engin.umich.edu/sim-dataset/', 'label': 'dataset_landing_page'}]",
        "repoID": 22725,
        "URL_gold_label": [
            {
                "URL": "https://fcav.engin.umich.edu/sim-dataset/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For KITTI http://www.cvlibs.net/datasets/kitti/raw_data.php , first download the dataset using this script http://www.cvlibs.net/download.php?file=raw_data_downloader.zip  provided on the official website of KITTI. Placing the dataset on SSD would increase the training speed.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/raw_data.php\", \"label\": \"dataset_landing_page\"}]\n[{\"URL\": \"http://www.cvlibs.net/download.php?file=raw_data_downloader.zip\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 22943,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Next, download MS COCO 2014 from the official website http://cocodataset.org/#download , extra annotation from here http://datasets.d2.mpi-inf.mpg.de/hosang17cvpr/coco_minival2014.tar.gz  and make symbolic links as follows.",
        "answer": "  [{\"URL\": \"http://cocodataset.org/#download\", \"label\": \"dataset_direct_link\"}, {\"URL\": \"http://datasets.d2.mpi-inf.mpg.de/hosang17cvpr/coco_minival2014.tar.gz\", \"label\": \"other\"}]",
        "repoID": 22943,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "bokeh http://bokeh.pydata.org  for training visualization",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://bokeh.pydata.org', 'label': 'Software'}]",
        "repoID": 22990,
        "URL_gold_label": [
            {
                "URL": "http://bokeh.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for logging to csv",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'Software'}]",
        "repoID": 22990,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the VisDial v1.0 dialog json files and images from here https://visualdialog.org/data .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://visualdialog.org/data', 'label': 'dataset_direct_link'}]",
        "repoID": 23012,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download MSCOCO http://cocodataset.org/#home  datasets and transfer the raw images into , according to the python script .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 23053,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MSCOCO http://cocodataset.org/#home  dataset is applied for the training of the proposed image reconstruction network.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'}]",
        "repoID": 23053,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "ECSSD http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 23329,
        "URL_gold_label": [
            {
                "URL": "http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "First download the Cityscapes dataset https://www.cityscapes-dataset.com/  and then downsample the images and label maps to 1024x512 using bilinear and nearest-neighbor interpolation, respectively ( augmentation https://github.com/Lextal/pspnet-pytorch/blob/master/augmentation.py ). Next either download the pretrained model https://drive.google.com/file/d/1fh2qSLAAMzX0J27Megisehz9tV62zKtv/view?usp=sharing  (FCN-8s trained to segment Cityscapes) or provide your own pretrained model on Cityscapes. Then run  with suitable arguments. Specify the root folder in which the cityscapes dataset is stored using  and the pretrained model's path with . If you are running a targeted experiment, you need to specify a target label map using .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://www.cityscapes-dataset.com/\nLabel: dataset_landing_page\n* URL: https://github.com/Lextal/pspnet-pytorch/blob/master/augmentation.py\nLabel: Other\n* URL: https://drive.google.com/file/d/1fh2qSLAAMzX0J27Megisehz9tV62zKtv/view?usp=sharing\nLabel: Software\n* URL: (not provided)\nLabel: Other",
        "repoID": 23377,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes : link https://www.cityscapes-dataset.com",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com', 'label': 'dataset_landing_page'}]",
        "repoID": 23482,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org  library to be able to save the dataset as a dataframe compatible with MatchZoo https://github.com/NTMC-Community/MatchZoo",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://pandas.pydata.org', 'label': 'Software'}]",
        "repoID": 23581,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download all the relevant datasets including the FlyingChairs dataset https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html , the FlyingThings3D dataset https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html  (we use  following the practice of FlowNet 2.0), the KITTI dataset http://www.cvlibs.net/datasets/kitti/index.php , and the MPI Sintel dataset http://sintel.is.tue.mpg.de/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html\nLabel: dataset_landing_page\n* URL: https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html\nLabel: dataset_landing_page\n* URL: http://www.cvlibs.net/datasets/kitti/index.php\nLabel: dataset_direct_link\n* URL: http://sintel.is.tue.mpg.de/\nLabel: dataset_landing_page",
        "repoID": 23587,
        "URL_gold_label": [
            {
                "URL": "https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the KITTI odometry dataset from here http://www.cvlibs.net/datasets/kitti/eval_odometry.php  - get the color images and ground truth poses. Unzip the data in .",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 23647,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Please download the COCO dataset from cocodataset http://cocodataset.org/#download . If you use  format, please specify  in  files or  if you unzip the downloaded dataset.",
        "answer": "  Sure! Here's the annotated input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 23714,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the VisDial dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://visualdialog.org/data\nLabel: dataset_direct_link",
        "repoID": 23901,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The dataset used in the paper can be downloaded here https://github.com/harvardnlp/boxscore-data . More specifically, you just need to download the RotoWire dataset https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2 :",
        "answer": "  [{'URL': 'https://github.com/harvardnlp/boxscore-data', 'label': 'dataset_direct_link'}]",
        "repoID": 23996,
        "URL_gold_label": [
            {
                "URL": "https://github.com/harvardnlp/boxscore-data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "KITTI Optical Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow  and KITTI Optical Flow 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow",
        "answer": "  Sure! Here are the annotations for the given URLs:\n\nInput: KITTI Optical Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\nOutput: [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\", \"label\": \"dataset_landing_page\"}]\n\nInput: KITTI Optical Flow 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow\nOutput: [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 24048,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The only significant dependency beyond standard packages found in most Python distributions (e.g. numpy, matplotlib etc) is PyTorch https://pytorch.org/  (the experiments were performed with version 1.2.0). Additionally, using the seaborn https://seaborn.pydata.org/  library will make all your plot's nicer!",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://pytorch.org/', 'label': 'Software'}]",
        "repoID": 24073,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pandas http://pandas.pydata.org",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas\":\n\n{\n\"URL\": \"http://pandas.pydata.org\",\n\"label\": \"Software\"\n}",
        "repoID": 24080,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/  version 0.25.3",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas https://pandas.pydata.org/\":\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 24166,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Associate RGB images and depth images using the python script associate.py http://vision.in.tum.de/data/datasets/rgbd-dataset/tools . We already provide associations for some of the sequences in . You can generate your own associations file executing:",
        "answer": "  [{\"URL\": \"http://vision.in.tum.de/data/datasets/rgbd-dataset/tools\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 24237,
        "URL_gold_label": [
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset/tools",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "ORB-SLAM2 is a real-time SLAM library for ,  and  cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the KITTI dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  as stereo or monocular, in the TUM dataset http://vision.in.tum.de/data/datasets/rgbd-dataset  as RGB-D or monocular, and in the EuRoC dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets  as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. . ORB-SLAM2 provides a GUI to change between a  and , see section 9 of this document.",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: dataset_landing_page\n* URL: http://vision.in.tum.de/data/datasets/rgbd-dataset\nLabel: dataset_landing_page\n* URL: http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nLabel: dataset_landing_page\n* URL: http://ros.org\nLabel: Other\n* URL:",
        "repoID": 24237,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            },
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "MS-COCO 2014 http://cocodataset.org/#download  and HPatches http://icvl.ee.ic.ac.uk/vbalnt/hpatches/hpatches-sequences-release.tar.gz  should be downloaded into . The Synthetic Shapes dataset will also be generated there. The folder structure should look like:",
        "answer": "  [INST: 'label', 'URL']\n\n[INST: 'dataset_direct_link', 'http://cocodataset.org/#download']\n[INST: 'dataset_landing_page', 'http://icvl.ee.ic.ac.uk/vbalnt/hpatches/hpatches-sequences-release.tar.gz']",
        "repoID": 24354,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the images (2017 Train and 2017 Val) from here http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 24356,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/ (Installation Help) https://github.com/seemethere/nba_py/wiki/Installing-pandas",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://pandas.pydata.org/', 'label': 'dataset_landing_page'}]\n\nExplanation: The URL \"http://pandas.pydata.org/\" is a landing page for the pandas dataset.",
        "repoID": 24392,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "The speech enhancement dataset used in the work can be found in Edinburgh DataShare http://datashare.is.ed.ac.uk/handle/10283/1942 . :",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://datashare.is.ed.ac.uk/handle/10283/1942', 'label': 'dataset_landing_page'}]",
        "repoID": 24472,
        "URL_gold_label": [
            {
                "URL": "http://datashare.is.ed.ac.uk/handle/10283/1942",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the VisDial v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  directory, for default arguments to work effectively.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://visualdialog.org/data\nLabel: dataset_direct_link",
        "repoID": 24481,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We also provide pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VIsDial v1.0 images from here https://visualdialog.org/data  instead. Extracted features for v1.0 train, val and test are available for download at these links.",
        "answer": "  [{\"URL\": \"https://visualdialog.org/data\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 24481,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas (see https://pandas.pydata.org https://pandas.pydata.org )",
        "answer": "  Sure! Here's the annotation for the URL \"Pandas (see <https://pandas.pydata.org>):\"\n\n{\n\"URL\": \"https://pandas.pydata.org\",\n\"label\": \"Software\"\n}",
        "repoID": 24690,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Visualize the ontology using WebVOWL http://vowl.visualdataweb.org/webvowl/ . It should be enough to click this link http://visualdataweb.de/webvowl/#iri=http://ethon.consensys.net/EthOn.ttl . WebVOWL is also developed on GitHub: https://github.com/VisualDataWeb/WebVOWL",
        "answer": "  [{\"URL\": \"http://vowl.visualdataweb.org/webvowl/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 24821,
        "URL_gold_label": [
            {
                "URL": "http://vowl.visualdataweb.org/webvowl/",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "If the data is not preprocessed as described here https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset , then copy the scripts in  into their corresponding directories. Finally run them each to save each validation files into their category folder. You can specify the dataset path using the system input argument passed to the  file.",
        "answer": "  [{\"URL\": \"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md\", \"label\": \"Software\"}]",
        "repoID": 24937,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Installation requires numpy https://numpy.org/ , pandas https://pandas.pydata.org/ , and matplotlib https://matplotlib.org/ . Some functions will optionally use scipy https://www.scipy.org/  and/or statsmodels https://www.statsmodels.org/  if they are available.",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://numpy.org/', 'label': 'Software'}]\n[{'URL': 'https://pandas.pydata.org/', 'label': 'Software'}]\n[{'URL': 'https://matplotlib.org/', 'label': 'Software'}]\n[{'URL': 'https://www.scipy.org/', 'label': 'Software'}]\n[{'URL': 'https://www.statsmodels.org/', 'label': 'Software'}]",
        "repoID": 25212,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Please note that the split files in  folder are created by Ravi and Larochelle https://openreview.net/pdf?id=rJY0-Kcll  ( GitHub link https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet ). Vinyals et al. http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf  didn't include their split files for mini-ImageNet when they first released their paper, so Ravi and Larochelle https://openreview.net/pdf?id=rJY0-Kcll  created their own splits. Additional split files are provided here https://github.com/yaoyao-liu/mini-imagenet-tools/tree/master/mini_imagenet_split .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://openreview.net/pdf?id=rJY0-Kcll\nLabel: DatasetDirectLink\n* URL: https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet\nLabel: DatasetDirectLink\n* URL: https://openreview.net/pdf?id=rJY0-Kcll\nLabel: DatasetLandingPage\n* URL: https://github.com/yaoyao-liu/mini-imagenet-tools/tree/master/mini_imagenet_split\nLabel: Other",
        "repoID": 25365,
        "URL_gold_label": [
            {
                "URL": "https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "We support LIBSVM datasets which can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . The downloaded file should be unzipped and put in the following folder",
        "answer": "  [{\"URL\": \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 25396,
        "URL_gold_label": [
            {
                "URL": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the COCO train2014 and val2014 data here http://cocodataset.org/#download . Put the COCO train2014 images in the folder , and put the file  in the folder . Similarly, put the COCO val2014 images in the folder , and put the file  in the folder . Furthermore, download the pretrained VGG16 net here https://ucsb.box.com/s/pj4gg3vpei57cf9xewttoqn01qqa3uj4  if you want to use it to initialize the CNN part.",
        "answer": "  Sure, here are the annotated URLs in the input:\n\n* URL: http://cocodataset.org/#download\nLabel: DatasetDirectLink\n* URL: https://ucsb.box.com/s/pj4gg3vpei57cf9xewttoqn01qqa3uj4\nLabel: Software",
        "repoID": 25456,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Oxford Flower 102 http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 25483,
        "URL_gold_label": [
            {
                "URL": "http://www.robots.ox.ac.uk/~vgg/data/flowers/102/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Get dataset from Cityscapes https://www.cityscapes-dataset.com/ , and from Lost and Found http://www.6d-vision.com/lostandfounddataset .",
        "answer": "  Sure, here are the annotations for the given URLs:\n\nInput: Get dataset from Cityscapes https://www.cityscapes-dataset.com/, and from Lost and Found http://www.6d-vision.com/lostandfounddataset.\n\nOutput:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'},\n {'URL': 'http://www.6d-vision.com/lostandfounddataset', 'label': 'dataset_direct_link'}]",
        "repoID": 25630,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The KITTI (raw) dataset used in our experiments can be downloaded from the KITTI website http://www.cvlibs.net/datasets/kitti/raw_data.php . For convenience, we provide the standard splits used for training and evaluation: eigen_zhou https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_zhou_files.txt , eigen_train https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_train_files.txt , eigen_val https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_val_files.txt  and eigen_test https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_test_files.txt , as well as pre-computed ground-truth depth maps: original https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_velodyne.tar.gz  and improved https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_groundtruth.tar.gz . The full KITTI_raw dataset, as used in our experiments, can be directly downloaded here https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar.gz  or with the following command:",
        "answer": "INST",
        "repoID": 25752,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We trained and tested on the KITTI dataset. Download the raw dataset here http://www.cvlibs.net/datasets/kitti/raw_data.php . We provide a dataloader, but we first require that the data be preprocessed. To do so, run  within  (be sure to specify the source and target directory). We preprocessed the data by resizing the images and removing 'static' frames.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://www.cvlibs.net/datasets/kitti/raw_data.php\nLabel: dataset_direct_link",
        "repoID": 25757,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pycocotools https://github.com/cocodataset/cocoapi",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'Software'}]",
        "repoID": 26101,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download images from COCO website http://cocodataset.org/#download , and put train2014/val2014 splits into  respectively.",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 26124,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install COCOAPI referring to cocoapi website https://github.com/cocodataset/cocoapi , or:",
        "answer": "  [{\"URL\": \"https://github.com/cocodataset/cocoapi\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 26124,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We evaludated HANet on Cityscapes https://www.cityscapes-dataset.com/  and BDD-100K https://bair.berkeley.edu/blog/2018/05/30/bdd/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://www.cityscapes-dataset.com/>\nLabel: DatasetLandingPage\n* URL: <https://bair.berkeley.edu/blog/2018/05/30/bdd/>\nLabel: DatasetLandingPage",
        "repoID": 26217,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Additionaly, to use , you will also need coco python api https://github.com/cocodataset/cocoapi . You can get this using",
        "answer": "  [{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_direct_link'}]",
        "repoID": 26336,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the MS-COCO 2014 training set http://cocodataset.org/#download  and unzip it at path .",
        "answer": "  Sure! Here's the annotated output for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 26459,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "StellarGraph is built on TensorFlow 2 https://tensorflow.org/  and its Keras high-level API https://www.tensorflow.org/guide/keras , as well as Pandas https://pandas.pydata.org  and NumPy https://www.numpy.org . It is thus user-friendly, modular and extensible. It interoperates smoothly with code that builds on these, such as the standard Keras layers and scikit-learn http://scikit-learn.github.io/stable , so it is easy to augment the core graph machine learning algorithms provided by StellarGraph. It is thus also easy to install with #installation .",
        "answer": "  Here are the annotated URLs in the input text:\n\n* \"https://tensorflow.org/\" - label: \"Software\"\n* \"https://www.tensorflow.org/guide/keras\" - label: \"Software\"\n* \"https://pandas.pydata.org\" - label: \"Software\"\n* \"https://www.numpy.org\" - label: \"Software\"\n* \"http://scikit-learn.github.io/stable\" - label: \"Software\"",
        "repoID": 26527,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For this evaluation, the KITTI odometry dataset http://www.cvlibs.net/datasets/kitti/eval_odometry.php  and  zip files must be downloaded. As above, we assume that the pngs have been converted to jpgs.",
        "answer": "  [{\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_odometry.php\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 26549,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "The train/test/validation splits are defined in the  folder. By default, the code will train a depth model using Zhou's subset https://github.com/tinghuiz/SfMLearner  of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new benchmark split http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction  or the odometry split http://www.cvlibs.net/datasets/kitti/eval_odometry.php  by setting the  flag.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://github.com/tinghuiz/SfMLearner\nLabel: DatasetDirectLink\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction\nLabel: DatasetLandingPage\n\n* URL: http://www.cvlibs.net/datasets/kitti/eval_odometry.php\nLabel: Other",
        "repoID": 26549,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "You can download the entire raw KITTI dataset http://www.cvlibs.net/datasets/kitti/raw_data.php  by running:",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_direct_link'}]",
        "repoID": 26549,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For COCO 2017, visit this link http://cocodataset.org/#download  and download the 2017 validation images, which is about 1.1GB. Un-tar this file. It will create a folder called , full of JPEGs. After uncompressing this archive, navigate to  and link (or copy) the dataset folder to . The contents of  should now be all of the  files.",
        "answer": "  Sure, here are the annotations for the input:\n\n* URL: http://cocodataset.org/#download\nLabel: dataset_direct_link",
        "repoID": 26603,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Download the train https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json  datasets and move them under",
        "answer": "  [{\"URL\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\", \"label\": \"dataset_direct_link\"}]\n[{\"URL\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 26623,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "FlyingChairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 26634,
        "URL_gold_label": [
            {
                "URL": "https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow  & KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
        "answer": "  Sure! Here are the annotations for the given URLs:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow', 'label': 'dataset_landing_page'}]\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow', 'label': 'dataset_landing_page'}]",
        "repoID": 26634,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Readable code leveraging NumPy https://numpy.org/ , SciPy https://www.scipy.org/scipylib/index.html  and Numba http://numba.pydata.org/",
        "answer": "  [{\"URL\": \"https://numpy.org/\", \"label\": \"Software\"}, {\"URL\": \"https://www.scipy.org/scipylib/index.html\", \"label\": \"Software\"}, {\"URL\": \"http://numba.pydata.org/\", \"label\": \"Software\"}]",
        "repoID": 26687,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KITTI http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 26720,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The training procedure needs gta5(G) https://download.visinf.tu-darmstadt.de/data/from_games/ , synthia_rand_citys(S) https://synthia-dataset.net/downloads/ , idd(I) https://idd.insaan.iiit.ac.in/ , mapillary(M) https://www.mapillary.com/dataset/vistas?pKey=1GyeWFxH_NPIQwgl0onILw  and cityscapes(C) https://www.cityscapes-dataset.com/ . Please download them and put to the same folder which can be specified in . The folder tree structure is as follows:",
        "answer": "  Sure, here are the annotated URLs:\n\nInput: The training procedure needs gta5(G) https://download.visinf.tu-darmstadt.de/data/from_games/, synthia_rand_citys(S) https://synthia-dataset.net/downloads/, idd(I) https://idd.insaan.iiit.ac.in/, mapillary(M) https://www.mapillary.com/dataset/vistas?pKey=1GyeWFxH_NPIQwgl0onILw  and cityscapes(C) https://www.cityscapes-dataset.com/. Please download them and put to the same folder which can be specified in. The folder tree structure is as follows:\n\nOutput:\n\n[{'URL': 'https://download.visinf.tu-darmstadt.de/data/from_games/', 'label': 'dataset_direct_link'},\n {'URL': 'https://synthia-dataset",
        "repoID": 26739,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The Cityscapes dataset can be downloaded at here https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 26758,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "the TUM Vision Groud RGBD datasets https://vision.in.tum.de/data/datasets/rgbd-dataset ;",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'https://vision.in.tum.de/data/datasets/rgbd-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 26774,
        "URL_gold_label": [
            {
                "URL": "https://vision.in.tum.de/data/datasets/rgbd-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Cityscapes & Foggy Cityscapes https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here are the annotations for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 26775,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Events and summaries are utilizing pandas http://pandas.pydata.org/  for data structures and analysis. New metrics can reuse already computed values from depending metrics.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://pandas.pydata.org/\nLabel: Software",
        "repoID": 26870,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Install COCOAPI https://github.com/cocodataset/cocoapi :",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_direct_link'}]",
        "repoID": 26894,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "All databases are implemented https://github.com/orbitdb/orbit-db-store  on top of ipfs-log https://github.com/orbitdb/ipfs-log , an immutable, cryptographically verifiable, operation-based conflict-free replicated data structure ( CRDT https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type ) for distributed systems. ipfs-log is formalized in the paper Merkle-CRDTs https://arxiv.org/abs/2004.00107 . You can also easily extend OrbitDB by implementing and using a custom data model https://github.com/orbitdb/orbit-db/blob/master/GUIDE.md#custom-stores  benefitting from the same properties as the default data models provided by the underlying Merkle-CRDTs.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://github.com/orbitdb/orbit-db-store>\nLabel: DatasetDirectLink\n* URL: <https://github.com/orbitdb/ipfs-log>\nLabel: DatasetLandingPage\n* URL: <https://arxiv.org/abs/2004.00107>\nLabel: Other\n* URL: <https://github.com/orbitdb/orbit-db/blob/master/GUIDE.md>\nLabel: Software",
        "repoID": 26931,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "is an immutable, operation-based conflict-free replicated data structure ( CRDT https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type ) for distributed systems. It's an append-only log that can be used to model a mutable, shared state between peers in p2p applications.",
        "answer": "  [{\"URL\": \"https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type\", \"label\": \"Software\"}]",
        "repoID": 26935,
        "URL_gold_label": [
            {
                "URL": "https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "This repository uses Pandas https://pandas.pydata.org  to process and normalize the data.",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://pandas.pydata.org', 'label': 'Software'}]",
        "repoID": 26945,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download EuRoC MAV Dataset http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets . Although it contains stereo cameras, we only use one camera. Before testing, copy the new  file to  path. Change the output frequency of VINS-Mono  to 15 Hz .",
        "answer": "  Sure, here are the annotations for the input:\n\n[{'URL': 'http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets', 'label': 'dataset_landing_page'}]",
        "repoID": 26989,
        "URL_gold_label": [
            {
                "URL": "http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The notebooks and the pref_voting library is built around a full SciPy stack: MatPlotLib https://matplotlib.org/ , Numpy https://numpy.org/ , Pandas https://pandas.pydata.org/ , numba http://numba.pydata.org/ , networkx https://networkx.org/ , and tabulate https://github.com/astanin/python-tabulate",
        "answer": "  Sure, here are the annotated URLs:\n\n[{\"URL\": \"https://matplotlib.org/\", \"label\": \"Software\"}]\n[{\"URL\": \"https://numpy.org/\", \"label\": \"Software\"}]\n[{\"URL\": \"https://pandas.pydata.org/\", \"label\": \"Software\"}]\n[{\"URL\": \"http://numba.pydata.org/\", \"label\": \"Software\"}]\n[{\"URL\": \"https://networkx.org/\", \"label\": \"Software\"}]\n[{\"URL\": \"https://github.com/astanin/python-tabulate\", \"label\": \"Other\"}]",
        "repoID": 27122,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            },
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KITTI Raw Data http://www.cvlibs.net/datasets/kitti/raw_data.php  (synced+rectified data, please refer MonoDepth2 https://github.com/nianticlabs/monodepth2#-kitti-training-data  for downloading all data more easily)",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]",
        "repoID": 27259,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "KITTI Scene Flow 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 27259,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download link: GoogleDrive https://www.cityscapes-dataset.com",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com', 'label': 'dataset_direct_link'}]",
        "repoID": 27365,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "COCO dataset http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 27431,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "\"covertype.mat\": Covertype dataset from the LIBSVM Data Repository https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html . It is also used in SVGD by Liu & Wang (2016) http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html', 'label': 'dataset_landing_page'}]\n\nExplanation:\n\n* The URL \"covertype.mat\" is a file URL, which falls under the category of \"dataset_direct_link\".\n* The URL \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\" is a landing page for a dataset, which is why it is annotated as \"dataset_landing_page\".",
        "repoID": 27454,
        "URL_gold_label": [
            {
                "URL": "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "We use Lasot https://cis.temple.edu/lasot/ , GOT-10k http://got-10k.aitestunion.com , TrackingNet https://tracking-net.org  and COCO http://cocodataset.org/#home  to train fcot. Before running the training scripts, you should download the datasets and set the correct datasets path in . Also remember to download the pretrained dimp50 https://drive.google.com/file/d/14zFM14cjJY-D_OFsLDlF1fX5XrSXGBQV/view?usp=sharing  model to initialize the backbone and classification-18 branch of fcot. Then switch to your conda environment using . The training scripts can be found at bash bash  folder. We use the two following stategies to train fcot (we report the results of the first training strategy in the paper).",
        "answer": "  Here are the annotated URLs:\n\n* URL: https://cis.temple.edu/lasot/\nLabel: dataset_landing_page\n* URL: http://got-10k.aitestunion.com\nLabel: dataset_direct_link\n* URL: https://tracking-net.org\nLabel: dataset_landing_page\n* URL: http://cocodataset.org/#home\nLabel: dataset_direct_link\n* URL: https://drive.google.com/file/d/14zFM14cjJY-D_OFsLDlF1fX5XrSXGBQV/view?usp=sharing\nLabel: software\n* URL: bash\nLabel: Other",
        "repoID": 27501,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:",
        "answer": "  [INST]\n    Output: [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 27545,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas http://pandas.pydata.org/  for storing data about components and time series",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 27622,
        "URL_gold_label": [
            {
                "URL": "http://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download Scene Flow https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo  and KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo  datasets.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: <https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html>\nLabel: DatasetLandingPage\n* URL: <http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo>\nLabel: DatasetDirectLink\n* URL: <http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo>\nLabel: DatasetDirectLink",
        "repoID": 27687,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Python 3 for code generation and with pandas https://pandas.pydata.org  installed for the evaluation scripts",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n* URL: \"Python 3\"\nLabel: \"Software\"",
        "repoID": 27812,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Follow the instruction on KITTI Odometry website http://www.cvlibs.net/datasets/kitti/eval_odometry.php  to download the KITTI odometry train set. Then train with",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_odometry.php', 'label': 'dataset_landing_page'}]",
        "repoID": 27840,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_odometry.php",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "In case you'd like to have fun with some random  meshes, you could download samples from Stanford's 3D Scanning Repository here http://graphics.stanford.edu/data/3Dscanrep/ .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'http://graphics.stanford.edu/data/3Dscanrep/', 'label': 'dataset_landing_page'}]",
        "repoID": 27970,
        "URL_gold_label": [
            {
                "URL": "http://graphics.stanford.edu/data/3Dscanrep/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "dlib http://dlib.net/ face_recognition https://github.com/ageitgey/face_recognition/ NumPy http://www.numpy.org/ pandas https://pandas.pydata.org/ scikit-learn http://scikit-learn.org/",
        "answer": "  Sure! Here are the annotated URLs:\n\nInput: dlib http://dlib.net/ face_recognition https://github.com/ageitgey/face_recognition/ NumPy http://www.numpy.org/ pandas https://pandas.pydata.org/ scikit-learn http://scikit-learn.org/\n\nOutput:\n\n[{\"URL\": \"http://dlib.net/\", \"label\": \"Software\"},\n {\"URL\": \"https://github.com/ageitgey/face_recognition/\", \"label\": \"Software\"},\n {\"URL\": \"http://www.numpy.org/\", \"label\": \"Software\"},\n {\"URL\": \"https://pandas.pydata.org/\", \"label\": \"Software\"},\n {\"URL\": \"http://scikit-learn.org/\", \"label\": \"Software\"}]",
        "repoID": 28035,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the VisDial v0.9 and v1.0 dialog json files from here https://visualdialog.org/data  and keep it under  and  directory, respectively.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: https://visualdialog.org/data\nLabel: DatasetDirectLink",
        "repoID": 28041,
        "URL_gold_label": [
            {
                "URL": "https://visualdialog.org/data",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "TransE https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data  ( code kge/model/transe.py , config kge/model/transe.yaml )",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data', 'label': 'dataset_landing_page'}]",
        "repoID": 28118,
        "URL_gold_label": [
            {
                "URL": "https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Data was created by using the Vader dataset from NLTK.org http://www.nltk.org/data.html .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.nltk.org/data.html', 'label': 'dataset_landing_page'}]",
        "repoID": 28202,
        "URL_gold_label": [
            {
                "URL": "http://www.nltk.org/data.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "SQuAD v1.1: train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json  and dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
        "answer": "  Sure! Here are the annotations for the URLs in the input:\n\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json', 'label': 'dataset_direct_link'}]\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json', 'label': 'dataset_direct_link'}]",
        "repoID": 28227,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
                "gold_label": "dataset_direct_link"
            },
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\",\n\"label\": \"dataset_direct_link\"\n}",
        "repoID": 28324,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json', 'label': 'dataset_direct_link'}]",
        "repoID": 28324,
        "URL_gold_label": [
            {
                "URL": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json",
                "gold_label": "dataset_direct_link"
            }
        ]
    },
    {
        "text": "In our experimental evaluation using a benchmark of time series datasets [LINK] http://www.cs.ucr.edu/~eamonn/time_series_data/ , TEASER is two to three times as early while keeping the same (or even a higher) level of accuracy, when compared to the state of the art.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: [http://www.cs.ucr.edu/~eamonn/time_series_data/](http://www.cs.ucr.edu/~eamonn/time_series_data/)\nLabel: dataset_landing_page",
        "repoID": 28481,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ucr.edu/~eamonn/time_series_data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The UCR Time Series Classification Archive http://www.cs.ucr.edu/~eamonn/time_series_data/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cs.ucr.edu/~eamonn/time_series_data/', 'label': 'dataset_landing_page'}]",
        "repoID": 28481,
        "URL_gold_label": [
            {
                "URL": "http://www.cs.ucr.edu/~eamonn/time_series_data/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Person names and location (country, city) names are multilingual, depending on the  language. We got the data notebooks/other/Acquiring%20multilingual%20lexicons%20from%20wikidata.ipynb  from wikidata https://www.wikidata.org , so there is a bias towards names on wikipedia.",
        "answer": "here are the annotated URLs:\n\n* URL: https://www.wikidata.org\nLabel: DatasetDirectLink\n* URL: notebooks/other/Acquiring multilingual lexicons from wikidata.ipynb\nLabel: DatasetLandingPage",
        "repoID": 28517,
        "URL_gold_label": [
            {
                "URL": "https://www.wikidata.org",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "Kitti http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://www.cvlibs.net/datasets/kitti/eval_tracking.php\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 28555,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Some functions will optionally use pandas https://pandas.pydata.org/ , bibtexparser https://bibtexparser.readthedocs.io/  and ase https://wiki.fysik.dtu.dk/ase/ .",
        "answer": "  Sure, here are the annotated URLs:\n\n[{'URL': 'https://pandas.pydata.org/', 'label': 'Software'}]\n[{'URL': 'https://bibtexparser.readthedocs.io/', 'label': 'Software'}]\n[{'URL': 'https://wiki.fysik.dtu.dk/ase/', 'label': 'Other'}]",
        "repoID": 28690,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Cityscapes Dataset https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n{\n\"URL\": \"https://www.cityscapes-dataset.com/\",\n\"label\": \"dataset_landing_page\"\n}",
        "repoID": 28720,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the  data of a KITTI RAW sequence http://www.cvlibs.net/datasets/kitti/raw_data.php . We choose the data  of the  category:",
        "answer": "  [{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_direct_link'}]",
        "repoID": 28755,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Please download the UCF101 dataset here http://crcv.ucf.edu/data/UCF101.php .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://crcv.ucf.edu/data/UCF101.php', 'label': 'dataset_landing_page'}]",
        "repoID": 28839,
        "URL_gold_label": [
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The SINDyBVP.sindy_bvp() method is then called. This method loads the relevant data, randomly selects trial data to use for regression (i.e. finding the model and parameters), and sends that data to a TermBuilder object to construct Pandas https://pandas.pydata.org/ DataFrames https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html  containing symbolic functions. Each trial is sent individually to the TermBuilder, which creates a DataFrame from each individual trial. The DataFrame has (p+1) columns, where (p) is the number of candidate functions to use for SINDy regression. The additional column corresponds to the regression outcome variable. After a DataFrame has been constructed for each of the trials, the data is sent to a grouper object, which takes the DataFrames and reorganizes them for regression, as described in the paper. The organized data is used by the GroupRegressor object to perform the SGTR algorithm. The results are reported, and the coefficients are computed for the operator L (rather than for the algebraically manipulated form learned by SINDy-BVP). The coefficients are returned with a configured plotter object, which can be used to visualize the results.",
        "answer": "  Here are the annotated URLs in the input text:\n\n* \"https://pandas.pydata.org/\": label = \"Software\"\n* \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\": label = \"Software\"\n* \"http://simongog.github.io/assets/data/sdsl-slides/tutorial\": label = \"Other\"\n* \"http://law.di.unimi.it/datasets.php",
        "repoID": 28943,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download the dataset from KITTI Tracking http://www.cvlibs.net/datasets/kitti/eval_tracking.php .",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/eval_tracking.php', 'label': 'dataset_direct_link'}]",
        "repoID": 29284,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_tracking.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "To run the docker image make sure a  is available (either on your notebook or in the cloud you prefer) and your Docker environment supports GPUs. To do so, you might follow the official nvidia guide: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html",
        "answer": "  [{\"URL\": \"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html\", \"label\": \"Software\"}]",
        "repoID": 29333,
        "URL_gold_label": [
            {
                "URL": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html",
                "gold_label": "other"
            }
        ]
    },
    {
        "text": "We use the , , ,  datasets from from 5-core subsets of the Amazon review corpus http://jmcauley.ucsd.edu/data/amazon/  by Prof. Julian McAuley.",
        "answer": "  Sure! Here are the annotated URLs:\n\n* URL: http://jmcauley.ucsd.edu/data/amazon/\nLabel: dataset_landing_page",
        "repoID": 29441,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Python3 libraries: h5py https://www.h5py.org , numba https://numba.pydata.org , numpy https://numpy.org , pandas https://pandas.pydata.org",
        "answer": "  Sure, here are the annotations for the URLs you provided:\n\n[{'URL': 'https://www.h5py.org', 'label': 'Software'}]\n[{'URL': 'https://numba.pydata.org', 'label': 'Software'}]\n[{'URL': 'https://numpy.org', 'label': 'Software'}]\n[{'URL': 'https://pandas.pydata.org', 'label': 'Software'}]",
        "repoID": 29669,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "MagnaTagATune Dataset http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset', 'label': 'dataset_landing_page'}]",
        "repoID": 29738,
        "URL_gold_label": [
            {
                "URL": "http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "v2e can convert recordings from DDD20 https://sites.google.com/view/davis-driving-dataset-2020/home  and the original DDD17 https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub  which are the first public end-to-end training datasets of automotive driving using a DAVIS event + frame camera. It lets you compare the real DVS data with the conversion. This dataset is maintained by the Sensors Research Group of Institute of Neuroinformatics.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: https://sites.google.com/view/davis-driving-dataset-2020/home\nLabel: dataset_landing_page\n* URL: https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub\nLabel: dataset_direct_link",
        "repoID": 29973,
        "URL_gold_label": [
            {
                "URL": "https://sites.google.com/view/davis-driving-dataset-2020/home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Some examples of MDEQ segmentation results on the Cityscapes https://www.cityscapes-dataset.com/  dataset.",
        "answer": "  Sure, here are the annotated URLs for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 30050,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "numba http://numba.pydata.org/  - for speeding up calculations of the statistics (available automatically with Anaconda).",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://numba.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 30079,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 30079,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/  - for the probability density function of the Hotel dataset presented in the article.",
        "answer": "  Sure! Here's the annotation for the input URL:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 30079,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Amazon datasets are derived from here http://jmcauley.ucsd.edu/data/amazon/ , tradesy dataset is introduced in here http://jmcauley.ucsd.edu/data/tradesy/ . Please cite the corresponding papers if you use the datasets.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://jmcauley.ucsd.edu/data/amazon/\nLabel: DatasetDirectLink\n* URL: http://jmcauley.ucsd.edu/data/tradesy/\nLabel: DatasetLandingPage",
        "repoID": 30202,
        "URL_gold_label": [
            {
                "URL": "http://jmcauley.ucsd.edu/data/amazon/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The code is a re-write of PythonAPI for COCO https://github.com/cocodataset/cocoapi . The core functionality is the same with LVIS specific changes.",
        "answer": "  [{\"URL\": \"https://github.com/cocodataset/cocoapi\", \"label\": \"Software\"}]",
        "repoID": 30209,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "numba http://numba.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"http://numba.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 30260,
        "URL_gold_label": [
            {
                "URL": "http://numba.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 30260,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Get ImageNet https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset  if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily.",
        "answer": "  [{\"URL\": \"https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md\", \"label\": \"Software\"}]",
        "repoID": 30340,
        "URL_gold_label": [
            {
                "URL": "https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download Scene Flow Datasets https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , KITTI 2012 http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo , KITTI 2015 http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo , ETH3D https://www.eth3d.net/ , Middlebury https://vision.middlebury.edu/stereo/",
        "answer": "  Sure, here are the annotated URLs:\n\n* \"Download Scene Flow Datasets\": \"dataset_landing_page\"\n* \"KITTI 2012\": \"dataset_direct_link\"\n* \"KITTI 2015\": \"dataset_direct_link\"\n* \"ETH3D\": \"dataset_landing_page\"\n* \"Middlebury\": \"dataset_landing_page\"",
        "repoID": 30420,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "The implementation of all tasks and algorithms is based on the qd-branch of the C++ Sferes2 https://github.com/sferes2/sferes2  library presented in Sferesv2: Evolvin' in the multi-core world https://ieeexplore.ieee.org/abstract/document/5586158/?casa_token=EhBJLkircvMAAAAA:ls8I90Y5H2vsJk5RxCYs8X1T9yZHDhDEz5S6g5gatOzETle1LK_ib8zwodx6t5J_-Uwq_YP9 , and the hexapod control task uses the Dart simulator introduced in Dart: Dynamic animation and robotics toolkit https://joss.theoj.org/papers/10.21105/joss.00500.pdf . Furthermore, the analysis of the results is based on Panda https://pandas.pydata.org/ , Matplotlib https://matplotlib.org/  and Seaborn https://seaborn.pydata.org/index.html  libraries.",
        "answer": "  Here are the annotated URLs in the input text:\n\n* URL: https://github.com/sferes2/sferes2\nLabel: dataset_landing_page\n* URL: https://ieeexplore.ieee.org/abstract/document/5586158/\nLabel: Other\n* URL: https://joss.theoj.org/papers/10.21105/joss.00500.pdf\nLabel: Other\n* URL: https://pandas.pydata",
        "repoID": 30532,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            },
            {
                "URL": "https://seaborn.pydata.org/index.html",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": ": Please follow the instructions in Cityscape https://www.cityscapes-dataset.com/  to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:",
        "answer": "  [INST]\n    Output: [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 30533,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ", please download from COCO download http://cocodataset.org/#download , 2017 Train/Val is needed for COCO keypoints training and validation. Download and extract them under {POSE_ROOT}/data, and make them look like this:",
        "answer": "  [{\"URL\": \"http://cocodataset.org/#download\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 30632,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Install COCOAPI https://github.com/cocodataset/cocoapi :",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_direct_link'}]",
        "repoID": 30632,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download and pre-process UCF101 http://crcv.ucf.edu/data/UCF101.php  and HMDB51 http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/  datasets as follows.",
        "answer": "  [{\"URL\": \"http://crcv.ucf.edu/data/UCF101.php\", \"label\": \"dataset_landing_page\"}, {\"URL\": \"http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 30657,
        "URL_gold_label": [
            {
                "URL": "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/",
                "gold_label": "dataset_landing_page"
            },
            {
                "URL": "http://crcv.ucf.edu/data/UCF101.php",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 30754,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "seaborn https://seaborn.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given input:\n\n[{'URL': 'https://seaborn.pydata.org/', 'label': 'Software'}]",
        "repoID": 30754,
        "URL_gold_label": [
            {
                "URL": "https://seaborn.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Download Cityscapes https://www.cityscapes-dataset.com/  and virtual Kitti https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/",
        "answer": "  Sure, here are the annotations for the given URLs:\n\nInput: Download Cityscapes https://www.cityscapes-dataset.com/\nOutput: [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]\n\nInput: virtual Kitti https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/\nOutput: [{'URL': 'https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/', 'label': 'other'}]",
        "repoID": 30775,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "For the needs of our application we utilized YOLOv3 detector, trained on the COCO dataset http://cocodataset.org/#home . You can download this detector from here https://convcao.hopto.org/index.php/s/mh8WIDpprE70SO3 . After downloading the file, extract the yolo-coco folder inside your local ConvCao_AirSim folder.",
        "answer": "  Sure, here are the annotated URLs:\n\n* URL: http://cocodataset.org/#home\nLabel: dataset_landing_page\n* URL: https://convcao.hopto.org/index.php/s/mh8WIDpprE70SO3\nLabel: dataset_direct_link",
        "repoID": 30854,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Make sure you have downloaded flying chairs https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html  and flying things https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html , and placed them under the same folder, say /ssd/.",
        "answer": "  [INST]\n    Output:\n    [\n        {\"URL\": \"https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html\", \"label\": \"dataset_landing_page\"},\n        {\"URL\": \"/ssd/FlyingChairs.en.html\", \"label\": \"dataset_direct_link\"}\n    ]",
        "repoID": 30887,
        "URL_gold_label": [
            {
                "URL": "https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ", please download from COCO download http://cocodataset.org/#download , 2017 Train/Val is needed for COCO detection/instance segmentation/keypoints training and validation. Download and extract them under {ROOT}/data, and make them look like this:",
        "answer": "  [{\"URL\": \"http://cocodataset.org/#download\", \"label\": \"dataset_direct_link\"}]",
        "repoID": 30993,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 31158,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Please follow the official COCO dataset website http://cocodataset.org/#download  to download the dataset. After downloading the dataset you should have the following directory structure:",
        "answer": "  [{\"URL\": \"http://cocodataset.org/#download\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 31309,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the dataset from the Cityscapes dataset server( Link https://www.cityscapes-dataset.com/ ). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in ../data/CityScapes/",
        "answer": "  [{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_direct_link'}]",
        "repoID": 31442,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "pandas https://pandas.pydata.org/  (tested with version 0.24.2)",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 31457,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "This repository is a PyTorch implementation for semantic segmentation / scene parsing. The code is easy to use for training and testing on various datasets. The codebase mainly uses ResNet50/101/152 as backbone and can be easily adapted to other basic classification structures. Implemented networks including PSPNet https://hszhao.github.io/projects/pspnet  and PSANet https://hszhao.github.io/projects/psanet , which ranked 1st places in ImageNet Scene Parsing Challenge 2016 @ECCV16 http://image-net.org/challenges/LSVRC/2016/results , LSUN Semantic Segmentation Challenge 2017 @CVPR17 https://blog.mapillary.com/product/2017/06/13/lsun-challenge.html  and WAD Drivable Area Segmentation Challenge 2018 @CVPR18 https://bdd-data.berkeley.edu/wad-2018.html . Sample experimented datasets are ADE20K http://sceneparsing.csail.mit.edu , PASCAL VOC 2012 http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6  and Cityscapes https://www.cityscapes-dataset.com .",
        "answer": "  Sure, here are the annotated URLs in the input text:\n\n* \"https://hszhao.github.io/projects/pspnet\" - label: \"dataset_landing_page\"\n* \"https://hszhao.github.io",
        "repoID": 31506,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download The Cityscapes Dataset https://www.cityscapes-dataset.com/",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 31575,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Pandas https://pandas.pydata.org/  (v1.0.1 or later)",
        "answer": "  Sure! Here's the annotation for the given URL:\n\n{\n\"URL\": \"https://pandas.pydata.org/\",\n\"label\": \"Software\"\n}",
        "repoID": 31664,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "For color images, KITTI Raw dataset is also needed, which is available at the KITTI Raw Website http://www.cvlibs.net/datasets/kitti/raw_data.php .",
        "answer": "  Sure! Here are the annotations for the input:\n\n[{'URL': 'http://www.cvlibs.net/datasets/kitti/raw_data.php', 'label': 'dataset_landing_page'}]",
        "repoID": 31665,
        "URL_gold_label": [
            {
                "URL": "http://www.cvlibs.net/datasets/kitti/raw_data.php",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "We provide evaluation tools for COCO-WholeBody dataset. Our evaluation tools is developed based on @cocodataset/cocoapi https://github.com/cocodataset/cocoapi .",
        "answer": "  Sure! Here are the annotated URLs:\n\n[{'URL': 'https://github.com/cocodataset/cocoapi', 'label': 'dataset_landing_page'}]",
        "repoID": 31807,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Next, set up the COCO dataset. You can download it from here http://cocodataset.org/#download , and update the paths in  to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from here https://github.com/cocodataset/cocoapi .",
        "answer": "  [{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 31808,
        "URL_gold_label": [
            {
                "URL": "https://github.com/cocodataset/cocoapi",
                "gold_label": "software"
            },
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ". Please follow the standard download and preparation guidelines on the official website https://www.cityscapes-dataset.com/ . We recommend to symlink its root folder  to  by:",
        "answer": "  [{\"URL\": \"https://www.cityscapes-dataset.com/\", \"label\": \"dataset_landing_page\"}]",
        "repoID": 31827,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Download the images (2014 Train, 2014 Val, 2017 Test) from here http://cocodataset.org/#download",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'http://cocodataset.org/#download', 'label': 'dataset_direct_link'}]",
        "repoID": 31962,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#download",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": ": Different machine learning frameworks have different strengths. Flower can be used with any machine learning framework, for example, PyTorch https://pytorch.org , TensorFlow https://tensorflow.org , Hugging Face Transformers https://huggingface.co/ , PyTorch Lightning https://pytorchlightning.ai/ , MXNet https://mxnet.apache.org/ , scikit-learn https://scikit-learn.org/ , JAX https://jax.readthedocs.io/ , TFLite https://tensorflow.org/lite/ , fastai https://www.fast.ai/ , Pandas https://pandas.pydata.org/  for federated analytics, or even raw NumPy https://numpy.org/  for users who enjoy computing gradients by hand.",
        "answer": "  Sure, here are the annotated URLs:\n\n* PyTorch: <https://pytorch.org/> (label: Software)\n* TensorFlow: <https://tensorflow.org/> (label: Software)\n* Hugging Face Transformers: <https://huggingface.co/> (label: Software)\n* PyTorch Lightning: <https://pytorchlightning.ai/> (label: Software)\n* MXNet: <https://mxnet.apache.org/> (label: Software)\n* scikit-learn: <https://scikit-learn.org/> (label: Software)\n* JAX: <https://jax.readthedocs.io/> (label: Software)\n* TFLite: <https://tensorflow.org/lite/> (label: Software)\n* fastai: <https://www.fast.ai/> (label: Software)\n* NumPy: <https://numpy",
        "repoID": 32006,
        "URL_gold_label": [
            {
                "URL": "https://pandas.pydata.org/",
                "gold_label": "software"
            }
        ]
    },
    {
        "text": "Datasets: MS COCO http://cocodataset.org/#home  for image instance segmentation and YouTube-VIS https://youtube-vos.org/dataset/vis/  for video instance segmentation.",
        "answer": "  Sure, here are the annotations for the given input:\n\n[{'URL': 'http://cocodataset.org/#home', 'label': 'dataset_landing_page'},\n {'URL': 'https://youtube-vos.org/dataset/vis/', 'label': 'dataset_landing_page'}]",
        "repoID": 32035,
        "URL_gold_label": [
            {
                "URL": "http://cocodataset.org/#home",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Segmentation performance measured in IoU/mIoU (%) on Cityscapes https://www.cityscapes-dataset.com/ . For class names: All referred to all classes average, and the rest single class codes are as follows: DeepGlobe: Ro.-road, Sw.-sidewalk, Bu.-building, W.-wall, F.-fence, P.-pole, T.L.-traffic light, R.S.-raffic sign, V.-vegetation, Te.-terrain, Sk.-sky, P.-person, RI.-rider, C.-car, Tru.-truck, Tra.-train, M.-motorcycle, Bi.-bicycle.",
        "answer": "  Sure, here are the annotated URLs for the given input:\n\n[{'URL': 'https://www.cityscapes-dataset.com/', 'label': 'dataset_landing_page'}]",
        "repoID": 32060,
        "URL_gold_label": [
            {
                "URL": "https://www.cityscapes-dataset.com/",
                "gold_label": "dataset_landing_page"
            }
        ]
    },
    {
        "text": "Mini-Imagenet as described here https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet",
        "answer": "  Sure! Here's the annotation for the input:\n\n[{'URL': 'https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet', 'label': 'dataset_landing_page'}]",
        "repoID": 32088,
        "URL_gold_label": [
            {
                "URL": "https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet",
                "gold_label": "dataset_direct_link"
            }
        ]
    }
]