repoId,link,text,prev_text,next_text
67,http://pandas.pydata.org/,pandas,"
",": library providing high-performance, easy-to-use data structures and data analysis tools"
70,http://www.rubydoc.info/github/heartcombo/devise/main/Devise/Models/Validatable,Validatable,"
",": provides validations of email and password. It's optional and can be customized, so you're able to define your own validations."
155,http://conda.pydata.org/miniconda.html,miniconda,"Though you can install all the requirements yourself, as most are available in the Python Package Index (PyPI) and can be installed with simple commands, the easiest way to get up and running is to use ",. Once you have the 
155,http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#datasets,supervised dataset format, above have the now-standard scikit-learn ,". This means we can use any classifier that satisfies the scikit-learn API. Below, we use a simple wrapper around the scikit-learn "
189,http://simongog.github.io/assets/data/sdsl-slides/tutorial,tutorial slides and walk-through,", ",.
189,http://simongog.github.io/assets/data/sdsl-slides/tutorial,presentation,A tutorial , with the 
189,http://simongog.github.io/assets/data/sdsl-slides/tutorial,tutorial,Next we suggest you look at the comprehensive , which describes all major features of the library or look at some of the provided 
231,http://www.tuhh.de/ibb/publications/databases-and-software.html,TUHH website,". The original networks from [GA'05] are included in the folder data (or at least, networks very similar to the ones described in the article), also for testing purposes. They were retrieved from the ",.
244,https://github.com/BorisMoore/jquery-datalink,JQuery Data Link,"
","
"
269,http://snap.stanford.edu/data/index.html,Stanford Large Network Dataset (SNAP),"
","
"
269,http://law.di.unimi.it/datasets.php,Laboratory for Web Algorithms,"
","
"
269,http://grouplens.org/datasets/movielens/,Movielens dataset GroupLens,"
","
"
269,http://stat-computing.org/dataexpo/2009/,Airline on time performance,"
","
"
269,http://aws.amazon.com/datasets,Amazon Web Services public datasets,"
","
"
301,http://shuyo.wordpress.com/2012/03/02/estimation-of-ldig-twitter-language-detection-for-liga-dataset/,Estimation of ldig (twitter Language Detection) for LIGA dataset,"
","
"
308,https://github.com/ucd-spatial/Datasets/tree/master/geresid-geo_relatedness_similarity_dataset,Geo-Relatedness and Similarity Dataset (GeReSiD),2. ,"
"
308,https://github.com/ucd-spatial/Datasets/tree/master/mdsm-similarity_dataset,MDSM Geo-Semantic Similarity Dataset,3. ,"
"
308,https://github.com/ucd-spatial/Datasets/tree/master/sentiment_detection_hotel_reviews_dataset,Sentiment Detection in Hotel Reviews Dataset,4. ,"
"
314,https://github.com/RDFLib/pymicrodata,pymicrodata,"
", - A module to extract RDF from an HTML5 page annotated with microdata.
346,https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr-renamed,Full time series,"
","
"
346,https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_9010,Extrapolation,"
","
"
346,https://github.com/jamesrobertlloyd/gpss-research/tree/master/data/tsdlr_5050,Interpolation,"
","
"
351,https://www.opensciencedatacloud.org/publicdata/heat-transfer/,Open Source Data Cloud,The heat transfer simulation data is available from the ,.
374,http://nilm-metadata.readthedocs.org,documentation is available online,The ,.
374,http://nilm-metadata.readthedocs.org/en/latest/tutorial.html,tutorial,If you're new to NILM Metadata then please read this README and then dive into the , to find out how to see a worked example.
374,http://nilm-metadata.readthedocs.org/en/latest/dataset_metadata.html,Dataset metadata,"Or, if you are already familiar with NILM Metadata then perhaps you want direct access to the full description of the ""","""."
374,https://github.com/nilmtk/nilm_metadata/tree/v0.1.0,version 0.1 of the schema,In ,", we wrote a very comprehensive (and complex) schema using "
374,http://nilm-metadata.readthedocs.org,human-readable documentation," in order to automate the validation of metadata instances.  JSON Schema is a lovely language and can capture everything we need but, because our metadata is quite comprehensive, we found that using JSON Schema was a significant time drain and made it hard to move quickly and add new ideas to the metadata.  As such, when we moved from v0.1 to v0.2, the JSON Schema has been dropped.  Please use the "," instead.  If there is a real desire for automated validation then we could resurrect the JSON Schema, but it is a fair amount of work to maintain."
374,https://github.com/nilmtk/nilm_metadata/issues/6,issue #6, files.  See ,.
389,https://github.com/girving/pentago/tree/master/data/edison/final.txt,data/edison/final.txt,"Warning: The total size of the data is 3.7 TB.  If you want something more manageable, you can download only the first few slices (say up to move 12).  To see the sizes of all slices before downloading, look at ",".  If you've gotten past this warning, you can download the data via"
395,http://datatracker.ietf.org/doc/draft-ietf-ppsp-peer-protocol/,video streaming," Aims to create a censorship-free Internet. Already deployed, used and incrementally improved for 8-years. Tribler uses an upcoming IETF Internet Standard for ", and is backward compatible with Bittorrent. Future aim is using smartphones to even bypass Internet kill switches. An early proof-of-principle 
446,https://en.wikipedia.org/wiki/Rope_%28data_structure%29,rope data structure,". In each B+ tree, an internal node keeps the count of symbols in the subtree descending from it; an external node keeps a BWT substring in the run-length encoding. The B+ tree achieve a similar purpose to the ",", which enables efficient query and insertion. RopeBWT2 uses this rope-like data structure to achieve incremental construction. This is where word 'rope' in ropeBWT2 comes from."
454,http://conda.pydata.org/docs/build.html,conda-build,To simplify the build we now use the , tool. The resulting binary is uploaded to the 
454,http://conda.pydata.org/,conda,", and can be installed using the ", package manager.  The installation will install all of the neuroproof binaries (including the interactive tool) and the python libraries.
454,http://conda.pydata.org/miniconda.html,Miniconda,The , tool first needs to installed:
463,http://www.lorisbazzani.info/code-datasets/sdalf-descriptor/,SDALF,The training / testing partition is generated following the approach ,"
"
513,ttic.edu/bansal/data/syntacticEmbeddings.zip,Tailoring Continuous Word Representations for Dependency Parsing,"Bansal, ","
"
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_preprocess.m,preprocessing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_region_growing.m,region growing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_regular_grid.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with regular grid of scales."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_basc_MSTEPS.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with scales selected by MSTEPS."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_regular_grid.m,Multiscale Statistical Parametric Connectome,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/MOTOR_pipeline_MSPC_MSTEPS.m,Multiscale Statistical Parametric Connectome,The ," pipeline, with scales selected by MSTEPS."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_preprocess.m,preprocessing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_region_growing.m,region growing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_regular_grid.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with regular grid of scales."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_basc_MSTEPS.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with scales selected by MSTEPS."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_regular_grid.m,Multiscale Statistical Parametric Connectome,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/BLIND_pipeline_MSPC_MSTEPS.m,Multiscale Statistical Parametric Connectome,The ," pipeline, with scales selected by MSTEPS."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_preprocess.m,preprocessing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_region_growing.m,region growing,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_regular_grid.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with regular grid of scales."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_basc_MSTEPS.m,Boostrap Analysis of Stable Clusters,The ," pipeline, with scales selected by MSTEPS."
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_regular_grid.m,Multiscale Statistical Parametric Connectome,The , pipeline.
559,https://github.com/SIMEXP/glm_connectome/blob/master/real_data/SCHIZO_pipeline_MSPC_MSTEPS.m,Multiscale Statistical Parametric Connectome,The ," pipeline, with scales selected by MSTEPS."
575,https://github.com/redpony/creg/tree/master/test_data,test_data,You will find example files for each type of model in the , directory.
580,http://nbviewer.ipython.org/github/ETCBC/laf-fabric-nbs/blob/master/extradata/annox_workflow.ipynb,annox_workflow,"
","
"
664,http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz,mnist.pkl.gz,Download , and change the shared_args['dataset'] to where you save it.
666,http://mscoco.org/dataset/#download,download,Visit MS COCO , page for more details.
666,http://mscoco.org/dataset/#format,format,Visit MS COCO , page for more details.
685,http://www.bbci.de/competition/iii/#data_set_i,"BCI Competition3, Data Set 1",An example for classification of motor imagery in ECoG recordings. For that example the , was used.
685,http://www.bbci.de/competition/iii/#data_set_ii,"BCI Competition 3, Data Set 2",An example for classification with a P300 Matrix Speller in EEG recordings. The , was used for that example.
711,http://smileclinic.alwaysdata.net/,Novi Quadrianto,"
",": letter n followed by dot then quadrianto and the at, and also sussex.ac.uk"
724,#jpcnn_create_image_buffer_from_uint8_data,jpcnn_create_image_buffer_from_uint8_data,"
","
"
741,https://dl.dropboxusercontent.com/s/ttw041hqgw64ymx/r-cnn-release1-data-caffe-proto-v0.tgz,VOC models," R-CNN has been updated to use the new Caffe proto messages that were rolled out in Caffe v0.999. The model package contains models in the up-to-date proto format. If, for some reason, you need to get the old (Caffe proto v0) models, they can still be downloaded: ","
"
741,https://dl.dropboxusercontent.com/s/c6aqns2bvoqi86q/r-cnn-release1-data-ilsvrc2013-caffe-proto-v0.tgz,ILSVRC13 model,"
",.
760,http://snap.stanford.edu/data/amazon/amazon_readme.txt,data set,"First, obtain the ",", and clone this repository. The outline of the processing steps is as follows:"
761,https://github.com/Spirals-Team/npe-dataset/,https://github.com/Spirals-Team/npe-dataset/,The evaluation dataset is at ,"
"
786,http://www.ams.org/mr-database,"
Mathematical Reviews
",The code in this repo makes use of 24 years of data from ,", available by request to the director of the American Mathematical Society. The data may require cleaning in order to match the format assumed by "
791,http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools,SICK dataset,"
", (semantic relatedness task)
791,http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools,Sentences Involving Compositional Knowledge (SICK),The goal of this task is to predict similarity ratings for pairs of sentences. We train and evaluate our models on the , dataset.
831,http://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data,Kaggle,Download the data files from ,. Place and extract the files in the following locations:
843,/data/,Data dashboard,"
",", "
843,http://www.micc.unifi.it/tagsurvey/#data,Florence data server,", ","
"
858,https://datamarket.com/data/list/?q=ebola&ref=search,here,Datamarket has made these data available through their API ,. The DataMarket API is documented 
858,https://datamarket.com/api/v1/,here,. The DataMarket API is documented ,". To access it programmatically you need a sharing key, which you can find in the file 'datamarket_sharingkey.txt'"
858,http://apps.who.int/gho/data/node.ebola-sitrep,WHO, contains data from the , that compare sitrep case counts with patient database counts for select cities and countries.
875,data/,here,I have prepared a simple sample ,", you may also check "
883,#partitioning-data-for-multiple-gpus,Partitioning Data for (Multiple) GPUs,"
","
"
886,http://www.w3.org/2014/data-shapes/charter,W3C RDF data shapes working group charter,"
","
"
924,http://cocodataset.org,COCO,"This project includes the front end user interfaces that are used to annotate COCO dataset. For more details, please visit ","
"
941,https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#,Assamese handwriting recognition,"
","
"
941,http://www.nlpr.ia.ac.cn/databases/handwriting/Online_database.html,Chinese handwriting for recognition,"
","
"
941,https://www.kaggle.com/c/datasciencebowl,"Kaggle plankton recognition competition, 2015","
", Third place. The competition solution is being adapted for research purposes in 
945,http://data-sorcery.org/contents/,,"
","
"
986,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format,Having a training.conll file and a development.conll formatted according to the ,", to train a parsing model with the LSTM parser type the following at the command line prompt:"
986,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format,Having a test.conll file formatted according to the ,"
"
996,http://torch7.s3-website-us-east-1.amazonaws.com/data/svhn.t7.tgz,here,"SVHN dataset in Torch format, available ",. Please note that running on SVHN requires roughly 28GB of RAM for dataset loading.
1004,https://github.com/klout/opendata/blob/master/wiki_annotation/README.md,DAWT: Densely Annotated Wikipedia Texts across multiple languages,"
", (
1008,classical-stats-and-social-data-101,Classical statistics applied to social data,"
","
"
1021,https://github.com/wnzhang/make-ipinyou-data,make-ipinyou-data,"For the large-scale experiment, please first check our GitHub project ", for pre-processing the 
1021,http://data.computational-advertising.org,iPinYou data, for pre-processing the ,". After downloading the dataset, by simplying "
1024,http://linkedgeodata.org,LinkedGeoData,Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the , project.
1024,http://stack.linkeddata.org,Linked Data Stack,Via the , (recommended)
1024,http://stack.linkeddata.org,Linked Data Stack,Sparqlify is distributed at the ,", which offers many great tools done by various contributors of the Semantic Web community."
1024,http://stack.linkeddata.org/download/repo.php,here,"
",.
1032,http://ranisz.iis.p.lodz.pl/indexes/data/text_files/200/,http://ranisz.iis.p.lodz.pl/indexes/data/text_files/200/,Before launching a benchmark please download the data files from , and extract them into the main project folder.
1047,https://github.com/FasterXML/jackson-databind,jackson-databind," like it, but this one is ours. Codec has its own reflection toolset, but we are moving to wards using more and more jackson in the back end. Currently, the most notable features are those that are built on top of ", and 
1067,http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz,cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/ubuntu_dialogs.tgz,": directory where 1on1 dialogs will downloaded and extracted, the data will be downloaded from ", (default = '.')
1068,https://www.dropbox.com/s/f7q3bbgvat2q1u2/cifar10-dataset.zip?dl=0,DropBox,", ",", "
1102,tree/master/core/src/main/scala/org/dbpedia/extraction/dataparser,org.dbpedia.extraction.dataparser, Parsers to extract data from nodes in the abstract syntax tree. All classes are located in the namespace ,"
"
1121,http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/,EDUB-Seg dataset,The training and validation of the code was performed using the ,.
1127,http://building-babylon.net/2016/01/26/metadata-embeddings-for-user-and-item-cold-start-recommendations/,Metadata Embeddings for User and Item Cold-start Recommendations,"
","
"
1139,http://www.cs.utexas.edu/users/ml/nldata/geoquery.html,Geoquery,A lexicon of , is available in lex/
1151,ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/HG00122/sequence_read/,ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/HG00122/sequence_read/, of the 1000 Genomes Project individual HG00122: ,"
"
1182,http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2016arXiv160309320M&data_type=BIBTEX&db_key=PRE&nocookieset=1,"
[BibTex]
",", abs/1603.09320. ","
"
1185,https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type,Conflict-free Replicated Data Types," (RON), a distributed live data format. In turn, RON is based on "," (CRDTs), a new math apparatus for distributed data."
1185,https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#PN-Counter_(Positive-Negative_Counter),Counter,[ ] ,", a positive-negative counter."
1186,https://travis-ci.org/tonsky/datascript,"
Build Status
",Latest version ,"
"
1186,https://cljdoc.org/d/datascript/datascript/CURRENT,"
cljdoc badge
",API Docs ,"
"
1186,https://github.com/tonsky/datascript/wiki/Getting-started,Getting started,"
","
"
1186,https://github.com/kristianmandrup/datascript-tutorial,Tutorials,"
","
"
1186,https://github.com/tonsky/datascript/wiki/Tips-&-tricks,Tips & tricks,"
","
"
1186,http://www.learndatalogtoday.org/,Quick tutorial into Datalog,"
","
"
1186,http://tonsky.me/blog/datascript-internals/,DataScript internals explained,"
","
"
1186,https://skillsmatter.com/skillscasts/6038-datascript-for-web-development,video,", ","
"
1186,https://github.com/tonsky/datascript-todo,app,", ","
"
1186,https://github.com/tonsky/datascript-transit,DataScript-Transit,"
",", transit serialization for database and datoms"
1186,https://github.com/typeetfunc/datascript-mori,DataScript-mori,"
",", DataScript & Mori wrapper for use from JS"
1186,https://github.com/djjolicoeur/datamaps,Datamaps,"
",",  lib designed to leverage datalog queries to query arbitrary maps."
1186,http://simonb.com/blog/2016/01/24/om-next-datascript-localisation-demo/,Localisation Demo with Om Next,"
","
"
1186,https://github.com/tonsky/datascript-todo,sources,"ToDo, task manager demo app (persistence via localStorage and transit, filtering, undo/redo): ",", "
1186,http://tonsky.me/datascript-todo/,live,", ","
"
1186,https://github.com/tonsky/datascript-chat,sources,"CatChat, chat demo app: ",", "
1186,http://tonsky.me/blog/datascript-chat/,code walkthrough,", ",", "
1186,http://tonsky.me/datascript-chat/,live,", ","
"
1186,http://thegeez.net/2014/04/30/datascript_clojure_web_app.html,blog post,", ","
"
1186,https://github.com/madvas/todomvc-omnext-datomic-datascript,OmNext TodoMVC,"
","
"
1186,test/datascript/test/,our acceptance test suite,"For more examples, see ",.
1186,https://www.npmjs.org/package/datascript,npm page,or as a CommonJS module (,):
1192,http://linkeddata.org/,Linked Data,Allowing , service descriptions
1204,http://github.com/rug-compling/dep-brown-data,induced clusters and experimental details, COLING. See also ,.
1221,http://grouplens.org/datasets/movielens/,MovieLens data,We have tested our matrix factorization algorithm on the ,". In particular, we have used the 1M, 10M, and 20M datasets (after a straightforward preprocessing step to make it compatible with the HAMSI input format)."
1258,http://www.nltk.org/data.html,here,(*) The stopword corpus is needed. Instructions ,.
1268,http://corpus-texmex.irisa.fr/,GIST,    | 60000   | 784  | images of hand-written digits | | News     | 262144  | 1000 | web pages converted into TF-IDF representation | | ,     | 1000000 | 960  | global color GIST descriptors | | 
1268,http://corpus-texmex.irisa.fr/,SIFT,     | 1000000 | 960  | global color GIST descriptors | | ,     | 2500000 | 128  | local SIFT descriptors | | 
1282,http://snap.stanford.edu/data/com-Amazon.html,SNAP,Here we use Amazon dataset (obtained from , website) as an illustration. You may switch to other datasets with corresponding file format as well. Note that some parameters might need to be adjusted accordingly based on the properties of network under test.
1282,http://snap.stanford.edu/data/com-Amazon.html,http://snap.stanford.edu/data/com-Amazon.html,amazon dataset (available at ,)
1301,http://ifcb-data.whoi.edu/mvco,IFCB Data Dashboard.,) starting in 2006 and continuing to the present. Near real time image data and the complete archive are accessible for browse and download at the ,"
"
1313,https://storage.googleapis.com/refexp/google_refexp_dataset_release.zip,link,You can download the GoogleRefexp data directly from this ,.
1347,http://www.cvlibs.net/download.php?file=data_stereo_flow.zip,KITTI 2012,the , data set and unzip it into 
1347,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI 2015,the , data set and unzip it into 
1347,http://www.cvlibs.net/datasets/kitti/user_submit.php,KITTI evaluation server, and can be used to submit to the ,.
1351,https://snap.stanford.edu/data/loc-gowalla.html,Gowalla,"
",: the pre-processed data that we used in the paper can be downloaded 
1351,http://dawenl.github.io/data/gowalla_pro.zip,here,: the pre-processed data that we used in the paper can be downloaded ,.
1355,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI dataset, cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the ," as stereo or monocular, in the "
1355,http://vision.in.tum.de/data/datasets/rgbd-dataset,TUM dataset," as stereo or monocular, in the "," as RGB-D or monocular, and in the "
1355,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC dataset," as RGB-D or monocular, and in the "," as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. "
1355,http://vision.in.tum.de/data/datasets/rgbd-dataset/tools,associate.py,Associate RGB images and depth images using the python script ,. We already provide associations for some of the sequences in 
1374,http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/,EDUB-Seg dataset,The , has been used in this paper. Please cite to 
1374,https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application,BigGraph TEC2013-43935-R, | |  This work has been developed in the framework of the project ," and and TIN2012-38187-C03-01, funded by the Spanish Ministerio de Economía y Competitividad and the European Regional Development Fund (ERDF).  | "
1407,http://groups.inf.ed.ac.uk/hbilen-data/data/WSDDN/EdgeBoxesVOC2007trainval.mat,trainval,b.  Pre-computed edge-boxes for , and 
1407,http://groups.inf.ed.ac.uk/hbilen-data/WSDDN/EdgeBoxesVOC2007test.mat,test, and , splits:
1407,http://groups.inf.ed.ac.uk/hbilen-data/data/WSDDN/wsddn.mat,VGGF-EB-BoxSc-SpReg,You can also download the pre-trained WSDDN model (,). Note that it gives slightly different performance reported than in the paper (34.4% mAP instead of 34.5% mAP)
1435,https://github.com/sstober/openmiir/wiki/How-to-import-the-raw-EEG-data-into-EEGLab,in the wiki,"This data format can, for instance, also be easily converted into the MAT format used by Matlab, which allows importing into EEGLab. A description on how to do this can be found ",.
1435,http://opendatacommons.org/licenses/pddl/1-0/,Open Data Commons Public Domain Dedication and Licence (PDDL),OpenMIIR is released under the ,", which means that you can freely use it without any restrictions."
1463,#dataset,Dataset,"
","
"
1463,http://vml.cs.sfu.ca/wp-content/uploads/volleyballdataset/volleyball.zip,Old Download Link,"
",.
1488,http://yaroslavvb.blogspot.fi/2011/09/notmnist-dataset.html,not_mnist, and , data sets
1506,https://github.com/Element-Research/dataload#dl.loadGBW,Google Billion Words dataset, language models on the ,. The example uses 
1506,https://github.com/Element-Research/dataload,dataload,"
", : a collection of torch dataset loaders;
1552,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford,This repository contains scripts to download the , and 
1552,http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris, and ," image retrieval benchmark datasets, as well as the VGG16 pre-trained convolutional neural network model for feature extraction. The Python scripts in this repository offer general utilities for extracting features from "
1574,https://github.com/v-m/PropagationAnalysis-dataset,here,Dataset generated for those papers can be found ,.
1586,http://202.112.113.8/d/DV-ngram/alldata-id_p1gram.zip,unigram,Preprocessed data for IMDB dataset can be downloaded here: ,", "
1586,http://202.112.113.8/d/DV-ngram/alldata-id_p2gram.zip,bigram,", ",", "
1586,http://202.112.113.8/d/DV-ngram/alldata-id_p3gram.zip,trigram,", ","
"
1591,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe: Global Vectors for Word Representation,"
","
"
1591,http://www.nltk.org/data.html,NLTK Data,"
",: punkt
1591,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe: Global Vectors for Word Representation,"
","
"
1591,https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/,WikiQA: A Challenge Dataset for Open-Domain Question Answering,"
","
"
1591,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe: Global Vectors for Word Representation,"
","
"
1608,https://github.com/JJ/splash-volunteer/tree/data,"
data
",Data files are in the ," branch, divided by experiment sets."
1615,https://github.com/wnzhang/make-ipinyou-data,iPinYou data formalizing repository,"
","
"
1616,http://vowl.visualdataweb.org/webvowl/,WebVOWL,Integration with diagram creators (,).
1616,https://oops.linkeddata.es/,OOPS! web service,Evaluation reports of your ontology (using the ,)
1616,doc/metadataGuide/guide.md,the Widoco metadata documentation,"To see how WIDOCO recognizes metadata annotations in your ontology to create the documentation files, see ",". To learn which metadata properties we recommend adding to your ontology for producing a nice-looking documentation, have a look at our "
1616,doc/metadataGuide/guide.md,Widoco metadata guide,"For more information, see the ","
"
1620,http://wikidata.org/,Wikidata,"There are many other projects developing schemas and ontologies for the Web, e.g. ", or the vocabulary projects in the 
1624,#example-data,Example data,"
","
"
1624,http://lov.okfn.org/dataset/lov/vocabs/mv,@lov.okfn.org,LOV entry: ,"
"
1624,http://www.visualdataweb.de/webvowl/#iri=http://schema.mobivoc.org/,@visualdataweb.de,WebVOWL Visualization: ,"
"
1624,http://oops.linkeddata.es/response.jsp?uri=http://schema.mobivoc.org/#,@oops.linkeddata.es,oops Report: , (slow)
1624,schema/Metadata.ttl,schema/Metadata.ttl,"
", - ontology metadata
1638,https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#loading-and-storing-persistent-data,Read-through and write-through,"
", caching patterns
1638,https://docs.hazelcast.com/hazelcast/latest/data-structures/map.html#map-eviction,Expiring items,"
", automatically based on certain criteria like TTL or last access time
1638,https://docs.hazelcast.com/hazelcast/latest/data-structures/distributed-data-structures.html,distributed data structures,"Hazelcast also provides additional data structures such as ReplicatedMap, Set, MultiMap and List. For a full list, refer to the ", section of the docs.
1638,https://docs.hazelcast.com/hazelcast/latest/data-structures/topic.html,docs,"For examples in other languages, please refer to the ",.
1638,https://docs.hazelcast.com/hazelcast/latest/data-structures/queue.html,docs,"For examples in other languages, please refer to the ",.
1657,http://datatables.net,DataTables,. This demo uses , to create a test page with a number of events attached to different elements.
1680,data,"
data
",Check the ," dir for the raw and processed files, "
1688,https://github.com/ekzhu/datasketch,Datasketch,"
","
"
1688,http://corpus-texmex.irisa.fr/,GIST, (217MB) | | ,"                            |        960 |  1,000,000 |     1,000 |       100 | Euclidean | "
1688,http://fimi.uantwerpen.be/data/,Kosarak, (918MB)           | | ,"                        |      27,983 |     74,962 |       500 |       100 | Jaccard   | "
1688,https://grouplens.org/datasets/movielens/10m/,MovieLens-10M, (217MB)         | | ,"  |      65,134 |     69,363 |       500 |       100 | Jaccard   | "
1688,https://archive.ics.uci.edu/ml/datasets/bag+of+words,NYTimes, (63MB)             | | ,"   |        256 |    290,000 |    10,000 |       100 | Angular   | "
1688,http://corpus-texmex.irisa.fr/,SIFT, (301MB)         | | ,"                           |        128 |  1,000,000 |    10,000 |       100 | Euclidean | "
1690,https://motchallenge.net/data/MOT15/,2D MOT 2015 benchmark dataset,Download the ,"
"
1709,https://www.force11.org/group/joint-declaration-data-citation-principles-final,Joint Declaration of Data Citation Principle,See , as a example of a similar deliverable.
1715,https://gsuite.google.com/marketplace/app/wikipedia_and_wikidata_tools/595109124715?pann=cwsdp&hl=en,Wikipedia Tools,Find the ," in the Google Add-on Store and install them by clicking on the blue ""+ Free"" button."
1734,http://numba.pydata.org,"
numba
",", ", or 
1737,https://cs.stanford.edu/~jhoffman/domainadapt/#datasets_code,Office,", we give the lists of three domains in ", dataset.
1742,https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz,here,Available ,"
"
1742,https://www.cs.jhu.edu/~mdredze/datasets/sentiment/,here,I got them from ,"
"
1762,http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets,here,All the benchmark datasets are publicly available ,. These datasets were first introduced by 
1762,http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets,here," dataset is provided as an example. To deploy a new benchmark dataset, download the zip file from ", and then uncompress it. You will get a dataset folder including two files 
1762,https://github.com/yuyuz/FLASH/tree/master/data,data directory,. Move this folder into the ,", just like the dataset folder "
1766,https://maven-badges.herokuapp.com/maven-central/info.debatty/java-datasets,"
Maven Central
","
","
"
1766,https://travis-ci.org/tdebatty/java-datasets,"
Build Status
","
","
"
1766,http://www.javadoc.io/doc/info.debatty/java-datasets,"
Javadocs
","
","
"
1766,./src/main/java/info/debatty/java/datasets/dblp/,DBLP dataset,"
","
"
1766,./src/main/java/info/debatty/java/datasets/reuters/,Reuters-21578 dataset,"
","
"
1766,./src/main/java/info/debatty/java/datasets/textfile/,Text file dataset,"
","
"
1766,./src/main/java/info/debatty/java/datasets/enron/,ENRON email dataset,"
","
"
1766,./src/main/java/info/debatty/java/datasets/wikipedia/,Wikipedia web pages,"
","
"
1766,./src/main/java/info/debatty/java/datasets/gaussian/,Synthetic gaussian mixture,"
","
"
1766,./src/main/java/info/debatty/java/datasets/sift/,Scale-invariant feature transform (SIFT) dataset,"
","
"
1766,https://github.com/tdebatty/java-datasets/releases,GitHub releases,Or check the ,.
1766,./src/main/java/info/debatty/java/datasets/examples,the examples,"For the other datasets, check ",", or "
1766,http://www.javadoc.io/doc/info.debatty/java-datasets,the documentation,", or ",.
1770,http://potree.entwine.io/data/custom.html?r=http://localhost:8080/red-rocks/ept.json,Potree,And view the data with , and 
1781,https://github.com/dune-community/dune-xt-data,dune-xt-data,"
","
"
1786,http://bokeh.pydata.org/en/latest/docs/installation.html,Bokeh,"
", 0.8.1+
1789,../master/data/lenet_memory_train_test.prototxt#L10-L12,illustrated,"As a distributed extension of Caffe, CaffeOnSpark supports neural network model training, testing, and feature extraction.  Caffe users can now perform distributed learning using their existing LMDB data files and minorly adjusted network configuration (as ",).
1801,https://github.com/facebook/zstd#the-case-for-small-data-compression,dictionary compression,LZ4 is also compatible with ,", both at "
1815,https://travis-ci.org/stanford-futuredata/macrobase,"
Build Status
","
","
"
1815,https://coveralls.io/github/stanford-futuredata/macrobase?branch=master,"
Coverage Status
","
","
"
1820,documents/data.md,documents/data.md,Data Formats: ,"
"
1821,http://grouplens.org/datasets/movielens/,MovieLens,"This script will turn an external raw dataset into torch format. The dataset will be split into a training/testing set by using the training ratio. When side inforamtion exist, they are automatically appended to the inputs. The ", and 
1821,https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban,Douban, and , dataset are supported by default.
1821,http://grouplens.org/datasets/movielens/1m/,MovieLens-1M,| Dataset       | user info | item info  | item tags | | :-------      | --------: | :--------: | --------: | | ,  | true      |  true      |  false    | | 
1821,http://grouplens.org/datasets/movielens/10m/,MovieLens-10M,  | true      |  true      |  false    | | , | false     |  true      |  true     | | 
1821,http://grouplens.org/datasets/movielens/20m/,MovieLens-20M, | false     |  true      |  true     | | , | false     |  true      |  true     | | 
1821,https://www.cse.cuhk.edu.hk/irwin.king/pub/data/douban,Douban, | false     |  true      |  true     | | ,       | true      |  info      |  false    |
1822,http://saliency.mit.edu/datasets.html,MIT300,"
",.
1822,https://imatge.upc.edu/web/projects/biggraph-heterogeneous-information-and-graph-signal-processing-big-data-era-application,BigGraph TEC2013-43935-R, | |  This work has been developed in the framework of the project ,", funded by the Spanish Ministerio de Economía y Competitividad and the European Regional Development Fund (ERDF).  | "
1824,http://data.computational-advertising.org,iPinYou dataset,"For running further large-scale experiments, you will rely on another repository which is written for ", feature engineering.
1824,https://github.com/wnzhang/make-ipinyou-data,make-ipinyou-data,Please check our GitHub project ,". After downloading the dataset, by simplying "
1830,https://github.com/hans/adversarial/tree/master/data/lfwcrop_color,in the data/lfwcrop_color folder,"You can perform your own training runs using these YAML files. The paths in the YAML files reference my own local data; you'll need to download the LFW dataset and change these paths yourself. The ""file-list"" and embedding files referenced in the YAML files are available for LFW ",". Once you have the paths in the YAML file, you can start training a model with the simple invocation of Pylearn2's "
1830,https://github.com/hans/adversarial/blob/master/sampler/data_browser.py,"
data_browser
","
","
"
1831,data-sample.txt,data sample,Please take a look the ," file. In general, each row is one sentence for monologue ("
1854,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, files formatted according to the ,. For the faster graph-based parser change directory to 
1854,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, file formatted according to the , with a previously trained model is:
1856,https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/skill-builder-data-2009-2010,Assistments 2009-2010 dataset,We have included the skill builder version of the , which is one of the datasets evaluated in the paper.
1857,https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_dataset.txt,here,"WCRP assumes that your student data are in a space-delimited text file with one row per trial. The columns should correspond to a trial's student ID, item ID, and whether the student produced a correct response in the trial. The IDs should be integers beginning at 0, and the trials for each student should be ordered from least to most recent. An example data file is available ","
"
1857,https://github.com/robert-lindsey/WCRP/blob/master/datasets/spanish_expert_labels.txt,here,"Our model's nonparametric prior distribution over skill labels can leverage skill labels provided by a human domain expert. If you want to provide them to our model, create a text file with one line per item. The ith line should be the expert-provided skill ID for the ith item. You can provide the file to WCRP via the command line argument --expertfile. An example file is available ","
"
1878,http://alt.qcri.org/semeval2014/task3/index.php?id=data-and-tools,SemEval 2014 Cross-level Semantic Similarity Task,[ ] , (TODO; 500 paragraph-to-sentence training items)
1878,http://allenai.org/data.html,AI2 8th Grade Science Questions,"
"," are 641 school Science quiz questions (A/B/C/D test format), stemming from "
1878,https://www.cs.york.ac.uk/semeval-2013/task7/index.php%3Fid=data.html,STS2013 Joint Student Response Analysis (RTE-8),[ ] ,"
"
1878,http://www.nist.gov/tac/data/,TAC tracks RTE-4 to RTE-7,[ ] ,. Printed user agreement required.
1885,https://carbondata.apache.org,Apache CarbonData,"
",","
1885,https://www.influxdata.com,InfluxDB, also use Roaring bitmaps with their own implementations. You find Roaring bitmaps in ,", "
1891,https://www.fc.up.pt/addi/ph2%20database.html,PH<sup>2</sup> database, package to study the ," of dermoscopic images. This image database contains a total of 200 dermoscopic images of melanocytic lesions, including, from benign to more serious, 80 common nevi, 80 atypical nevi, and 40 melanomas. The goal is to verify if images of the three types of lesions form statistically distinguishable samples."
1903,https://github.com/rodrigo-pena/load-data,load-data,"), you will need to clone another repository, ",", and follow the installation instructions in the README therein."
1915,http://www.comp.leeds.ac.uk/mat4saj/lspet_dataset.zip,LSP Extended dataset,"
","
"
1915,http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz,Annotation,"
","
"
1915,http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz,Images,"
","
"
1929,http://disi.unitn.it/~dle/dataset/TUHOI.html,TUHOI, and , datasets.
1950,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/19.08/mini.h5,19.08/mini.h5,  | , | | 17.06    | 
1950,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5,17.06/mini.h5,  | , | | 17.04    | 
1950,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.05/mini.h5,17.05/mini.h5, | , | | 17.02    | 
1950,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/16.09/numberbatch.h5,16.09/numberbatch.h5,  |                              | | 16.09    |                                         |                                           | , |
1968,https://snap.stanford.edu/data/egonets-Facebook.html,facebook,"
",", should find 5"
1971,http://www.datasyslab.org/,DataSys Lab,Hippo index is one of the projects under ," at Arizona State University. The mission of DataSys Lab is designing and developing experimental data management systems (e.g., database systems)."
1980,https://github.com/countries/countries-data-yaml,YAML,The data used in this gem is also available as git submodules in , and 
1980,https://github.com/countries/countries-data-json,JSON, and , files.
2003,https://github.com/keunwoochoi/lstm_real_book/blob/master/more_data_to_play_with/jazz_xlab.zip,2486 .lab files,Use , to do even more interesting!
2005,https://www.crossref.org/services/metadata-delivery/,a variety of tools and APIs,The Crossref REST API is one of , that allow anybody to search and reuse our members' metadata in sophisticated ways.
2005,https://www.crossref.org/services/metadata-delivery/plus-service/,have a service-level offering," guidelines? Well, if you’re interested in using these tools or APIs for production services, we "," called ""Plus"". This service provides you with access to all supported APIs and metadata, but with extra service and support guarantees."
2005,https://www.crossref.org/services/metadata-delivery/plus-service/,Metadata Plus subscribers, (to ,) or 
2005,https://www.crossref.org/services/metadata-delivery/plus-service/,Metadata Plus subscribers, (to ,) or 
2006,https://github.com/kermitt2/article-dataset-builder,article-dataset-builder,"Finally, the following python utilities can be used to create structured full text corpora of scientific articles. The tool simply takes a list of strong identifiers like DOI or PMID, performing the identification of online Open Access PDF, full text harvesting, metadata aggregation and Grobid processing in one workflow at scale: ","
"
2006,https://grobid.readthedocs.io/en/latest/Principles/#training-data-qualitat-statt-quantitat,"small, high quality sets","GROBID does not use training data derived from existing publisher XML documents, but ", of manually labeled training data.
2006,https://github.com/kermitt2/datastet,datastet,"
",: identification of named and implicit research datasets and associated attributes in scientific articles
2006,https://github.com/dataseer/dataseer-ml,dataseer-ml,"
",": identification of sections and sentences introducing datasets in a scientific article, and classification of the type of these datasets"
2013,#datasets,Datasets,"
","
"
2021,https://people.cs.umass.edu/~pat/data/naacl-data.tar.gz,here," by Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, Andrew McCallum. NAACL 2016. (You can download our training data ", and some pretrained models 
2021,https://people.cs.umass.edu/~pat/data/EACL_rowless_entity_types.tar.gz,here, branch. You can download our entity type data , )
2024,http://crcv.ucf.edu/data/UCF101.php,UCF101,Pre-computed optical flow images and resized rgb frames for the , and 
2024,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB51, and , datasets
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.001,part1,UCF101 RGB: ,"
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.002,part2,"
","
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.003,part3,"
","
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.001,part1,UCF101 Flow: ,"
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.002,part2,"
","
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.003,part3,"
","
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_jpegs_256.zip,part1,HMDB51 RGB: ,"
"
2024,http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_tvl1_flow.zip,part1,HMDB51 Flow: ,"
"
2031,https://www.kaggle.com/c/rossmann-store-sales/data,Kaggle, files on , and put them in this folder.
2043,http://www.princeton.edu/~jzthree/datasets/ICML2014/,website,"For cb513+profile_split1.npy.gz, cullpdb+profile_6133_filtered.npy.gz, please download from this ",". For CASP10 and CASP11, please download from this "
2046,http://snap.stanford.edu/data/index.html,SNAP format, converts a graph in ," and converts it to Ligra's adjacency graph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name. The ""-s"" flag may be used to symmetrize the input file. This converter works for any format that lists the two endpoints of each edge separated by white space per line, with lines starting with '#' ignored."
2046,http://snap.stanford.edu/data/index.html,SNAP format, converts a communities network in , and converts it to symmetric adjacency hypergraph format. The first required parameter is the input (SNAP) file name and second required parameter is the output (Ligra) file name.
2047,https://lrs.icg.tugraz.at/datasets/prid/prid_2011.zip,PRID,"
","
"
2047,http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip,3DPeS,"
","
"
2047,http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan,Shinpuhkan,"
", (need to send an email to the authors)
2104,http://caffe2.ai/docs/datasets.html,Datasets,"
","
"
2111,https://www.coursera.org/specializations/tensorflow-data-and-deployment,TensorFlow: Data and Deployment from Coursera,"
","
"
2124,http://numba.pydata.org/,Numba,"
","
"
2139,http://acube.di.unipi.it/rlzap-dataset/,here,"This project is an implementation of the compressed relative index strategy outlined in the paper ""RLZAP: Relative Lempel-Ziv with Adaptive Pointers"". The dataset used for the experiments is available ","
"
2210,https://archive.ics.uci.edu/ml/datasets/Adult,UCI Adult Data Set,"We cannot release our dataset publicly, so the a toy dataset needs to be set up. We use the "," for this purpose. Although it is a regular tabular dataset, and has a Gausian distribution instead of a heavy tailed one over the attributes, it is fine for experimenting. Then you can work on your own."
2211,http://google.github.io/rappor/doc/data-flow.html,RAPPOR Data Flow,"
","
"
2224,http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/,here,This will download the COCO-QA dataset from , and generate two files under the 
2224,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro.json,here," model, you should use the corresponding json file form ","
"
2224,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/vqa_data_prepro_all.json,here," model, you should use the corresponding json file form ","
"
2224,http://datasets.d2.mpi-inf.mpg.de/mateusz14visual-turing/calculate_wups.py,here," folder. If you need to evaluate based on WUPS, download the evaluation script from ","
"
2224,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/co_atten/data_file/,here,Some of the data file can be download at ,"
"
2225,http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/,Airport,"
", Partition comes with dataset
2225,http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/dataset/DukeMTMC4ReID.zip,DukeMTMC4ReID,"
", Partition comes with dataset
2232,./data/US/processed_HealthMap/,Processed HealthMap U.S.,"
","
"
2232,./data/China/processed_HealthMap/,Processed HealthMap China,"
","
"
2232,./data/India/processed_HealthMap/,Processed HealthMap India,"
","
"
2232,./data/US/topic_distributions/,Topic distributions U.S.,"
","
"
2232,./data/China/topic_distributions/,Topic distributions China,"
","
"
2232,./data/India/topic_distributions/,Topic distributions India,"
","
"
2232,./data/US/case_counts/,Case counts U.S.,"
","
"
2232,./data/China/case_counts/,Case counts China,"
","
"
2232,./data/India/case_counts/,Case counts India,"
","
"
2243,http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip,The pre-trained GloVe vectors,"
", were trained using Common Crawl (release unknown) containing 
2252,http://conll.cemantix.org/2012/data.html,here,Download the CoNLL training data from ,.
2264,https://github.com/theodi/data-definitions#open-and-personal,Below is more on personal data meets open data,"
",.
2264,http://theodi.github.io/data-definitions,Click here for the Venn diagram page,"
",.
2264,http://www.opentracker.net/article/definitions-big-data,many,There is no formal definition of Big Data – though ," have tried. Most of them agree that it is more than 'large data', more than just its size."
2264,https://theodi.org/blog/data-sharing-is-not-open-data,data sharing is not open data,Note how ,"
"
2264,http://theodi.org/blog/data-sharing-is-not-open-data,Shared data is not open data.,. ,"
"
2270,https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4,open-source dataset, (see below for full citation). The repository comes with an , with dense voxel-level annotations for vasculature that we hope that stimulate further research on vascular segmentation using deep learning networks.
2270,https://github.com/petteriTeikari/vesselNN/blob/master/configs/ZNN_configs/datasetPaths/dataset_forVD2D.spec,dataset_forVD2D3D.spec,"
"," defines path for image/label files of your dataset. For example images 1-12 refer to the images from the microscope, 13-24 refer to the output images from the VD2D part of the recursive ""two-stage"" approach of ZNN. The outputs of the VD2D part are provided in the "
2270,https://github.com/petteriTeikari/vesselNN_dataset/tree/4daf46cee49f411b759f04ff92b92dd1dbbc25b4/experiments/VD2D_tanh,dataset repository," defines path for image/label files of your dataset. For example images 1-12 refer to the images from the microscope, 13-24 refer to the output images from the VD2D part of the recursive ""two-stage"" approach of ZNN. The outputs of the VD2D part are provided in the ",", and can be used for re-training of the VD2D3D stage, or if you may you can obtain new VD2D outputs for your dataset if you wish."
2282,http://cs.stanford.edu/~danqi/data/cnn.tar.gz,http://cs.stanford.edu/~danqi/data/cnn.tar.gz,CNN: , (546M)
2282,http://cs.stanford.edu/~danqi/data/dailymail.tar.gz,http://cs.stanford.edu/~danqi/data/dailymail.tar.gz,Daily Mail: , (1.4G)
2282,https://github.com/deepmind/rc-data,https://github.com/deepmind/rc-data,The original datasets can be downloaded from , or 
2282,http://nlp.stanford.edu/data/glove.6B.zip,http://nlp.stanford.edu/data/glove.6B.zip,glove.6B.zip: ,"
"
2285,https://github.com/harvardnlp/BSO/tree/master/data_prep,data_prep/,First prepare the data as in ,.
2288,http://web.science.mq.edu.au/~dqnguyen/papers/TACL-datasets.zip,[Datasets],"
","
"
2298,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/vqaRelease/train_only/data_train_val.zip,here,", you can download the features from ",", and the pretrained model from "
2298,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/vqaRelease/train_val/data_train-val_test.zip,here,", you can download the feature from ",", and the pretrained model from "
2315,#preparing-and-using-the-coco-and-pascal-datasets,Preparing and using the COCO and PASCAL datasets,"
","
"
2317,http://mscoco.org/dataset/#detections-challenge2015,COCO,"This repository contains the original models (ResNet-50, ResNet-101, and ResNet-152) described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). These models are those used in [ILSVRC] (http://image-net.org/challenges/LSVRC/2015/) and "," 2015 competitions, which won the 1st places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
2334,http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip,here,Download the official SemEval evaluation materials from , and put the file 
2346,https://github.com/tensorpack/dataflow,"
tensorpack.dataflow
",Squeeze the best data loading performance of Python with ,.
2346,https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions,does not,) , offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of 
2354,http://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/,here,The JIGSAWS dataset is available ,". After registering, an automated system will send you a download link. To run this code, you'll need to download the Suturing Kinematics data."
2361,https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp,Recursive Length Prefix,  | Developer utility tool to convert binary RLP (,) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. 
2365,http://cdn.cai.fi/datasets/shapes50k_20x20_compressed_v2.h5,here,) can also be downloaded from ,"
"
2365,http://cdn.cai.fi/datasets/freq20-1MNIST_compressed.h5,here,) can also be downloaded from , and 
2365,http://cdn.cai.fi/datasets/freq20-2MNIST_compressed.h5,here, and ,"
"
2370,http://buckeyecorpus.osu.edu/,buckeyecorpus.osu.edu,Buckeye corpus: ,"
"
2404,http://conda.pydata.org/miniconda.html,"Install conda
","
","
"
2405,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE,The program also provides the implementation of the embedding model ,. See an overview of embedding models of entities and relationships for knowledge base completion at 
2414,https://github.com/antlr/codebuff/tree/master/corpus,corpora, dir for leave-one-out formatting of the various ,". But, here are some sample formatting results."
2414,https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava,guava corpus,"
", and 
2414,https://github.com/antlr/codebuff/tree/master/corpus/java/training/guava,guava corpus,"
", and 
2414,https://github.com/antlr/codebuff/tree/master/corpus/antlr4/training,antlr corpus,"
", and 
2416,https://github.com/flashxio/knor/blob/master/test-data/matrix_r50_c5_rrw.bin?raw=true,"
$KNOR_HOME/test-data/matrix_r50_c5_rrw.bin
",We maintain a very small dataset for testing: ,.
2449,#supplementary-data,Supplementary data,"
","
"
2449,https://fasttext.cc/docs/en/dataset.html#content,YFCC100M data,The preprocessed , used in [2].
2456,https://github.com/numenta/NAB/tree/master/data,data readme,"The majority of the data is real-world from a variety of sources such as AWS server metrics, Twitter volume, advertisement clicking metrics, traffic data, and more. All data is included in the repository, with more details in the ",. Please contact us at 
2468,#data_encription,Data Encryption,"
","
"
2468,#datasets,Datasets and Simulators,"
","
"
2468,https://zhangjiqing.com/dataset/,dataset,", ",.
2468,http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/,dataset,", ",.
2468,http://wp.doc.ic.ac.uk/pb2114/datasets/,Dataset: 4 sequences,", ","
"
2468,https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0,Dataset,", ","
"
2468,https://datasets.hds.utc.fr/share/er2aA4R0QMJzMyO,Dataset,", ",", "
2468,https://github.com/wl082013/ESIM_dataset,Dataset,", ","
"
2468,#1mpx_detection_dataset,1Mpx Detection Dataset,", Advances in Neural Information Processing Systems 33 (NeurIPS), 2020. ","
"
2468,#dvsgesture_dataset,Dataset,", ","
"
2468,#ncars_dataset,N-CARS Dataset,"
",: A large real-world event-based dataset for car classification.
2468,https://sites.google.com/a/udayton.edu/issl/software/dataset,Dataset,", ","
"
2468,http://rpg.ifi.uzh.ch/davis_data.html,"The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM","
",","
2468,http://rpg.ifi.uzh.ch/davis_data.html,Dataset,", ",.
2468,http://sensors.ini.uzh.ch/databases.html,Datasets from the Sensors group at INI,"
"," (Institute of Neuroinformatics), Zurich:"
2468,http://wp.doc.ic.ac.uk/pb2114/datasets/,Four sequences,", ","
"
2468,https://sites.google.com/a/udayton.edu/issl/software/dataset?authuser=0,DVSMOTION20 Dataset,. ,"
"
2468,http://wp.doc.ic.ac.uk/pb2114/datasets/,Four sequences,", ","
"
2468,http://rpg.ifi.uzh.ch/davis_data.html,"The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM","
",","
2468,http://rpg.ifi.uzh.ch/davis_data.html,Dataset,", ",.
2468,http://sensors.ini.uzh.ch/databases.html,Dataset," Int. Conf. Machine Learning, Workshop on Machine Learning for Autonomous Vehicles, 2017. ","
"
2468,https://sites.google.com/view/davis-driving-dataset-2020/home,Dataset,", IEEE Intelligent Transportation Systems Conf. (ITSC), 2020. ",", "
2468,http://sensors.ini.uzh.ch/databases.html,More datasets,", ","
"
2468,https://vision.in.tum.de/data/datasets/visual-inertial-event-dataset,TUM-VIE: The TUM Stereo Visual-Inertial Event Data Set,"
","
"
2468,https://visibilitydataset.github.io/,ViViD++: Vision for Visibility Dataset,"Lee, A. J., Cho, Y., Shin, Y., Kim, A., Myung, H., ",", IEEE Robotics and Automation Letters (RA-L), 2022. "
2468,https://visibilitydataset.github.io/,Dataset,", IEEE Robotics and Automation Letters (RA-L), 2022. ","
"
2468,https://star-datasets.github.io/vector/,Dataset,", ",", "
2468,https://github.com/mgaoling/mpl_dataset_toolbox,MPL Dataset Toolbox,", ",.
2468,http://www.garrickorchard.com/datasets/n-mnist,Neuromorphic-MNIST (N-MNIST) dataset,"
", is a spiking version of the original frame-based MNIST dataset (of handwritten digits). 
2468,http://www.garrickorchard.com/datasets/n-caltech101,The Neuromorphic-Caltech101 (N-Caltech101) dataset,"
", is a spiking version of the original frame-based Caltech101 dataset. 
2468,http://dgyblog.com/projects-term/dvs-dataset.html,Dataset," Front. Neurosci. (2016), 10:405. ","
"
2468,http://www.prophesee.ai/dataset-n-cars/,N-CARS Dataset,"
",: A large real-world event-based dataset for car classification.     
2468,https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox,Code,", arXiv, 2020. ",", "
2468,https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/,News,", ","
"
2468,https://www.prophesee.ai/2020/11/24/automotive-megapixel-event-based-dataset/,1Mpx Detection Dataset," Perot, E., de Tournemire, P., Nitti, D., Masci, J., Sironi, A.,  ",: 
2468,https://n-rod-dataset.github.io/home/,Project page,", IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), 2021. ",", "
2468,https://sites.google.com/a/udayton.edu/issl/software/dataset,DVSNOISE20,"
", associated to the paper 
2468,https://sites.google.com/view/dnd21/datasets?authuser=0,DND21 DeNoising Dynamic vision sensors dataset,"
", associated to the paper 
2468,https://github.com/mgaoling/mpl_dataset_toolbox,events_bag2h5,"
", Python code to convert event data from ROSbags to HDF5.
2468,https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox,Code,"Prophesee automotive dataset toolbox, ","
"
2469,./data/README.md,data/README,"To run on the enwik8 dataset, first download and prepare the data (see ", for details):
2485,http://www.nature.com/articles/sdata201555#data-records,http://www.nature.com/articles/sdata201555#data-records,. This spatial tessellation is released under a Open Data Commons Open Database License (ODbL) and it is obtained from the publicly available dataset described in this paper ,.
2489,http://pandas.pydata.org/,Pandas,", ",", and "
2496,https://gitter.im/xdata-tick/Lobby,"
Gitter chat
","
","
"
2496,https://portail.polytechnique.edu/datascience/en,Datascience initiative,"The project was started in 2016 by Emmanuel Bacry, Martin Bompaire, Stéphane Gaïffas and Søren Vinther Poulsen at the ", of 
2496,https://x-datainitiative.github.io/tick/auto_examples/index.html,https://x-datainitiative.github.io/tick/auto_examples/index.html,"
","
"
2496,https://x-datainitiative.github.io/tick/,https://x-datainitiative.github.io/tick,"
","
"
2496,https://portail.polytechnique.edu/datascience/fr/node/329,A joint work,"
"," with the French national social security (CNAMTS) to analyses a huge health-care database, that describes the medical care provided to most of the French citizens. For this project, "
2496,https://x-datainitiative.github.io/tick/,https://x-datainitiative.github.io/tick,"
","
"
2532,http://people.bath.ac.uk/hc551/dataset.html,Photo-Art-50 dataset,We provide a link to the page on which the , is hosted. Please cite the papers of the authors of the dataset if you use it. The models for testing SwiDeN can be found 
2540,http://mscoco.org/dataset/#overview,mscoco," folder, which can be from "," COCO's images are used for RefCOCO, RefCOCO+ and refCOCOg. For RefCLEF, please add "
2540,http://imageclef.org/SIAPRdata,imageCLEF," folder. We extracted the related 19997 images to our cleaned RefCLEF dataset, which is a subset of the original ",. Download the 
2540,https://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip,subset,. Download the , and unzip it to 
2553,https://github.com/yjxiong/temporal-segment-networks/wiki/Working-on-custom-datasets.,How to add a custom dataset,",    ","
"
2553,#code--data-preparation,Code & Data Preparation,"
","
"
2553,http://crcv.ucf.edu/data/UCF101.php,UCF-101,We experimented on two mainstream action recognition datasets: , and 
2553,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB51, and ,". Videos can be downloaded directly from their websites. After download, please extract the videos from the "
2592,#datasets,Datasets,"
","
"
2617,https://serv.cusp.nyu.edu/projects/urbansounddataset/index.html,UrbanSound8k dataset," format [1,2] for the ", [3]. It also contains the extended JAMS files returned by the 
2630,https://github.com/tesseract-ocr/tessdata/tree/3.04.00,trained data file,A v3.04 , for a language. Data files must be copied to the Android device in a subdirectory named 
2633,#data,Data,"
","
"
2678,https://linkeddatafragments.org/,Linked Data Fragments (LDF),This repository contains modules for , servers.
2678,http://dbpedia.org/page/Linked_data,subject pages,", ",", and "
2678,http://linkeddatafragments.org/,"
Linked Data Fragment
",. We call each such part a ,.
2678,https://linkeddatafragments.org/specification/quad-pattern-fragments/,Quad Pattern Fragments,"Instead, this server offers ", (a.k.a. 
2678,https://linkeddatafragments.org/specification/triple-pattern-fragments/,Triple Pattern Fragments,"
","
"
2678,http://data.linkeddatafragments.org/dbpedia?subject=&predicate=rdf%3Atype&object=dbpedia-owl%3ARestaurant,example,(,)
2678,http://data.linkeddatafragments.org/dbpedia?subject=&predicate=rdf%3Atype&object=,example,(,)
2678,http://data.linkeddatafragments.org/dbpedia?subject=&predicate=&object=%22John%22%40en,example,(,)
2678,http://data.linkeddatafragments.org/,data.linkeddatafragments.org,An example server is available at ,.
2678,https://linkeddatafragments.org/specification/quad-pattern-fragments/,Quad Pattern Fragments,: Feature that enables , (a.k.a. 
2678,https://linkeddatafragments.org/specification/triple-pattern-fragments/,Triple Pattern Fragments, (a.k.a. ,).
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-hdt,"
@ldf/datasource-hdt
","
",: Datasource that allows HDT files to be loaded.
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-jsonld,"
@ldf/datasource-jsonld
","
",: Datasource that allows JSON-LD files to be loaded.
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-rdfa,"
@ldf/datasource-rdfa
","
",: Datasource that allows RDFa files to be loaded.
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-n3,"
@ldf/datasource-n3
","
",: Datasource that allows 
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-sparql,"
@ldf/datasource-sparql
","
",: Datasource that allows SPARQL endpoints to be used as a data proxy.
2678,https://github.com/LinkedDataFragments/Server.js/tree/master/packages/datasource-composite,"
@ldf/datasource-composite
","
",: Datasource that delegates queries to an sequence of other datasources.
2683,https://codecov.io/gh/datactive/bigbang,"
codecov
","
","
"
2683,https://gitter.im/datactive/bigbang?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge,"
Gitter
","
","
"
2683,https://github.com/datactive/bigbang/issues/412,#412,"Docstrings are preferred, so that auto-generated web-based documentation will be possible (",). You can follow the 
2683,https://gitter.im/datactive/bigbang,development chatroom," and let us know your suggestions, questions, requests and comments. A ", is also available.
2683,https://github.com/datactive/bigbang/wiki/Governance,Governance, for its text. This license may be changed at any time according to the principles of the project ,.
2689,https://retriever.readthedocs.io/en/latest/datasets_list.html,List of Available Datasets,"
","
"
2689,https://en.wikipedia.org/wiki/Iris_flower_data_set,"
Iris flower dataset",These examples are using the ,. More examples can be found in the Data Retriever documentation.
2689,http://www.data-retriever.org/,Data Retriever website,For more information see the ,.
2689,https://www.moore.org/initiative-strategy-detail?initiativeId=data-driven-discovery,Gordon and Betty Moore Foundation's Data-Driven Discovery Initiative,Development of this software was funded by the , through 
2719,http://data.co60.ca/ai/checkpoint.pth.tar.gz,my server,Download the external checkpoint from , and place in the root directory.
2741,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,"
","
"
2741,https://www.cityscapes-dataset.com,Cityscapes,"
","
"
2741,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI stereo 2015, corresponds to the 200 official training set pairs from ,.
2757,https://web.archive.org/web/20131118073324/https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable,https://www.infochimps.com/datasets/word-list-350000-simple-english-words-excel-readable,While searching for a list of english words (for an auto-complete tutorial) I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to , (archived).
2769,http://www.doc.ic.ac.uk/~sleutene/datasets/elasticfusion/dyson_lab.klg,here,We have provided a sample dataset which you can run easily with ElasticFusion for download ,. Launch it as follows:
2785,https://www.physionet.org/physiobank/database/mitdb,MIT-BIH Arrhythmia Database, from the , (the output will be stored in the 
2785,https://www.physionet.org/physiobank/database/qtdb,QT database, from the ,", and store the result in the "
2791,http://www.cvlibs.net/datasets/kitti/,http://www.cvlibs.net/datasets/kitti/,"[1] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ""Vision meets robotics: The KITTI dataset,"" Int. J. Robot. Research (IJRR), vol. 32, no. 11, pp. 1231–1237, Sep. 2013. ","
"
2795,https://github.com/harvardnlp/sent-conv-torch/tree/master/data,HarvardNLP,More data in the paper can be found at ,.
2797,https://gitter.im/openeventdata/petrarch2?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge,"
Join the chat at https://gitter.im/openeventdata/petrarch2
","
","
"
2797,https://travis-ci.org/openeventdata/petrarch2,"
Build Status
","
","
"
2797,https://landscape.io/github/openeventdata/petrarch2/master,"
Code Health
","
","
"
2797,https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/,intro guide,"For a step-by-step guide to running text through the complete Petrarch2 processing pipeline, see our ",.
2797,https://github.com/openeventdata/phoenix_pipeline,Phoenix pipeline,"It is possible to run PETRARCH-2 as a stand-alone program. Most of our development work has gone into incorporating PETRARCH-2 into a full pipeline of utilities, through, e.g., the ",. There's also a RESTful wrapper around PETRARCH and CoreNLP named 
2797,https://andrewhalterman.com/2017/05/08/making-event-data-from-scratch-a-step-by-step-guide/,guide,"Alternatively, see this ", on running the complete 
2797,https://github.com/openeventdata/phoenix_pipeline,"""phoenix_pipeline""", on running the complete ,.
2797,http://eventdata.parusanalytics.com/software.dir/tabari.html,TABARI,Because it used the verb dictionaries from ,", a coder based on shallow parsing, PETRARCH-1 made relatively little use of the CoreNLP constituency parse beyond parts-of-speech markup and noun-phrase markup. PETRARCH-2 makes full use of the deep parse."
2797,https://gitter.im/openeventdata/petrarch2,gitter,". In general, we welcome contributions from anyone and everyone, be it in the form of pull requests, bug reports, or feature requests. If you would like to engage in more real-time conversation with us, please visit our ", channel.
2816,http://corpus-texmex.irisa.fr/,SIFT1M and GIST1M,"
","
"
2822,https://github.com/ZZUTK/Delay_Embedding/tree/master/data,data,"
","
"
2822,https://archive.ics.uci.edu/ml/datasets/Character+Trajectories,Character Trajectories Data Set , is the , from UCI
2832,https://github.com/hadiasghari/pyasn#ipasn-data-files,are online,The tool takes as input a FQDN and an ASN database.  Instructions on how to build such a database ,.
2851,http://smalldata.io,Small Data Lab,The , ResearchStack Extensions package is the easiest way to include SDL visual surveys (
2851,http://yadl.smalldata.io,YADL, ResearchStack Extensions package is the easiest way to include SDL visual surveys (,", MEDL, PAM) into a ResearchStack application."
2852,http://smalldata.io,Small Data Lab,The , ResearchKit Extensions package is the easiest way to include SDL AVA (
2852,http://yadl.smalldata.io,YADL, ResearchKit Extensions package is the easiest way to include SDL AVA (,", MEDL, PAM) and Behavioral extensions (Go / No Go, Delayed Discounting, BART) into a ResearchKit application."
2858,#getting-australian-senate-election-data,Getting Australian Senate Election Data, option is the file path to all Australian Senate Election data (see , for details).
2858,#getting-australian-senate-election-data,Getting Australian Senate Election Data, option is the file path to all Australian Senate Election data (see , for details).
2858,#getting-australian-senate-election-data,Getting Australian Senate Election Data, option is the file path to all Australian Senate Election data (see , for details).
2891,#data-movement,Data Movement, 7.2 , 7.3 
2895,https://fcav.engin.umich.edu/sim-dataset,10k dataset, on our , and evaluate on 
2895,http://www.cvlibs.net/datasets/kitti/eval_object.php,KITTI, and evaluate on ,.
2895,https://fcav.engin.umich.edu/sim-dataset/,our website,"Create a directory and download the archive files for 10k images, annotations and image sets from ",. Assuming you have downloaded these to a directory named 
2895,http://www.cvlibs.net/datasets/kitti/eval_object.php,KITTI's object detection landing page,Visit , and follow the links named:
2897,http://mscoco.org/dataset/#overview,COCO, and , joint workshop at 
2897,http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar,here,. Download the Places365 standard easyformat split at ,. Untar it to some folder. Then run the following:
2911,http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2016arXiv160309320M&data_type=BIBTEX&db_key=PRE&nocookieset=1,"
[BibTex]
",", abs/1603.09320. ","
"
2913,http://corpus-texmex.irisa.fr,"
Sift
","
", consists of 1 million 128-d SIFT vectors.
2913,http://corpus-texmex.irisa.fr/,"
Bann
","
"," is used to evaluate the scalability of the algorithms, where 1M, 2M, 4M, 6M, 8M, and 10M data points are sampled from 128-dimensional SIFT descriptors extracted from natural images."
2913,http://phototour.cs.washington.edu/datasets,"
Notre
","
", contains about 0.3 million 128-d features of a set of Flickr images and a reconstruction.
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Train_mscoco.zip,training question files,"
","
"
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Val_mscoco.zip,validation question files,"
","
"
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Questions_Train_mscoco.zip,here,"Question files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Train_mscoco.zip,training annotation files,"
","
"
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Val_mscoco.zip,validation annotation files,"
","
"
2924,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Annotations_Train_mscoco.zip,here,"Annotation files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
2924,http://mscoco.org/dataset/#download,MS COCO website,"For real, create a directory with name mscoco inside this directory. For each of train, val and test, create directories with names train2014, val2014 and test2015 respectively inside mscoco directory, download respective images from ", and place them in respective folders.
2925,http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering,"""Exploring models and data for image question answering.""","Ren, Mengye, Ryan Kiros, and Richard Zemel. "," In Advances in Neural Information Processing Systems, pp. 2935-2943. 2015. "
2925,http://gitxiv.com/posts/6pFP3b8gqxWZdBfjf/exploring-models-and-data-for-image-question-answering,[code]," In Advances in Neural Information Processing Systems, pp. 2935-2943. 2015. ","
"
2927,https://github.com/yoosan/sentpair/releases/download/predata/data.zip,here,You can download the preprocessed data (recommend) from ,. Alternatively you can process them by yourself. The original links are:
2930,https://maven-badges.herokuapp.com/maven-central/com.datastax.oss/java-driver-core,"
Maven Central
","
","
"
2930,http://docs.datastax.com/en/developer/java-driver/,DataStax Docs,"If you're reading this on github.com, please note that this is the readme for the development version and that some features described here might not yet have been released. You can find the documentation for latest version through "," or via the release tags, e.g. "
2930,https://github.com/datastax/java-driver/tree/4.15.0,4.15.0," or via the release tags, e.g. ",.
2930,https://www.datastax.com/products/datastax-enterprise,DataStax Enterprise, (2.1+) and ," (4.7+), and "
2930,https://www.datastax.com/products/datastax-astra,DataStax Astra," (4.7+), and ",", using exclusively Cassandra's binary protocol and Cassandra Query Language (CQL) v3."
2930,http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datastax.oss%22,com.datastax.oss,"The driver artifacts are published in Maven central, under the group id ","; there are multiple modules, all prefixed with "
2930,https://docs.datastax.com/en/drivers/java/4.14,API docs,"
","
"
2930,https://datastax-oss.atlassian.net/browse/JAVA,JIRA,Bug tracking: ,"
"
2930,https://groups.google.com/a/lists.datastax.com/forum/#!forum/java-driver-user,Mailing list,"
","
"
2930,https://twitter.com/datastaxeng,@DataStaxEng, tweets Java driver releases and important announcements (low frequency). ," has more news, including other drivers, Cassandra, and DSE."
2960,https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst,these instructions, and download MNIST following ,.
2961,https://github.com/HUJI-Deep/TMM/blob/master/exp/norb_small/generate_norb_small_missing_data.py,script,Inpainting images with missing data locations given by a mask image is meant to be used with a dataset in the format created by this ,.
2961,https://github.com/mila-udem/fuel/blob/master/docs/built_in_datasets.rst,these instructions, and download MNIST following ,.
2967,https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua,fb.resnet.torch,The pretrained ResNet-152 model and related scripts can be found in ,.
2995,http://www.select.cs.cmu.edu/code/graphlab/datasets/netflix_mm,netflix_mm,The netflix_mm and netflix_mme are original data files downloaded from , and 
2995,http://www.select.cs.cmu.edu/code/graphlab/datasets/netflix_mme,netflix_mme, and ,". As the download link no longer works, we put them on the above google drive link. If you are interested on how to transform ""netflix_train/netflix_test"" to ""netflix_train.bin/netflix_test.bin"", please check out ./data/netflix/prepare.sh"
3006,https://stacks.stanford.edu/file/druid:cs392kv3054/life_demo_data.tar.gz,here,Download the LiFE demo data set from the repository ,.
3011,data/get_data.sh,"
data/get_data.sh
",Executing the , script will automatically download the major datasets of WMT '15 in many languages and save training sets in 
3011,data/build_alphabet.py,"
data/build_alphabet.py
","The alphabets are included in this git repository, but new ones can be built based on other datasets by editing and running ",.
3014,http://bigdata.poly.edu/~fchirigati/,Fernando Chirigati,"
", (New York University)
3014,#33-data,3.3. Data,"
","
"
3014,#62-datasets,6.2. Datasets,"
","
"
3014,data-polygamy,"
data-polygamy/
","
", includes the source code of the framework.
3014,data,"
data/
","
"," includes scripts to download datasets, and some additional data and metadata that is necessary to run the framework."
3014,#33-data,later, is a directory containing all the datasets and metadata associated with the datasets (more information ,);
3014,data/load-hdfs-structure,"
load-hdfs-structure
","To automatically create the required directories, take a look at the ", script.
3014,data-polygamy/src/main/java/edu/nyu/vida/data_polygamy/resolution/,"
resolution
",". The grid resolution has only been used for testing, and not in our final experiments. Note that the framework assumes that all the data fed to the pre-processing step corresponds to a single city; therefore, if you are handling data from more than one city, you probably need to provide a suitable resolution conversion under the ", directory.
3014,data/neighborhood.txt,"
neighborhood.txt
",The files , and 
3014,data/zipcode.txt,"
zipcode.txt
", and , are examples of such file for New York City.
3014,data/neighborhood-graph.txt,"
neighborhood-graph.txt
"," file represents a graph for the resolution, where each region of the resolution is a node, and there is an edge between two regions if these are neighboring regions. The first line of this file contains the number of nodes and number of edges, and the following lines represent the edges of the graph (one line per edge). The files ", and 
3014,data/zipcode-graph.txt,"
zipcode-graph.txt
", and , are examples of such file for New York City.
3014,data/load-spatial,"
load-spatial
",The script , can be used to automatically upload our spatial resolutions files to HDFS.
3014,data/datasets.txt,here," directory, containing a mapping between dataset name and dataset id. An example of such file is available ",.
3014,data-polygamy/lib,"
data-polygamy/lib
",", since these libraries are not available in the central repository. Therefore, we include these libraries, as well as their corresponding licenses, under ",. It is worth mentioning that we 
3014,data-polygamy/src/main/java/edu/nyu/vida/data_polygamy/utils/FrameworkUtils.java#L2906,FrameworkUtils.java, in file ,". For each identifier, information related to the corresponding machine is declared (e.g.: number of cores, amount of memory, and number of disks). Such information is used to set a few Hadoop configuration parameters."
3014,#dataset-attributes,attributes,"The Pre-Processing step is responsible for selecting data (from a dataset) that correspond to spatial, temporal, identifier, and numerical ",. This step also does a pre-aggregation that is fed to the scalar function computation step.
3014,sigmod16/data-polygamy.jar,"
sigmod16/data-polygamy.jar
",We provide a pre-built jar file for the Data Polygamy framework at ,". If you want to build the code yourself, follow the instructions "
3014,sigmod16/data-polygamy.jar,"
sigmod16/data-polygamy.jar
"," builds the Data Polygamy framework; note this is optional, since we provide a pre-built jar file available at ",.
3014,https://data.cityofnewyork.us/d/h9gi-nx95?category=Public-Safety&view_name=NYPD-Motor-Vehicle-Collisions,NYC Open Data portal,The original dataset is available at the ,.
3014,https://data.cityofnewyork.us/d/erm2-nwe9?category=Social-Services&view_name=311-Service-Requests-from-2010-to-Present,NYC Open Data portal,The original dataset is available at the ,.
3014,https://www.citibikenyc.com/system-data,Citi Bike website,The original dataset is available at the ,.
3014,http://www7.ncdc.noaa.gov/CDO/dataproduct,National Climatic Data Center website,The original dataset is available at the ,.
3014,http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml,website,"The version of the Taxi dataset that we used in the experiments is not open source, and therefore, we cannot make it available online. However, the Taxi and Limousine Commission has made the trip data available on their ",.
3046,https://github.com/deepmind/rc-data/,CNN/DailyMail,"
",: This repository contains a script to download CNN and Daily Mail articles from the Wayback Machine.
3051,#data,Data,"
","
"
3057,https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md,license,We have collected birth record data from the United States and the United Kingdom across a number of years for all births in the two countries and are releasing the collected and cleaned up data here. We have also generated a simple gender classified based on incidence of gender by name. You can use this data for any purpose compatible with the ,.
3057,http://weblog.bocoup.com/global-name-data/,blog,You can read about some uses of this data along with code examples at the Bocoup ,.
3057,https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets,assets directory,"If you're mainly interest in the data, pre and post classified name data is available in the ",. If you install the package these will not be included in the install as 
3057,https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md,LICENSE,Processed data are provided under the Open Government License or the public domain where appropriate. See the , for details.
3057,https://github.com/OpenGenderTracking/globalnamedata/blob/master/LICENSE.md,LICENSE,See the , file for details.
3060,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist,"
mnist
",Users can also compare on the multi-class dataset ," with the follow command (Note that we only shuffle the training data once in this example, so the standard deviation is zero):"
3074,http://nlp.stanford.edu/data/glove.6B.zip,Wikipedia+Gigaword 5, | | ," | 50 | Wikipedia+Gigaword 5 (6B) | 400,000 | GloVe | GloVe | AdaGrad | 10+10 | "
3074,http://nlp.stanford.edu/data/glove.6B.zip,Wikipedia+Gigaword 5, | | ," | 100 | Wikipedia+Gigaword 5 (6B) | 400,000 | GloVe | GloVe | AdaGrad | 10+10 | "
3074,http://nlp.stanford.edu/data/glove.6B.zip,Wikipedia+Gigaword 5, | | ," | 200 | Wikipedia+Gigaword 5 (6B) | 400,000 | GloVe | GloVe | AdaGrad | 10+10 | "
3074,http://nlp.stanford.edu/data/glove.6B.zip,Wikipedia+Gigaword 5, | | ," | 300 | Wikipedia+Gigaword 5 (6B) | 400,000 | GloVe | GloVe | AdaGrad | 10+10 | "
3074,http://nlp.stanford.edu/data/glove.42B.300d.zip,Common Crawl 42B, | | , | 300 | Common Crawl (42B) | 1.9M | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://nlp.stanford.edu/data/glove.840B.300d.zip,Common Crawl 840B, | | , | 300 | Common Crawl (840B) | 2.2M | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://www-nlp.stanford.edu/data/glove.twitter.27B.zip,Twitter (2B Tweets), | | , | 25 | Twitter (27B) | ? | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://www-nlp.stanford.edu/data/glove.twitter.27B.zip,Twitter (2B Tweets), | | , | 50 | Twitter (27B) | ? | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://www-nlp.stanford.edu/data/glove.twitter.27B.zip,Twitter (2B Tweets), | | , | 100 | Twitter (27B) | ? | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://www-nlp.stanford.edu/data/glove.twitter.27B.zip,Twitter (2B Tweets), | | , | 200 | Twitter (27B) | ? | GloVe | GloVe | GloVe | AdaGrad | 
3074,http://u.cs.biu.ac.il/~yogo/data/syntemb/deps.words.bz2,Wikipedia dependency, | | ," | 300 | Wikipedia (?) | 174,015 | Levy & Goldberg | word2vec modified | word2vec | syntactic dependencies | "
3081,http://www.di.ens.fr/data/software/,libsvm-compact,", and ",.
3083,code/collect_data/data_collection.ipynb,The data collection IPython notebook,"
","
"
3083,#dataset-summary,Dataset Summary,"
","
"
3083,#exploratory-data-analysis,Exploratory Data Analysis,"
","
"
3083,http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset,Million Song Dataset,"A 10,000-song subset was downloaded from the ",.
3084,https://github.com/Atomu2014/make-ipinyou-data,make-ipinyou-data,"For simplicity, we provide iPinYou dataset at ",. Follow the instructions and update the soft link 
3094,test/data.100.fine.all.ud.expect,English,UD Bank: ,", "
3119,http://fivethirtyeight.com/datalab/mta-new-york-lost-and-found-subway-most-common/,Mona's story,", to revisit ", in a year (or some longer period of time)
3131,http://torch7.s3-website-us-east-1.amazonaws.com/data/cifar-10-torch.tar.gz,CIFAR 10,Download ,. Use 
3134,https://github.com/tesseract-ocr/tessdata,tessdata," files which support the legacy engine, for example those from the ", repository.
3166,https://github.com/edwin-de-jong/mnist-digits-stroke-sequence-data/wiki/MNIST-digits-stroke-sequence-data,wiki page,See the ,"
"
3208,http://crcv.ucf.edu/data/UCF101.php,here,The data used to train this model is located ,.
3215,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,move validation images, dataset and , to labeled subfolders
3216,datasets/facemodel/README.md,datasets/facemodel/README.md,"This demo requires a database of high resolution images, which is used to select source and target images for the transformation. Follow the instructions at ", to collect the database.
3216,datasets/test/,datasets/test/,The source of each test image and our test masks are in ,". We find that DFI works well on photographs of natural faces which are: un-occluded, front-facing, and lit by natural or office-environment lighting."
3219,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow,KITTI2012,"The code was developed on Ubuntu 14.04, using Theano+Lasagne+OpenCV. You can see the performance it achieved on the ",", "
3219,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI2015,", ", and 
3220,https://github.com/dmlc/mxnet/tree/master/example/image-classification#prepare-datasets,MXNet Example, files for different datasets. Please refer to ,.
3221,http://ai.stanford.edu/~jkrause/cars/car_dataset.html,CARS196,"
","
"
3223,http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/,here, and the dataset ,.
3230,docs/collect_data.md,Training the color segmenter,"
",: a quick overview of how you can train the color segmenter.
3230,docs/collect_data.md,the block stacking data collection notes,"For more information on how to collect data for the ""block stacking"" task, check out ","
"
3230,costar_tools/alvar_data_collection/README.md,alvar_data_collection,"
",: utilities to define the black and white printed AR tags we use for 
3245,datasets/bibtex/facades.tex,Citation,. [,]
3245,https://www.cityscapes-dataset.com/,Cityscapes training set,: 2975 images from the ,.  [
3245,datasets/bibtex/cityscapes.tex,Citation,.  [,]
3245,datasets/bibtex/shoes.tex,Citation, edge detector + post-processing. [,]
3245,datasets/bibtex/handbags.tex,Citation, edge detector + post-processing. [,]
3245,datasets/bibtex/transattr.tex,Citation, [,]. To train a 
3248,http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/,http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/,"For small norb dataset, please download the raw images in .MAT format from ", and run datasets_norb.convert_orig_to_np() to convert it into numpy format.
3261,http://mscoco.org/dataset/#download,COCO dataset,Please download , and annotations for the 5k image 
3263,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,Flying Chairs, model and , pre-trained 
3283,http://pytorch.org/docs/torchvision/datasets.html,API,"
",.
3296,https://github.com/fvisin/dataset_loaders,The dataset loader,"
",". Thanks a lot to Francesco Visin for its data loader, please cite it if you use it."
3308,http://data.influenceexplorer.com/api,Influence Explorer API,: Grabs updated FEC and OpenSecrets IDs from the ,. Will only work for members with a Bioguide ID.
3311,https://www.cityscapes-dataset.com/downloads/,Cityscapes dataset,Download the ,", and put the extracted images into the directory:"
3311,https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results,leaderboard,"For more information, refer to the official ",.
3324,http://mscoco.org/dataset/#captions-leaderboard,MS COCO leaderboard,"On MS COCO, with 5 reference captions scores are typically in the range 0.15 - 0.20. With 40 reference captions, scores are typically in the range 0.03 - 0.07. This is the expected result due to the impact of the recall component of the metric. To make the scores more readable, on the ",", C40 SPICE scores are multiplied by 10."
3328,https://github.com/NorThanapon/dict-definition/tree/master/data,Data,"For detail of the data, see ","
"
3347,https://github.com/openeventdata/scraper,"
scraper
",The EL:DIABLO event coding platform is comprised of two primary applications: a web scraper and a processing pipeline (, and 
3347,https://github.com/openeventdata/phoenix_pipeline,"
phoenix_pipeline
", and ," specifically). The scraper is a simple web scraper that makes use of a whitelist of RSS feeds to pull stories from popular news outlets. The pipeline moves the news stories from storage in a database to the event coder, such as TABARI or PETRARCH, and outputs event data. More information about the details of these projects can be found in their respective documentation, linked to above. If you use the standard "
3347,https://github.com/openeventdata/eldiablo,Github repository," for VirtualBox. Once that software is installed, EL:DIABLO needs to be downloaded from the ",. For those familiar with 
3349,https://gitter.im/openeventdata/scraper?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge,"
Join the chat at https://gitter.im/openeventdata/scraper
","
","
"
3350,http://workshop.colips.org/dstc5/data.html,DSTC5,"Although we only mentioned DSTC4 in our paper, users can also locate these sub-dialogs from ",", since it provides all the same data in DSTC4, and plus two extra Chinese dialogs (055, 056)."
3355,http://data-bdd.berkeley.edu,here,BDD-V dataset will be released ,.
3366,#6-paris-street-view-dataset,Download Dataset,"
","
"
3372,https://os.unil.cloud.switch.ch/fma/fma_metadata.zip,"
fma_metadata.zip
","
","
"
3372,https://pandas.pydata.org/,pandas, (342 MiB). The below tables can be used with , or any other data analysis tool. See the 
3372,https://towardsdatascience.com/music-genre-classification-with-tensorflow-3de38f0d4dbb,Music Genre Classification With TensorFlow,"
",", Towards Data Science, 2020-08-11."
3372,https://towardsdatascience.com/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58,Music Genre Classification: Transformers vs Recurrent Neural Networks,"
",", Towards Data Science, 2020-06-14."
3372,https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af,Using CNNs and RNNs for Music Genre Recognition,"
",", Towards Data Science, 2018-12-13."
3372,https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad,Over 1.5 TB’s of Labeled Audio Datasets,"
",", Towards Data Science, 2018-11-13."
3372,https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/,25 Open Datasets for Deep Learning Every Data Scientist Must Work With,"
",", Analytics Vidhya, 2018-03-29."
3372,https://tensorflow.blog/2017/03/14/fma-a-dataset-for-music-analysis,FMA: A Dataset For Music Analysis,"
",", tensorflow.blog, 2017-03-14."
3372,http://datajamdays.org,Data Jam days, presented at the ,", Lausanne, 2017-11-24."
3372,https://github.com/caesar0301/awesome-public-datasets,https://github.com/caesar0301/awesome-public-datasets,"
","
"
3372,https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis,https://archive.ics.uci.edu/ml/datasets/FMA:+A+Dataset+For+Music+Analysis,"
","
"
3372,http://deeplearning.net/datasets,http://deeplearning.net/datasets,"
","
"
3372,http://www.audiocontentanalysis.org/data-sets,http://www.audiocontentanalysis.org/data-sets,"
","
"
3372,https://github.com/ismir/mir-datasets,https://github.com/ismir/mir-datasets,"
","
"
3372,https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research,https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research,"
","
"
3372,https://cloudlab.atlassian.net/wiki/display/datasets/FMA:+A+Dataset+For+Music+Analysis,https://cloudlab.atlassian.net/wiki/display/datasets/FMA:+A+Dataset+For+Music+Analysis,"
","
"
3372,https://www.datasetlist.com,https://www.datasetlist.com,"
","
"
3372,https://data-flair.training/blogs/deep-learning-project-ideas,https://data-flair.training/blogs/deep-learning-project-ideas,"
","
"
3372,https://datascience.ch/collaboration-and-partnerships,Swiss Data Science Center,We are grateful to the , (
3373,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/COCO/,COCO,Download the corresponding Vocabulary file for , and 
3373,https://filebox.ece.vt.edu/~jiasenlu/codeRelease/AdaptiveAttention/data/Flickr30k/,Flickr30k, and ,"
"
3397,http://mscoco.org/dataset/#detections-challenge2015,COCO,"This repository contains the original models (ResNet-50, ResNet-101, and ResNet-152) described in the paper ""Deep Residual Learning for Image Recognition"" (http://arxiv.org/abs/1512.03385). These models are those used in [ILSVRC] (http://image-net.org/challenges/LSVRC/2015/) and "," 2015 competitions, which won the 1st places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
3399,http://conda.pydata.org/miniconda.html,Download,We won't need the entire distribution. , a Python 3.7+ & install a minimal version of anaconda.
3400,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,flowers, and , image data. Extract them to 
3410,http://nlp.stanford.edu/data/glove.42B.300d.zip,Pretrained Glove vectors,"
","
"
3418,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/19.08/mini.h5,19.08/mini.h5,  | , | | 17.06    | 
3418,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5,17.06/mini.h5,  | , | | 17.04    | 
3418,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.05/mini.h5,17.05/mini.h5, | , | | 17.02    | 
3418,http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/16.09/numberbatch.h5,16.09/numberbatch.h5,  |                              | | 16.09    |                                         |                                           | , |
3424,data.csv,"
data.csv
","The dataset with the energy profiles used in the above paper is not publicly available. Instead, we provide a sample "," file with 10 profiles to show the format of the input data. Specifically, each profile is specified in one line, and it refers to one day of energy consumption, with a measure every 30 minutes (i.e., each line contains 48 measures)."
3459,https://github.com/m2ci-msp/ematoblender-example-data,ematoblender-example-data,"
","
"
3459,https://github.com/m2ci-msp/ematoblender-example-data,https://github.com/m2ci-msp/ematoblender-example-data,example EMA data from ,","
3470,#data-format,Data format,"
","
"
3470,http://pandas.pydata.org/,pandas,These files are suitable to be further processed with software such as ,.
3472,http://trec.nist.gov/data/docs_eng.html,TREC website,Change directory to IR for experimenting on information Retrieval task. IR Datasets mentioned in the paper can be downloaded from ,.
3482,http://corpus-texmex.irisa.fr/,SIFT1M and GIST1M,"
","
"
3483,http://corpus-texmex.irisa.fr/,SIFT1M and GIST1M,"
","
"
3488,http://mscoco.org/dataset/#download,download,Visit MS COCO , page for more details.
3488,http://mscoco.org/dataset/#format,format,Visit MS COCO , page for more details.
3492,http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df,state-of-the-art,"MultiNet is able to jointly perform road segmentation, car detection and street classification. The model achieves real-time speed and ",  performance in segmentation. Check out our 
3492,http://www.cvlibs.net/download.php?file=data_road.zip,http://www.cvlibs.net/download.php?file=data_road.zip,Retrieve kitti data url here: ,"
"
3492,README.md#manage-data-storage,Manage data storage, instead of downloading the data yourself. The script will also extract and prepare the data. See Section , if you like to control where the data is stored.
3492,data/demo/um_000005.png,demo.png, to obtain a prediction using , as input.
3519,http://www.cvlibs.net/download.php?file=data_stereo_flow.zip,KITTI 2012,Download the , data set and unzip it into 
3519,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI 2015,Download the , data set and unzip it into 
3531,#databases--orms,Databases & ORMs,"
","
"
3531,#databases,Databases,"
","
"
3531,https://github.com/ardatan/graphql-toolkit,graphql-toolkit,"
"," - A set of utils for faster development of GraphQL tools (Schema and documents loading, Schema merging and more)."
3531,https://github.com/engagingspaces/vertx-dataloader,vertx-dataloader,"
"," - Port of Facebook DataLoader for efficient, asynchronous batching and caching in clustered GraphQL environments."
3531,https://github.com/Yelp/dataloader-codegen,dataloader-codegen,"
"," - An opinionated JavaScript library for automatically generating predictable, type safe DataLoaders over a set of resources (e.g. HTTP endpoints)."
3531,https://medium.com/@gajus/using-dataloader-to-batch-requests-c345f4b23433,Using DataLoader to batch GraphQL requests,"
","
"
3531,https://hasura.io/blog/level-up-your-serverless-game-with-a-graphql-data-as-a-service-layer/,Level up your serverless game with a GraphQL data-as-a-service layer,"
","
"
3552,https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data,LSHBOX-sample datasets,"
",: a dataset for performance tests
3552,https://github.com/RSIA-LIESMARS-WHU/LSHBOX-sample-data,LSHBOX-sample-data,", if the link is invalid, you can also get it from ",.
3559,https://github.com/deepdsl/deepdsl/tree/master/deepdsl-java/dataset/mnist/,dataset/mnist/," uses Mnist, which is assumed to be located at ", (please use the script described in the previous section to prepare the dataset).
3560,documentation/libxsmm_aux.md#user-data-dispatch,Dispatching user-data and multiple kernels,"
","
"
3566,https://github.com/wnzhang/make-ipinyou-data,make-ipinyou-data,"Note these results are produced from a subset (the first 350,000 lines of each campaign in iPinYou) under T = 1000 and c0 = 1/32. For the full small-scale evaluation and large-scale evaluation, please first check the GitHub project ", for pre-processing the 
3566,http://data.computational-advertising.org,iPinYou data, for pre-processing the ,". After downloading the dataset, by ""make all"" you can generate the standardised data."
3576,http://www.ims.uni-stuttgart.de/data/dLCE/wiki_en_dLCE_100d_minFreq_100.bin,dLCE_100d_minFreq_100,"Wikipedia corpus, 100dim, min-count=100: ","
"
3591,http://trec-kba.org/data/fakba1/,FAKBA1,""" dataset from the paper (i.e., "," with ""entity age timestamps"")."
3591,http://aws-publicdatasets.s3.amazonaws.com/trec/kba/FAKBA1/index.html,FAKBA1,Freebase Annotations for the TREC KBA 2014 StreamCorpus: , (~200GB)
3613,https://github.com/IITDBGroup/gprom/wiki/datalog_prov,Provenance Graphs for Datalog,"
","
"
3635,https://seaborn.pydata.org/index.html,seaborn,"
","
"
3640,https://github.com/avivt/VIN/tree/master/data,author's repository,Download the 16x16 and 28x28 GridWorld datasets from the ,. This repository contains the 8x8 GridWorld dataset for convenience and its small size.
3641,docs/data-flow.md,the data flow diagrams,"
","
"
3658,#data,Data,"
","
"
3668,https://travis-ci.org/fultonms/drivedata,"
Build
",DRIVEDATA ,"
"
3695,https://github.com/yiling-chen/flickr-cropping-dataset,Flickr cropping dataset,We provide the evaluation script to reproduce our evaluation results on ,". For example,"
3695,https://github.com/yiling-chen/flickr-cropping-dataset,Flickr cropping dataset, and the test images from the , and specify the path of your model when running 
3703,http://archive.org/~vinay/archive-analysis/sample-dataset/crawl-data/warcs/EOT-2016-20161117233212076-00004-13036-wbgrp-crawl004.us.archive.org-8443.warc.gz,Click to download an example WARC file,. , from the 
3717,https://github.com/cod3licious/simec/blob/master/04_noisy_data.ipynb,"
04_noisy_data.ipynb
","
",": Show how SimEc deals with noise in the input data (random/correlated, either added to the data or as additional dimensions). While kPCA can only handle moderate amounts of noise, SimEc is capable of filtering out noise even if it is several times the standard deviation of the underlying data."
3728,https://archive.ics.uci.edu/ml/datasets/Ionosphere,Ionosphere (UCI Machine Learning Repository),Data set: ," (#features = 34, #classes = 2)"
3733,http://lisi1.unal.edu.co/mmimdb/metadata.npy,metadata.npy,) and , (
3733,https://archive.org/download/mmimdb/metadata.npy,archive.org mirror, (,).
3736,http:/xinleic.xyz/data/tf-faster-rcnn/,here,Another server ,.
3737,http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools,SemEval-2016 official scorer,Evaluation scripts for SemEval were adapted & modified from ,.
3740,http://image.ee.tsinghua.edu.cn/data/icip2017-ren/models.zip,here, or ,. Please put them in the 
3746,https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MIMIC&templateURL=https://aws-bigdata-blog.s3.amazonaws.com/artifacts/biomedical-informatics-studies/mimic-iii-athena.yaml,"
cloudformation-launch-stack
","
","
"
3766,http://wp.doc.ic.ac.uk/wbai/data,placing the landmarks,"To initialise image registration, we use six landmarks and perform point-based registration, which is then followed by image-based registration. The landmarks are defined as in the ", section and they are manually selected using the 
3767,#13-required-data-pre-processing,1.4. Required Data Pre-Processing,"
","
"
3767,#4-how-to-run-deepmedic-on-your-data,4. How to run DeepMedic on your data,"
","
"
3767,examples/dataForExamples/,examples/dataForExamples/,"The above configuration files are pre-set to point to accompanying .nii files, provided in ",. Those NIFTIs serve as input to the networks in our examples. This data are modified versions of images from the Brain Tumor Segmentation challenge (
3767,#14-required-data-pre-processing,1.4, as described in Sec. ,". Do not forget to normalise them to a zero-mean, unit-variance space. Produce ROI masks (for instance brain masks) if possible for the task."
3772,http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html,http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html,"
","
"
3773,mailto:data@fivethirtyeight.com,let us know,". If you find this information useful, please ",.
3802,https://lrs.icg.tugraz.at/datasets/prid/prid_2011.zip,PRID,"
","
"
3802,http://imagelab.ing.unimore.it/3DPeS/3dPES_data/3DPeS_ReId_Snap.zip,3DPeS,"
","
"
3802,http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan,Shinpuhkan,"
", (need to send an email to the authors)
3819,data/facebook-fact-check.csv,"
here
","You can find a spreadsheet of all the posts, fact-check ratings, and Facebook engagement figures ",. The methodology for collecting and rating the pages can be at the beginning and end of 
3820,http://vis-www.cs.umass.edu/~narayana/castanza/I2Rdataset/,Perception Test Images Sequences," contains ipython notebooks to apply the online rpca algorithms for real-world video survillance data (separating foreground from background in the video). Before running the corresponding ipython notebooks, please first download the video data from the ", of the 
3829,http://mscoco.org/dataset/#download,MS-COCO_2014,"
",": 82,783 images in ""train2014"" for training, and 40,504 images in ""val2014"" for testing."
3844,http://vision.in.tum.de/data/datasets/rgbd-dataset,TUM RGBD datasets,The example application takes input frames and poses from the ,", and requires that your create an "
3844,http://vision.in.tum.de/data/datasets/rgbd-dataset/tools,association file,", and requires that your create an "," to associate the RGB, Depth, and pose information. Instructions for this process can be found "
3848,./data/confusion_matrix/,confusion_matrix,The confusion matrices corresponding to the performance of each model for a clinical feature can be found at ,.
3863,https://github.com/happynear/caffe-windows/blob/ms/src/caffe/layers/image_data_layer.cpp,image_data_layer.cpp,My Caffe (https://github.com/happynear/caffe-windows/tree/ms). If you don't want to train with class-balance sampling (,) and observing Pearson Correlation during training (
3864,http://www.robots.ox.ac.uk/~vgg/data/vgg_face/,VGG-Face,", ",", "
3876,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/#,Microsoft 7-Scenes,"In air data: Any RGB-D dataset, e.g. ",", "
3876,http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html,NYU Depth,", ",", "
3876,http://kinectdata.com/,B3DO,", ","
"
3883,https://code.google.com/p/data-shrinker,shrinker 0.1,"
", - WARNING: it can throw SEGFAULT compiled with gcc 4.9+ -O3
3884,https://datatracker.ietf.org/doc/html/rfc8878,RFC8878,Zstandard's format is stable and documented in ,". Multiple independent implementations are already available. This repository represents the reference implementation, provided as an open-source dual "
3902,ortools/lp_data,lp_data,"
", Data structures for linear models.
3915,http://lmb.informatik.uni-freiburg.de/resources/datasets/,FBMS-59 dataset,The datasets included originate from the ,". The datasets are provided only for research purposes and without any warranty. When using the BMS-26 or FBMS-59 in your research work, you should cite the appropriate papers in the link above."
3922,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Oxford Flowers 102,", ",", "
3922,https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/merge_data_layer.hpp,MergeData,"
","
"
3922,https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/split_data_layer.hpp,SplitData,"
","
"
3922,https://github.com/ZYYSzj/Selective-Joint-Fine-tuning/blob/master/selective_joint_ft/additional_layers/random_crop_boosted_data_layer.hpp,RandomCropBoostedData,"
","
"
3923,https://www.dropbox.com/s/ewwvbu1d0drh9wu/all_dataset.zip?dl=0,here,All the datasets used in the paper can be downloaded ,.
3942,https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4,PMLB: a large benchmark suite for machine learning evaluation and comparison,"Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). ",. 
3955,http://www.vs.inf.ethz.ch/res/show.html?what=eco-data,The ECO dataset,"
",: Together with 
3956,http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database,here,The first thing you need to do is to download the data. You have to register , and download these two files:
3962,http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config,Makefile.config,You can download my , for reference. 2. Python packages you might not have: 
3997,https://github.com/stanford-futuredata/tensorflow-noscope/tree/speedhax,here,This repository contains the code for the optimization step in the paper. The inference code is ,.
4003,http://dataminingtutorial.com,dataminingtutorial.com,The data corpus used in the research is publicly available and can be requested at ,"
"
4003,src/users_likes_data_set.R,users_likes_data_set.R,"
", - holds data set definition with functions to get batches of train/validation samples
4004,#code-and-data-preparation,Code and Data Preparation,"
","
"
4004,#download-datasets,Download Datasets,"
","
"
4004,#evaluating-on-benchmark-datasets,Evaluating on benchmark datasets,"
","
"
4004,https://s3-us-west-2.amazonaws.com/awscv-public-data/ssn-actionness/binaryclassifier_thumos14_BNInception_rgb_checkpoint.pth.tar,RGB Actionness Model,Pretrained actionness classifier on THUMOS14 can be downloaded from , and 
4004,https://s3-us-west-2.amazonaws.com/awscv-public-data/ssn-actionness/binaryclassifier_thumos14_BNInception_flow_checkpoint.pth.tar,Flow Actionness Model, and ,"
"
4008,http://datamarket.azure.com/dataset/bing/search,Bing search API, and the ,. Attribute exploration is a method based on Formal Concept Analysis - for an introduction I recommend the book 
4027,http://deim.urv.cat/~manlio.dedomenico/data.php,personal Web page,", an open source tool by Manlio de Domenico. Note his ", additionally proposes many multilayer networks to play with.
4027,http://deim.urv.cat/~manlio.dedomenico/data.php,Manlio de Domenico's page,"
",.
4027,http://vlado.fmf.uni-lj.si/pub/networks/data/ucinet/ucidata.htm,Ucinet IV Datasets,"
",.
4040,https://www.robots.ox.ac.uk/~vgg/data/lip_reading/,Lip Reading in the Wild,Download the , dataset
4042,https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption,https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption,The repository supports optimization of the above models on artifical multivariate noisy AR time series and household electricity conspumption dataset , The dataset has to be specified alongside the paremeters in each of the files listed above.
4044,http://www.ltr-data.se/opencode.html/#ImDisk,IMDisk,Install a RAM disk like , and set 
4054,http://www.pjm.com/markets-and-operations/ops-analysis/historical-load-data.aspx,PJM,"Experiments considering a realistic grid-scheduling task, in which electricity generation is scheduled based on some (unknown) distribution over electricity demand. Historical load data for these experiments were obtained from ",.
4064,https://www.kaggle.com/c/ultrasound-nerve-segmentation/data,Provided data,"
", is processed by 
4066,http://ganymed.imib.rwth-aachen.de/irma/datasets/,IRMA Skin Lesion Dataset,The ," has 747 dermoscopic images (187 melanomas). This dataset is unlisted, but available under special request, and the signing of a license agreement."
4066,http://www.fc.up.pt/addi/ph2%20database.html,PH2 Dataset,The , has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.
4072,#suggested-datasets-and-models,Suggested Datasets and Models,"
","
"
4072,#datasets,Datasets,"
","
"
4072,#adding-a-dataset,Adding a dataset,"
","
"
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py,"
all_problems.py
", protocol buffers. All problems are imported in , or are registered with 
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen,"
t2t-datagen
",. Run , to see the list of available problems and download them.
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir,"
example_usr_dir
",See the , for an example user directory.
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py,"
Problem
","To add a new dataset, subclass ", and register it with 
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py,"
TranslateEndeWmt8k
",. See , for an example. Also see the 
4072,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md,data generators README, for an example. Also see the ,.
4075,https://github.com/helm/helm/tree/master/cmd/helm/testdata/testcharts/alpine,alpine example chart,Take a look at the , for reference when you're writing your first few charts.
4095,http://www.census.gov/topics/population/genealogy/data/2000_surnames.html,Frequently Occurring Surnames from the Census 2000,"
",. Surnames occurring >= 100 more times in the 2000 census. Details here: 
4095,http://deron.meranda.us/data/,Female/male first names from the Census 1990,"
","
"
4095,http://data.okfn.org/data/core/s-and-p-500,"Standard and Poor's (S&P) 500 Index Data including Dividend, Earnings and P/E Ratio",S&P500 data was retrieved from Open Knowledge's ,"
"
4097,https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/,"
FLAML
","
", provides automated tuning for LightGBM (
4100,https://archive.ics.uci.edu/ml/datasets/Wine+Quality,winequality dataset,Download ,", and other datasets and change utils.py to add new datasets to test out the pruning algorithm."
4113,http://datahub.io/dataset/amsterdam-museum-as-edm-lod,http://datahub.io/dataset/amsterdam-museum-as-edm-lod,Amsterdam Museum (,)
4133,http://mscoco.org/dataset/#download,MSCOCO images,Download , and 
4135,https://fra.xtil.net/simfiles/data/tsunamix/III/Tsunamix%20III%20[SM5].zip,(Fraxtil) Tsunamix III,"
","
"
4135,https://fra.xtil.net/simfiles/data/arrowarrangements/Fraxtil's%20Arrow%20Arrangements%20[SM5].zip,(Fraxtil) Fraxtil's Arrow Arrangements,"
","
"
4135,https://fra.xtil.net/simfiles/data/beastbeats/Fraxtil's%20Beast%20Beats%20[SM5].zip,(Fraxtil) Fraxtil's Beast Beats,"
","
"
4139,https://www.cheminformania.com/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/,"
Smiles Enumeration Header
","
","
"
4139,https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/,blog post,SMILES enumeration is the process of writing out all possible SMILES forms of a molecule. It's a useful technique for data augmentation before sequence based modeling of molecules. You can read more about the background in this , or 
4139,https://www.wildcardconsulting.dk/useful-information/smiles-enumeration-as-data-augmentation-for-molecular-neural-networks/,blog.,"If you find it useful, feel welcome to leave a comment on the ","
"
4148,https://motchallenge.net/data/MOT16/,MOT16 benchmark,The following example starts the tracker on one of the , sequences. We assume resources have been extracted to the repository root directory and the MOT16 benchmark data is in 
4158,http://www.nature.com/articles/sdata201635,MIMIC-III paper,Please be sure also to cite the original ,.
4158,http://www.nature.com/articles/sdata201635,paper,"Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database (",", "
4159,http://www.nature.com/articles/sdata201635,MIMIC-III paper,Please be sure also to cite the original ,.
4159,http://www.nature.com/articles/sdata201635,paper,"Here we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database (",", "
4161,docs/collect_data.md,Training the color segmenter,"
",: a quick overview of how you can train the color segmenter.
4161,docs/collect_data.md,the block stacking data collection notes,"For more information on how to collect data for the ""block stacking"" task, check out ","
"
4161,costar_tools/alvar_data_collection/README.md,alvar_data_collection,"
",: utilities to define the black and white printed AR tags we use for 
4162,http://mscoco.org/dataset/#download,Download,"
"," or use symlink, such that the MS COCO images are under "
4162,http://www.eecs.berkeley.edu/~ronghang/projects/cvpr16_text_obj_retrieval/referitdata.tar.gz,Download,"
"," or use symlink, such that the ReferItGame data are under "
4179,MBM_data.zip,GitHub,Available from ,","
4180,https://github.com/Unidata/awips2/blob/c9f28fd5943170b88cac2e3af3b0234ac444b705/cave/com.raytheon.uf.viz.collaboration.ui/src/com/raytheon/uf/viz/collaboration/ui/login/ServerListListener.java,source,"
","
"
4189,http://marsyasweb.appspot.com/download/data_sets/,Gtzan genre,"
","
"
4189,http://marsyasweb.appspot.com/download/data_sets/,Gtzan speech/music,"
","
"
4189,http://cvml.unige.ch/databases/emoMusic/,emoMusic,"
","
"
4189,http://www.mathieuramona.com/wp/data/jamendo/,Jamendo singing voice,"
","
"
4189,https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html,Urbansound8K,"
","
"
4199,http://lmb.informatik.uni-freiburg.de/data/ogn/data.zip,here," voxel grids into octrees. Three of the datasets used in the paper (ShapeNet-cars, FAUST and BlendSwap) can be downloaded from ",". For ShapeNet-all, we used the voxelizations(ftp://cs.stanford.edu/cs/cvgl/ShapeNetVox32.tgz) and the renderings(ftp://cs.stanford.edu/cs/cvgl/ShapeNetRendering.tgz) provided by Choy et al. for their "
4199,http://lmb.informatik.uni-freiburg.de/data/ogn/examples.zip,here,Example models can be downloaded from ,". Run one of the scripts (train_known.sh, train_pred.sh or test.sh) from the corresponding experiment folder. You should have the caffe executable in your $PATH."
4200,http://datashare.is.ed.ac.uk/handle/10283/1942,Edinburgh DataShare, can be found in ,". However, "
4207,ihttp://conda.pydata.org/docs/index.html,conda,", ",", "
4207,https://github.com/escorciav/daps/tree/master/data/models,Pre-trained models,"
",. Our generalization experiment suggests that you may expect decent results for other kind of action classes with similar lengths. Check out the models trained on the validation set of THUMOS14.
4207,https://github.com/escorciav/daps/blob/master/data/samples/c3d_after_pca.hdf5,here,Download the C3D representation of a couple of videos from ,.
4207,https://github.com/escorciav/daps/blob/master/data/models/T512K64_thumos14.npz,model,Download our ,.
4209,https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb,get_geographic_data.ipynb,Sampling the Data using , in Python2 (the only part requiring Python2) followed by 
4209,https://github.com/comp-journalism/2016-03-wapo-uber/blob/master/get_geographic_data.ipynb,get_geographic_data.ipynb,Python 3 (and python 2 only for , )
4212,lib/dataset/imagenet_vid_eval_motion.py,motion-specific evaluation code,"We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The ", is included in this repository.
4212,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,Flying Chairs, model and , pre-trained 
4212,lib/dataset/imagenet_vid_eval_motion.py,Motion-specific evaluation code,"
", is available!
4219,#freebase-data-dumps,Freebase Data Dumps,"
","
"
4222,http://en.wikipedia.org/wiki/Graph_database,,Titan is a highly scalable ," optimized for storing and querying large graphs with billions of vertices and edges distributed across a multi-machine cluster. Titan is a transactional database that can support thousands of concurrent users, complex traversals, and analytic graph queries."
4223,https://www.alluxio.io/data-orchestration-summit-2019/,Data Orchestration Summit, or visit our first community conference (,) to learn from other community members!
4233,https://pytorch.org/docs/stable/data.html,"
torch.utils
"," | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training | | ", | DataLoader and other utility functions for convenience |
4233,http://numba.pydata.org/,Numba, and ,. Our goal is to not reinvent the wheel where appropriate.
4235,https://github.com/louismartin/dress-data/raw/master/data-simplification.tar.bz2,Github, datasets can be downloaded on , or on 
4254,https://github.com/zhenjl/encoding/tree/master/benchmark/data,Timestamps: ts.txt(sorted),Test file  ,"
"
4254,https://github.com/ot/partitioned_elias_fano/tree/master/test/test_data,Test data,Raw 32 bits binary data file ,"
"
4254,https://github.com/zhenjl/encoding/tree/master/benchmark/data,Test data: ts.txt(sorted) and lat.txt(unsorted),Text file: 1 entry per line. ,)
4254,http://lemire.me/data/integercompression2014.html,Document identifier data set,"
","
"
4259,http://mscoco.org/dataset/#captions-challenge2015,MSCOCO Image Caption dataset, and , using an auto-tagging system for each image. Some of the results are shown below (left and right columns are selected samples from Clickture and MSCOCO caption dataset respectively.)
4270,https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime,"
--icu-data-dir node flag", module and the , at launch.
4279,http://stackoverflow.com/questions/4116067/purge-or-recreate-a-ruby-on-rails-database,Purge or recreate a Ruby on Rails database,"
","
"
4280,refdiff-evaluation/data/java-evaluation/evaluation-data-public.xlsx,Java evaluation data,"
","
"
4284,https://github.com/ryancotterell/sigmorphon2016/tree/master/data/,here,", first download training, validatation and test data ",.
4285,https://www.kdd.org/kdd2017/papers/view/learning-certifiably-optimal-rule-lists-for-categorical-data,KDD,"
",", 2017"
4285,#data-format,Data format,"
","
"
4285,#example-dataset,Example dataset,"
","
"
4285,#data-structures,Data structures,"
","
"
4292,https://snap.stanford.edu/data/index.html,SNAP Database,. It should accept any graph file you can find from , or the 
4294,http://corpus-tools.org/pepper/,Pepper,Luke Gessler has written a module for the ," tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See "
4297,data_engine/README.md,data_engine/README.md,See , for detailed information.
4304,http://www.comp.leeds.ac.uk/mat4saj/lspet_dataset.zip,LSP Extended dataset,"
", (10000 train images)
4304,http://datasets.d2.mpi-inf.mpg.de/leonid14cvpr/mpii_human_pose_v1_u12_1.tar.gz,Annotation,"
","
"
4304,http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz,Images,"
","
"
4309,http://people.ee.ethz.ch/~ihnatova/#dataset,DPED dataset,Download , (patches for CNN training) and extract it into 
4328,http://numba.pydata.org/numba-doc/dev/user/installing.html,here,"For numba instructions, you can find a tutorial and installation guideline ",.
4331,https://lrs.icg.tugraz.at/datasets/prid/,PRID 2011,We use the ,", "
4331,www.eecs.qmul.ac.uk/.../downloads_qmul_iLIDS-VID_ReID_dataset.html,iLIDS-VID,", ", and [LPW] (coming soon) datasets for evaluation. They are video-based tasks for person re-identification.
4336,https://github.com/rbgirshick/py-faster-rcnn/blob/96dc9f1dea3087474d6da5a98879072901ee9bf9/data/README.md,here,"[Optional] If you want to use COCO, please see the notes ",.
4352,https://www.dropbox.com/s/9t770jhcjqo3mmg/release_data.zip,Horse Dataset,Download the , (580 MB)
4353,https://github.com/davidstutz/superpixel-benchmark-data,Datasets, | , | 
4353,https://github.com/davidstutz/superpixel-benchmark-data,data repository,"The converted (i.e. pre-processed) NYUV2, SBD and SUNRGBD datasets are now available in the ",.
4354,data,"
VID
",1.Download the training data. (,)
4366,#example-data,Example data,"
","
"
4366,#stand-alone-data-generation,Stand-alone data generation,"
","
"
4366,#loading-extracted-data,Loading extracted data,"
","
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-oneshape/data.html,[One shape],Existential:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-colfree/data.html,[Collision-free],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-full/data.html,[Full],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/existential-chinese/data.html,[Chinese],  ·  ,"
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial_twoshapes/data.html,[Spatial two shapes],Relational:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-spatial/data.html,[Spatial],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-attribute/data.html,[Attribute],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-comparative/data.html,[Comparative],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/relational-full/data.html,[Full],  ·  ,"
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-positive/data.html,[Positive],Selection:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-superlative/data.html,[Superlative],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/selection-full/data.html,[Full],  ·  ,"
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-count/data.html,[Count],Quantification:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-ratio/data.html,[Ratio],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification-full/data.html,[Full],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/quantification_complex-full/data.html,[Complex],  ·  ,"
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-existential/data.html,[Existential],Logical:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/agreement/logical-full/data.html,[Full],  ·  ,"
"
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-single/data.html,[Single],Shape:     ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-multi/data.html,[Multi],  ·  ,  ·  
4366,https://rawgit.com/AlexKuhnle/ShapeWorld/master/examples/classification/shape-count/data.html,[Count],  ·  ,"
"
4366,https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets,dataset arguments," if necessary, see ", for details)
4366,https://github.com/AlexKuhnle/ShapeWorld/tree/master/shapeworld/datasets,dataset arguments," if necessary, see ", for details)
4376,http://www.qizhexie.com//data/RACE_leaderboard,Leaderboard of RACE,"
","
"
4376,http://www.cs.cmu.edu/~glai1/data/race/,here,RACE: Please submit a data request ,". The data will be automatically sent to you. Create a ""data"" directory alongside ""src"" directory and download the data."
4376,http://nlp.stanford.edu/data/glove.6B.zip,http://nlp.stanford.edu/data/glove.6B.zip,glove.6B.zip: ,"
"
4403,https://github.com/ppuliu/GloRE/tree/master/data,"
data
",We provide the following pre-processed files in ,:
4405,data/,"
data
"," dataset, which contains 8 domains including Basketball, Calendar, and Restaurants. The dataset is already pre-processed and can be found under ",.
4407,http://www.cvlibs.net/datasets/kitti/user_login.php,kitti submit,You can submit the result at ,". If you don't have time to train your model, you can download a pre-trained model from the link as follow. "
4411,data/conll,data,"
", — Persuasive Essay data set in CONLL format. Essay and paragraph level given. The original data can be found here: https://www.ukp.tu-darmstadt.de/data/argumentation-mining/argument-annotated-essays-version-2/
4427,http://www.cs.columbia.edu/~andrews/mil/datasets.html,here,This script will download datasets from , and make necessary changes.
4438,http://en.wikipedia.org/wiki/Semi-structured_data,semi-structured information,The goal of this project is to extract , from Wiktionary and construct 
4438,https://github.com/componavt/wikokit/blob/wiki/File_wikt_parsed_empty_sql.md#machine-readable-database-schema,Machine-readable database schema,", see ",;
4438,http://en.wiktionary.org/wiki/User:AKA_MBG/Statistics:Parameters_of_the_database_created_by_the_Wiktionary_parser,total,English Wiktionary: ,", "
4440,http://sourceforge.net/projects/jobimtext/files/data/models/en_news120M_stanford_lemma/LMI_p1000_l200.gz,http://sourceforge.net/projects/jobimtext/files/data/models/en_news120M_stanford_lemma/LMI_p1000_l200.gz,"For an example of how to use the CW clustering algorithm for word sense induction, compile the code as shown above and download example data, like this word similarity graph extracted from a 120-million-lines English news corpus taken from the JoBimText project: ",.
4457,https://www.kaggle.com/c/GiveMeSomeCredit/data/,Credit scoring,"
","
"
4458,https://travis-ci.org/databricks/spark-perf,"
Build Status
","
","
"
4458,https://github.com/databricks/spark-perf/issues,open an issue on GitHub,"For questions, bug reports, or feature requests, please ",.
4461,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,For ,", first download the dataset using this "
4461,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,", first download the dataset using this "," provided on the official website, and then run the following command"
4461,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,here,"For the pose experiments, we used the KITTI odometry split, which can be downloaded ",. Then you can change 
4461,https://www.cityscapes-dataset.com/,Cityscapes,For ,", download the following packages: 1) "
4461,https://vision.in.tum.de/data/datasets/rgbd-dataset/tools#evaluation,TUM evaluation toolkit, consistent with the ,. Then you could run
4469,https://pandas.pydata.org,pandas,"
","
"
4475,#preprocess-vqa-dataset,Preprocess VQA dataset,"
","
"
4475,#pretrained-models-and-data-files,Pretrained models and data files,"
","
"
4495,http://www.cse.cuhk.edu.hk/leojia/projects/dblurdetect/dataset.html,Blur Detection Dataset,Test images are from , [2].
4500,http://128.2.220.95/multilingual/data,Crosslingual word embeddings and corpora,"
","
"
4504,./data/finance,folder," once downloaded if you wish to keep using the same helper functions that I use in the code please put the individual training, test, trail data in this ", or else change the 
4522,#mscoco-captioning-dataset,preprocessing MSCOCO data,Script for ,"
"
4544,https://s3-us-west-2.amazonaws.com/credbank/stream_tweets_byTimestamp.data,(Download),"
","
"
4544,https://s3-us-west-2.amazonaws.com/credbank/eventNonEvent_annotations.data,(Download),"
","
"
4544,https://s3-us-west-2.amazonaws.com/credbank/cred_event_TurkRatings.data,(Download),"
", This file contains more than 1300 events and their corresponding credibility ratings and reasons behind the ratings entered by Turkers. It contains 4 fields:
4544,https://s3-us-west-2.amazonaws.com/credbank/cred_event_SearchTweets.data,(Download),"
", This file contains more than 80 million tweets spread across event topics. The tweets are fetched using the Twitter search API. The search query is constructed by taking a boolean AND over all 3 terms of a topic. Each line in the file contains 4 fields:
4549,https://github.com/sajao/CrisisLex/tree/master/data/CrisisLexT26,CrisisLexT26 Dataset,"
","
"
4549,http://www.crowdflower.com/data-for-everyone,CrowdFlower10K Dataset,"
","
"
4552,http://nyc.lti.cs.cmu.edu/datasets/cross_distill.tar.gz,link,download dataset from this ,"
"
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,"
probing tasks",SentEval also includes a series of , to evaluate what linguistic properties are encoded in your sentence embeddings:
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SentLen,| Task     	| Type                         	| #train 	| #test 	| needs_train 	| set_classifier | |----------	|------------------------------	|-----------:|----------:|:-----------:|:----------:| | ,	| Length prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,WC,	| Length prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Word Content analysis	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,TreeDepth,	| Word Content analysis	| 100k     	| 10k    	| 1 | 1 | | ,	| Tree depth prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,TopConst,	| Tree depth prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Top Constituents prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,BShift,	| Top Constituents prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Word order analysis	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,Tense,	| Word order analysis	| 100k     	| 10k    	| 1 | 1 | | ,	| Verb tense prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SubjNum,	| Verb tense prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Subject number prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,ObjNum,	| Subject number prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Object number prediction	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SOMO,	| Object number prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Semantic odd man out	| 100k     	| 10k    	| 1 | 1 | | 
4567,https://github.com/facebookresearch/SentEval/tree/master/data/probing,CoordInv,	| Semantic odd man out	| 100k     	| 10k    	| 1 | 1 | | ,	| Coordination Inversion | 100k     	| 10k    	| 1 | 1 |
4601,https://belinkov.mit.edu/data,here,"Download the train and test data created by Yonatan Belinkov, available ",", and set them up in the same format as the sample test data, with labels indicating the index of the correct head words. The dataset comes with POS tags of head words, so you will not need a POS tagger for this. Concatenate the preposition and the dependent of the preposition at the end of the set of head words. Your train and test sets should have tab separated lines that look like this:"
4612,http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html,site, (ECSSD dataset ,)
4633,https://github.com/ctuning/ctuning-datasets-min,'ctuning-datasets-min' repository,) to the ,.
4644,https://omnomnom.vision.rwth-aachen.de/data/lunet2c-noscale-nobg-2to32-aug.pkl,the weights we used can be downloaded here, and ,.
4644,https://omnomnom.vision.rwth-aachen.de/data/bbmtrack-results/,can be found here,Final raw bounding box results ,.
4647,https://github.com/Lab41/Magnolia/tree/master/data,"
data
",Directory: ,"
"
4661,https://www.researchgate.net/publication/320475411_When_to_use_what_data_set_for_your_self-driving_car_algorithm_An_overview_of_publicly_available_driving_datasets,"""When to use what data set for your self-driving car algorithm: An overview of publicly available driving datasets""","H. Yin, C. Berger: ","
"
4686,http://qweb.cs.aau.dk/pipe/data.tar.gz,here,Odyssey's statistics are available ,". All the files with the statistics (statistics*, cps*, css*) should be put in the folder fedbench/"
4688,#adding-a-new-dataset,Adding a new dataset,"
","
"
4688,data/conll2003/en,"
data/conll2003/en
","
",": annotated dataset with the CoNLL-2003 format, containing 3 files ("
4688,data/example_unannotated_texts,"
data/example_unannotated_texts
","
",": unannotated dataset with the BRAT format, containing 1 folder ("
4704,#data_prep,Data Preparation,"
","
"
4705,http://datadryad.org/resource/doi:10.5061/dryad.jp917,data set," concept unique identifiers (CUIs), derived from 20 million clinical notes spanning 19 years of data from Stanford Hospital and Clinics, using a  ", released in a 
4705,http://www.nature.com/articles/sdata201432,paper, released in a ," by Finlayson, LePendu & Shah."
4709,https://github.com/yburda/iwae/tree/master/datasets/BinaryMNIST,link,static MNIST: links to the datasets can found at ,;
4709,https://github.com/yburda/iwae/blob/master/datasets/OMNIGLOT/chardata.mat,link,OMNIGLOT: the dataset could be downloaded from ,;
4709,https://people.cs.umass.edu/~marlin/data/caltech101_silhouettes_28_split1.mat,link,Caltech 101 Silhouettes: the dataset could be downloaded from ,.
4709,https://github.com/jmtomczak/vae_householder_flow/tree/master/datasets/histopathologyGray,link,Histopathology Gray: the dataset could be downloaded from ,;
4711,#get-the-data,Get the Data,"
","
"
4711,#loading-data-with-other-machine-learning-libraries,Many ML libraries,"
"," already include Fashion-MNIST data/API, give it a try!"
4711,#get-the-data,downloaded the data,Make sure you have , and placed it in 
4711,https://docs.activeloop.ai/datasets/fashion-mnist-dataset,Activeloop Hub,"
","
"
4711,https://mxnet.apache.org/api/python/docs/api/gluon/data/vision/datasets/index.html#mxnet.gluon.data.vision.datasets.FashionMNIST,Apache MXNet Gluon,"
","
"
4711,https://github.com/tensorflow/tfjs-examples/blob/master/fashion-mnist-vae/data.js,TensorFlow.js,"
","
"
4711,https://pytorch.org/vision/stable/datasets.html#fashion-mnist,Pytorch,"
","
"
4711,https://keras.io/api/datasets/fashion_mnist/,Keras,"
","
"
4711,https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist,Tensorflow,"
","
"
4711,https://www.tensorflow.org/datasets/catalog/fashion_mnist,TensorFlow Datasets,"
","
"
4711,https://juliaml.github.io/MLDatasets.jl/latest/datasets/FashionMNIST/,JuliaML,"
","
"
4711,https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.get_fashion_mnist.html,Chainer,"
","
"
4711,https://huggingface.co/datasets/fashion_mnist,HuggingFace Datasets,"
","
"
4711,https://jamesmccaffrey.wordpress.com/2013/11/23/reading-the-mnist-data-set-with-c/,C#,"
","
"
4711,https://github.com/AbhirajHinge/CNN-with-Fashion-MNIST-dataset,:link:, | ,"| |2 Conv | Normalization, random horizontal flip, random vertical flip, random translation, random rotation. | 0.919 |0.971 | "
4745,/terngrad/split_dataset.sh,split_dataset.sh,We also provide , to 
4745,inception/data/download_and_preprocess_imagenet.sh,script,We provide a single , for downloading and converting ImageNet data to TFRecord format. Downloading and preprocessing the data may take several hours (up to half a day) depending on your network and computer speed. Please be patient.
4745,inception/data/build_image_data.py,"
build_image_data.py
"," If you wish to prepare a custom image data set for transfer learning, you will need to invoke ", on your custom data set. Please see the associated options and assumptions behind this script by reading the comments section of [
4745,inception/imagenet_data.py,"
imagenet_data.py
","] (inception/data/build_image_data.py). Also, if your custom data has a different number of examples or classes, you need to change the appropriate values in ",.
4745,inception/data/build_image_data.py,"
build_image_data.py
",One can use the existing scripts supplied with this model to build a new dataset for training or fine-tuning. The main script to employ is ,". Briefly, this script takes a structured directory of images and converts it to a sharded "
4746,https://github.com/davidsandberg/facenet/wiki/Training-using-the-VGGFace2-dataset,wiki,| Date     | Update | |----------|--------| | 2018-04-10 | Added new models trained on Casia-WebFace and VGGFace2 (see below). Note that the models uses fixed image standardization (see ,"). | | 2018-03-31 | Added a new, more flexible input pipeline as well as a bunch of minor updates. | | 2017-05-13 | Removed a bunch of older non-slim models. Moved the last bottleneck layer into the respective models. Corrected normalization of Center Loss. | | 2017-05-06 | Added code to "
4746,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,VGGFace2, dataset has been used for training. This training set consists of total of 453 453 images over 10 575 identities after face detection. Some performance improvement has been seen if the dataset has been filtered before training. Some more information about how this was done will come later. The best performing model has been trained on the , dataset consisting of ~3.3M faces and ~9000 classes.
4747,http://jmcauley.ucsd.edu/data/amazon/,Amazon dataset,"
","
"
4748,https://dataskeptic.com/blog/episodes/2017/pix2code,Data Skeptic,"
", (podcast)
4752,https://bmcfee.github.io/data/aotm2011.html,AotM-2011,The paper presents a thorough off-line evaluation conducted on two playlist datasets: the publicly available , dataset (derived from the 
4752,http://drive.jku.at/ssf/s/readFile/share/8197/5021896040269493362/publicLink/data_HybridPlaylistContinuation_.zip,Download the data,We share the filtered playlists and song features corresponding to the AotM-2011 collection. We can not share any information related to the 8tracks collection. ,", decompress it, and place the obtained "
4770,http://pandas.pydata.org/,pandas,"
", for logging to csv
4770,http://bokeh.pydata.org,bokeh,"
", for training visualization
4789,https://cs.adelaide.edu.au/users/hwong/doku.php?id=data,AdelaideRMF dataset,Download the , into 
4789,http://vision.jhu.edu/data/,Hopkings dataset,Download , into 
4795,#synthetic-data,Synthetic Data,"
","
"
4795,#custom-data,Custom data,"
","
"
4795,#data-preparation,data preparation, must lead to the directory where the results will be stored. See , section for an explanation of how to set 
4795,#data-preparation,data preparation,: path to the data; see , for a detailed discussion; this parameter is required
4795,http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html,digits,To train the model on a custom dataset you need to define a class with a specific interface. Suppose we want to train the model on the , dataset. This datasets consists of 8x8 images of digits. Let's suppose that the data is stored in 
4799,dataset/multi_paste/cattle_gcs500_copy_rb5.png,cattle dataset," of the blocks group and the total number of the blocks it is formed with. If this list is not empty, we can assume that the image is being tampered. For example, running the ", with 32 px of block size will result in:
4807,http://www.let.rug.nl/rikvannoord/AMR/silver_data/,here,The silver data that I used in the experiments for the CLIN paper can be downloaded ,. The silver data was obtained by parsing all sentences in the 
4817,http://dhsprogram.com/data/dataset_admin/download-datasets.cfm,host website for the Demographic and Health Surveys data,Visit the ,"
"
4833,http://graphics.stanford.edu/data/3Dscanrep/,XYZ RGB Asian Dragon,Model: ,"
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/full.tar.gz,the whole dataset (screenshots + trajectories) for all five games,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/trajectories.tar.gz,trajectories only for all five games,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/spaceinvaders.tar.gz,Space Invaders,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/qbert.tar.gz,Q*bert,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/mspacman.tar.gz,Ms. Pacman,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/pinball.tar.gz,Video Pinball,"
","
"
4839,https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/revenge.tar.gz,Montezuma's Revenge,"
","
"
4847,https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/config,data/generator/config,"The values associated to the keys can be either one value or a list of values. In the case of a list of values, the PIMC generator will consider all the possible combinations of values between all the keys. Some examples of configuration files for PIMC generation can be found in ",. Some examples of PRISM files can be found in 
4847,https://github.com/anicet-bart/pimc_pylib/tree/master/data/generator/inputs,data/generator/inputs,. Some examples of PRISM files can be found in , (from 
4851,#open-source-speech-recognition-recipe-and-corpus-for-building-german-acoustic-models-with-kaldi,Open source speech recognition recipe and corpus for building German acoustic models with Kaldi,"
","
"
4851,#Get-LM-text-data,Get LM text data,"
","
"
4851,#getting-data-files-separately,Getting data files separately,"
","
"
4851,#speech-corpus,Speech corpus,"
","
"
4851,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,m-ailabs read speech data corpus,", containing about 285h of additional data and the German subset of ","
"
4851,https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/,(mirror),"
", (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_HCLG_s.fst.bz2,HCLG_s,". If you need an overall faster model, you can also replace the default HCLG with this much smaller one: ",.
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,download,A new pretrained model with a vocabulary of 400 thousand words is available: ,"
"
4851,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,m-ailabs speech data corpus,"We added more aligned speech data (630h total now), thanks to the ","
"
4851,https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/,(mirror),"
",. We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_350k_nnet3chain_tdnn1f_1024_sp_bi.tar.bz2,download,A new pretrained model with a vocabulary of 350 thousand words is available: ,"
"
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k.tar.bz2,full archive,The ivector extractor had been missing from the acoustic model binary archive. You can download it separately from  https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k_ivector_extractor.tar.bz2 or redownload the ,.
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv8_voc900k,| Acoustic model + FST | Training data | Tuda dev WER (FST) | Tuda test WER (FST) | | --- | --- | --- | --- | | , | 1700h (tuda+SWC+m-ailabs+cv8) | 9.30  | 10.17 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_const_arpa.tar.bz2,lm_v6_voc900k, | 1700h (tuda+SWC+m-ailabs+cv8) | 9.30  | 10.17 | | + , const arpa rescoring | 140 million sentences | 7.23 | 7.96  | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_rnnlm_lstm_4x.tar.bz2,rnn_lmv6_lstm4x_voc900k, const arpa rescoring | 140 million sentences | 7.23 | 7.96  | | + , rnnlm rescoring | 140 million sentences | 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_HCLG_s.fst.bz2,HCLG_s,", e.g. drei und sechzig -> dreiundsechzig. If you need an overall faster model, you can also replace the default HCLG with this much smaller one: ",". Together with RNNLM rescoring, the WER result will only be minimally bigger."
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k.tar.bz2,tuda_swc_voc126k,| Acoustic model + FST | Training data | Tuda dev WER (FST) | Tuda test WER (FST) | | --- | --- | --- | --- | | , / 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_350k_nnet3chain_tdnn1f_1024_sp_bi.tar.bz2,tuda_swc_voc350k, | 375h (tuda+SWC) | 20.30 | 21.43 | | , / 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_voc400k, | 375h (tuda+SWC) | 15.32 | 16.49 | | , / 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_683k_nnet3chain_tdnn1f_2048_sp_bi_smaller_fst.tar.bz2,tuda_swc_mailabs_cv_voc683k_smaller_fst, | 630h (tuda+SWC+m-ailabs) | 14.78 | 15.87 | | , | 1000h (tuda+SWC+m-ailabs+cv) | 12.69 | 14.29 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/carpa_rescoring_language_model_v5_voc683k.tar.bz2,lm_v5_voc683k_smaller_fst, | 1000h (tuda+SWC+m-ailabs+cv) | 12.69 | 14.29 | | + , const arpa rescoring | 100 million sentences | 10.92 | 12.37 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_683k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv3_voc683k, | e.g. drei und sechzig -> dreiundsechzig |  8.94  | 10.26 | | , | 1000h (tuda+SWC+m-ailabs+cv3) | 12.26 | 13.79 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/carpa_rescoring_language_model_v5_voc683k.tar.bz2,lm_v5_voc683k, | 1000h (tuda+SWC+m-ailabs+cv3) | 12.26 | 13.79 | | + , const arpa rescoring | 100 million sentences | 10.47 | 11.85 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv8_voc722k, | e.g. drei und sechzig -> dreiundsechzig | 8.61  | 9.85 | | , | 1700h (tuda+SWC+m-ailabs+cv8) | 10.94 | 12.09 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_const_arpa.tar.bz2,lm_v5_voc722k, | 1700h (tuda+SWC+m-ailabs+cv8) | 10.94 | 12.09 | | + , const arpa rescoring | 100 million sentences | 9.25 | 10.17 | | + 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_rnnlm_lstm_2x.tar.bz2,rnn_lm_lstm2x_voc722k, | e.g. drei und sechzig -> dreiundsechzig | 7.51 | 8.53 | | + , rnnlm rescoring | 100 million sentences | 
4851,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/german-speechdata-package-v2.tar.gz,here,The corpus can be downloaded ,. The license is 
4851,http://kaldi.sourceforge.net/data_prep.html#data_prep_lang_creating,lexion_p.txt,export_lexicon.py will export such a serialised python dictionary into KALDIs ," format (this allows to model different phonetic realisations of the same word with probabilities). Stress markers in the phoneme set are grouped with their unstressed equivalents in KALDI using the extra_questions.txt file. It is also possible to generate a CMU Sphinx formated dictionary with the same data using the -spx option. The Sphinx format also allows pronunciation variants, but cannot model probabilities for these variants."
4862,#11-train-with-a-face-dataset-celeba,Face dataset: CelebA,"
","
"
4862,#12-train-with-a-digit-dataset-mnist,Digit dataset: MNIST,"
","
"
4869,https://github.com/seg/2016-ml-contest/blob/master/validation_data_nofacies.csv,in the repo,F1 scores of models against secret blind data in the STUART and CRAWFORD wells. The logs for those wells are available ,", but contestants do not have access to the facies."
4874,#save-data,Save Data,"
","
"
4903,http://slate.cse.ohio-state.edu/UTSAuthenticatedDownloader/index.html?dataset=BMASS,Download the dataset,"
", (requires a valid 
4904,https://github.com/amarasovic/abstract-anaphora-data,repo,Check this ,.
4919,https://data.vision.ee.ethz.ch/zzhiwu/trailerFaces-tfrecords.zip,TrailerFaces,"
", (
4958,https://www.ndsl.kaist.edu/~changho/cmcl/dataset/cifar10_whitened.t7,pre-processed data (1.37GB),CIFAR-10 whitened: ,"
"
4958,https://www.ndsl.kaist.edu/~changho/cmcl/dataset/svhn_preprocessed.t7,pre-processed data (2.27GB),SVHN (excluding the extra dataset): ,"
"
4968,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUM_RGBD,"Two short RGB-D sequences are included as examples. To add more sequences, download from: ", or 
4969,#device-data,Device data,"
", (samples from mobile phones)
4969,#device-data-filtered,Filtered device data,"
", (activity and movement filtered)
4969,https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/csv,csv folder,CSV-versions of the data are available in the ,. The 
4969,https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/psql,psql folder,. The , contains instructions and scripts to import the data into a PostgreSQL database and produce reports and comparison tables between the manual log and recognised trips. Output from three sample methods is included for testing.
4969,https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/example-positioning-problem.png,here,", which combines data from satellite, WLAN and cellular positioning. Despite that, sometimes there can be problematic positioning fixes, which should be taken care of by filtering. One example is shown ",.
4969,#device-data,device_data,"device_id (integer, same as in ",)
4969,https://github.com/aalto-trafficsense/public-transport-dataset/blob/master/doc/transit-data-bounding-box.png,around the coordinates," fleet obtained by sampling http://dev.hsl.fi/siriaccess/vm/json every 30 seconds, restricting the timeperiod to the time of the trial and geoboxing the area ", sampled from the test participants.
4969,https://github.com/aalto-trafficsense/public-transport-dataset/tree/master/trains-json,trains-json folder,JSON-format information in the , is fetched from: http://rata.digitraffic.fi/api/v1/history?departure_date=2016-08-26
4969,http://www.liikennevirasto.fi/web/en/open-data/services/digitraffic#.V9BlOxB96Ho,Digitraffic, from , offered by the 
4976,http://data.caida.org/datasets/as-relationships/serial-1/,here,as_relationship : CAIDAs  AS relationship files found ,"
"
5011,#data-input-pipeline,Data Input Pipeline,"
","
"
5016,http://pytorch.org/audio/main/datasets.html,Dataloaders for common audio datasets,"
","
"
5036,https://s3-us-west-1.amazonaws.com/nico-opendata-distribution/550000.model,model files,Please download , at 
5040,src/main/java/it/cnr/jatecs/indexing/corpus/CorpusReader.java,CorpusReader.java,", which might be used independently, or in combination with ", and 
5040,src/example/java/apps/dataset,dataset, to construct an index from a raw corpus of documents (several examples of this latter could be consulted in directory ,", including, e.g., the "
5040,src/example/java/apps/dataset/IndexReuters21578.java,IndexReuters21578.java, collection -file ,")-, the "
5040,src/example/java/apps/dataset/IndexRCV1.java,IndexRCV1.java, collection -file ,"-, among many others). JaTeCS provides common feature extractors to represent features from raw textual data, like the "
5040,src/main/java/it/cnr/jatecs/indexing/corpus/BagOfWordsFeatureExtractor.java,BoW (bag-of-words),"-, among many others). JaTeCS provides common feature extractors to represent features from raw textual data, like the "," extractor, or the "
5040,src/main/java/it/cnr/jatecs/indexing/corpus/CharsNGramFeatureExtractor.java,characters n-grams," extractor, or the ", extractor. The 
5040,src/example/java/apps/dataset,dataset, extractor. The , directory contains many examples of corpus indexing. Both extractors are subclasses of the generic class 
5040,src/main/java/it/cnr/jatecs/indexing/corpus/FeatureExtractor.java,FeatureExtractor.java, directory contains many examples of corpus indexing. Both extractors are subclasses of the generic class ," which provides additional capabilities like stemming, stopword removal, etc."
5053,https://github.com/drethage/speech-denoising-wavenet#dataset,below,Download the dataset as described ,"
"
5053,http://datashare.is.ed.ac.uk/handle/10283/1942,Download here,"
","
"
5055,http://cs.stanford.edu/~bdlijiwei/visual_data.tar,data,download ,"
"
5056,https://www.notion.so/Analyzing-fMRI-data-with-deep-learning-models-62e0c032d0e244dab1fb077da136b214,blog, [,]
5063,https://www.dropbox.com/s/4i9u4y24pt3paba/personalized-dialog-dataset.tar.gz?dl=1,this link,". Alternatively, it is accessable using ", or through the 
5065,https://www.instacart.com/datasets/grocery-shopping-2017,Instacart,"
","
"
5066,https://hemanthdv.github.io/officehome-dataset/,Office-Home,"
","
"
5073,http://www.mathieuramona.com/wp/data/jamendo/,Jamendo,", ",", "
5074,https://www.researchgate.net/profile/Marius_Miron/publication/318322107_Generating_data_to_train_convolutional_neural_networks_for_classical_music_source_separation/links/59637cc3458515a3575b93c6/Generating-data-to-train-convolutional-neural-networks-for-classical-music-source-separation.pdf?_iepl%5BhomeFeedViewId%5D=WchoMnlUL1Hk9hBLVTeR8Amh&_iepl%5Bcontexts%5D%5B0%5D=pcfhf&_iepl%5BinteractionType%5D=publicationDownload&origin=publication_detail&ev=pub_int_prw_xdl&msrp=p3lQ8M4uZlb4TF5Hv9a2U3P2y4wW7ant5KWj4E5-OcD1Mg53p1ykTKHMG9_zVTB9n6mI8fvZOCL2Xhpru186pCEY-2ZxiYR-CB8_QvwHc1kUG-QE4SHdProR.LoJb2BDOiiQth3iR9xgZUxxCWEJgtTBF4whFrFa01OD49-3YYRxA0WQVN--zhtQU_7C2Pt0rKdwoFxT1pfxFvnKXSXmy2eT1Jpz-pw.U1QLoFO_Uc6aQVr2Nm2FcAi6BqAUfngH2Or5__6wegbCgVvTYoIGt22tmCkYbGTOQ_4PxBgt1LrvsFQiL0oMyogP8Yk8myTj0gs9jw.fGpkufGqAI4R2v8Hfe0ThcXL7M7yN2PuAlx974BGVn50SdUWvNhhIPWBD-zWTn8NKtVJx3XrjKXFrMgi9Cx7qGrNP8tBWpha6Srf6g,Generating data to train convolutional neural networks for classical music source separation, | | 2017 | , | 
5074,datasets.md,datasets,53 datasets used. See the list of ,. Datasets pie chart: 
5074,http://www.audiocontentanalysis.org/data-sets/,AudioContentAnalysis nearly exhaustive list of music-related datasets,"
","
"
5074,https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#cite_ref-215,Wikipedia's list of datasets for machine learning research,"
","
"
5074,http://deeplearning.net/datasets/,Datasets for deep learning,"
","
"
5074,https://github.com/caesar0301/awesome-public-datasets,Awesome public datasets,"
","
"
5074,https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0,Estimating Optimal Learning Rate,"
", - Blog post on the learning rate optimisation
5074,https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750,Battle of the Deep Learning frameworks,"
", - DL frameworks comparison and evolution
5102,http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,LibSVM Datasets," dataset, available at ",.
5102,https://archive.ics.uci.edu/ml/datasets/HIGGS,link,| Data     |      Task     |  Link | #Examples | #Feature| Comments| |----------|---------------|-------|-------|---------|---------| | Higgs    |  Binary classification | ," |10,500,000|28| use last 500,000 samples as test set  | | Epsilon  |  Binary classification | "
5102,http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,link," |10,500,000|28| use last 500,000 samples as test set  | | Epsilon  |  Binary classification | "," | 400,000 | 2,000 | use the provided test set | | Bosch    |  Binary classification | "
5102,https://www.kaggle.com/c/bosch-production-line-performance/data,link," | 400,000 | 2,000 | use the provided test set | | Bosch    |  Binary classification | "," | 1,000,000 | 968 | use the provided test set | | Yahoo LTR|  Learning to rank      | "
5102,https://webscope.sandbox.yahoo.com/catalog.php?datatype=c,link," | 1,000,000 | 968 | use the provided test set | | Yahoo LTR|  Learning to rank      | ","     |473,134|700|   set1.train as train, set1.test as test | | MS LTR   |  Learning to rank      | "
5102,http://stat-computing.org/dataexpo/2009/,link," |2,270,296|137| {S1,S2,S3} as train set, {S5} as test set | | Expo     |  Binary classification (Categorical) | "," |11,000,000|700| use last 1,000,000 as test set |"
5113,https://github.com/gittar/k-means-u-star/blob/master/notebooks/dataset_class.ipynb,dataset_class.ipynb,"
","
"
5116,https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/data/denoise/mit_saliency,here, and put all the image files ,; b) launch 
5116,https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_training_data.m,this script,; b) launch , to generate training data; c) launch 
5116,https://github.com/jimmy-ren/vcnn_double-bladed/blob/master/applications/image_denoise/gen_data/gen_val_data.m,this script, to generate training data; c) launch , to generate validation data; d) launch 
5125,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,here,Download KITTI dataset from ,. We need 
5125,http://www.cvlibs.net/download.php?file=data_tracking_image_2.zip,left color images,. We need , and 
5125,http://www.cvlibs.net/download.php?file=data_tracking_label_2.zip,tracking labels, and ,.
5126,http://us.data.yt8m.org/1/ground_truth_labels/train_labels.csv,http://us.data.yt8m.org/1/ground_truth_labels/train_labels.csv,"
","
"
5126,http://us.data.yt8m.org/1/ground_truth_labels/validate_labels.csv,http://us.data.yt8m.org/1/ground_truth_labels/validate_labels.csv,"
","
"
5127,http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE,", ",", "
5131,http://data.iana.org/TLD/tlds-alpha-by-domain.txt,IANA,The valid TLDs file (get if from ,)
5140,http://nlp.stanford.edu/data/glove.6B.100d.zip,Download," of 100 dimensions, trained on 6B tokens. ", and extract under 
5140,https://github.com/Noahs-ARK/semafor/tree/master/training/data,frame-elements file format, and in the , to 
5146,https://datasets.maluuba.com/NewsQA,NewsQA, to answer questions on ,.
5166,http://corpus-texmex.irisa.fr/,SIFT1M and GIST1M,"
","
"
5167,#datasets,Datasets,"
","
"
5167,#performance-on-taobaos-e-commerce-data,Performance on Taobao's E-commerce Data,"
","
"
5167,http://corpus-texmex.irisa.fr/,SIFT1M and GIST1M,"
","
"
5168,#data,Data,"
","
"
5168,#corpus,Corpus,"
","
"
5168,#kbp-dataset,Processing KBP Dataset,"
","
"
5189,#download-dataset-locally,Download Dataset Locally,"
","
"
5189,#create-your-own-dataset-files,Create Your Own Dataset Files,"
","
"
5189,https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/youtube8m#steps-to-run-the-youtube-8m-inference-graph-with-the-yt8m-dataset,MediaPipe inference demo,To run inference with your model in ,", you need to export your checkpoint to a SavedModel."
5194,http://spatialrelations.cs.uni-freiburg.de/#dataset,dataset,Download the ,"
"
5222,https://en.oxforddictionaries.com/explore/what-can-corpus-tell-us-about-language/,Oxford English Corpus,This repo is useful as a corpus for typing training programs. According to analysis of the ,", the 7,000 most common English lemmas account for approximately 90% of usage, so a 10,000 word training corpus is more than sufficient for practical training applications."
5237,https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations.csv,data/sensor_graph/graph_sensor_locations.csv,"Besides, the locations of sensors in Los Angeles, i.e., METR-LA, are available at ",", and the locations of sensors in PEMS-BAY are available at "
5237,https://github.com/liyaguang/DCRNN/blob/master/data/sensor_graph/graph_sensor_locations_bay.csv,data/sensor_graph/graph_sensor_locations_bay.csv,", and the locations of sensors in PEMS-BAY are available at ",.
5246,./parallel-corpus/,the Readme in this folder,"| directory | description | |---        |---          | | parallel-corpus | Main parallel corpus with a canonical split in  109108 training triples, 2000 validation triples and 2000 test triples. Each triple is annotated by metadata (repository owner, repository name, source file and line number). Also two versions of the above corpus reassembled into pairs: (declaration+body, docstring) and (declaration+docstring, body), for  code documentation tasks and code generation tasks, respectively. You may refer to "," for descriptions about escape tokens| | code-only-corpus | A code-only corpus of 161630 pairs of function declarations and function bodies, annotated with metadata. | | backtranslations-corpus | A corpus of docstrings automatically generated from the code-only corpus using Neural Machine Translation, to enable data augmentation by ""backtranslation"" | | nmt-outputs | Test and validation outputs of the baseline Neural Machine Translation models. | | repo_split.parallel-corpus | An alternate train/validation/test split of the parallel corpus which is ""repository-consistent"": no repository is split between training, validation or test sets. | | repo_split.code-only-corpus | A ""repository-consistent"" filtered version of the code-only corpus: it only contains fragments which appear in the training set of the above repository. | | scripts | Preprocessing scripts used to generate the corpora. | | V2 | code-docstring-corpus version 2, with class declarations, class methods, module docstrings and commit SHAs. |"
5264,http://pandas.pydata.org/,Pandas,"
", (
5293,https://github.com/vered1986/LexNET/tree/v2/datasets,LexNet,"To reproduce our experimental results, you need dictionaries ("," for English, "
5293,http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz500-w10-cb0-it3-min5.w2v,Russian Distributional Thesaurus," for English, "," for Russian). Since our implementation uses Python 3 and TensorFlow 0.12, please install them, too."
5295,http://curtis.ml.cmu.edu/datasets/quasar/,here,Both Quasar-S and Quasar-T are available for download ,. See the accompanying 
5306,https://github.com/datafl4sh/diskpp,DiSk++,The code for handling the meshes is a prelimary version of what became the , library by Matteo Cicuttin. The code for assembling the high order operators is based on code written by Danielle Di Pietro for similar HHO schemes.
5308,#data,Data,"
","
"
5308,data.txt,"
data.txt
",", see ",.
5320,data/images/,README, as described in the relevant , file.
5322,http://data.statmt.org/wmt17/translation-task/news.2016.tr.shuffled.gz,"
news.2016.shuffled
", where the target (TR) side samples are from monolingual Turkish data ,. The sentences are translated into EN with a single TR->EN NMT system (~14 BLEU on newstest2016):
5342,/data/,sample data, Data format follow the ,"
"
5359,https://github.com/cocodataset/cocoapi,pycocotools,"
","
"
5374,https://github.com/georgid/otmm_vocal_segments_dataset/tree/master/scripts,similar repository,"scripts:  python scripts for loading data, more scripts are in the ","
"
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/match.py,by this script,.  Then match ,"
"
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/fetch_midi.py,fetch_midi,get the matched MIDI from lakh-matched MIDI ,"  (if more than one match, pick the MIDI for the best match)"
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_note_annotations.py,this script,derive singing voice note annotations  by ," Since MIDI standard does not define an instrument for singing voice, the singing voice track is given a different program # in a random  channel in each MIDI. Thus one needs to manually identify the  MIDI channel # that corresponds to the melody of the singing voice track Optionally, doing in advance an annotation of segments with active vocal  is helpful."
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/derive_beat_annotations.py,this script,derive beat annotations by ,"
"
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/open_in_sv.sh,open_in_sv.sh, by script 'sh ,'
5374,https://github.com/georgid/lakh_vocal_segments_dataset/blob/master/scripts/shift_time_annotaion.py,shift time of annotation,"if systematic delay/advance of timestamps, measure the difference to onsets with SV's measure tool and run ","
"
5382,data,data,"You can either use the simulation reports and pre-processed Enron dataset files that we have produced, or you reproduce them yourself. You can download our data package from Zenodo (see the "," folder), or by running "
5385,https://github.com/codeneuro/neurofinder-datasets,"
neurofinder-datasets
","
", example scripts for loading the datasets
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.zip,"
neurofinder.00.00
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.zip,"
neurofinder.00.01
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.02.zip,"
neurofinder.00.02
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.03.zip,"
neurofinder.00.03
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.04.zip,"
neurofinder.00.04
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.05.zip,"
neurofinder.00.05
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.06.zip,"
neurofinder.00.06
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.07.zip,"
neurofinder.00.07
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.08.zip,"
neurofinder.00.08
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.09.zip,"
neurofinder.00.09
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.10.zip,"
neurofinder.00.10
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.11.zip,"
neurofinder.00.11
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.zip,"
neurofinder.01.00
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.zip,"
neurofinder.01.01
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.zip,"
neurofinder.02.00
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.zip,"
neurofinder.02.01
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.zip,"
neurofinder.03.00
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.00.zip,"
neurofinder.04.00
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.04.01.zip,"
neurofinder.04.01
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.00.test.zip,"
neurofinder.00.00.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.00.01.test.zip,"
neurofinder.00.01.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.00.test.zip,"
neurofinder.01.00.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.01.01.test.zip,"
neurofinder.01.01.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.00.test.zip,"
neurofinder.02.00.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.02.01.test.zip,"
neurofinder.02.01.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip,"
neurofinder.03.00.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip,"
neurofinder.04.00.test
","
","
"
5385,https://s3.amazonaws.com/neuro.datasets/challenges/neurofinder/neurofinder.03.00.test.zip,"
neurofinder.04.01.test
","
","
"
5399,https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh,Yoon Kim's script,. For ease of use you can use the ,", which downloads these data and saves them into the relevant folders."
5404,http://cs.ucsb.edu/~xwhan/datasets/NELL-995.zip,NELL-995,Download the knowledge graph dataset ,"
"
5411,https://data.4tu.nl/repository/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54,BPIC 2011,"Together with the code, we make available 22 datasets that were used in the evaluation section in the paper (2 datasets used in the paper are private). These datasets correspond to different prediction tasks, formulated on 8 publicly available event logs (namely, the ",", "
5411,https://data.4tu.nl/repository/uuid:3926db30-f712-4394-aebc-75976070e91f,BPIC 2012,", ",", "
5411,http://data.4tu.nl/repository/uuid:31a308ef-c844-48da-948c-305d167a0ec1,BPIC 2015,", ",", "
5411,http://data.4tu.nl/repository/uuid:5f3067df-f10b-45da-b98b-86ae4c7a310b,BPIC 2017,", ",", "
5411,https://data.4tu.nl/repository/uuid:915d2bfb-7e84-49ad-a286-dc35f063a460,Sepsis Cases,", ",", "
5411,https://data.4tu.nl/repository/uuid:76c46b83-c930-4798-a1c9-4be94dfeb741,Hospital Billing,", ",", "
5411,https://data.4tu.nl/repository/uuid:270fd440-1057-4fb9-89a9-b699b47990f5,Road Traffic Fine Management,", ",", "
5411,https://data.4tu.nl/repository/uuid:68726926-5ac5-4fab-b873-ee76ea412399,Production log,", ", event logs). These (labeled and preprocessed) benchmark datasets can be found at https://drive.google.com/open?id=154hcH-HGThlcZJW5zBvCJMZvjOQDsnPR.
5435,http://www.bigdatalab.ac.cn/~gjf/,Homepage,"
","
"
5435,http://www.bigdatalab.ac.cn/~lanyanyan/,Homepage,"
","
"
5435,http://www.bigdatalab.ac.cn/~cxq/,Homepage,"
","
"
5457,http://cs.stanford.edu/~danqi/data/cnn.tar.gz,http://cs.stanford.edu/~danqi/data/cnn.tar.gz,"
"," already preprocessed, and the original one from "
5457,https://github.com/deepmind/rc-data,https://github.com/deepmind/rc-data," already preprocessed, and the original one from ", for the CNN news article
5457,http://nlp.stanford.edu/data/glove.6B.zip,http://nlp.stanford.edu/data/glove.6B.zip,"The dataset must be downloaded and save it in the data folder (on the right data folder). Moreover, you have to download in the data folder the glove embeddings (",)
5459,https://github.com/harvardnlp/boxscore-data,boxscore-data repo,The boxscore-data associated with the above paper can be downloaded from the ,", and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar."
5459,https://github.com/ratishsp/data2text-plan-py,data2text-plan-py," For an improved implementation of the extractive evaluation metrics (and improved models), please see the ", repo associated with the Puduppully et al. (AAAI 2019) 
5459,https://github.com/harvardnlp/boxscore-data,boxscore-data repo, models and results reflecting the newly cleaned up data in the , are now given below.
5464,https://bitbucket.org/nfitzgerald/genx-referring-expression-corpus,GenX corpus,Data is from the , produced by 
5470,./data/rank_tid2013,folder,. The source code can be found in this ,.
5484,https://www.robots.ox.ac.uk/~vgg/data/text/#sec-synth,here,"Following our text recognition experiments might be a little difficult, because we can not offer the entire dataset used by us. But it is possible to perform the experiments based on the Synth-90k dataset provided by Jaderberg et al. ",. After downloading and extracting this file you'll need to adapt the groundtruth file provided with this dataset to fit to the format used by our code. Our format is quite easy. You need to create a 
5493,http://data.statmt.org/romang/gec-emnlp16/models.tgz,our baseline models,You can download and run ," (1,3G)."
5493,http://data.statmt.org/romang/gec-emnlp16/wikilm.tgz,Wikipedia language model,"
", (22G)
5493,http://data.statmt.org/romang/gec-emnlp16/cclm.tgz,Common Crawl language model,"
", (26G)
5495,https://www.hankcs.com/nlp/corpus/introduction-to-chinese-abstract-meaning-representation.html,CAMR, | , | | 
5495,https://hanlp.hankcs.com/docs/data_format.html,《格式规范》,及,。我们购买、标注或采用了世界上量级最大、种类最多的语料库用于联合多语种多任务学习，所以HanLP的标注集也是覆盖面最广的。
5496,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google NGrams,Create a dictionary with inverse document frequency (idf) values from the , dataset.
5496,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google NGrams, files for your language from the , site into a common folder.
5503,data/VerbsWithAttributes,Our annotations are available in this folder.,"
", See the readme there for more information.
5507,#datasets,Datasets,"
","
"
5507,https://global.oup.com/academic/product/data-science-and-complex-networks-9780199639601,Data Science and Complex Networks: Real Case Studies with Python,"
","
"
5507,https://www.packtpub.com/big-data-and-business-intelligence/gephi-cookbook,Gephi Cookbook,"
","
"
5507,https://www.packtpub.com/big-data-and-business-intelligence/network-graph-analysis-and-visualization-gephi,Network Graph Analysis and Visualization with Gephi,"
","
"
5507,http://math.bu.edu/people/kolaczyk/datasets.html,Eric D. Kolaczyk’s Network Datasets,"
",.
5507,https://CRAN.R-project.org/package=igraphdata,igraphdata,"
", - R data-centric package.
5507,http://eh.net/database/international-currencies-1890-1910/,International Currencies 1890-1910,"
", - Historical data on the international connections between 45 currencies.
5507,http://moreno.ss.uci.edu/data.html,Linton Freeman’s Network Data,"
"," - Over 300 datasets of all sorts, in UCINET format."
5507,https://comunelab.fbk.eu/data.php,Manlio De Domenico’s Complex Multilayer Networks,"
",.
5507,http://www-personal.umich.edu/~mejn/netdata/,Mark E.J. Newman’s Network Data,"
", (
5507,http://networksciencebook.com/translations/en/resources/data.html,Network Science Book - Network Datasets,"
", - Network data sets from Albert-László Barabási’s 
5507,http://www.boardsandgender.com/data.php,"Norwegian Interlocking Directorate, 2002-2011","
", - Two-mode and one-mode data on gender representation in Norwegian firms.
5507,http://vlado.fmf.uni-lj.si/pub/networks/data/,Pajek Datasets,"
",.
5507,http://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm,Siena Datasets,"
",.
5507,http://www.sociopatterns.org/datasets/,SocioPatterns Datasets,"
", - Network data obtained through the 
5507,http://snap.stanford.edu/data/index.html,Stanford Large Network Dataset Collection,"
",.
5507,https://toreopsahl.com/datasets/,tnet Datasets,"
", - Weighted network data.
5507,http://networkdata.ics.uci.edu/,UCI Network Data Repository,"
",.
5507,https://sites.google.com/site/ucinetsoftware/datasets,UCINET Datasets,"
", - Network data in UCINET format.
5507,http://datasciencegroup.pl/,Data Science Group,"
"," -  Wroclaw-based research group that studies, among many things, complex networks and other network-related topics."
5507,http://www.studienverlag.at/data.cfm?vpath=openaccess/oezg-12012-lemercier&download=yes,Formale Methoden der Netzwerkanalyse in den Geschichtswissenschaften: Warum und Wie? [Formal Network Methods in History: Why and How?],"
",", in German ("
5507,http://www.pfeffer.at/data/visposter/,poster version,", 2009; ",).
5507,http://eh.net/database/international-currencies-1890-1910/,data,", 2009), both by Marc Flandreau and Clemens Jobst - Network analysis of the foreign exchange system in the late 19th century (",).
5507,http://nos.alwaysdata.net/,Node Overlap and Segregation Software,"
", - Web-based tool to compute 
5507,https://github.com/schochastics/networkdata,networkdata,"
", - Includes 979 network datasets containing 2135 networks.
5507,https://www.data-imaginist.com/2017/introducing-tidygraph/,Introducing tidygraph,"
","
"
5507,https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/,Using Metadata to Find Paul Revere,"
", and 
5507,http://bactra.org/notebooks/network-data-analysis.html,Analysis of Network Data,"
",.
5507,http://bokeh.pydata.org/en/latest/docs/gallery/les_mis.html,"Character Co-Occurrences in Victor Hugo’s Les Misérables
","
",", plotted as an adjacency matrix, written in Python (+ Javascript)."
5507,https://solomonmessing.wordpress.com/2012/09/30/working-with-bipartiteaffiliation-network-data-in-r/,Working with Bipartite/Affiliation Network Data in R,"
", (2012).
5530,http://www.cs.ucr.edu/~eamonn/time_series_data/,UCR Time Series Repository,) and sample dataset from the , (
5535,http://segchd.csail.mit.edu/data.html,HVSMR,Download , dataset (phase 2) and put them in folder 
5538,http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html,iLIDS-VID,Download and extract datasets ,", "
5539,https://github.com/rbgirshick/fast-rcnn/blob/master/data/scripts/fetch_fast_rcnn_models.sh,selective search windows, and Ross Girshick's , by manually downloading the 
5544,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html,documentation repository,"Product documentation including an architecture overview, platform support, installation and usage guides can be found in the ",.
5544,https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html,NVIDIA driver,Make sure you have installed the , and Docker engine for your Linux distribution
5544,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker,installation guide,"For instructions on getting started with the NVIDIA Container Toolkit, refer to the ",.
5544,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html,user guide,The , provides information on the configuration and command line options available when running GPU containers with Docker.
5559,#custom-model-and-dataset,templates,". To implement custom models and datasets, check out our ",". To help users better understand and adapt our codebase, we provide an "
5559,docs/datasets.md,Datasets,"
","
"
5559,data/template_dataset.py,template,"If you plan to implement custom models and dataset for your new applications, we provide a dataset ", and a model 
5564,https://wiki.haskell.org/Algebraic_data_type,Haskell algebraic data types,The input specification format mimics that of , where in addition each type constructor may be annotated with an additional 
5581,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html, file is a JSON file with format mentioned in ,.
5581,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object.html,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object.html, and have JSON formated mentioned in ,"
"
5585,https://pan.baidu.com/s/1o7RME86#list/path=%2FVQA2.0%20data,Baidu Pan,Our training question and answer data for VQA2.0: ,.
5585,https://pan.baidu.com/s/1o7RME86#list/path=%2FVQA2.0%20data%2Fmodel,Here, respectively. , is the model on VQA-2.0.
5587,https://dl.fbaipublicfiles.com/stardata/original_replays.tar.gz,Link to the original replays,"
","
"
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/0.tar.gz,00,"
",. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/1.tar.gz,01,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/2.tar.gz,02,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/3.tar.gz,03,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/4.tar.gz,04,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/5.tar.gz,05,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/6.tar.gz,06,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/7.tar.gz,07,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/8.tar.gz,08,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/9.tar.gz,09,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/10.tar.gz,10,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/1.tar.gz,11,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/12.tar.gz,12,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/13.tar.gz,13,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/14.tar.gz,14,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/15.tar.gz,15,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/16.tar.gz,16,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/17.tar.gz,17,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/18.tar.gz,18,. ,. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/19.tar.gz,19,. ,.
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/train.list,train,Standardized ,", "
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/valid.list,valid,", ",", and "
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/test.list,test,", and ", sets are also available. 
5587,https://dl.fbaipublicfiles.com/stardata/dumped_replays/all.list,Here, sets are also available. , is a list of all the files.
5592,https://github.com/CornellNLP/ConvoKit#datasets,conversational datasets, inspired by (and compatible with) scikit-learn.  Several large , are included together with scripts exemplifying the use of the toolkit on these datasets. The latest version is 
5592,http://zissou.infosci.cornell.edu/convokit/datasets/,here,.  Alternatively you can access them directly ,.
5592,https://zissou.infosci.cornell.edu/convokit/datasets/wikiconv-corpus/blocks.json,block data retrieved directly from the Wikipedia block log,". Note that due to the large size of the data, it is split up by year. We separately provide ",", for reproducing the "
5592,https://convokit.cornell.edu/documentation/wiki-articles-for-deletion-corpus.html,Wikipedia Articles for Deletion Corpus,"
","
"
5592,https://convokit.cornell.edu/documentation/casino-corpus.html,CaSiNo Corpus,"
","
"
5592,https://github.com/CornellNLP/ConvoKit/blob/master/examples/converting_movie_corpus.ipynb,This example script, object. , shows how to construct a Corpus from custom data.
5596,http://128.2.220.95/multilingual/data,Crosslingual word embeddings and corpora,"
","
"
5619,http://pandas.pydata.org/,pandas,"
","
"
5620,https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/,"Very fast Data cleaning of product names, company names & street names","
","
"
5620,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google Books Ngram data,"
","
"
5620,https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries,Frequency dictionaries,"
","
"
5620,https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data,Frequency dictionaries,"
","
"
5624,#input-data-format,Input Data Format,"
","
"
5624,#grab-a-dataset,Grab a Dataset,"
","
"
5624,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,"For this example, we'll use the ""a1a"" dataset, acquired from ",. Currently the Photon ML dataset converter supports only the LibSVM format.
5628,http://ltdata1.informatik.uni-hamburg.de/sensegram/,"pre-trained models for English, German, and Russian",You can downlooad ,. Note that to run examples from the QuickStart you only need files with extensions 
5633,#data,data tuple,": integer to determine k in k-fold validation, defaults to 10. Must be an integer (or left default) for k-fold validation experiment along with ","
"
5643,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC MAV Dataset,Download ,". Although it contains stereo cameras, we only use one camera. The system also works with "
5643,http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/,ETH-asl cla dataset,". Although it contains stereo cameras, we only use one camera. The system also works with ",. We take EuRoC as the example.
5659,grouplens.org/datasets/movielens/latest,MovieLens,"We use the same input format as the LibFM toolkit (http://www.libfm.org/). In this instruction, we use ",". The MovieLens data has been used for personalized tag recommendation, which contains 668,953 tag applications of users on movies. We convert each tag application (user ID, movie ID and tag) to a feature vector using one-hot encoding and obtain 90,445 binary features. The following examples are based on this dataset and it will be referred as "
5665,./data,"
data/
","
", contains Traditional Chinese evaluation datasets. 
5679,https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes,this link,", followed by their descriptions from ",:
5686,http://grupolys.org/software/UUUSA/sisa-data.zip,here,Data/Resources used for our SISA (Syntactic Iberian Polarity classification) model can be found ,"
"
5688,doc/tutorials/datasetmapper.md,DatasetMapper,"
","
"
5693,https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0,here,The sample data and processing results of detected corners can be downloaded from , (181M) for panoramic image and 
5693,https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0,here, (181M) for panoramic image and , (29M) for perspective image. 
5693,https://www.dropbox.com/s/m0ogerftqav0fyx/ILCC_sample_data_and_result.zip?dl=0,link,6     |       7.5       |      1.2 ~ 2.6      | , | 
5693,https://www.dropbox.com/s/et0o4k2sp485nz1/ILCC_sample_perspective_data.zip?dl=0,link,6     |       7.5       |      1.2 ~ 2.6      | , | 
5701,#data,Data,"
","
"
5701,#twitter-dataset,Twitter Dataset,"
","
"
5714,http://lxcenter.di.fc.ul.pt/datasets/models/vanilla.tar.gz,Vanilla model,"
","
"
5714,http://lxcenter.di.fc.ul.pt/datasets/models/,All models,"
","
"
5714,http://lxcenter.di.fc.ul.pt/datasets/models/2.2b/,LX-DSemVectors 2.2b,"
","
"
5717,http://www.cs.cornell.edu/people/pabo/movie-review-data/,Cornell Movie Review,"
", -- 
5723,https://www.kaggle.com/c/data-science-bowl-2017/data,kaggle challenge,. Here are some randomly picked denoised results on low dose CTs from this ,. 
5734,http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE,", ",", "
5753,https://github.com/shiralkarprashant/knowledgestream/blob/master/datastructures/test_graph.py,KG generation script," which contains datastructures in binary format as required by the code. If you are interested in applying methods in this repository on your own knowledge graph, you may use the following script to generate the required graph files (.npy): ","
"
5772,https://github.com/bing-jian/gmmreg/tree/master/data,this folder,". For examples of config INI file, please check ",.
5772,http://graphics.stanford.edu/data/3Dscanrep/,Stanford dragon_stand dataset, using the ,.
5772,http://qianyi.info/scenedata.html,Stanford lounge dataset, can be used to register depth frames in the ,. Please see section below for results.
5772,http://qianyi.info/scenedata.html,Stanford lounge dataset,Results of running GMMReg-Rigid on ,"
"
5780,https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools,pycocotools,"
","
"
5808,http://crcv.ucf.edu/data/UCF101.php,UCF-101,We experimented on three mainstream action recognition datasets: ,", "
5808,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB51,", ", and 
5821,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe: Global Vectors for Word Representation,"
","
"
5840,http://research.mangaki.fr/2017/07/18/mangaki-data-challenge-en/,Data Challenge,"
", (now finished)
5840,http://research.mangaki.fr/2017/07/18/mangaki-data-challenge-en/,"

","
","
"
5840,https://research.mangaki.fr/2017/10/08/mangaki-data-challenge-winners-en/,See the results.,We organized a data challenge about recommender systems with Kyoto University. ,"
"
5850,https://data.mendeley.com/datasets/hx3rzw5dwt/draft?a=377d5571-af17-4e61-bf77-1b77b88316de,Here,Please download the data from ,.
5851,/toy_data/ALICE_l2.py,"
ALICE_l2.py
",(a) Explicit cycle-consistency (,)
5851,/toy_data/ALICE_A.py,"
ALICE_A.py
",(b) Implicit cycle-consistency (,)
5851,/toy_data/ALICE_l2_l2.py,"
ALICE_l2_l2.py
",(c) Explicit mapping  (,)
5851,/toy_data/ALICE_A_A.py,"
ALICE_A_A.py
",(d) Implicit mapping  (,)
5855,https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/sketch-dataset,exp-src/data/sketch-dataset,"For instructions to get annotated sketch dataset, navigate to ",. Pose dataset is present in 
5855,https://github.com/val-iisc/sketch-parse/blob/master/exp-src/data/lists/Pose_all_label.txt,"
exp-src/data/lists/Pose_all_label.txt
",. Pose dataset is present in ,. Find instruction regarding pose dataset 
5855,https://github.com/val-iisc/sketch-parse/tree/master/exp-src/data/lists,here,. Find instruction regarding pose dataset ,.
5856,http://lixirong.net/data/mm2015/rucmmc_irc2015_data.tar.gz,dataset,Download , without image visual feature.
5856,http://lixirong.net/data/mm2015/rucmmc_irc2015_required_feature.tar.gz,required (5.0GB),Download image visual feature [ , | 
5856,http://lixirong.net/data/mm2015/rucmmc_irc2015_optional_feature.tar.gz,optional (7.9GB), | , ].
5879,https://anndata.readthedocs.io/en/latest/,AnnData, and ,.
5881,http://staffhome.ecm.uwa.edu.au/~00061811/parallel_benchdata.tar.gz,(705MB),. The raw benchmark data ,", used in this paper is available online, including the comparison with PolSAT "
5881,http://staffhome.ecm.uwa.edu.au/~00061811/polsat_benchdata.tar.gz,(26MB),", used in this paper is available online, including the comparison with PolSAT ",. There is some 
5881,https://github.com/gmatht/leviathan/blob/master/samples/benchmark_data_DOC.txt,Documentation,. There is some , for the benchmark data.
5893,https://github.com/openvenues/libpostal/tree/master/scripts/geodata,geodata,"Libpostal is a bit different because it's trained on open data that's available to everyone, so we've released the entire training pipeline (the "," package in this repo), as well as the resulting training data itself on the Internet Archive. It's over 100GB unzipped."
5893,https://github.com/openvenues/libpostal/blob/master/scripts/geodata/addresses/components.py,normalizations," to construct formatted, tagged traning examples for every inhabited country in the world. Many types of ", are performed to make the training data resemble real messy geocoder input as closely as possible.
5893,https://github.com/openvenues/libpostal/tree/master/scripts/geodata,geodata,The ," Python package in the libpostal repo contains the pipeline for preprocessing the various geo data sets and building training data for the C models to use. This package shouldn't be needed for most users, but for those interested in generating new types of addresses or improving libpostal's training data, this is where to look."
5898,https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0,Dropbox,We provide demo attack data and classifier on , and 
5899,/data,data,Download and place data in the ," directory, then uncompress them. First run "
5899,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3UWZT,Dataverse,The data is hosted on , and 
5916,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI odometry benchmark,To use the ," to train DPC-Net, you can use the scripts "
5938,http://nlp.stanford.edu/data/glove.840B.300d.zip,300-d Glove embedding,Download , and save it as data/glove.840B.300d.txt
5939,/data/input,data/input,The dataset can be downloaded at ,.
5945,#data,Data,"
","
"
5947,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
5950,http://malicia-project.com/dataset.html,Malicia, or ,.
5954,https://www.kaggle.com/c/multinli-matched-open-evaluation/data,matched,"Then, manually download download MultiNLI 0.9 ", and 
5954,https://www.kaggle.com/c/multinli-mismatched-open-evaluation/data,mismatched, and , test set under data/multinli_0.9 folder
5956,http://marsyasweb.appspot.com/download/data_sets/,GTZan,"
",": (30s, 10 genres, 1,000 mp3)"
5956,http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset,MagnaTagATune,"
",": (29s, 188 tags, 25,880 mp3) for tagging and triplet similarity"
5956,http://www.mathieuramona.com/wp/data/jamendo/,Jamendo,"
",: 61/16/24 songs for vocal activity detection
5956,https://www.audiocontentanalysis.org/data-sets/,MIR datasets,"
",: An awesome list of MIR datasets
5965,#label-data,Label data,) , with the 
5967,docs/data_collection.md,Collect measurements from RIPE Atlas,"
","
"
5967,docs/auxiliary_data.md,Collect auxiliary data,"
","
"
5973,http://mscoco.org/dataset/#download,MS COCO,"Alternatively, you can extract image features yourself, you should download images from "," dataset first. Please make sure that we can find the image on ./datasets/ms_coco/images/ (should have at least train2014 and val2014 folder). After that, type:"
5988,http://exploredata.net,Maximal Information Coefficient (MIC),"Our best model has an accuracy greater than 99.0% with mean squared error of ~6.00 watts. For correlation analysis, we applied a metric called ", that has the equitability and generality properties.
6020,http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html,AT&T faces,Scripts are in the 'face' folder. Need databases , and 
6021,data_organization.md,data organization,"The dataset consists of several types of annotations: color and depth images, camera poses, textured 3D meshes, building floor plans and region annotations, object instance semantic annotations.  For details see the ", document.
6028,http://rpg.ifi.uzh.ch/davis_data.html,The Event Camera Dataset and Simulator,", such as those of ",. 
6028,http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.bag,slider_depth.bag,"Download a squence of the dataset, such as ","
"
6029,https://github.com/remega/LEDOV-eye-tracking-database,LEDOV,"As introduced in our paper, our model is trained by our newly-established eye-tracking database,  ",", which is also available at "
6046,https://travis-ci.org/Wikidata-lib/PropertySuggester,"
Build Status
","
","
"
6046,https://coveralls.io/r/Wikidata-lib/PropertySuggester?branch=master,"
Coverage Status
","
","
"
6046,https://github.com/Wikidata-lib/PropertySuggester-Python,PropertySuggester-Python,"This extension adds a new table ""wbs_propertypairs"" that contains the information that is needed to generate suggestions. You can use ", to generate this data from a wikidata dump.
6065,https://sites.google.com/site/iitaffdataset/,IIT-AFF dataset,). This weight is trained on the training set of the ,:
6065,https://sites.google.com/site/iitaffdataset/,IIT-AFF dataset,We train AffordanceNet on ,"
"
6065,https://sites.google.com/site/iitaffdataset/,IIT-AFF dataset,If you use ,", please consider citing:"
6068,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-2,"word level language models over the Penn Treebank (PTB), "," (WT2), and "
6068,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-103," (WT2), and ", (WT103) datasets
6073,http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU Depth V2,Download the preprocessed , and/or 
6073,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI, and/or , datasets in HDF5 formats and place them under the 
6075,https://github.com/tesseradata/install-vagrant,Vagrant,Probably the easiest way to build RHIPE is to provision a ," machine that has all the prerequisites configured.  Another option is to set up a local pseudo-distributed Hadoop cluster, for example see "
6079,https://www.caida.org/data/internet-topology-data-kit/,Macroscopic Internet Topology Data Kit,The , from Caida including measured real world internet topologies
6091,https://github.com/tesseract-ocr/tessdata,tessdata repository,"If you want to find a language data set to run Tesseract, then look at our ", instead.
6113,https://github.com/datastax/spark-cassandra-connector/actions?query=branch%3Amaster,"
CI
","
","
"
6113,https://community.datastax.com/index.html,Datastax and Cassandra Q&A,| What       | Where                                                                                                                                                                                                                                                                                                                                 | | ---------- |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Community  | Chat with us at ,                                                                                                                                                                                                                                               | | Scala Docs | Most Recent Release (3.3.0): 
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector,                                                                                                                                                                                                                                               | | Scala Docs | Most Recent Release (3.3.0): ,", "
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/driver/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector-Driver,", ", | | Latest Production Release | 
6113,https://search.maven.org/artifact/com.datastax.spark/spark-cassandra-connector_2.12/3.3.0/jar,3.3.0, | | Latest Production Release | ,                                                                                                                                                                                                                                |
6113,https://github.com/datastax/spark-cassandra-connector/tree/master,master,"Currently, the following branches are actively supported: 3.3.x (","), 3.2.x ("
6113,https://github.com/datastax/spark-cassandra-connector/tree/b3.2,b3.2,"), 3.2.x (","), 3.1.x ("
6113,https://github.com/datastax/spark-cassandra-connector/tree/b3.1,b3.1,"), 3.1.x (","), 3.0.x ("
6113,https://github.com/datastax/spark-cassandra-connector/tree/b3.0,b3.0,"), 3.0.x (",) and 2.5.x (
6113,https://github.com/datastax/spark-cassandra-connector/tree/b2.5,b2.5,) and 2.5.x (,).
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.3.0/connector/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector,"
","
"
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.2.0/connector/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector,"
","
"
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.1.0/connector/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector,"
","
"
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/3.0.1/connector/com/datastax/spark/connector/index.html,Spark-Cassandra-Connector,"
","
"
6113,https://datastax.github.io/spark-cassandra-connector/ApiDocs/2.5.2/connector/#package,Spark-Cassandra-Connector,"
","
"
6113,http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector/,Spark-Cassandra-Connector,"
","
"
6113,http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.4.2/spark-cassandra-connector-embedded/,Embedded-Cassandra,"
","
"
6113,doc/14_data_frames.md,DataFrames,"
","
"
6113,https://academy.datastax.com/courses/ds320-analytics-with-apache-spark,DS320: Analytics with Spark,DataStax Academy provides free online training for Apache Cassandra and DataStax Enterprise. In ,", you will learn how to effectively and efficiently solve analytical problems with Apache Spark, Apache Cassandra, and DataStax Enterprise. You will learn about Spark API, Spark-Cassandra Connector, Spark SQL, Spark Streaming, and crucial performance optimization techniques."
6113,https://datastax-oss.atlassian.net/browse/SPARKC/,JIRA,New issues may be reported using ,". Please include all relevant details including versions of Spark, Spark Cassandra Connector, Cassandra and/or DSE. A minimal reproducible case with sample code is ideal."
6113,https://groups.google.com/a/lists.datastax.com/forum/#!forum/spark-connector-user,user mailing list,Questions and requests for help may be submitted to the ,.
6113,https://community.datastax.com/index.html,DataStax Community,The , provides a free question and answer website for any and all questions relating to any DataStax Related technology. Including the Spark Cassandra Connector. Both DataStax engineers and community members frequent this board and answer questions.
6113,https://cla.datastax.com/,DataStax Spark Cassandra Connector Contribution License Agreement,"To protect the community, all contributors are required to sign the ",. The process is completely electronic and should only take a few minutes.
6113,https://datastax-oss.atlassian.net/projects/SPARKC/issues,SPARKC JIRA,Create a ,"
"
6122,https://github.com/rasvaan/accurator/wiki/2.-Collection-data,Collection data,"
","
"
6123,https://github.com/VUAmsterdam-UniversityLibrary/ubvu_bibles/wiki/2.-Collection-data,Collection data,"
","
"
6129,https://github.com/osplanning-data-standards/GTFS-PLUS,GTFS-PLUS,"
", -  GTFS-based data transit network data standard suitable for dynamic transit modeling
6129,https://github.com/zephyr-data-specs/GMNS,General Modeling Network Specification,"
", - GMNS defines a common human and machine readable format for sharing routable road network files. It is designed to be used in multi-modal static and dynamic transportation planning and operations models.
6143,https://papers.nips.cc/paper/6629-structured-embedding-models-for-grouped-data,"M. Rudolph, F. Ruiz, S. Athey, D. Blei, Structured Embedding Models for Grouped Data, Neural Information Processing Systems, 2017","
","
"
6149,https://www.dropbox.com/s/yq1gpygw3oq1grq/eq2_grammar_dataset.h5?dl=0,grammar,The equation dataset can be downloaded here: ,", "
6149,https://www.dropbox.com/s/gn3iq2ykrs0dqwb/eq2_str_dataset.h5?dl=0,string,", ","
"
6163,#data,Data,"
","
"
6164,#data,Data,"
","
"
6165,https://codecov.io/gh/Wikidata/Wikidata-Toolkit,"
Coverage status
","
","
"
6165,http://search.maven.org/#search|ga|1|g%3A%22org.wikidata.wdtk%22,"
Maven Central
","
","
"
6165,https://www.openhub.net/p/Wikidata-Toolkit,"
Project Stats
","
","
"
6165,https://www.mediawiki.org/wiki/Wikidata_Toolkit,Wikidata Toolkit homepage,"
",": project homepage with basic user documentation, including guidelines on how to setup your Java IDE for using Maven and git."
6165,https://github.com/Wikidata/Wikidata-Toolkit-Examples,Wikidata Toolkit examples,"
",: stand-alone Java project that shows how to use Wikidata Toolkit as a library for your own code.
6165,http://wikidata.github.io/Wikidata-Toolkit/,Wikidata Toolkit Javadocs,"
",: API documentation
6165,https://github.com/Wikidata/Wikidata-Toolkit/graphs/contributors,other contributors, and ,"
"
6165,https://meta.wikimedia.org/wiki/Grants:IEG/Wikidata_Toolkit,Wikibase Toolkit Individual Engagement Grant,The development of Wikidata Toolkit has been partially funded by the Wikimedia Foundation under the ,", and by the German Research Foundation (DFG) under "
6185,https://github.com/FNNDSC/ChRIS_ultron_backEnd/wiki/ChRIS-backend-database-design,here,Available ,.
6186,https://github.com/zenvisage/zenvisage/wiki/Instructions-for-uploading-new-datasets,here,"For uploading your datasets, you can follow the instructions explained ",.
6211,data/keywords.txt,keywords.txt,The file , lists the 67 keywords used in the task. The semantic labelling of utterances using these keywords are given in two files.
6211,data/semantic_flickraudio_labels.csv,semantic_flickraudio_labels.csv,The CSV file , gives hard annotations for keywords that are relevant for specific utterances. The CSV file 
6211,data/semantic_flickraudio_counts.csv,semantic_flickraudio_counts.csv, gives hard annotations for keywords that are relevant for specific utterances. The CSV file ," gives the actual annotator counts. In this file, "
6212,http://cocodataset.org/#download,MSCOCO,"
","
"
6230,https://www.webmproject.org/docs/container/#cueing-data,cueing data,Can parse , elements for DASH's SegmentBase@indexRange and SegmentTemplate@index
6232,https://ram-lab.com/file/tai_icra_2018_dataset.zip,pedestrian navigation dataset,The collected , contains:
6242,../../wiki/Metadata,Metadata Handling,"
","
"
6247,neon/data/convert_manifest.py,this,"Between neon v2.1.0 and v2.2.0, the aeon manifest file format has been changed. When updating from neon < v2.2.0 manifests have to be recreated using ingest scripts (in examples folder) or updated using ", script.
6264,http://www.nltk.org/data.html,the data installed,", with ", so that WordNet is available.
6267,http://mscoco.org/dataset/#download,MS-COCO download page,"For the details of the annotation format, please see ",.
6271,http://nlp.stanford.edu/data/OpenSubData.tar,link,#Download Data Processed traning datasets can be downloaded at , (unpacks to 8.9GB). All tokens have been transformed to indexes (dictionary file found at data/movie_2500)
6274,http://ftp.tugraz.at/pub/feichtenhofer/detect-track/data/proposals/RPN_proposals_DET.zip,[FTP server],ImageNet DET: ,"
"
6274,http://ftp.tugraz.at/pub/feichtenhofer/detect-track/data/proposals/RPN_proposals_VID_train.zip,[FTP server],ImageNet VID_train: ,"
"
6274,http://ftp.tugraz.at/pub/feichtenhofer/detect-track/data/proposals/RPN_proposals_VID_val.zip,[FTP server],ImageNet VID_val: ,"
"
6283,https://github.com/facebookresearch/MUSE/blob/master/data/get_evaluation.sh#L99-L100,here,"Note: Requires bash 4. The download of Europarl is disabled by default (slow), you can enable it ",.
6287,http://ntcir-lifelog.computing.dcu.ie/data/NTCIR_Lifelog_formal_run_Dataset.zip,here,The NTCIR-12 dataset is available ,.
6287,https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/annotations.txt,annotations,Our , and 
6287,https://github.com/gorayni/egocentric_photostreams/blob/master/datasets/ntcir/categories.txt,categories, and , are available in the 
6297,http://dev.w3.org/html5/webdatabase/,Database Storage via SQLite,HTML5 ,"
"
6309,#data-structures,Data Structures,"
","
"
6309,#data-manipulation-files,Data Manipulation Files,"
","
"
6315,https://github.com/haozhenWu/lightchem/tree/master/datasets,Datasets,"
","
"
6315,http://pandas.pydata.org/,pandas,"
", version = 0.18.1
6320,http://www.bigdatalab.ac.cn/~pangliang/,HomePage,"
","
"
6320,http://www.bigdatalab.ac.cn/~lanyanyan/,HomePage,"
","
"
6320,http://www.bigdatalab.ac.cn/~gjf/,HomePage,"
","
"
6320,http://www.bigdatalab.ac.cn/~junxu/,HomePage,"
","
"
6320,http://www.bigdatalab.ac.cn/~cxq/,HomePage,"
","
"
6322,https://github.com/vs-uulm/vnc2017-CACC-data,here,"To save some time, we've also published the data set generated by this code in a separate (very large!) repository, which can be found ",.
6325,http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip,http://ml.cs.tsinghua.edu.cn/~yinpeng/adversarial/dataset.zip,"We use a subset of ImageNet validation set containing 1000 images, most of which are correctly classified by those models. Our dataset can be downloaded at ",. You can alternatively use the NIPS 2017 competition official 
6325,https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset,dataset,. You can alternatively use the NIPS 2017 competition official ,.
6336,https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools,pycocotools,"
","
"
6337,https://preferredjp.box.com/v/pfn-pic-dataset-main,Download (dataset-main.zip),"
","
"
6338,http://www.mohamedaly.info/datasets/caltech-lanes,Caltech Lanes Dataset,Download ,.
6346,http://groups.csail.mit.edu/vision/datasets/ADE20K/,ADE20k,Download the , dataset and put it in 
6347,https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html,Speech Commands Dataset,"Honk is a PyTorch reimplementation of Google's TensorFlow convolutional neural networks for keyword spotting, which accompanies the recent release of their ",". For more details, please consult our writeup:"
6347,http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz,Speech Commands Dataset, trains or evaluates the model. It expects all training examples to follow the same format as that of ,". The recommended workflow is to download the dataset and add custom keywords, since the dataset already contains many useful audio samples and background noise."
6348,https://github.com/mmalekzadeh/replacement-autoencoder/blob/master/skoda/RAE_on_Skoda_dataset.ipynb,RAE_on_Skoda_dataset.ipynb,"Codes and files are available under ""skoda"" folder: ","
"
6350,https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split,MSD_split,", ",)
6356,https://www.wikidata.org,Wikidata,"The aim of the SLING project is to learn to read and understand Wikipedia articles in many languages for the purpose of knowledge base completion, e.g. adding facts mentioned in Wikipedia (and other sources) to the ", knowledge base. We use 
6356,doc/guide/wikiflow.md#wikidata-import,convert, can take a raw dump of Wikidata and , this into one big frame graph. This can be loaded into memory so we can do fast graph traversal for inference and reasoning over the knowledge base. The Wiki flow pipeline can also take raw Wikipedia dumps and 
6359,data-types.md,Data types,"
","
"
6362,http://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration,"
Challenging data sets for point cloud registration algorithms
","
","
"
6365,http://www.cvlibs.net/datasets/kitti/,kitti dataset,"In this example we create a cblox map using the lidar data and ""ground-truth"" pose estimates from the ",. This simple example demonstrates the 
6365,http://www.cvlibs.net/datasets/kitti/raw_data.php,kitti raw dataset,To run the example download a ,". To produce the map above, we ran the ""2011_09_30_drive_0018"" dataset under the catagory ""residential"". Convert the data to a rosbag using "
6366,http://datasets.maluuba.com/FigureQA,Project Page, [,].
6366,http://datasets.maluuba.com/FigureQA/dl,Download,"
", the FigureQA data set 
6367,https://www.microsoft.com/en-us/research/project/figureqa-dataset/,The dataset is available for download here,Code to generate the FigureQA dataset. ,.
6368,#custom-data,Custom data,"
","
"
6378,https://github.com/FilippoMB/TCK_AE/blob/master/Data/TCK_data.mat,/Data/TCK_data.mat,"We can see that the matrix has a block structure: the first larger block on the diagonal are the similarities between MTS of class 1, the second smaller block is relative to the elements of class 2. Results are saved in ", and they are used in the next section to train the dkAE.
6391,http://cocodataset.org/#home,COCO,Multi-label Image Annotation(COCO) Please download ," dataset first. Specifically, 2014 train, val and test splits are used in the paper. To prepare a well trained multi-label model for feedback-prop, you can either train a new model by running "
6401,https://angular.io/guide/displaying-data,Components and Templates,"
","
"
6409,https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip,"
https://hobbitdata.informatik.uni-leipzig.de/bengal/en_surface_forms.tsv.zip
",Download the surface forms file which is available at ,.
6414,https://github.com/huynhlvd/corpca/blob/master/dataGeneration.m,dataGeneration.m,"
",: Generating data for numerical simulations
6424,https://pandas.pydata.org/,"
pandas
",The notebooks use ," for data analysis. We used v0.20.3 but anything above and some below should do as well. For plotting, "
6425,#download-and-setup-stochastic-world-database,Stochastic World database,Download and setup the ,.
6433,http://vowl.visualdataweb.org/,VOWL,(See ," for good online tools for visualizing OWL and data from endpoints as node-edge diagrams, but no textual visualization.)"
6433,https://miriadax.net/web/semantic-web-and-linked-data,another MOOC,. And ,.
6433,https://www.coursera.org/specializations/genomic-data-science,MOOC specialization,. This other , has a Python for BioInformatics course. A 
6433,https://www.coursera.org/learn/data-genes-medicine,MOOC for big data and bioinformatics, has a Python for BioInformatics course. A ,.
6433,http://visualdataweb.de/webvowl/#,WebVOWL,"
", for visualizing OWL ontologies online. It is very good. The 
6433,http://vowl.visualdataweb.org/,VOWL, for visualizing OWL ontologies online. It is very good. The , project includes online querying an endpoint for visualizing the structure of data and other tools.
6444,http://pandas.pydata.org/,pandas,"
", for logging to csv
6444,http://bokeh.pydata.org,bokeh,"
", for training visualization
6447,https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/,Download, IWSLT2015: ,"
"
6450,http://pandas.pydata.org/,pandas,"
","
"
6450,http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset,here,Download audio data and tag annotations from ,. Then you should see 3 
6458,https://github.com/ramey/datamicroarray/wiki/Chin-%282006%29,Chin (2006),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Chowdary-%282006%29,Chowdary (2006),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Gravier-%282010%29,Gravier (2010),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Sorlie-%282001%29,Sorlie (2001),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/West-%282001%29,West (2001),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Pomeroy-%282002%29,Pomeroy (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Burczynski-%282006%29,Burczynski (2006),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29,Alon (1999),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Sun-%282006%29,Sun (2006),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Borovecki-%282005%29,Borovecki (2005),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Chiaretti-%282004%29,Chiaretti (2004),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29,Golub (1999),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Yeoh-%282002%29,Yeoh (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Gordon-%282002%29,Gordon (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Shipp-%282002%29,Shipp (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Tian-%282003%29,Tian (2003),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Singh-%282002%29,Singh (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Nakayama-%282007%29,Nakayama (2007),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Khan-%282001%29,Khan (2001),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Christensen-%282009%29,Christensen (2009),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Su-%282002%29,Su (2002),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Subramanian-%282005%29,Subramanian (2005),"
","
"
6458,https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29,Alon et al. (1999) Colon Cancer data set," command. For example, to load the well-known ",", type the following at the R console:"
6458,https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29,Alon et al. (1999) Colon Cancer data set,Here is a summary for the ,.
6461,#download-and-setup-pvs-hm-database,"
database
",This repository provides ,", "
6461,#download-PVS-HMEM-database,Download the PVS-HMEM database,"
",.
6461,#details-of-the-mat-data-file,here," file, refer to ", (Note that you do not have to read the details of the mat file if you just want to run our code and reproduce the numbers).
6462,#download-and-setup-pvs-hm-database,"
database
",This repository provides ,", "
6462,#download-PVS-HMEM-database,Download the PVS-HMEM database,"
",.
6462,#details-of-the-mat-data-file,here," file, refer to ", (Note that you do not have to read the details of the mat file if you just want to run our code and reproduce the numbers).
6471,[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).,End-to-end training with on-the-fly data processing,"
","
"
6475,https://github.com/xuchen/jacana/tree/master/tree-edit-data/answerSelectionExperiments/data,answer sentence selection dataset,We use the , from TREC QA as our source of indirect supervision. We ran Stanford NER to extract entity mentions on both question and answer sentences and process the dataset into JSON format containing QA-pairs. Details of how we construct QA-pairs can be found in our paper.
6476,http://www.cs.jhu.edu/~xuchen/packages/jacana-data.tar.bz2,data files,": Original repo was at Google Code (https://code.google.com/p/jacana/). It was too big and after conversion, commit history was lost. Also, the data files had to be separated. Please download the ", and untar it to the same folder as the project files.
6480,https://neurodata.io>,NeuroData,). AND REFERENCES FOR PUBLIC ACCESS TO DATA TO ,.
6480,http://www.robots.ox.ac.uk/~andreww/synapse_data.html,http://www.robots.ox.ac.uk/~andreww/synapse_data.html,Data will be made available for download at ,". For this code to operate without adjustment, this data must be downloaded and unzipped into the root file of this repository so that the scripts can automatically grab the required data."
6484,assets/data/modl/components_64.nii.gz,64-components dictionary covering the whole brain,"
","
"
6484,assets/data/modl/components_128.nii.gz,128-components dictionary covering the whole brain,"
","
"
6484,assets/data/modl/components_453_gm.nii.gz,453-components dictionary covering the grey-matter,"
","
"
6484,assets/data/modl/loadings_128_gm.npy,128-components dictionary loadings over the 453-components dictionary,"
","
"
6488,https://github.com/pytorch/tnt/blob/master/torchnet/dataset/concatdataset.py,pytorch/tnt,ConcatDataset from ,.
6503,https://rose1.ntu.edu.sg/dataset/actionRecognition/,https://rose1.ntu.edu.sg/dataset/actionRecognition/,"
","
"
6504,https://github.com/OSGeo/PROJ-data,PROJ-data GitHub repository,More info on the contents of the proj-data package can be found at the ,.
6516,https://github.com/learningtitans/data-depth-design/issues,submitting an issue,"Please, help us to improve this code, by ", if you find any problems.
6516,http://www.fc.up.pt/addi/ph2%20database.html,PH2 Dataset,The , has 200 dermoscopic images (40 melanomas). It's freely available after signing a short online registration form.
6524,http://mpqa.cs.pitt.edu/corpora/mpqa_corpus/mpqa_corpus_2_0/,MPQA 2.0 corpus,Download ,.
6539,./testdata,testdata,The Python package provides an interface for aligning and estimating priors. Here is a simple example using the files in ,:
6542,http://accad.osu.edu/research/mocap/mocap_data.htm,ACCAD,This motion capture data was taken from Ohio State University's , centre.
6546,https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip,data,Download ,"
"
6551,http://opendatacommons.org/licenses/by/1.0/,Open Data Commons Attribution License (ODC-By) v1.0,"
","
"
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb,Introduction to MNIST Dataset,"
",.
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb,notebook, (,"). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...)."
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb,this notebook,"Some examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples. MNIST is a database of handwritten digits, for a quick description of that dataset, you can check ",.
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb,Introduction to MNIST Dataset,"
",.
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb,notebook, (,) (
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py,code,) (,"). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file."
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb,notebook, (,) (
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py,code,) (,). Introducing TensorFlow Dataset API for optimizing the input data pipeline.
6568,https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb,notebook, (,"). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...)."
6572,https://www.indigo-datacloud.eu/,INDIGO-DataCloud,The present document describes how Apache Mesos is used by the , PaaS layer. 
6572,https://deep-hybrid-datacloud.eu,DEEP Hybrid-DataCloud," INDIGO-DataCloud (start date: 01/04/2015, end date: 30/09/2017) is a project funded under the Horizon2020 framework program of the European Union and led by the National Institute for Nuclear Physics (INFN). It developed a data and computing platform targeting scientific communities, deployable on multiple hardware and provisioned over hybrid (private or public) e-infrastructures. The INDIGO solutions are being evolved in the context of other European projects like ",", "
6572,http://www.extreme-datacloud.eu/,eXtreme-DataCloud,", ", and 
6573,https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/orchestrator/job/master/,"
Jenkins
","
","
"
6573,https://deep-hybrid-datacloud.eu/,DEEP-HybridDataCloud project,"
", (Horizon 2020) under Grant number 777435.
6573,http://www.extreme-datacloud.eu/,eXtreme-DataCloud project,"
", (Horizon 2020) under Grant number 777367.
6573,https://www.indigo-datacloud.eu/,INDIGO-DataCloud project,"
", (Horizon 2020) under Grant number 653549.
6577,https://jenkins.indigo-datacloud.eu:8080/job/tts-packaging/platform=bcentos7/,centos,WaTTS has nightly builds for packages for , and 
6577,https://jenkins.indigo-datacloud.eu:8080/job/tts-packaging/platform=bubuntu14/,Ubuntu/Debian, and ,". Download and install the package on your system and start configuring and using it, following the "
6581,https://jenkins.indigo-datacloud.eu/job/Pipeline-as-code/job/im/job/master/,"
Build Status
","
","
"
6582,http://onedata.org,Onedata,This is the main code repository of ," - a global data management system, providing easy access to distributed storage resources, supporting wide range of use cases from personal data management to data-intensive scientific computations."
6582,https://onedata.org/docs/doc/administering_onedata/onezone_overview.html,Onezone,"
"," - allows to connect multiple storage providers into a larger distributed domain and provides users with Graphical User Interface for typical data management tasks,"
6582,https://onedata.org/docs/doc/administering_onedata/provider_overview.html,Oneprovider,"
"," - the main data management component of Onedata, deployed at each storage provider site, responsible for unifying and controlling access to data over low level storage resources of the provider,"
6582,https://onedata.org/docs/doc/using_onedata/oneclient.html,Oneclient,"
", - command line tool which enables transparent access to users data spaces through 
6582,https://onedata.org/docs/doc/administering_onedata/onepanel_overview.html,Onepanel,"
", - administration and configuration interface for 
6582,https://onedata.org/docs/doc/administering_onedata/luma.html,LUMA,"
"," - service which allows mapping of between Onedata user accounts and local storage ID's, here we provide an example implementation of this service."
6582,https://onedata.org/docs/index.html,documentation,The easiest way to get started with using or deploying Onedata is to start with our official ,.
6582,https://github.com/onedata/getting-started,example configurations and scenarios,"In order to try deploying Onedata, or specific components we have prepared a set of ",.
6582,https://hub.docker.com/u/onedata/,Docker Hub,The best way to use Onedata is to use our Docker images available at , or the binary packages available 
6582,https://get.onedata.org/,here, or the binary packages available ,. Currently the binary packages are only available for 
6582,https://github.com/onedata/onedata/issues,GitHub issues,Please use , mechanism as the main channel for reporting bugs and requesting support or new features.
6582,https://onedata.org/support,here,More information about support can be found ,.
6585,https://www.indigo-datacloud.eu,https://www.indigo-datacloud.eu,INDIGO DataCloud ,"
"
6585,https://deep-hybrid-datacloud.eu,https://deep-hybrid-datacloud.eu,DEEP-Hybrid-DataCloud ,"
"
6591,https://www.ukp.tu-darmstadt.de/data/spelling-correction/rwse-datasets,corpus1, 2013 ,"
"
6591,https://www.ukp.tu-darmstadt.de/data/spelling-correction/spelling-difficulty-prediction/,corpus2,"
","
"
6602,https://www.sajari.com/public-data,data-sets," is a search, recommendation and matching (e.g. dating website) service. On their site, they also have aggregated a bunch of useful ",.
6602,http://data.mr-dlib.org,data," A recommender-system as-a-service for academic organisations such as digital libraries and reference managers. Mr. DLib provides 'related-article' recommendations, is open-source, and publishes most of it's ",.
6605,http://simongog.github.io/assets/data/sdsl-slides/tutorial,tutorial slides and walk-through,", ",.
6605,http://simongog.github.io/assets/data/sdsl-slides/tutorial,presentation,A tutorial , with the 
6605,http://simongog.github.io/assets/data/sdsl-slides/tutorial,tutorial,Next we suggest you look at the comprehensive , which describes all major features of the library or look at some of the provided 
6605,https://users.dcc.uchile.cl/~jfuentess/datasets/graphs.php,here,A corpus with planar embeddings is available ,"
"
6611,https://research.fb.com/category/data-science/,Core Data Science team, released by Facebook's ,. It is available for download on 
6621,https://github.com/ziqizhang/data#ate,TTC,: the index files created by this version of Solr is not compatible with the previous versions; 2) fixing a couple of minor bugs documented in the Issues page; 3) added two more example configrations for the ," corpora; 4) added two new algorithms, "
6621,#datac,Data contribution,"
","
"
6621,https://github.com/ziqizhang/data#ate,research data page,Ziqi Zhang's , contains 4 datasets used for ATE research.
6623,#scholarlydata,Scholarly Data Linking,"
","
"
6623,https://github.com/ziqizhang/data/tree/master/hate%20speech,/hate speech,: ,"
"
6623,https://github.com/ziqizhang/data/tree/master/ontology%20mapping,/ontology mapping,: ,"
"
6623,https://github.com/ziqizhang/data/tree/master/procedural%20knowledge,/procedural knowledge,: ,"
"
6623,http://www.scholarlydata.org/papers/eswc2017/dataCleaning.html,Entity Deduplication on ScholarlyData," Z. Zhang, A. N. Nuzzolese, and A. L. Gentile. ",". In Proceedings of ESWC 2017, pp 85-100, Lecture Notes in Computer Science. Springer, 2017."
6623,http://www.scholarlydata.org/,scholarlydata,: ,"
"
6623,https://github.com/ziqizhang/data/tree/master/scholarly%20data%20linking,/scholarly data linking,: ,"
"
6623,https://github.com/ziqizhang/data/tree/master/terminology%20extraction,/terminology extraction,: ,"
"
6623,https://github.com/ziqizhang/data/tree/master/webtable%20entity%20linking,/webtable entity linking,: ,"
"
6649,https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet,Ravi and Larochelle - splits, can be downloaded from ,. For more information on how to obtain the images check the original source 
6663,#trained-models-and-data,Trained Models and Data,"This repository includes code, data, and pre-trained models for processing and querying Wikipedia as described in the paper -- see ",". We also list several different datasets for evaluation, see "
6663,#qa-datasets,QA Datasets,". We also list several different datasets for evaluation, see ",. Note that this work is a refactored and more efficient version of the original code. Reproduction numbers are very similar but not exact.
6663,#trained-models-and-data,download, DrQA and , our models to start asking open-domain questions!
6663,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json,train,SQuAD: ,", "
6663,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json,dev,", ","
"
6663,http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.train.json.bz2,train,WebQuestions: ,", "
6663,http://nlp.stanford.edu/static/software/sempre/release-emnlp2013/lib/data/webquestions/dataset_11/webquestions.examples.test.json.bz2,test,", ",", "
6682,#datasets,datasets,. Check the , section for details.
6691,examples/fully_sharded_data_parallel/README.md,Added full parameter and optimizer state sharding + CPU offloading,March 2021 ,"
"
6691,examples/fully_sharded_data_parallel/README.md,full parameter and optimizer state sharding,"
","
"
6691,examples/fully_sharded_data_parallel/README.md,offloading parameters to CPU,"
","
"
6697,https://ai.baidu.com/broad/leaderboard?dataset=dureader,[Leaderboard],"
","
"
6697,https://ai.baidu.com/broad/leaderboard?dataset=dureader,[Leaderboard],"
","
"
6703,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,Standford cars dataset,a script to finetune a pretrained resnet50 on the , to 90% accuracy in 60 epochs.
6706,https://www.cityscapes-dataset.com/,Cityscapes training set,: 2975 images from the ,.
6716,https://datamarket.azure.com/account/keys,Bing,", ",", "
6719,https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/Flickr30k-prepare,Flickr30k,. You can choose one dataset to run. Three datasets need different prepocessing. I write the instruction for ,", "
6719,https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/MSCOCO-prepare,MSCOCO,", ", and 
6719,https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare,CUHK-PEDES, and ,.
6724,https://www.cityscapes-dataset.com/,Cityscapes training set, 2975 images from the ,. (113M) 
6742,https://www.dropbox.com/s/f7q3bbgvat2q1u2/cifar10-dataset.zip?dl=0,DropBox,", ",", "
6743,http://www.robots.ox.ac.uk/~lz/DEM_cvpr2017/data.zip,here,Download data from , and unzip it 
6754,http://crcv.ucf.edu/data/UCF101.php,UCF101 website, which can be downloaded from the ,.
6754,https://github.com/google/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/kinetics_dataset.py,this,"For additional details on preprocessing, check ",", refer to our paper or contact the authors."
6774,https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712/README.md,data/VOC0712/README.md,Follow the , to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2007 training and testing.
6774,https://github.com/sfzhang15/RefineDet/blob/master/data/VOC0712Plus/README.md,data/VOC0712Plus/README.md,Follow the , to download VOC2007 and VOC2012 dataset and create the LMDB file for the VOC2012 training and testing.
6774,https://github.com/sfzhang15/RefineDet/blob/master/data/coco/README.md,data/coco/README.md,Follow the , to download MS COCO dataset and create the LMDB file for the COCO training and testing.
6774,https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/pascal_voc.py,"
test/lib/datasets/pascal_voc.py
",Change the ‘self._devkit_path’ in , to yours.
6774,https://github.com/sfzhang15/RefineDet/blob/master/test/lib/datasets/coco.py,"
test/lib/datasets/coco.py
",Change the ‘self._data_path’ in , to yours.
6786,http://cocodataset.org/#download,http://cocodataset.org/#download,Download MSCOCO images from ,. We train in COCO 
6786,https://github.com/cocodataset/cocoapi/tree/master/PythonAPI,PythonAPI, dataset. Then put the data and evaluation , in $CPN_ROOT/data/COCO/MSCOCO. All paths are defined in config.py and you can modify them as you wish.
6794,https://visualdialog.org/data,VisDial v1.0,"
", dataset can be downloaded and preprocessed as specified below. The path provided as 
6794,http://images.cocodataset.org/zips/train2014.zip,"
train2014
", must have four subdirectories - , and 
6794,http://images.cocodataset.org/zips/val2014.zip,"
val2014
", and ," as per COCO dataset, "
6794,https://visualdialog.org/data,here, which can be downloaded from ,.
6794,https://visualdialog.org/data,Visdial v0.9,To download and preprocess ," dataset, provide an extra "
6794,https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_data.h5,"
visdial_data.h5
","
",": Tokenized captions, questions, answers, image indices"
6794,https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json,"
visdial_params.json
","
",: Vocabulary mappings and COCO image ids
6794,https://s3.amazonaws.com/visual-dialog/data/v0.9/data_img_vgg16_relu7.h5,"
data_img_vgg16_relu7.h5
","
",: VGG16 
6794,https://s3.amazonaws.com/visual-dialog/data/v0.9/data_img_vgg16_pool5.h5,"
data_img_vgg16_pool5.h5
","
",: VGG16 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/visdial_data_train.h5,"
visdial_data_train.h5
","
",": Tokenized captions, questions, answers, image indices, for training on "
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/visdial_params_train.json,"
visdial_params_train.json
","
",: Vocabulary mappings and COCO image ids for training on 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/data_img_vgg16_relu7_train.h5,"
data_img_vgg16_relu7_train.h5
","
",: VGG16 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/data_img_vgg16_pool5_train.h5,"
data_img_vgg16_pool5_train.h5
","
",: VGG16 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/visdial_data_trainval.h5,"
visdial_data_trainval.h5
","
",": Tokenized captions, questions, answers, image indices, for training on "
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/visdial_params_trainval.json,"
visdial_params_trainval.json
","
",: Vocabulary mappings and COCO image ids for training on 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/data_img_vgg16_relu7_trainval.h5,"
data_img_vgg16_relu7_trainval.h5
","
",: VGG16 
6794,https://s3.amazonaws.com/visual-dialog/data/v1.0/data_img_vgg16_pool5_trainval.h5,"
data_img_vgg16_pool5_trainval.h5
","
",: VGG16 
6800,http://conda.pydata.org/,Conda,An additional method using , is also possible:
6801,https://github.com/harvardnlp/BSO/tree/master/data_prep/MT,Harvard NLP repo,Run the script (borrowed from ,) to download and preprocess IWSLT'14 dataset:
6801,https://github.com/pcyin/pytorch_nmt/tree/master/data,this repo,"NOTE: this script requires Lua and luaTorch. As an alternative, you can download all necessary files from ","
"
6803,#datasets,Datasets,"
","
"
6803,http://pytorch.org/docs/torchvision/datasets.html,API,"
",.
6803,https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md,instructions,", default is ~/data/COCO. Following the ", to prepare 
6805,pandas.pydata.org/,pandas,"
","
"
6811,https://datalab.snu.ac.kr/data/SNeCT/pancan12_tensor.tar.gz,DOWN,"| Name | Structure | Size | Number of Entries | Download | | :------------ | :-----------: | :-------------: |------------: |:------------------: | | PanCan12     | Patient - Gene - Platform | 4,555 × 14,351 × 5 | 183,211,020 | "," | | Pathway    | Gene - Gene | 14,351 × 14,351 | 665,429 | "
6811,https://datalab.snu.ac.kr/data/SNeCT/pathway_network.tar.gz,DOWN," | | Pathway    | Gene - Gene | 14,351 × 14,351 | 665,429 | ", |
6816,http://cocodataset.org/#home,COCO Dataset,Download COCO2017 image from ,"
"
6833,http://pandas.pydata.org,pandas,"
","
"
6833,https://seaborn.pydata.org/,Seaborn,"
","
"
6836,https://www.dropbox.com/s/2x509u80g5zkuea/MagNet_support_data.zip?dl=0,Dropbox,We provide demo attack data and classifier on , and 
6838,http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php,here,"HitL-SLAM can be used on other datasets as well, as long as they are 2D, and based on depth scans or depth images. Many well-known datasets of this nature can be found ",". After downloading a dataset or generating some data yourself, it needs to be put into the right format."
6846,https://data.pyg.org/slack.html,"
Slack
","
","
"
6846,https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py,"
DataPipe
"," support, "," support, a large number of common benchmark datasets (based on simple interfaces to create your own), the "
6846,https://data.pyg.org/slack.html,Click here to join our Slack community!,"
","
"
6846,https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html,Cora,"In the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph. For this, we load the "," dataset, and create a simple 2-layer GCN model using the pre-defined "
6846,https://data.pyg.org/whl,here," wheels for all major OS/PyTorch/CUDA combinations, see ",.
6846,https://data.pyg.org/whl,here, in order to prevent a manual installation from source. You can look up the latest supported version number ,.
6854,http://cocodataset.org/#download,MSCOCO,"Download train2014, val2014 images and their annotations from the ", webpage and put them in ./data/coco
6871,http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html,notMNIST dataset," does the same accuracy comparisons, but for the ",. We omit the textual explanations since it would be redundant with what's in the MNIST notebook.
6872,#build-your-own-dataset,Build Your Own Dataset,"
","
"
6873,utils/datasetcollections.py,dataset collections,This runs all experiments included in the results section of our paper. It is up to the user to download the datasets and link to them in the ," file. The output folders including the folders where the data will be stored (in this case testoutput/fullruns/2003_np/) have to exist before running the code, if folders are missing an error message will indicate this. Speeding up the simulations can be done by allocating more processes using the n_proc flag."
6874,utils/datasetcollections.py,dataset collections, to be installed. It is up to the user to download the datasets and link to them in the ," file. The output folders including the folders where the data will be stored (in this case testoutput/fullruns/2003_np/) have to exist before running the code, if folders are missing an error message will indicate this. Speeding up the simulations can be done by allocating more processes using the n_proc flag."
6884,http://crcv.ucf.edu/data/UCF101.php,here,Download videos and train/test splits ,.
6884,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,here,Download videos and train/test splits ,.
6887,https://github.com/EdinburghNLP/spot-data,link,The SPOT dataset used in the above paper is available on the EdinburghNLP github page: ,"
"
6888,https://github.com/DavidGrangier/wikipedia-biography-dataset,WIKIBIO,The dataset for evaluation is , from 
6909,https://archive.ics.uci.edu/ml/datasets/Online+Handwritten+Assamese+Characters+Dataset#,Assamese handwriting recognition,"
","
"
6909,http://www.nlpr.ia.ac.cn/databases/handwriting/Online_database.html,Chinese handwriting for recognition,"
","
"
6909,https://www.kaggle.com/c/datasciencebowl,"Kaggle plankton recognition competition, 2015","
", Third place. The competition solution is being adapted for research purposes in 
6910,https://github.com/mil-tokyo/bc_learning_sound/tree/master/dataset_gen,this page,Prepare datasets following ,.
6929,https://ieee-dataport.org,IEEE DataPort, and , are two of 
6929,https://www.re3data.org,many available options, are two of ,.
6940,http://www.technologyreview.com/news/428497/your-laptop-can-now-analyze-big-data/?nlid=nldly&nld=2012-07-17,"""Your laptop can now analyze big data""",MIT Technology Review article about GraphChi: ,"
"
6942,datasets/bibtex/facades.tex,Citation,. [,]
6942,datasets/bibtex/shoes.tex,Citation, edge detector + post-processing. [,]
6942,datasets/bibtex/handbags.tex,Citation, edge detector + post-processing. [,]
6942,datasets/bibtex/transattr.tex,Citation, [,]
6943,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC,First obtain either the , or the 
6943,http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag,Vicon Room 1 01,"
","
"
6943,http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_easy/V1_02_easy.bag,Vicon Room 1 02,"
","
"
6943,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC,"), there are two launch files prepared for the ", and 
6959,http://ml.cs.tsinghua.edu.cn/~chenxi/dataset/val224_compressed.pkl,Tsinghua, (, /  
6962,http://cocodataset.org/#download,MS-COCO website,) for validation. Follow ," to download images/annotations, and set-up the COCO API."
6980,https://github.com/bredele/datastore,datastore, and uses parts of ,". The two projects are combined such that changes two types of data (observable and replicated data) can be used. Changes to these data types are propagated to server or clients via remote procedure calls. This way, custom failure handling can be added when for example no network is available. Proxies are used to ensure that every assignment on these objects are propagated to the server and every connected client."
6989,https://gbfs.mobilitydata.org/,GBFS Resource Center," endorsement, support, and hosting was key to its success starting in 2015. In 2019, NABSA chose MobilityData to govern and facilitate the improvement of GBFS. MobilityData hosts a ", and a 
6989,https://bit.ly/mobilitydata-slack,public GBFS Slack channel, and a , - you are welcome to contact us there or at 
6989,mailto:sharedmobility@mobilitydata.org,sharedmobility@mobilitydata.org, - you are welcome to contact us there or at , with questions.
6989,https://bit.ly/mobilitydata-slack,public GBFS Slack channel, Questions can also be addressed to the community via the , or to the shared mobility staff at MobilityData: 
6989,mailto:sharedmobility@mobilitydata.org,sharedmobility@mobilitydata.org, or to the shared mobility staff at MobilityData: ,"
"
6989,https://mobilitydata.org/cities-gbfs-v2-2-is-here-for-you/,v2.2 Article,| , |  
6989,https://mobilitydata.org/gbfs-now-fully-supports-dockless-systems-%f0%9f%9b%b4%f0%9f%91%8f/,v2.1 Article,| , |  
6989,https://mobilitydata.org/whats-new-in-gbfs-v2-0-%f0%9f%9a%b2%f0%9f%9b%b4/,v2.0 Article, | , | |  
6989,https://mobilitydata-io.slack.com,GBFS Slack channel,The person calling for the vote should announce the vote in the , with a link to the PR. The message should conform to this template:
6989,https://gbfs.mobilitydata.org/toolbox/resources/,here,"Including APIs, datasets, validators, research, and software can be found ",.
6989,https://github.com/openmobilityfoundation/mobility-data-specification,MDS,There are many similarities between GBFS and ," (Mobility Data Specification), however, their intended use cases are different. GBFS is a real-time or near real-time specification for public data primarily intended to provide transit advice through consumer-facing applications. MDS is not public data and is intended for use only by mobility regulators. Publishing a public GBFS feed is a "
6989,https://github.com/openmobilityfoundation/mobility-data-specification#gbfs-requirement,requirement," (Mobility Data Specification), however, their intended use cases are different. GBFS is a real-time or near real-time specification for public data primarily intended to provide transit advice through consumer-facing applications. MDS is not public data and is intended for use only by mobility regulators. Publishing a public GBFS feed is a ", of all MDS compatible 
6989,https://mobilitydata.org/,MobilityData,The copyright for GBFS is held by the ,.
6998,#data,Data,"
","
"
6999,http://www.robots.ox.ac.uk/~vgg/data/dtd/,Describable Textures Dataset,"get a dataset with backgrounds, e.g. the ","
"
7000,https://archive.ics.uci.edu/ml/datasets/airfoil+self-noise,"
UCI Airfoil Self-Noise Data Set
", trained on the ,", converted to the TT format via cross-approximation):"
7006,https://iie.fing.edu.uy/~jlezama/datasets/Facescrub500/,here,Download dataset used in the paper ,"
"
7008,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/N8XJME,Dataverse,The data set is available on , (361 MB). This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in our experiments.
7011,./res/python-results/datalist.csv,"
datalist.csv
","
"," - Details of the .vec and .sca files of all simulation runs including the names, and other details"
7025,examples/fully_sharded_data_parallel/README.md,Added full parameter and optimizer state sharding + CPU offloading,March 2021 ,"
"
7025,examples/fully_sharded_data_parallel/README.md,full parameter and optimizer state sharding,"
","
"
7025,examples/fully_sharded_data_parallel/README.md,offloading parameters to CPU,"
","
"
7027,http://robotics.ethz.ch/~asl-datasets/segmap/segmap_data/,segmap_data,"To download all necessary files, copy the content of the ", into 
7028,https://github.com/Yuliang-Liu/Curve-Text-Detector/tree/master/data,here,Additional point annotation for each character is included. Example can be referred to ,.
7030,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,3D Object,Table: Comparison of results with other published methods on the KITTI , and 
7030,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev,BEV, and ," benchmarks (accessed Apr 11, 2018)."
7030,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,Kitti Object Detection Dataset,To train on the ,:
7039,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,this page,Go to , to prepare ImageNet 1K data.
7039,datasets/cityscapes/drn-d-105.csv,here,"DRN-D-105 gets 76.3% mIoU on Cityscapes testing set with multi-scale testing, poly learning rate and data augmentation with random rotation and scaling in training. Full results are ",.
7039,datasets/cityscapes,document,"To set up Cityscapes data, please check this ",.
7042,http://pytorch.org/docs/master/torchvision/datasets.html#torchvision.datasets.ImageFolder,torchvision.datasets.ImageFolder,Our script uses ," for loading ImageNet data, which expects folders organized as follows:"
7042,https://www.cityscapes-dataset.com/benchmarks/#scene-labeling-task,Cityscapes,"We have successfully used InPlace-ABN with a DeepLab3 segmentation head that was trained on top of the WideResNet38 model above. Due to InPlace-ABN, we can significantly increase the amount of input data to this model, which eventually allowed us to obtain #1 positions on ",", "
7042,http://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015,Kitti,", ", and 
7045,https://grouplens.org/datasets/movielens/,this link, folder. It can be also downloaded from the ,. The Yelp! and KDD datasets are in 
7064,http://www.datanucleus.org/javadocs/core/6.0/,6.0, : ,", "
7064,http://www.datanucleus.org/javadocs/core/5.2/,5.2,", ",", "
7064,http://www.datanucleus.org/javadocs/core/5.1/,5.1,", ",", "
7064,http://www.datanucleus.org/javadocs/core/5.0/,5.0,", ",", "
7064,http://www.datanucleus.org/javadocs/core/4.1/,4.1,", ",", "
7064,http://www.datanucleus.org/javadocs/core/4.0/,4.0,", ","
"
7064,https://repo1.maven.org/maven2/org/datanucleus/datanucleus-core,Maven Central, : ,"
"
7064,http://www.datanucleus.org/support.html,DataNucleus Support Page, : ,"
"
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/flush/FlushOrdered.html,[Javadoc],"
",. Other datastores typically use a 
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/flush/FlushNonReferential.html,[Javadoc],"
",.
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/metadata/ClassMetaData.html,[Javadoc],"
",. This in turn has a Collection of 
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/metadata/FieldMetaData.html,[Javadoc],"
", andy/or 
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/metadata/PropertyMetaData.html,[Javadoc],"
"," depending whether the metadata is specified on a field or on a getter/setter method. Fields/properties are numbered alphabetically, with the "
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/JDOQLSingleStringParser.html,[Javadoc],"
", and 
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/JPQLSingleStringParser.html,[Javadoc],"
",.
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/compiler/QueryCompilation.html,[Javadoc],"
",.
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/compiler/JDOQLCompiler.html,[Javadoc],"
", and 
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/compiler/JPQLCompiler.html,[Javadoc],"
",. These each have a Parser that performs the extraction of the different components of the clauses and generation of the Node tree. Once a Node tree is generated it can then be converted into the compiled Expression tree; this is handled inside the JavaQueryCompiler.
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/compiler/SymbolTable.html,[Javadoc],"
"," which is a lookup table (map) of identifiers and their value. So, for example, an input parameter will have a name, so has an entry in the table, and its value is stored there. This is then used during evaluation."
7064,http://www.datanucleus.org/javadocs/core/latest/org/datanucleus/store/query/inmemory/InMemoryExpressionEvaluator.html,[Javadoc],"
",. This takes in each candidate object one-by-one and evaluates whichever of the query clauses are desired to be evaluated. For example we could just evaluate the filter clause. Evaluation makes use of the values of the fields of the candidate objects (and related objects) and uses the SymbolTable for values of parameters etc. Where a candidate fails a particular clause in the filter then it is excluded from the results.
7065,https://camel.apache.org/components/latest/dataformats/,https://camel.apache.org/components/latest/dataformats/,Data Formats: ,"
"
7075,https://towardsdatascience.com/image-colorization-using-convolutional-autoencoders-fdabc1cb1dbe,Medium,Eryk Lewinson on ,"
"
7089,https://github.com/yoonkim/lstm-char-cnn/tree/master/data/ptb,here,"Download the preprocessed PTB training, testing, and validation files e.g. from ", (the default location used by the training script is 
7091,https://github.com/jpuigcerver/Laia/tree/iam_new/egs/iam#data-preparation,IAM data preparation,Follow steps for ,". IAM consists of approx. 10k images of handwritten text lines and their transcriptions. The code in the linked repo binarizes the images in a manner that preserves the original grayscale information, converts to JPEG, and scales to 64 pixel height. The code creates a folder for preprocessed images "
7093,https://github.com/fjxmlzn/PacGAN/tree/master/synthetic_data_experiments,Synthetic data experiments,"
","
"
7106,https://www.unidata.ucar.edu/software/netcdf/,netCDF 4,"After the command finishes, the extracted spectrograms have been stored in ", format in a file called 
7106,https://www.unidata.ucar.edu/software/netcdf/,netCDF 4,"Extracts metadata, such as labels, cross-validation information, or partition information, from a data set on disk, and extracts spectrograms from raw audio files. The extracted spectrograms together with their metadata are stored in "," format. For a description of our data model, see "
7106,#data-model,Data Model," format. For a description of our data model, see ",.
7106,#data-model,Data Model,". For an overview of our data model, see ",. | | 
7106,#data-model,Data Model,"Export a data set into CSV or ARFF format. Cross validation and partition information are represented through the output directory structure, whereas filename, chunk number and labels are stored within the CSV or ARFF files (for a description of these attributes, see ",).
7106,#data-model,Data Model,". For an overview of our data model, see ",. |
7106,#data-model,Data Model,"Validate data set integrity constraints. This functionality is provided by a separate command, since integrity constraint checking can be rather time-consuming. For a list of integrity constraints, see ",.
7106,http://xarray.pydata.org,xarray,"Internally, we rely on ", to represent data sets.
7106,https://github.com/karoldvl/paper-2015-esc-dataset,ESC-10 and ESC-50,Parses the , data sets. This parser requires that audio files are sorted into separate directories according to their class. Each class directory name must adhere to the regex 
7106,https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html,UrbanSound8K,Parses the , data set. This parser requires the following directory structure below the data set base directory.
7117,https://github.com/marytts/dfki-semaine-data,DFKI SEMAINE Data,A male British English unit selection voice built from the ,"
"
7119,https://montrealcorpustools.github.io/Montreal-Forced-Aligner/,Montreal Forced Aligner,-based , to phonetically segment audio data based on corresponding text files. It uses 
7119,https://montrealcorpustools.github.io/Montreal-Forced-Aligner/,Montreal Forced Aligner," file is on the library search path, since it is not bundled with the Linux release of ",". For details, see the "
7140,http://www.robots.ox.ac.uk/~vgg/data/text/,here,"Unfortunately, we can not offer our entire train dataset for download, as it is way too huge. But if you want to train a text recognition model on your own, you can use the ""Synthetic Word Dataset"" (download it ","). After you've downloaded the dataset, you will need to do some post processing and create a groundtruth similar to the one for the FSNS dataset. We provide a sample dataset at the location, where you can also download the text recognition model (which is "
7147,http://redwood-data.org/3dscan/dataset.html?c=chair,Redwood Dataset,"
","
"
7147,http://redwood-data.org/3dscan/dataset.html?c=chair,Redwood, on , Depth dataset and move it to 
7157,#visget_window_data,"
vis.get_window_data
","
",: get current data for a window
7158,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip,test_LR_bicubic_X4,"Other links for DIV2K, in case you can't find it : ",", "
7158,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip,train_HR,", ",", "
7158,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip,train_LR_bicubic_X4,", ",", "
7158,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip,valid_HR,", ",", "
7158,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip,valid_LR_bicubic_X4,", ",.
7159,https://circleci.com/gh/datavisyn/lineupjs,"
CircleCI
","
","
"
7159,https://circleci.com/gh/datavisyn/lineupjs/tree/develop,"
CircleCI
","
","
"
7159,https://lineup.js.org/master/docs/modules/_builder_databuilder_.html#builder,builder,"
", returning a new 
7159,https://lineup.js.org/master/docs/classes/_builder_databuilder_.databuilder.html,DataBuilder, returning a new ,"
"
7159,https://lineup.js.org/master/docs/classes/_provider_localdataprovider_.localdataprovider.html,LocalDataProvider,", and ",. A 
7159,https://github.com/datavisyn/lineupjsx,lineupjsx, wrapper is located at ,.
7159,https://github.com/datavisyn/nglineup,nglineup, wrapper is located at ,.
7159,https://github.com/datavisyn/vue-lineup,vue-lineup, wrapper is located at ,.
7159,https://github.com/datavisyn/lineup-element,lineup-element, web component wrapper is located at ,.
7159,https://github.com/datavisyn/lineup_htmlwidget,lineup_htmlwidget, wrapper for R is located at ,. It can be used within standalone 
7159,https://github.com/datavisyn/lineup_widget,lineup_widget, wrapper for Python is located at ,.
7159,http://mybinder.org/repo/datavisyn/lineup_widget/examples,"
Launch Binder
","
","
"
7159,https://github.com/datavisyn/lineup_powerbi,lineup_powerbi, wrapper is located at ,.
7166,http://www.nltk.org/data.html,Installing NLTK Data," needs to be downloaded. For installing packages, see the official guide ",.
7166,https://github.com/deepmind/rc-data,DeepMind,"The dataset for query-based abstractive summarization is created by converting an existing dataset for question answering, released by ",. Archives containing the processed DeepMind dataset can be downloaded at 
7167,https://github.com/helmertz/querysum-data,separate repo,Instructions for acquiring the dataset released along with this model can be found at a ,.
7172,https://github.com/DrSleep/tensorflow-deeplab-resnet#using-your-dataset,topic,Please refer to this ,.
7172,https://github.com/DrSleep/tensorflow-deeplab-resnet#using-your-dataset,these instructions, are added to ease the usability of the scripts on new datasets. Check out , on how to set up the training process on your dataset.
7172,https://github.com/DrSleep/tensorflow-deeplab-resnet/tree/master/dataset,here,Create a file with instances of your dataset in the same format as in files ,;
7191,https://archive.ics.uci.edu/ml/datasets/HIGGS,HIGGS,download data from , and uncompress gz file.
7207,https://bitbucket.org/rjust/fault-localization-data,Scripts and annotations for evaluating FL techniques,"
","
"
7278,https://github.com/afagarap/pt-datasets,"
pt-datasets
","To use the dataset from this repository, you can intall ",","
7310,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,German Traffic Sign Dataset,"This project creates and trains a deep convolutional neural network from scratch, with the task of classifying German traffic signs using the ",". The trained model is subsequently tested on German traffic signs found on the web, as well as US traffic signs."
7310,https://d17h27t6h515a5.cloudfront.net/topher/2016/October/580d53ce_traffic-sign-data/traffic-sign-data.zip,here,This project uses the German Traffic Sign Dataset. A pickled version that has images resized to 32x32 is available ,. The pickle file was used as the raw data for this project.
7354,./data/example.xml,example.xml,"convert ground truth into ""xml"" form: ","
"
7355,https://datalab.snu.ac.kr/~ljw9111/,Jungwoo Lee*,"
",", "
7355,https://datalab.snu.ac.kr/data/GIFT/GIFT.zip,GIFT-v1.0,Refer to the code directory in our repository or download the following zip file. [,]
7355,https://datalab.snu.ac.kr/data/GIFT/total.zip,DOWN,"| Name | Structure | Size | Number of Nonzeros | Download | | :------------ | :-----------: | :-------------: |------------: |:------------------: | | PANCAN12 tensor     | Patient - Gene - Experiment Type | 4,555 × 14,351 × 5 | 180M | "," | | Mask matrix, "
7355,https://datalab.snu.ac.kr/data/GIFT/mask.zip,DOWN,"	    | Gene - Gene set | 14,351 × 50 | 7K | ", |
7356,https://disq.us/url?url=https%3A%2F%2Fd17h27t6h515a5.cloudfront.net%2Ftopher%2F2017%2FFebruary%2F5898cd6f_traffic-signs-data%2Ftraffic-signs-data.zip%3AWO3Nq9Ds8s63rCvcn6CrIqXkNk0&cuid=4444009,here," , a research project under the INSPIRE group in the Electrical Engineering Department at Princeton University. It is the same code that we used to run the experiments, but excludes some of the run scripts as well as the datasets used. Please download the dataset in pickle format ",", or visit the original "
7371,https://www.unidata.ucar.edu/netcdf/software.html,Software for Manipulating or Displaying NetCDF Data,"
","
"
7371,https://www.unidata.ucar.edu/netcdf/,Unidata Network Common Data Form (NetCDF),"
","
"
7371,http://github.com/Unidata/netcdf-c,C library and utilities,"
","
"
7371,http://github.com/Unidata/netcdf-fortran,Fortran,"
","
"
7371,https://downloads.unidata.ucar.edu/netcdf-java/,Java,"
","
"
7371,http://github.com/Unidata/netcdf4-python,Python,"
","
"
7371,http://github.com/Unidata/netcdf-cxx4,C++,"
","
"
7371,https://www.unidata.ucar.edu/software/netcdf/copyright.html,here,Copyright and licensing information can be found ,", as well as in the COPYRIGHT file accompanying the software"
7371,https://docs.unidata.ucar.edu/netcdf-c/current/winbin.html,Building NetCDF,"
","
"
7371,https://docs.unidata.ucar.edu/nug/current/index.html#user_guide,Language-independent User's Guide,"
","
"
7371,https://docs.unidata.ucar.edu/netcdf-c/current/tutorial_8dox.html,NetCDF-C Tutorial,"
","
"
7371,https://docs.unidata.ucar.edu/netcdf-fortran/current/f90_The-NetCDF-Fortran-90-Interface-Guide.html,Fortran-90 User's Guide,"
","
"
7371,https://docs.unidata.ucar.edu/netcdf-fortran/current/nc_f77_interface_guide.html,Fortran-77 User's Guide,"
","
"
7371,https://docs.unidata.ucar.edu/netcdf-java/current/userguide/,netCDF-Java/Common Data Model library,"
","
"
7371,http://unidata.github.io/netcdf4-python/,netCDF4-python,"
","
"
7371,https://www.unidata.ucar.edu/netcdf/mailing-lists.html,Unidata netCDF Mailing-Lists,"
","
"
7371,mailto:support-netcdf@unidata.ucar.edu,support-netcdf@unidata.ucar.edu,"We appreciate feedback from users of this package.  Please send comments, suggestions, and bug reports to ",.
7385,http://seaborn.pydata.org/,Seaborn,", ",", and "
7392,#2-berkeley-adobe-perceptual-patch-similarity-bapps-dataset,Berkeley-Adobe Perceptual Patch Similarity (BAPPS) dataset,"
","
"
7392,#a-downloading-the-dataset,Download, a. ,"
"
7392,#b-evaluating-a-perceptual-similarity-metric-on-a-dataset,Evaluation, b. ,"
"
7392,#c-about-the-dataset,About the dataset, c. ,"
"
7392,#d-using-the-dataset-to-train-the-metric,Train the metric using the dataset, d. ,"
"
7392,https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py,py-faster-rcnn, repository. The average precision (AP) code is borrowed from the , repository. 
7402,http://data.vision.ee.ethz.ch/mentzerf/imgcomp-ckpts/ckpts.tar.gz,Download checkpoints/pre-trained models here,"
", and extract them to 
7402,https://github.com/tensorflow/models/blob/master/research/inception/inception/data/download_imagenet.sh,Inception download_imagenet.sh script, (a good resource is the ,"). For the following instructions, we assume both tar files are located in a directory "
7441,http://www.hezhenyu.cn/UpLoadFiles/dataset/tirsequences_new.rar,Local, or ,.
7441,http://www.hezhenyu.cn/UpLoadFiles/dataset/results_OPE_all.rar,Local, or ,.
7441,http://www.dabi.temple.edu/~hbling/code_data.htm,[Project]," Bao, C, et al,  Real time robust L1 tracker using accelerated proximal gradient approach, CVPR, 2012. ","
"
7444,#data,Data,"
","
"
7464,https://www.kaggle.com/c/asap-aes/data,here,We have used 5-fold cross validation on ASAP dataset to evaluate our system. This dataset (training_set_rel3.tsv) can be downloaded from ,". After downloading the file, put it in the "
7464,https://github.com/nusnlp/nea/tree/master/data,data,". After downloading the file, put it in the "," directory and create training, development and test data using "
7474,http://neon.nervanasys.com/docs/latest/datasets.html#imagenet,macrobatches,See the neon documentation for how to generate the imagenet , used above.
7482,http://lil.nlp.cornell.edu/resources/Misra-EMNLP-2018/Dataset_and_Simulators/data/house/,here,CHALET has a large corpus of natural language instructions paired with human demonstrations. The corpus is available ,. The corpus was published by 
7488,https://gite.lirmm.fr/multi-contact/mc_rtc_ros_data,mc_rtc_ros_data,"
",: ROS environment and object descriptions for mc_rtc
7492,https://www.kaggle.com/c/quora-question-pairs/data,here, contains ~ 2300k question pairs. Both the files can be found ,.
7504,recognition/_datasets_,dataset,"The training data includes, but not limited to the cleaned MS1M, VGG2 and CASIA-Webface datasets, which were already packed in MXNet binary format. Please ", page for detail.
7508,data_conversions/README.md,data_conversions,Please refer to ," for downloading S3DIS, then:"
7508,data_conversions/README.md,data_conversions,Please refer to ," for downloading ScanNet, then:"
7511,http://ir.hit.edu.cn/%7Edytang/paper/acl2015/dataset.7z,[Download],"The original datasets are released by the paper [Tang et al., 2015]. ","
"
7522,https://influxdata.com/technical-papers/,detailed technical writeups for each here,This repo contains code for benchmarking InfluxDB against other databases and time series solutions. You can access the ,.
7522,https://influxdata.com/blog/influxdb-markedly-elasticsearch-in-time-series-data-metrics-benchmark/,announcement blog here,Elasticsearch (,)
7522,https://www.influxdata.com/influxdb-vs-cassandra-benchmark-time-series-metrics/,InfluxDB Tops Cassandra in Time-Series Data & Metrics Benchmark,Cassandra (,)
7522,https://www.influxdata.com/influxdb-is-27x-faster-vs-mongodb-for-time-series-workloads/,InfluxDB is 27x Faster vs MongoDB for Time-Series Workloads,MongoDB (,)
7528,http://cocodataset.org/#download,annotations and images, to name COCO's API as inheritance. Download the , into 
7528,https://github.com/rbgirshick/py-faster-rcnn/blob/77b773655505599b94fd8f3f9928dbf1a9a776c7/data/README.md,here,. Note the valminusminival and minival can be downloaded ,.
7537,https://hobbitdata.informatik.uni-leipzig.de/homes/mroeder/palmetto/Wikipedia_bd.zip,The index,"
"," should be downloaded and extracted to some path (for example, "
7540,data,data,"
",          : data loaders for loading datasets
7564,https://www.researchgate.net/publication/363668453_RDD2022_A_multi-national_image_dataset_for_automatic_Road_Damage_Detection,here,: The article for data released through CRDDC'2022 can be accessed ,!
7564,https://crddc2022.sekilab.global/data/,data,[2022-08-11]: The , for 
7564,http://bigdataieee.org/BigData2020/BigDataCupChallenges.html,"2020 IEEE International Conference on Big Data, Atlanta, GA, USA",[2021-03-23]: IEEE Big Data Cup - GRDDC 2020: The proceedings for , are available now! The published version of the paper summarizing GRDDC'2020 can be accessed 
7564,https://data.mendeley.com/datasets/5ty2wb6gvg/1,RDD2020 dataset,[2021-03-19]: , is now available at Mendeley in a citable and easy to share form!
7564,http://bigdataieee.org/BigData2020/BigDataCupChallenges.html,IEEE International Conference on Big Data 2020,[2020-12-10]: IEEE Big Data Cup - GRDDC 2020: The workshop is being conducted in association with the ,! Check out the recordings at 
7564,http://bigdataieee.org/BigData2020/,IEEE BigData Cup Challenge 2020,"[2020-09-02]: The citation information and the article explaining the latest India-Japan-Czech (InJaCz) Road Damage Dataset, being used for ",", is now "
7564,http://bigdataieee.org/BigData2020/,IEEE Bigdata Cup, will be held as one of the ,. How about joining the data cup now? Exciting prizes await you!
7564,https://www.researchgate.net/publication/363668453_RDD2022_A_multi-national_image_dataset_for_automatic_Road_Damage_Detection,article,The , providing detailed statistics and other information for data released through CRDDC'2022 can be accessed 
7564,https://www.researchgate.net/publication/363668453_RDD2022_A_multi-national_image_dataset_for_automatic_Road_Damage_Detection,here, providing detailed statistics and other information for data released through CRDDC'2022 can be accessed ,!
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/RDD2022.zip,RDD2022.zip,"
","
"
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Directory_Structure_CRDDC_RDD2022.txt,Directory_Structure_CRDDC_RDD2022.txt,"
","
"
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/File_List_CRDDC_RDD2022.txt,File_List_CRDDC_RDD2022.txt,"
","
"
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/label_map.pbtxt,label_map.pbtxt,"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/sampleSubmission.txt,sampleSubmission_covering_India_Japan_and_Czech.txt,"
","
"
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_Japan.zip,RDD2022_Japan.zip,"
", (1022.9 MB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_India.zip,RDD2022_India.zip,"
", (502.3 MB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_Czech.zip,RDD2022_Czech.zip,"
",  (245.2 MB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_Norway.zip,RDD2022_Norway.zip,"
",  (9.9 GB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_United_States.zip,RDD2022_United_States.zip,"
",  (423.8 MB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_China_MotorBike.zip,RDD2022_China_MotorBike.zip,"
",  (183.1 MB - train and test)
7564,https://bigdatacup.s3.ap-northeast-1.amazonaws.com/2022/CRDDC2022/RDD2022/Country_Specific_Data_CRDDC2022/RDD2022_China_Drone.zip,RDD2022_China_Drone.zip,"
",  (152.8 MB - only train)
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz,train.tar.gz,"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/test1.tar.gz,test1.tar.gz,"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/sampleSubmission.txt,sampleSubmission.txt,"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/test2.tar.gz,test2.tar.gz,"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/Japan/CACAIE2020/frozen_inference_graph_resnet.pb,Resnet(128MB),"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/Japan/CACAIE2020/frozen_inference_graph_mobilenet.pb,Mobilenet(18MB),"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/Japan/RDD2020_data.tar.gz,RoadDamageDataset_2019 (2.4GB),"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/Japan/CACAIE2018/trainedModels.tar.gz,trainedModels (70MB),"
","
"
7564,https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/Japan/CACAIE2018/RoadDamageDataset.tar.gz,RoadDamageDataset_v1 (1.7GB),"
","
"
7582,https://github.com/hkmztrk/DeepDTA/blob/master/data/README.md,readme,Please see the , for detailed explanation.
7597,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,KITTI Depth Completion benchmark,Table: Comparison of results with other published unguided methods on the ,.
7605,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,Download , dataset and use the full resolution images.
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Train_mscoco.zip,training question files,"
","
"
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Val_mscoco.zip,validation question files,"
","
"
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Questions_Train_mscoco.zip,here,"Question files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Train_mscoco.zip,training annotation files,"
","
"
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Val_mscoco.zip,validation annotation files,"
","
"
7606,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Annotations_Train_mscoco.zip,here,"Annotation files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
7606,http://mscoco.org/dataset/#download,MS COCO website,"For real, create a directory with name mscoco inside this directory. For each of train, val and test, create directories with names train2014, val2014 and test2015 respectively inside mscoco directory, download respective images from ", and place them in respective folders.
7608,#dataset-set-up,Dataset set-up,"
","
"
7635,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/,CoNLL (YAGO),". Recognizing entities in documents can be quite challenging since there are often millions of possible answers. However, when using a type system to constrain the options to only those that semantically ""type check,"" we shrink the answer set and make the problem dramatically easier to solve. Our new results suggest that learning types is a very strong signal for understanding natural language: if types were given to us by an oracle, we find that it is possible to obtain accuracies of 98.6-99% on two benchmark tasks ", and the 
7638,https://www.thedatum.org/datasets/UCR2022_DATASETS.zip,here,Download all 128 preprocessed datasets ,.
7638,https://www.thedatum.org/datasets/UAE2022_DATASETS_1.zip,here,Download the first 14 preprocessed datasets ,.
7638,https://www.thedatum.org/datasets/UAE2022_DATASETS_2.zip,here,Download the remaining 14 preprocessed datasets ,.
7641,http://conda.pydata.org/miniconda.html,conda,We highly recommended to use , as your Python distribution. Once downloading and installing 
7641,http://conda.pydata.org/miniconda.html,conda, as your Python distribution. Once downloading and installing ,", this project can be installed by using the "
7642,http://cocodataset.org/#download,here,Download MS-COCO 2017 dataset from ,.
7651,https://gridcal.readthedocs.io/en/latest/data_sheets.html,Equipment catalogue,"
"," (Wires, Cables and Transformers) ready to use in GridCal"
7668,https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/data/README.md#coco-minival-annotations,Detectron, sets from ,"
"
7668,https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/datasets/evaluation/coco/coco_eval.py,coco_eval.py,"To calculate mAP for each class, you can simply modify a few lines in ",. See 
7668,maskrcnn_benchmark/data/datasets/coco.py,"
maskrcnn_benchmark/data/datasets/coco.py
"," is implemented, check ",.
7668,maskrcnn_benchmark/data/datasets/__init__.py,"
maskrcnn_benchmark/data/datasets/__init__.py
","
",: add it to 
7668,maskrcnn_benchmark/data/datasets/evaluation/__init__.py,"
maskrcnn_benchmark/data/datasets/evaluation/__init__.py
","To enable your dataset for testing, add a corresponding if statement in ",:
7670,http://corpus-texmex.irisa.fr/,Datasets for approximate nearest neighbor search,"dataset, ","
"
7682,https://github.com/rkadlec/ubuntu-ranking-dataset-creator,Ubuntu Dialogue Corpus V2,We used ,". In order to easily reproduce results in the above paper, the processed dataset has been provided."
7684,https://data.mendeley.com/datasets/pvn3xc3wy5/1,Mendeley Data repository,"To download and better understand the training dataset, visit the ",.
7684,https://s3-us-west-2.amazonaws.com/lukedeo-data/lbl/calogan/shower-shapes.h5,on S3,"For quick handling, we have pre-extracted the shower shape variables into a pandas dataframe (stored as HDF5) and made it available available ",". To load, you can simply do "
7694,https://archive.org/details/comma-dataset,archive.org comma dataset,or get it at ,"
"
7709,#read-bad-a-new-dataset-and-evaluation-scheme-for-baseline-detection-in-archival-documents,[3],The demo will load a trained model and perform inference for five sample images of the cBad test set ,", "
7709,#read-bad-a-new-dataset-and-evaluation-scheme-for-baseline-detection-in-archival-documents,[3],The example images are sampled from the cBad test set ,", "
7709,#train-data,train data,See ,"
"
7712,https://github.com/jlmelville/uwot#nearest-neighbor-data-format,Nearest Neighbor Data Format section, parameter. See the ," for more details. Please note that the Hamming support is a lot slower than the other metrics. I do not recommend using it if you have more than a few hundred features, and even then expect it to take several minutes during the index building phase in situations where the Euclidean metric would take only a few seconds."
7712,https://github.com/jlmelville/snedata,snedata,". Below are results for the 70,000 MNIST digits (downloaded using the ", package). On the left is the result of using the official Python UMAP implementation (via the 
7712,https://github.com/jlmelville/snedata,snedata,"For R packages, the MNIST data was downloaded via the "," package. For Python packages, the "
7712,https://github.com/jlmelville/uwot#nearest-neighbor-data-format,Nearest Neighbor Data Format,", only Euclidean, Cosine, Hamming, Pearson Correlation, and Manhattan distances are supported for finding nearest neighbors from data frame and dense matrix input. For other metrics, you can pass nearest neighbor data directly: see the "," section. Or if you can calculate a distance matrix for your data, you can pass it in as "
7712,https://github.com/jlmelville/uwot#mixed-data-types,Mixed Data Types,As discussed under the ," section, you can apply multiple distance metrics to different parts of matrix or data frame input data. if you do this, then "
7716,https://github.com/fMoW/dataset,fMoW-rgb dataset,Download the ," and uncompress it. To run our fMoW submission for a test set without running the training, execute the following sequence of commands:"
7726,docs/data_config_en.md,Dataset,"Dataset will be automatically configured in current path, or download manually your data in ",",  step-by step."
7731,kernet/datasets/,kernet/datasets,You can add dataset and model by modifying , and  
7733,#download-preprocessed-data,Download preprocessed data,"
","
"
7733,#download-preprocessed-data,below,Preprocess data using the following scripts OR directly download preprocessed data ,.
7745,#datasets,Datasets,"
","
"
7751,https://data.4tu.nl/articles/dataset/MTL_Music_Representation_data_underlying_the_publication_One_deep_music_representation_to_rule_them_all_A_comparative_analysis_of_different_representation_learning_strategies/12692300/1,dataset page,"To extract CNN features from provided model, one should first download the model files from the ",". Once the model is stored locally, one then can extract features by calling following command."
7762,https://bitbucket.org/franrruiz/augment-reduce-data/src,the datasets,You can also obtain , used in the paper.
7772,https://github.com/twitter/torch-dataset,torch-Dataset,"
","
"
7782,https://archive.ics.uci.edu/ml/datasets/Adult,link,adult.csv ,"
"
7782,https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data),link,german_categorical.csv (Modified from ,"
"
7800,http://archive.ics.uci.edu/ml/datasets/Musk+(Version+1),musk1 dataset,An example script is included that trains classifiers on the ,; see:
7801,https://warwick.ac.uk/fac/sci/dcs/research/tia/data/crchistolabelednucleihe/,Colon Cancer, and ,. In the histopathology experiments we used a similar model to the model in 
7803,#data,Data Preparation,"
","
"
7805,#using-shapes-as-input-data,Using shapes as input data,"
","
"
7809,https://geometrica.saclay.inria.fr/data/Steve.Oudot/clustering/,ToMATo,: implementation of ,. 
7816,https://github.com/deepmind/dsprites-dataset,here,Download the npz file from , and place it into 
7822,https://archive.ics.uci.edu/ml/datasets/Daphnet+Freezing+of+Gait,Daphnet Freezing of Gait dataset at UCI,Please download the , to run this code with an openly available dataset.
7823,#data,Data,"
","
"
7823,data/LBIDD/README.md,LBIDD README file,", as well as labeled and unlabeled data of treatment assignment, treatment effect and censoring data from simulated models based on it. More details regarding the LBIDDb data can be found in the ",.
7824,#data,Data,"
","
"
7824,data/LBIDD/README.md,LBIDD README file,", as well as labeled and unlabeled data of treatment assignment, treatment effect and censoring data from simulated models based on it. More details regarding the LBIDDb data can be found in the ",.
7845,https://travis-ci.org/giagiannis/data-profiler,"
Build Status
",data-profiler ,"
"
7845,https://goreportcard.com/report/github.com/giagiannis/data-profiler,"
goreport
","
","
"
7845,https://coveralls.io/github/giagiannis/data-profiler?branch=master,"
Coverage Status
","
","
"
7845,https://hub.docker.com/r/ggian/data-profiler/,"
Docker Automated build
","
","
"
7848,https://archive.ics.uci.edu/ml/datasets/HIGGS,higgs,"|Dataset Name| #Training | #Testing | #Features |      Task      | Link | |------------|------------|----------|-----------|----------------|------| |    Higgs   | 10,000,000 | 500,000  |     28    | Classification | "," | |   Epsilon  | 400,000 | 100,000  |     2000    | Classification | "
7848,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,epsilon," | |   Epsilon  | 400,000 | 100,000  |     2000    | Classification | "," | |  HEPMASS   | 7,000,000 | 3,500,000 | 28 | Classification | "
7848,https://archive.ics.uci.edu/ml/datasets/HEPMASS,hepmass," | |  HEPMASS   | 7,000,000 | 3,500,000 | 28 | Classification | ","| |  SUSY   | 4,000,000 | 1,000,000 | 18 | Classification | "
7848,https://archive.ics.uci.edu/ml/datasets/SUSY,susy,"| |  SUSY   | 4,000,000 | 1,000,000 | 18 | Classification | ","| | CASP | 29,999 | 15,731 | 9 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure,casp,"| | CASP | 29,999 | 15,731 | 9 | Regression | "," | | SGEMM | 193,280 | 48,320 | 14 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance,sgemm," | | SGEMM | 193,280 | 48,320 | 14 | Regression | "," | | SUPERCONDUCTOR | 17,008 | 4,255 | 81 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data,superconductor," | | SUPERCONDUCTOR | 17,008 | 4,255 | 81 | Regression | "," | | CT | 29,999 | 15,731 | 384 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis,ct," | | CT | 29,999 | 15,731 | 384 | Regression | "," | | Energy | 29,999 | 15,788 | 27 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/Energy+efficiency,energy," | | Energy | 29,999 | 15,788 | 27 | Regression | "," | | Year | 412,206 | 103,139 | 90 | Regression | "
7848,https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD,year," | | Year | 412,206 | 103,139 | 90 | Regression | ", |
7863,https://speakerdeck.com/gdfm/samoa-a-platform-for-mining-big-data-streams,"
SAMOA Slides
","
","
"
7864,https://ci.apache.org/projects/flink/flink-docs-stable/dev/datastream_api.html,Streaming API,A Parameter Server implementation based on the , of 
7864,https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/datastream_api.html#iterations,iterations,We implement the two-way communication of workers and the parameter server with Flink Streaming ,", which is not yet production-ready. The main issues are"
7878,http://data.dws.informatik.uni-mannheim.de/rdf2vec/,here,. You can find the graph embeddings ,.
7885,http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz,this link,. You may download the new commentary training corpus using ,.
7904,#data-format,data format,. See the section , for more explanations.
7904,#data-format,data format, is also partitioned vertically with the same number of blocks for consistency during the matrix-vector multiplication. See the section , for more explanations.
7904,#data-format,data format, in number of blocks. See the section , for more explanations.
7908,http://conda.pydata.org/docs/commands/conda-install.html,this," environment for testing, the following command can be issued to ensure that all required dependencies are in place (see ", for more info):
7908,http://grouplens.org/datasets/movielens/,Movielens 1M,", which stresses on the ease of use of the framework. For example, that's how you build a pure SVD recommender on top of the ", dataset:
7915,cifar_data/,cifar_data/,", unzip it, then put the extracted folder into ",. Then simply run
7918,https://github.com/epfl-dlab/when_sheep_shop/blob/master/data/README.md,this file,). Feel free to use any part of the code or use the data after requesting it (have a look at ," for the request) at the condition that our article is explicitly mentionned, see section "
7918,https://github.com/epfl-dlab/when_sheep_shop/blob/master/data/README.md,this file,: This notebook is used to match the data from the parsed data (see ," for more information about the data). This process takes a lot of time, therefore the matched data are also available upon request."
7921,https://github.com/bsharchilev/influence_boosting/blob/master/data/adult/catboost_params.json,"
catboost_params.json
",": in this notebook, CatBoost parameters are loaded from the "," file. In particular, the "
7931,http://cocodataset.org/#home,MS COCO,", ","). For the preprocessed corpora and pre-trained models, see below."
7931,https://github.com/jasonleeinf/non-auto-decoding/blob/96f7765399133c79ad4d23768dd530ee3eb07990/data.py#L44,"
data.py
", function located in ,:
7932,https://devblogs.nvidia.com/parallelforall/how-optimize-data-transfers-cuda-cc/,host-pinned memory, of ,", where dimensions can be given as an argument list of sizes or a "
7933,http://www.cs.cornell.edu/~arb/data/,this web site,The collection of datasets from the paper are available from ,. You can also download them wholesale and use them as follows.
7961,#dataset,Dataset,"
","
"
7961,#prepare-training-data,Prepare Training Data,"
","
"
7961,#prepare-ontonotes-evaluation-data,Prepare OntoNotes Evaluation Data,"
","
"
7961,#prepare-training-data,Prepare Training Data, from Stanford CoreNLP. (Check step 5 in , for configuring CoreNLP)
7961,#prepare-training-data,Prepare Training Data,You will also need to have a CoreNLP parsed wsj corpus. (Check step 5 and 6 in ,)
7974,https://www.researchgate.net/publication/340847577_Learning_double_weights_via_data_augmentation_for_robust_sparse_and_collaborative_representation-based_classification,Paper,", 79, pp. 20617–20638. [",", "
7974,http://d.wanfangdata.com.cn/Periodical/jsjjy201612038,Paper,", ",]
7974,http://d.wanfangdata.com.cn/Periodical/jsjjy201612038,Paper,", ",]
7977,#input-data-format,Input data format,"
","
"
7977,https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2FFlow-Based-Cartograms%2Fgo_cart&text=Create%20your%20own%20cartograms%20today%20-%20and%20see%20what%20your%20world%20really%20looks%20like%21&hashtags=data%2Cmap%2Ccartogram%2Ctech,"
Tweet
","
","
"
7982,https://github.com/mmalekzadeh/motion-sense/tree/master/data,in the current repository,The MotionSense dataset is publicly available , and also 
7986,src/data/preprocessing/README.md,README, module for efficiently decoding videos on the fly. Check the dataprocessing , for more information about how to rescale the videos.
7994,http://www.robots.ox.ac.uk/~vgg/data/flowers/102,flowers, and , and 
7994,http://mscoco.org/dataset/#download,COCO, and , image data.
8002,http://pytorch.org/docs/data.html,torch.utils.data.Dataset, I recommend to write your own dataloader using , since 
8003,./datasets.py,datasets.py,"Dataloaders for FlyingChairs, FlyingThings, ChairsSDHom and ImagesFromFolder are available in ",. 
8004,https://github.com/yoonkim/lstm-char-cnn/blob/master/get_data.sh,Yoon Kim's script,. For ease of use you can use the ,", which downloads these data and saves them into the relevant folders."
8025,http://bollin.inf.ed.ac.uk/public/direct/Refresh-NAACL18-preprocessed-input-data.tar.gz,Preprocessed CNN and DailyMail data,"
",": Articles are tokenized/segmented with the original case. Then, words are replaced with word ids in the word embedding file with (PAD_ID = 0, UNK_ID = 1). (1.9GB)"
8025,http://bollin.inf.ed.ac.uk/public/direct/Refresh-NAACL18-baseline-gold-data.tar.gz,Gold Test and Validation highlights,"
",: These files are used to estimate ROUGE scores. (11MB)
8035,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K dataset,Download DIV2K training data (800 training + 100 validtion images) from , or 
8045,https://www.cityscapes-dataset.com,Link,Cityscapes: ,"
"
8045,http://synthia-dataset.net,Link,Synthia: ,"
"
8045,http://cocodataset.org/#home,Link,Coco: ,"
"
8045,http://www.ipb.uni-bonn.de/data/sugarbeets2016/,Link,Crop-Weed (CWC): ,"
"
8046,https://github.com/TellinaTool/nl2bash/tree/master/data/bash,here,"Our corpus contains a diverse set of Bash utilities and flags: 102 unique utilities, 206 unique flags and 15 reserved tokens. (Browse the raw data collection ",.)
8046,/data,data,"To change the data-processing workflow, go to ", and modify the utility scripts.
8046,https://github.com/TellinaTool/nl2bash/tree/master/data/bash/manual_judgements,the examples annotated with their annotations,"If you run manual evaluation, please release ",. This helps others to replicate the results and reuse these annotations.
8050,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,LIBSVM library,Each .ipynb file corresponds to the particular set of experiments with given dimension of the problem (for Nesterov's function) or given dataset. All datasets are taken from ,.
8054,https://archive.ics.uci.edu/ml/datasets/HIGGS,Higgs,"
","
"
8054,https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption,Power,"
","
"
8067,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/,LIBSVM Data," in the MATLAB terminal, a small demo using dense dataset Adults(a9a)  from ",", to generate a plot shown as below."
8067,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/,LIBSVM Data,There is also a demo for sparse dataset rcv1(binary) from ,", generate a plot as below by running "
8084,http://conda.pydata.org/,Conda,An additional method using , is also possible:
8091,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/,AIDA-CoNLL,The preprocessed , data ('AIDA-PPR-processed.json') is available in the 
8091,data,data, data ('AIDA-PPR-processed.json') is available in the , folder:
8091,data,data,"The system also uses entity-entity features, which can be quickly computed on-the-fly. Here, we provide pre-computed entity-entity features (3 features per entity pair) for the AIDA-CoNLL dataset, which is available in the ", folder ('ent_ent_feats.txt.gz').
8096,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5 Dataset,Download the ," as the source domain, and put it in the "
8096,https://www.cityscapes-dataset.com/,Cityscapes Dataset,Download the ," as the target domain, and put it in the "
8108,https://www.census.gov/topics/population/genealogy/data/2000_surnames.html,Frequently Occurring Surnames from the Census 2000, is a subsampling of a file of ,.
8119,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove embeddings, to download , and 
8119,http://www.cs.cmu.edu/~glai1/data/race/,RACE dataset,Pretrain our model with , for 10 epochs.
8120,https://github.com/explosion/spacy-lookups-data,"
spacy-lookups-data
", or install ," separately. The lookups package is needed to create blank models with lemmatization data, and to lemmatize in languages that don't yet come with pretrained models and aren't powered by third-party libraries."
8123,https://datarep.app.ist.ac.at/61/,Marre et al. 2014,The folder named data contains the retinal data used for Fig. 3 in the ICLR paper. The whole data set can be found in ,. The folder also contains the data generated by the 
8124,http://ai.stanford.edu/~jkrause/cars/car_dataset.html,here,The cars dataset is available ,.
8141,http://datashare.is.ed.ac.uk/handle/10283/1942,Edinburgh DataShare, which also can be found in ,". However, the following script downloads and prepares the second dataset for TensorFlow format:"
8164,https://papers.nips.cc/paper/7587-training-deep-learning-based-denoisers-without-ground-truth-data,Paper and supplementary materials,"
","
"
8166,data/,Data,"
","
"
8166,data/,data folder,More details can be found in the ,.
8179,http://cocodataset.org/#detections-leaderboard,COCO Instance Segmentation Challenge 2017,"', which ranked 1st place of "," , 2nd place of "
8179,http://cocodataset.org/#detections-leaderboard,COCO Detection Challenge 2017," , 2nd place of ", (Team Name: 
8182,https://github.com/PanpanZheng/OCAN/tree/master/data,data/, on two real-world datasets: wiki and credit-card which have been attached in folder ,.
8184,https://github.com/stanford-futuredata/momentsketch,"
companion repository
",. Please see the , for a more readable implementation of the sketch suitable for experimentation and development.
8196,http://www.cvlibs.net/datasets/kitti/index.php,KITTI,"
","
"
8196,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI raw dataset," tasks, the training data is ", and you can download them by the 
8196,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,official script, and you can download them by the ,;
8196,http://www.cvlibs.net/download.php?file=data_odometry_color.zip,KITTI odometry dataset," task, the training data is ", and you should download the calibration files as well as ground truth poses (for evaluation).
8196,https://www.cityscapes-dataset.com/,Cityscapes,"
","
"
8196,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI flow 2015 dataset,Firstly you need to download the , and its 
8196,http://www.cvlibs.net/download.php?file=data_scene_flow_multiview.zip,multi-view extension, and its ,". For replicating our flow results in the paper, a "
8209,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,Kitti Stereo 2015, to download , and 
8209,http://www.cvlibs.net/datasets/kitti/raw_data.php,Kitti Raw, and , datasets.
8214,https://www.cityscapes-dataset.com/,CityScapes dataset for urban scenes,"Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the      different design choices for segmentation. In RTSeg, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The code and the experimental results are presented on the ",.
8259,#part-2-prepare-dataset,Prepare dataset,"
","
"
8259,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI Driving Dataset,The main dataset used in this project is ,. Please follow the instruction in 
8259,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry benchmark,"
"," contains 22 stereo sequences, in which 11 sequences are provided with ground truth. The 11 sequences are used for evaluation or training of visual odometry."
8270,http://cocodataset.org/,COCO, for the above models trained on popular datasets such as , and 
8270,http://luminoth.readthedocs.io/en/latest/usage/dataset.html,Adapting a dataset,See ,.
8273,http://pandas.pydata.org/,pandas,"
","
"
8289,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip,test_LR_bicubic_X4,"Other links for DIV2K, in case you can't find it : ",", "
8289,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip,train_HR,", ",", "
8289,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip,train_LR_bicubic_X4,", ",", "
8289,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip,valid_HR,", ",", "
8289,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip,valid_LR_bicubic_X4,", ",.
8303,https://cocodataset.org/#download,MS Coco,GuessWhat?! relies on two datasets: GuessWhat?! which contains the dialogue inputs and , which contains the image inputs.
8303,guesswhat/src/guesswhat/preprocess_data/extract_img_features.py,python script,"GuessWhat?! requires both to compute the image features from the full image. In order to compute the crop image features(and image features if you would like to compute them from scratch), you need to use the ",", as shown below."
8303,guesswhat/src/guesswhat/preprocess_data/create_dico.py,designated script,"Creating a dictionary is a necessary step before running the model. To create the GuessWhat?! dictionary, you need to use the ", .
8305,https://github.com/mit-nlp/MITIE/releases/download/v0.4/freebase_wikipedia_binary_relation_training_data_v1.0.tar.bz2,"Wikipedia, Freebase",", a high-performance machine-learning library[1], MITIE makes use of several state-of-the-art techniques including the use of distributional word embeddings[2] and Structural Support Vector Machines[3].  MITIE offers several pre-trained models providing varying levels of support for both English, Spanish, and German trained using a variety of linguistic resources (e.g., CoNLL 2003, ACE, ",", and Gigaword). The core MITIE software is written in C++, but bindings for several other software languages including Python, R, Java, C, and MATLAB allow a user to quickly integrate MITIE into his/her own applications."
8316,https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/dataset_files,datasets,Download and extract , such as:
8316,https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre/dataset_files,challenge's dataset page,"Note that this script can take many hours to complete on the whole 60k tracks. For you to play with the data right away, you'll find those features pre-computed on the ",.
8321,http://persoal.citius.usc.es/manuel.fernandez.delgado/papers/jmlr/data.tar.gz,UCI,"
","
"
8321,https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip,HTRU2,"
","
"
8326,#1-getting-data,the getting data section,", we only provide extracted features for the model trained on 28x28 feature maps, so if you want to use the 14x14 feature maps you'll need to extract those features yourself. See ", for details on that. The download options for the script are:
8329,#download-data,Download Data,"
","
"
8329,#data-preparation,Data Preparation,"
","
"
8329,https://sheffieldnlp.github.io/fever/data.html,our website,Download the FEVER dataset from , into the data directory:
8329,#data-preparation,Data Preparation,Run the oracle evaluation for the Decomposable Attention model on the dev set (requires sampling the NEI class for the dev dataset - see ,)
8333,#setup-datamodels,Setup data/models,"To train the model, download the data and word embeddings (see ", above).
8342,https://github.com/yburda/iwae/blob/master/datasets/OMNIGLOT/chardata.mat,link,OMNIGLOT: the dataset can be downloaded from ,;
8342,https://people.cs.umass.edu/~marlin/data/caltech101_silhouettes_28_split1.mat,link,Caltech 101 Silhouettes: the dataset can be downloaded from ,.
8347,https://vissarion.github.io/tutorials/volesti_tutorial_pydata.html,Tutorial given to PyData meetup,"
","
"
8361,#importing-time-series-data,Importing time series data,"
","
"
8361,#evaluating-an-stl-formula-on-data,Evaluating an STL formula on data,"
","
"
8367,#dataset,Dataset,"
","
"
8423,https://github.com/OpenDataExpMechanics/Survey/tree/master/data,data,In the sub folder ," the 11 articles per year (2000-2016), which were utilized in the study, are stored in the bibtex format. In the folder "
8423,https://github.com/OpenDataExpMechanics/Survey/tree/master/data/WoS_export,data/WoS_export," the 11 articles per year (2000-2016), which were utilized in the study, are stored in the bibtex format. In the folder ", the raw exported from Web of Science is stored.
8431,https://github.com/gidariss/FeatureLearningRotNet/blob/master/dataloader.py#L21,dataloader.py,You must download the desired datasets and set in , the paths to where the datasets reside in your machine. We recommend creating a 
8432,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2," using a simple blending approach. As the depth images contain many missing values (corresponding to shiny, bright, transparent, and distant surfaces, which are common in the dataset) we apply a simple crossbilateral filter based on the ", code to fill all but the largest holes. A couple of things to keep in mind:
8438,https://www.cs.ucr.edu/~eamonn/time_series_data/,UCR,", and ", public repositories.
8439,http://ms-multimedia-challenge.com/2016/dataset,MSR-VTT,"
","
"
8441,http://mscoco.org/dataset/#download,MSCOCO dataset,"For ease-of-use, we make pretrained features available for the entire ",. It is not necessary to clone or build this repo to use features downloaded from the links below. Features are stored in tsv (tab-separated-values) format that can be read with 
8458,http://cerc-datascience.polymtl.ca/person/mehdi-taobane/,Mehdi Taobane,", ",", Diane Bernier, "
8465,http://www.cvlibs.net/datasets/kitti/eval_stereo.php,KITTI Stereo,"
","
"
8465,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow,"
","
"
8465,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,Leaderboard Link,"
","
"
8489,https://github.com/BingyaoHuang/single-shot-pro-cam-calib/tree/ismar18/data/calibration-11-13-17/MT/Set10.yml,2D structured light point pairs,"After we calibrate the camera-projector pair, we reconstruct a point cloud using ", and 
8489,https://github.com/BingyaoHuang/single-shot-pro-cam-calib/tree/ismar18/data/calibration-11-13-17/results,calibration data, and ,". To calculate reconstruction accuracy, we also capture the "
8489,https://github.com/BingyaoHuang/single-shot-pro-cam-calib/tree/ismar18/data/calibration-11-13-17/recon-10.ply,ground truth point cloud,". To calculate reconstruction accuracy, we also capture the ", using an Intel RealSense F200 RGBD camera. The point cloud 3D alignment error (Euclidean distance) between the reconstructed point cloud and the ground truth point cloud are given by:
8498,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, files formatted according to the ,. For the faster graph-based parser change directory to 
8498,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, file formatted according to the , with a previously trained model is:
8505,annotation_tool/data/code_solution_labeled_data/source/python_how_to_do_it_by_classifier_multiple_iid_to_code.pickle,Python,Code snippets for , and 
8505,annotation_tool/data/code_solution_labeled_data/source/sql_how_to_do_it_by_classifier_multiple_iid_to_code.pickle,SQL, and ,": A dict of {(question id, code index): code snippet}."
8505,annotation_tool/data/code_solution_labeled_data/source/python_how_to_do_it_by_classifier_multiple_qid_to_title.pickle,Python,Question titles for , and 
8505,annotation_tool/data/code_solution_labeled_data/source/sql_how_to_do_it_by_classifier_multiple_qid_to_title.pickle,SQL, and ,: A dict of {question id: question title}.
8505,annotation_tool/data/code_solution_labeled_data/source/python_how_to_do_it_qid_by_classifier_unlabeled_single_code_answer_qid_to_code.pickle,Python,Code snippets for , and for 
8505,annotation_tool/data/code_solution_labeled_data/source/sql_how_to_do_it_qid_by_classifier_unlabeled_single_code_answer_qid_to_code.pickle,SQL, and for ,): A dict of {question id: accepted code snippet}.
8505,annotation_tool/data/code_solution_labeled_data/source/python_how_to_do_it_qid_by_classifier_unlabeled_single_code_answer_qid_to_title.pickle,Python,Question titles for , and 
8505,annotation_tool/data/code_solution_labeled_data/source/sql_how_to_do_it_qid_by_classifier_unlabeled_single_code_answer_qid_to_title.pickle,SQL, and ,: A dict of {question id: question title}.
8505,data/data_hnn,our processed data, or ,"
"
8505,annotation_tool/data/code_solution_labeled_data/source/python_text_content/,text vocab,Python: ,", "
8505,annotation_tool/data/code_solution_labeled_data/source/python_code_gram5/,code vocab,", ",.
8505,annotation_tool/data/code_solution_labeled_data/source/sql_text_content/,text vocab,SQL: ,", "
8505,annotation_tool/data/code_solution_labeled_data/source/sql_code_gram5/,code vocab,", ",.
8505,data_processing/howto_features.py#L106,here,"The script that extracts features for constructing a ""how-to-do-it"" question type classifier can be found ",. The 250 manually annotated posts for Python and SQL can be found 
8505,annotation_tool/data/question_type_labeled_data/,here,. The 250 manually annotated posts for Python and SQL can be found ," (label '1' denotes ""how-to-do-it""). For details, please refer to Section 2.2.1 in our paper."
8505,data_processing/code_processing.py#L311,here,The script for processing code snippets can be found ,". For details, please read Section 5.1 in our paper. The implementation of the SQL parser is adapted from https://github.com/sriniiyer/codenn."
8505,data/data_hnn/,here,We provide processed training/validation/testing files in our experiments ,.
8510,#additional-datasets,Additional datasets,"
","
"
8510,#creating-and-preprocessing-a-new-java-dataset,creating a new dataset,For , or 
8510,https://github.com/tech-srl/code2seq/blob/master/README.md#datasets,https://github.com/tech-srl/code2seq/blob/master/README.md#datasets," paper, using the code2vec preprocessing. These datasets are available in raw format (i.e., .java files) at ",", and are also available to download in a preprocessed format (i.e., ready to train a code2vec model on) here:"
8516,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
8520,dataset-code/baselines.py,baselines.py,"
",: run and evaluate baseline methods reported in the paper (apart from lang-model)
8520,dataset-code/build_json_dataset.py,build_json_dataset.py,"
",: several functions for creating the dataset from BMJ Case Reports (code for scraping and processing HTML data from the web will be added later)
8520,dataset-code/build_queries.py,build_queries.py,"
",: creating queries from learning points
8520,dataset-code/embedding_eval.py,embedding_eval.py,"
",: embedding metrics for evaluation
8520,dataset-code/evaluate.py,evaluate.py,"
",": evaluation code for EM, F1, BLEU-2, BLEU-4, embedding-average"
8520,dataset-code/evaluation_test.py,evaluation_test.py,"
",: unit test for the evaluation code
8520,dataset-code/expand_answers.py,expand_answers.py,"
",: answer set extension using UMLS
8520,dataset-code/json_to_plain.py,json_to_plain.py,"
",: convert from CliCR json structure to plain format for applying SA and GA readers
8520,dataset-code/randomized_hyperparameter_search.py,randomized_hyperparameter_search.py,"
",: finding optimal parameters for word embeddings and neural readers
8520,dataset-code/refine_json_dataset.py,refine_json_dataset.py,"
",: removing instances matching partly with a part of the passage
8520,dataset-code/run_emb_baseline.py,run_emb_baseline.py,"
",: find the best embedding hyper-parameters by running word2vec
8520,dataset-code/util.py,util.py,"
",: various utilities
8536,./utils/datasets.py,./utils/datasets.py,"In the paper, we present the Continuous task-agnostic Permuted MNIST experiment, where task-switch is performed slowly over time. To create similar experiment, you can use the included sampler - ContinuousMultinomialSampler. The sampler, and the function of creating data for the continuous experiment are at ", (relevant function: ds_padded_cont_permuted_mnist()).
8537,http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html,notMNIST dataset,The , is a image recognition dataset of font glypyhs for the letters A through J useful with simple neural networks. It is quite similar to the classic 
8540,pywiktionary/data,[pywiktionary/data],Use the example XML dump in ,:
8551,https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman,Images (667MB) & Annotations,"
","
"
8552,http://images.cocodataset.org/zips/train2017.zip,COCO 2017 Train images [118K/18GB],"
","
"
8552,http://images.cocodataset.org/zips/val2017.zip,COCO 2017 Val images [5K/1GB],"
","
"
8552,https://github.com/liruilong940607/Pose2Seg/releases/download/data/person_keypoints_train2017_pose2seg.json,COCOPersons Train Annotation (person_keypoints_train2017_pose2seg.json) [166MB],"
","
"
8552,https://github.com/liruilong940607/Pose2Seg/releases/download/data/person_keypoints_val2017_pose2seg.json,COCOPersons Val Annotation (person_keypoints_val2017_pose2seg.json) [7MB],"
","
"
8552,https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman,images [667MB] & annotations,"
","
"
8552,http://images.cocodataset.org/annotations/annotations_trainval2017.zip,COCO2017 Train/Val annotations, (in ,). We choose those instances with both keypoint and segmentation annotations for our experiments.
8561,https://github.com/MISP/misp-taxonomies/tree/main/data-classification,data-classification,"
", : Data classification for data potentially at risk of exfiltration based on table 2.1 of Solving Cyber Risk book. 
8561,https://www.misp-project.org/taxonomies.html#_data_classification,Overview, : Data classification for data potentially at risk of exfiltration based on table 2.1 of Solving Cyber Risk book. ,"
"
8561,https://fpf.org/2016/04/25/a-visual-guide-to-practical-data-de-identification/,visual guide to practical de-identification, : The Future of Privacy Forum (FPF) ," taxonomy is used to evaluate the degree of identifiability of personal data and the types of pseudonymous data, de-identified data and anonymous data. The work of FPF is licensed under a creative commons attribution 4.0 international license. "
8561,https://github.com/MISP/misp-taxonomies/tree/main/information-security-data-source,information-security-data-source,"
", : Taxonomy to classify the information security data sources. 
8561,https://www.misp-project.org/taxonomies.html#_information_security_data_source,Overview, : Taxonomy to classify the information security data sources. ,"
"
8572,docs/data_preparation.md,Data Preparation,"
","
"
8588,http://mscoco.org/dataset/#download,(download),": path to training dataset, the path should point to a folder containing another folder with all the training images. I used COCO 2014 Training images dataset [80K/13GB] ",.
8597,https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.json,SQuAD json file,"Each wav file is indexed by the format ""TopicIndex_ParagraphIndex_SentenceIndex.wav"" which follows the structure in the ",. -- 
8605,making-metadata-changes.md,How to make metadata changes,For porters: ,"
"
8605,FAQ.md#metadata_definition,metadata,"Otherwise, including when a release contains only "," changes, we publish a sub-minor release, e.g. 7.7.3 to 7.7.4."
8634,https://www.slideshare.net/guohuixiao/ontop-answering-sparql-queries-over-relational-databases,Ontology-based Data Access (OBDA) systems, specifically designed for benchmarks of , such as 
8634,http://www.semantic-web-journal.net/content/vig-data-scaling-obda-benchmarks-1,Longest Technical Description and Evaluation of VIG (Semantic Web 10(2): 413-433 (2019)),"
","
"
8645,crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/,Collecting data from threads,"
",: this example demonstrates how the controller can collect data/statistics from crawling threads.
8646,http://cocodataset.org/#download,coco,Download the , image data. Extract them to 
8650,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from , using:
8651,./doc/sample-data.md,sample data,:open_file_folder: Don't have access to a RealSense camera? Check-out ,"
"
8675,"https://www.google.com/maps/place/Featheringill+Hall/@36.1447708,-86.8055131,17z/data=!3m1!4b1!4m5!3m4!1s0x886466bd355f5741:0x3ae06da3496821a0!8m2!3d36.1447708!4d-86.8033244",Featheringill Hall 136,"Event Timing: May 31 - June 2, 2016, 9:00 - 16:00 Event Address: ","
"
8677,https://arxiv.org/help/bulk_data_s3,arXiv data dump,downloading the , from S3 to generate the arXiv paper labels.
8681,http://www.icwsm.org/data/,ICWSM 2011 Spinn3r dataset,The ,"
"
8703,https://vincentarelbundock.github.io/Rdatasets/doc/survival/flchain.html,Flchain,"
","
"
8703,./data,"
data
","For convenience, we provide pre-processing scripts of all datasets (except EHR). In addition, the ", directory contains downloaded 
8703,https://vincentarelbundock.github.io/Rdatasets/doc/survival/flchain.html,Flchain, directory contains downloaded , and 
8726,https://robotcar-dataset.robots.ox.ac.uk/,Oxford Robotcar,Added the rough MATLAB code that was used for submap generation upon requests. Some functions are gotten from the toolbox of ,.
8743,https://github.com/behas/ransomware-dataset,this repository,This repository contains the official procedure for analyzing the expanded ransomware dataset produced by the extraction procedure available in ,.
8743,https://storage.googleapis.com/graphsense-dumps/ransomware/ransomware_dataset_04_11_2018.tar.gz,expanded dataset,Possibility 2a: Download the dataset , and save it into 
8746,#dataset,Dataset,"
","
"
8752,http://publishing-statistical-data.googlecode.com/svn/trunk/specs/src/main/html/cube.html,RDF Data Cube vocabulary,TabLinker is experimental software for converting manually annotated Microsoft Excel workbooks to the ,. It is used in the context of the 
8752,http://www.data2semantics.org,Data2Semantics,. It is used in the context of the , project to investigate the use of Linked Data for humanities research (
8752,http://www.slideshare.net/rinkehoekstra/linked-census-data,available from SlideShare,"A presentation about Linked Census Data, including TabLinker is ",.
8775,https://www.cityscapes-dataset.com/,Cityscapes training set, 2975 images from the ,. (113M) 
8781,https://ember.elastic.co/ember_dataset.tar.bz2,https://ember.elastic.co/ember_dataset.tar.bz2,| Year | Feature Version | Filename                     | URL                                                                                                            | sha256                                                             | |------|-----------------|------------------------------|----------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------| | 2017 | 1               | ember_dataset.tar.bz2        | ,               | 
8781,https://ember.elastic.co/ember_dataset_2017_2.tar.bz2,https://ember.elastic.co/ember_dataset_2017_2.tar.bz2, | | 2017 | 2               | ember_dataset_2017_2.tar.bz2 | , | 
8781,https://ember.elastic.co/ember_dataset_2018_2.tar.bz2,https://ember.elastic.co/ember_dataset_2018_2.tar.bz2, | | 2018 | 2               | ember_dataset_2018_2.tar.bz2 | , | 
8787,http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html,LFW," ,",", "
8787,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,Voxceleb,", ", and 
8809,http://pr.cs.cornell.edu/grasping/rect_data/data.php,Cornell Grasping Dataset,"Currently, both the ", and 
8809,http://pr.cs.cornell.edu/grasping/rect_data/data.php,Cornell Grasping Dataset,Download the and extract ,.
8817,#data,Data,". To faciliate testing the functionality, we have included sample train and test images (8 images for each label) for Melanoma. For training the patch, use the full ",. We couldn't include the pretrained models object due to size. Create a directory 
8820,#how-can-i-use-my-own-dataset,How can I use my own dataset?,"
","
"
8823,https://archive.org/download/armancohan-long-summarization-paper-code/arxiv-dataset.zip,mirror, (,) PubMed dataset: 
8823,https://archive.org/download/armancohan-long-summarization-paper-code/pubmed-dataset.zip,mirror, (,)
8823,https://www.tensorflow.org/datasets/catalog/scientific_papers,Tensorflow Datasets,The dataset is also available on , which makes it easy to use within Tensorflow or colab.
8842,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html,"
pandas.Dataframe
", or ," dataframes. The typical pattern is as follows (for loading, e.g., the "
8842,https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data,"MNIST dataset provided by tensorflow
"," dataframes. The typical pattern is as follows (for loading, e.g., the ",", as already done in "
8842,deepconcolic/datasets.py,"
deepconcolic.datasets
",", as already done in ",):
8842,dc_plugins/toy_datasets/random.py,an example dataset plugin,"For further illustrative purposes, we provide ",", which can be used to randomly generate classification tasks. This plugin registers several datasets (named, e.g., "
8847,#configuring-datasets,Configuring Datasets,"
","
"
8847,#adding-a-dataset,Adding a Dataset,"
","
"
8847,#adding-a-dataset,Adding a Dataset,Methods to import and configure datasets correctly can be found in the section ,.
8874,http://www.phontron.com/data/qi18naacl-dataset.tar.gz,qi18naacl-dataset.tar.gz,Datasets for the specific language pairs used in the experiments mentioned in this paper: ,.
8874,http://phontron.com/data/ted_talks.tar.gz,ted_talks.tar.gz,"The train, dev and test splits for the above TED talks: ",.
8887,https://github.com/gyglim/video2gif_dataset,here,. You can find the implementation on mAP and nMSD ,.
8889,#data,Data Format,"
","
"
8889,sample_data/conllu/sample.conllustag,sample,"The biaffine parser takes as input a file in the Conllu+Supertag (conllustag) format, in which one column for supertags is added to the original conllu format at the end. See a ",.
8889,sample_data/config_demo.json,sample json file,All you need to do is to create a new directory for your data in the conllustag format  and a json file for the model configuration and data information. We provide a , for the 
8889,sample_data,sample, for the , data directory. You can train a parser on the sample data by the following command:
8891,http://www.merlin-platform.eu/C_data.php,original MERLIN corpus,The , is available under a 
8899,https://github.com/vs-uulm/2017-SUEE-data-set/releases/download/v1.1/SUEE1.pcap,SUEE1,| data set      | start date    | duration | hosts | external hosts | internal hosts | internal hosts wifi (eduroam/welcome) | | ------------- |:------------- | -----: |-----: |-----: |-----: |-----: | | ,        | 2017-11-02    | 24 h | 1634 | 1192 | 442 | 243 (97/146) | | 
8899,https://github.com/vs-uulm/2017-SUEE-data-set/releases/download/v1.1/SUEE8.pcap,SUEE8,        | 2017-11-02    | 24 h | 1634 | 1192 | 442 | 243 (97/146) | | ,       | 2017-11-05    |  8 d | 8286 | 6755 | 1531 | 705 (328/377) |
8899,https://github.com/vs-uulm/2017-SUEE-data-set/blob/master/anon.py,anon.py,The IP and MAC addresses of the benign clients were anonymized with ,", all IP addresses in the anonymized data sets are in the 192.168/16 block. The original IP addresses were in part from the Ulm University network and mostly from diverse networks in Ulm and surrounding areas. Keep in mind, that the same IP address in SUEE1 and SUEE8 are not affiliated. However, every packet sent (or received) by an IP within one data set was originally sent (or received) from the same IP address."
8911,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json,train-v1.1.json,"
","
"
8911,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json,dev-v1.1.json,"
","
"
8911,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json,train-v2.0.json,"
","
"
8911,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json,dev-v2.0.json,"
","
"
8911,https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py,WordPiece,": Apply whitespace tokenization to the output of the above procedure, and apply ", tokenization to each token separately. (Our implementation is directly based on the one from 
8911,https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html,Project Guttenberg Dataset, no longer have it available for public download. The , is a somewhat smaller (200M word) collection of older books that are public domain.
8911,https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py,tensor2tensor's WordPiece generation script,"
","
"
8920,https://www.twentybn.com/datasets/something-something,jester dataset,Download the , or 
8920,http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html,ChaLearn LAP IsoGD dataset, or ,. Decompress them into the same folder and use 
8920,process_dataset.py,process_dataset.py,. Decompress them into the same folder and use ," to generate the index files for train, val, and test split. Poperly set up the train, validatin, and category meta files in "
8920,datasets_video.py,datasets_video.py," to generate the index files for train, val, and test split. Poperly set up the train, validatin, and category meta files in ",". Finally, use directory "
8920,https://github.com/metalbubble/TRN-pytorch/blob/master/process_dataset.py,process_dataset.py,", from which we imported ", to our project.
8926,http://www.doc.ic.ac.uk/~sleutene/datasets/elasticfusion/dyson_lab.klg,here,We have provided a sample dataset which you can run easily with ElasticFusion for download ,. Launch it as follows:
8932,https://github.com/huggingface/datasets,"
datasets
", integrates with , to manage task data
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,"
probing tasks",SentEval also includes a series of , to evaluate what linguistic properties are encoded in your sentence embeddings:
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SentLen,| Task     	| Type                         	| #train 	| #test 	| needs_train 	| set_classifier | |----------	|------------------------------	|-----------:|----------:|:-----------:|:----------:| | ,	| Length prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,WC,	| Length prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Word Content analysis	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,TreeDepth,	| Word Content analysis	| 100k     	| 10k    	| 1 | 1 | | ,	| Tree depth prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,TopConst,	| Tree depth prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Top Constituents prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,BShift,	| Top Constituents prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Word order analysis	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,Tense,	| Word order analysis	| 100k     	| 10k    	| 1 | 1 | | ,	| Verb tense prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SubjNum,	| Verb tense prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Subject number prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,ObjNum,	| Subject number prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Object number prediction	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,SOMO,	| Object number prediction	| 100k     	| 10k    	| 1 | 1 | | ,	| Semantic odd man out	| 100k     	| 10k    	| 1 | 1 | | 
8933,https://github.com/facebookresearch/SentEval/tree/master/data/probing,CoordInv,	| Semantic odd man out	| 100k     	| 10k    	| 1 | 1 | | ,	| Coordination Inversion | 100k     	| 10k    	| 1 | 1 |
8944,#211-parsing-noun-similarity-data,2.1.1 Parsing NOUN Similarity Data,"
","
"
8944,#212-parsing-shapes-similarity-data,2.1.2 Parsing Shapes Similarity Data,"
","
"
8944,#214-parsing-shape-features-data,2.1.4 Parsing Shape Features Data,"
","
"
8944,#215-exporting-feature-data-to-csv,2.1.5 Exporting Feature Data to CSV,"
","
"
8944,#22-analysis-of-the-data-set,2.2 Analysis of the Data Set,"
","
"
8944,#31-preparing-the-data-set-for-machine-learning,3.1 Preparing the Data Set for Machine Learning,"
","
"
8944,#311-data-augmentation,3.1.1 Data Augmentation,"
","
"
8944,#314-data-set-creation-for-shapes-study,3.1.4 Data Set Creation for Shapes study,"
","
"
8949,#data,Data,"
","
"
8949,http://dmserv4.cs.illinois.edu/ner_dataset.pk,Download Link,| NER | Chunking | | ------------- |------------- | | , | 
8949,http://dmserv4.cs.illinois.edu/np_dataset.pk,Download Link, | , |
8949,http://dmserv4.cs.illinois.edu/ner_dataset.pk,Download Link,| NER | Chunking | | ------------- |------------- | | , | 
8949,http://dmserv4.cs.illinois.edu/np_dataset.pk,Download Link, | , |
8950,https://github.com/arangesh/HandyNet/blob/master/prepare_data.m,this MATLAB script,3. Create train-val split using ,.
8951,data/README.md,data/README.md,see ,"
"
8960,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data.html,[here],Download the 'English lexical sample' train and test datasets from ,.
8963,data/mini.sprl,data/mini.sprl,", which runs by default on the sample data ",:
8963,data/mini.sprl,data/mini.sprl,predict.py assumes an input file in .json format. See ," for an example. Output is identical .json structure, with additional field for SPR label predictions."
8970,https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data,PennTree Bank,"
",: sentence data
8970,https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/,Stanford Multi-domain Dialog,"
",: human-woz task-oriented dialogs.
8980,#data-configs,"
data configs
","
",: Data configs define a mapping from columns in a one-word-per-line formatted file (e.g. the CoNLL-X format) to named features and labels that will be provided to the model as batches.
8980,config/data_configs/conll05.json,"
conll05.json
",An full example data config can be seen here: ,.
8980,src/data_converters.py,"
src/data_converters.py
"," | json| A json object defining a function (name and, optionally, parameters) for converting the raw input. These functions are defined in ",. | 
8980,src/data_generator.py,"
src/data_generator.py
"," are read in and provided to the given converter. Data generators, which take the data config and data file as input to perform this mapping, are defined in ",.
8980,src/data_converters.py,"
src/data_converters.py
",New converter functions can be defined in ,". At a minimum, every converter function takes two parameters: "
8980,src/dataset.py,"
src/dataset.py
", function in ,.
8981,https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_tf_checkpoint/model.ckpt-935588.data-00000-of-00001,1,"
","
"
8981,https://github.com/allenai/bilm-tf/blob/master/bilm/data.py#L220,see here,"
",". As a result, set "
8989,https://travis-ci.org/epfldata/dblab,"
Build Status
","
","
"
8997,#data,Data,"
","
"
8998,http://mscoco.org/dataset/#download,COCO dataset,Please download , and annotations for the 5k image 
9004,https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering/blob/master/WikidataHowTo.md,here, and wrote a guide on the installation ," (in a different repository). This step takes a lot of time!. Right now this is the only way to run the models at test time, we are working to providing a smaller Wikidata dump just for the training/evaluation on the data sets."
9004,https://public.ukp.informatik.tu-darmstadt.de/starsem18-entity-linking/Wikidata_TransE_50.zip,here,. Download ,.
9009,http://nbviewer.ipython.org/urls/networkit.iti.kit.edu/data/uploads/docs/NetworKit_UserGuide.ipynb,NetworKit UserGuide,"Once you have installed NetworKit, please make sure to check out our ", for an overview of the features provided in NetworKit.
9020,https://www.nlm.nih.gov/databases/download/pubmed_medline.html,MEDLINE, was constructed from the ," dataset, whereas "
9031,https://github.com/shentianxiao/language-style-transfer/tree/master/data/yelp,link,You can find the data used in the sentiment modification experiment described in the paper at this ,". The train, dev, test and classtrain splits are given as is."
9036,https://github.com/eric-xw/AREL/tree/master/data/save,IRL-ini-iter100-*,We uploaded our checkpoints and meta files to the ,. Please load the model from these folders by running
9042,data/templates.tsv,data/templates.tsv,"s may also be replaced with the semantically bleached referent ""someone."" There are 120 templates (60 occupations, two templates per occupation); these are located in ",". Fully instantiated, the templates generate 720 full sentences (120 templates x {female, male, neutral} x {participant, ""someone""}); the 720 sentences are located in "
9042,data/all_sentences.tsv,data/all_sentences.tsv,". Fully instantiated, the templates generate 720 full sentences (120 templates x {female, male, neutral} x {participant, ""someone""}); the 720 sentences are located in ",. They were generated with 
9044,http://opendatacommons.org/licenses/odbl/1.0/,Open Database License,The database is made available under the ,"
"
9044,http://opendatacommons.org/licenses/dbcl/1.0/,Database Contents License,"Any rights in individual contents of the database (i.e., the data) are licensed under the ","
"
9046,https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract-round1.html,neurips.cc,", ","
"
9050,https://github.com/gidariss/FewShotWithoutForgetting/blob/master/dataloader.py#L28,dataloader.py, and set in , the path to where the dataset resides in your machine. We recommend creating a 
9050,https://github.com/gidariss/FewShotWithoutForgetting/blob/master/dataloader.py#L29,dataloader.py," First, you must download the ImageNet dataset and set in ", the path to where the dataset resides in your machine. We recommend creating a 
9052,http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,here,)from ,. Extract it to 
9056,https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip,Glove,Download the pretrained , and the 
9068,docs/src/site/markdown/data-model.md,data model,a JSON ," for render, tile, and transform specifications,"
9077,https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html#sec_datasets,here,Dataset(s) can be downloaded using the list of URLs provided ,.
9093,get_dataset_from_youtube.py,get_dataset_from_youtube.py,"
"," Directy download from youtube the videos and audio files of youtube audioset. Inside the script you will note the following hardcoded files : 2000max_subset_unbalanced_train_segments.csv, subset_eval_segments.csv,etc... This csv files are derived from the audioset "
9099,#input-data-format,Input data format,"
","
"
9132,https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu%20data.zip?dl=0,this dropbox link,Step 1: Download the data. you can download the Ubuntu Dialog Corpus(UDC) data from , used in several 
9134,http://marsyasweb.appspot.com/download/data_sets/,(link),: download the data ,. Download the (.txt) files that list which audios are in every partition 
9134,https://github.com/jongpillee/music_dataset_split/tree/master/GTZAN_split,(link),. Download the (.txt) files that list which audios are in every partition ,. Set the config file: 
9134,https://serv.cusp.nyu.edu/projects/urbansounddataset/urbansound8k.html,(link),: download the data ,". Partitions were already defined by the dataset authors, and we have some code to get those! Just list all the audios in a file, and set the config file: "
9142,https://github.com/jilljenn/ktm/blob/master/dataio.py#L12,dataio.py," and reproduce the experiments of the paper, you want to look at how folds are created in ",.
9142,https://sites.google.com/site/assistmentsdata/home/2009-2010-assistment-data,Assistments 2009,"
","
"
9142,https://jiji.cat/weasel2018/data.csv,reformatted version of the Assistments 2009 dataset,Our ,.
9142,https://jiji.cat/weasel2018/data.csv,Assistments 2009 dataset,You can also download the , into 
9147,https://github.com/whole-tale/girder_wt_data_manager,"
GitHub Project
","
","
"
9147,https://circleci.com/gh/whole-tale/girder_wt_data_manager,"
Build Status
","
","
"
9147,https://codecov.io/gh/whole-tale/girder_wt_data_manager,"
Coverage
","
","
"
9148,#datasets,Datasets,"
","
"
9148,#adding-a-dataset,Adding a dataset,"
","
"
9148,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen,data generator, protocol buffers. All datasets are registered and generated with the , and many common sequence datasets are already available for generation and use.
9148,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem_hparams.py,"
problem_hparams.py
"," (e.g. symbol, image, audio, label) and vocabularies, if applicable. All problems are defined either in ", or are registered with 
9148,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py,"
Problem
","To add a new dataset, subclass ", and register it with 
9148,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py,"
TranslateEndeWmt8k
",. See , for an example.
9148,https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md,data generators README,Also see the ,.
9173,https://github.com/stanford-futuredata/swag-python/,here, (GitHub ,) and 
9175,data/README.md,"instructions on how to process standard datasets like PTB, CTB, and the SMPRL 2013/2014 Shared Task data","Prior to training the parser, you will first need to obtain appropriate training data. We provide ",". After following the instructions for the English WSJ data, you can use the following command to train an English parser using the default hyperparameters:"
9182,http://www.cs.cornell.edu/~arb/data/index.html#pvc,here,The data used in the paper is also available ,.
9183,https://github.com/coseal/aslib_data/tree/master/OPENML-WEKA-2017,OpenML-WEKA-2017 scenario, contains all algorithm runs that were used for the ,.
9204,https://github.com/udacity/self-driving-car/tree/master/datasets/CH2,Udacity dataset,"In order to learn steering angles, the publicly available ", has been used. It provides several hours of video recorded from a car. We additionally recorded an outdoor dataset to learn the probability of collision by riding a bicycle in the streets of our city.
9204,data_preprocessing/time_stamp_matching.py,time_stamp_matching.py, file to extract data corresponding only to the central camera. It is important to sync images and the corresponding steerings matching their timestamps. You can use the script , to accomplish this step.
9204,http://rpg.ifi.uzh.ch/data/collision.zip,here,Collision dataset is ready to be used after downloading. It can be directly downloaded from ,. Its structure is as follows:
9204,http://rpg.ifi.uzh.ch/data/dronet_model.zip,best_model,A folder containing the trained model that we used in our real world experiments can be download here: ,". To evaluate it, use the following command:"
9234,./src_g2s/G2S_data_stream.py,data loader,The ," of our model requires simplified AMR graphs where variable tags, sense tags and quotes are removed. For example, the following AMR"
9234,./data/,./data/,"After having the JSON files, you can extract vocabularies with our released scripts in the ", directory. We also encourage you to write your own scripts.
9243,https://figshare.com/articles/dataset/Agent-based_Planning_Portfolio/7806548,here,Choose dataset. Default is gaussian with 10 agents. You can download other dataset from ,.
9247,https://github.com/vega/datalib,Datalib,This project depends on ," for data processing, "
9250,https://open.quiltdata.com/b/janelia-cosem-publications/tree/heinrich-2021a/,s3://janelia-cosem-publications/heinrich-2021a/,The training data for organelle segmentation can be downloaded from ,"
"
9250,https://open.quiltdata.com/b/janelia-cosem-publications/tree/heinrich-2021a/evaluations,s3://janelia-cosem-publications/heinrich-2021a/evaluations,", you'll need to download the results of the manual evaluation procedures from the s3 bucket: ",. 
9266,https://www.idiap.ch/dataset,EYEDIAP dataset, scripts. The ," is required. If using another dataset, these scripts have to be changed accordingly. The scripts also load the 3D landmarks computed using "
9269,https://sites.google.com/view/reside-dehaze-datasets,here, to where you put the RESIDE SOTS set. Please use absolute path. You can obtain this testing set ,". We used the 1,000-image version of SOTS that contains 500 indoor and 500 outdoor images, and we assume that they are all put into "
9273,http://u.cs.biu.ac.il/~yogo/data/syntemb/bow2.words.bz2,here,"In the default setup, we use SGNS vectors with bag-of-words contexts trained on Wikipedia, available ","
"
9285,http://www.nltk.org/data.html,the data installed,", with ", so that WordNet is available.
9287,#datasets,Datasets,"
","
"
9294,https://hpi.de/naumann/projects/repeatability/data-profiling/metanome-ind-algorithms.html,other CSV files,"
","
"
9294,https://archive.ics.uci.edu/ml/datasets/HIGGS,HIGGS,"
","
"
9294,http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,Other datasets,"
","
"
9294,https://github.com/rheem-ecosystem/rheem-benchmark/blob/master/src/test/resources/kmeans-datagenerator.py,data generator, We provide a , to generate files that can be clustered. You can further load these files into the database assuming the following schema:
9317,https://ebiquity.umbc.edu/resource/html/id/351/UMBC-webbase-corpus,UMBC,"To build everything from scratch, first download corpora such as Wikipedia, ",", and "
9322,#prepare-datasets,Prepare datasets,"
","
"
9329,https://www.cityscapes-dataset.com/,official website,"We use the Cityscapes dataset. To train a model on the full dataset, please download it from the "," (registration required). After downloading, please put it under the "
9334,numba.pydata.org,numba,This requires Python 3 and the , package. The easiest way is to use the 
9335,https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md,OASIS data, on ,", which we processed and released for free for HyperMorph."
9335,data/readme.md#models,here,See list of pre-trained models available ,.
9335,https://github.com/adalca/medical-datasets,a list of medical imaging datasets here,We encourage users to download and process their own data. See ,". Note that you likely do not need to perform all of the preprocessing steps, and indeed VoxelMorph has been used in other work with other data."
9335,https://surfer.nmr.mgh.harvard.edu/ftp/data/voxelmorph/synthmorph/shapes-dice-vel-3-res-8-16-32-256f.h5,"""shapes"" variant",We provide model files for a ," of SynthMorph, that we train using images synthesized from random shapes only, and a "
9335,https://surfer.nmr.mgh.harvard.edu/ftp/data/voxelmorph/synthmorph/brains-dice-vel-0.5-res-16-256f.h5,"""brains"" variant"," of SynthMorph, that we train using images synthesized from random shapes only, and a ",", that we train using images synthesized from brain label maps. We train the brains variant by optimizing a loss term that measures volume overlap of a "
9335,https://surfer.nmr.mgh.harvard.edu/ftp/data/voxelmorph/synthmorph/fs-labels.npy,selection of brain labels,", that we train using images synthesized from brain label maps. We train the brains variant by optimizing a loss term that measures volume overlap of a ",". For registration with either model, please use the "
9335,https://surfer.nmr.mgh.harvard.edu/ftp/data/voxelmorph/synthmorph/ref.nii.gz,reference image,"Accurate registration requires the input images to be min-max normalized, such that voxel intensities range from 0 to 1, and to be resampled in the affine space of a ",". The affine registration can be performed with a variety of packages, and we choose FreeSurfer. First, we skull-strip the images with "
9335,https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md,re-released OASIS1,"While we cannot release most of the data used in the voxelmorph papers as they prohibit redistribution, we thorough processed and ", while developing 
9338,data/acdc.lineage,acdc.lineage,Instruction to download the data are contained in the lineage files ,", "
9338,data/zenodo_spine.lineage,zenodo_spine,", ", and 
9338,data/prostate.lineage,prostate.lineage, and ,. They are just text files containing the md5sum (or sha256sum) of the original zip.
9341,https://github.com/gabrielStanovsky/oie-benchmark/blob/master/snapshot_oie_corpus_with_pronouns.tar.gz,here,"Since the publication of this resource, we made several changes, outlined below. The original version of the corpus, with 10,359 extractions, as reported in the paper, is available ",.
9341,https://dada.cs.washington.edu/qasrl/#dataset,QA-SRL corpus,"
", and place it under 
9341,oie_corpus,oie_corpus,"If everything runs fine, this should create an Open IE corpus (split between wiki and newswire domain) under ",. A snapshot of the corpus is also 
9341,snapshot_oie_corpus.tar.gz,available,. A snapshot of the corpus is also ,.
9341,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/software/clausie/,ClausIE,"
","
"
9342,https://github.com/dair-iitd/OpenIE-standalone/tree/master/data,[data],"
","
"
9356,https://jupyterhub.readthedocs.io/en/latest/reference/database.html?highlight=NFS#sqlite,Jupyterhub-Notes and Tips: SQLite,Are you running on an NFS filesystem? It's possible for Jupyter to experience issues due to varying implementations of the fcntl() system call. (See also ,)
9358,https://github.com/calpolydatascience/jupyterhub-deploy-data301,this deployment,The repository started from ," of JupyterHub for ""Introduction to Data Science"" at Cal Poly. It is designed to be a simple and reusable JupyterHub deployment, while following best practices."
9362,http://cemantix.org/data/ontonotes.html,CoNLL-2012,You have to follow the instructions below to get CoNLL-2012 data ,", this would result in a directory called "
9364,https://github.com/snakeztc/NeuralDialog-ZSDG/tree/master/data/simdial,SimDial Data,"
",: synthetic multi-domain dialog generator. The data generator can be found 
9364,https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/,Stanford Multi-domain Dialog,"
",: human-woz task-oriented dialogs.
9371,https://github.com/rizwan09/awd-lstm-lm/tree/master/data,data path,1. Setting the ," accordingly, we use command "
9371,https://github.com/rizwan09/awd-lstm-lm/tree/master/raw_data,uncleaned data is here," with default params to train baseline AWD_LSTM model, and type model. ",.
9371,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-2,. The model comes with instructions to train a word level language model over the Penn Treebank (PTB) and ," (WT2) datasets, though the model is likely extensible to many other datasets. The model can be composed of an LSTM or a "
9372,http://coai.cs.tsinghua.edu.cn/hml/dataset/,here,You can download our data ,". Press ctrl+f and search for ""Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders"" and you can find the link to our dataset."
9391,https://www.yelp.com/dataset/challenge,Yelp,"
","
"
9391,https://snap.stanford.edu/data/web-FineFoods.html,Amazon,"
","
"
9392,https://snap.stanford.edu/data/index.html,SNAP dataset collection,Graphs from the , are commonly used for graph algorithm benchmarks. We provide a tool that converts the most common SNAP graph format to the adjacency graph format that GBBS accepts. Usage example:
9399,#semeval-task-data,see the section on task data above, (,") for data download, you first need to get the trial data "
9399,https://competitions.codalab.org/my/datasets/download/d8e0b7e1-1c4f-4171-93e9-74339e6c759e,here,") for data download, you first need to get the trial data ",.
9399,#running-and-evaluating-the-amore-upf-model-on-the-semeval-test-data,Section above,. See the , for the description of the files stored therein.
9399,#running-and-evaluating-the-amore-upf-model-on-the-semeval-test-data,Section above,See the , for details.
9405,https://industrial-data-space.github.io/trusted-connector-documentation/docs/dev_core/,Github documentation page,Please see the ,"
"
9405,https://github.com/industrial-data-space/trusted-connector/blob/develop/.github/CONTRIBUTING.md,contribution guide,Please refer to the ,"
"
9415,http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html,here,. The content of the documents can be extract from the GOV2 dataset which need permission from ,.
9422,https://github.com/EagleW/ACL_titles_abstracts_dataset,ACL_titles_abstracts_dataset,"
","
"
9427,http://inverseprobability.com/2014/07/01/open-data-science/,Open Data Science," and Google sheets was created (initially just for reading, then later for updating). This made it much easier to import reviewer suggestions and export information about paper statuses to reviewers. That software has been spun out as part of a suite of tools for ", that is 
9450,#train-on-custom-dataset,Train on Custom Dataset,"
","
"
9451,https://github.com/feichtenhofer/twostreamfusion#data,https://github.com/feichtenhofer/twostreamfusion#data,For HMDB [1] and UCF101 [2] datasets: ,.
9454,http://www.cs.ubc.ca/~mbrown/patchdata/patchdata.html,Phototour Patch dataset,This repository contains reference source code for evaluating MatchNet models on ,.
9473,https://seaborn.pydata.org/,seaborn,"
", - helper plotting library for some charts
9473,https://pandas.pydata.org/,Pandas,"
", - helper data manipulation library
9478,http://alt.qcri.org/semeval2014/task1/data/uploads/sick_train.zip,SICK, you will also need , data.
9481,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Driving, - ,"
"
9499,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,GTSRB,"
",: download the training dataset and test dataset from the 
9499,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,website,: download the training dataset and test dataset from the ,.
9499,https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/data,ImageNet,"
",: scripts are provided to download the dataset.
9503,http://www.garrickorchard.com/datasets/n-mnist,the link,Get the N-MNIST dataset by ,. Then unzip the ''Test.zip'' and ''Train.zip''.
9517,https://pypi.python.org/pypi/chatterbot-corpus/,"
Package Version
","
","
"
9517,https://travis-ci.org/gunthercox/chatterbot-corpus,"
Build Status
","
","
"
9517,http://chatterbot.readthedocs.io/en/latest/training.html#training-with-corpus-data,project documentation,"For instructions on how to use these data sets, please refer to the ",.
9520,data/,"
Referring Expressions
","
","
"
9526,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,this repo, folders with 1000 subfolders each). See , for detailed instructions how to download and set up the dataset.
9532,https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons,10x Genomics,"
","
"
9532,https://archive.ics.uci.edu/ml/datasets/gas+sensor+array+drift+dataset,Gas sensors,"
","
"
9536,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html,obtained here,This is a freely available dataset set and can be ,. It contains m4a-format audiofiles which need to be converted to .wav files to enable processing. You may use the audio_converter.py script to do so or alternatively use a tool of your choice (e.g. 
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb,This directory contains code to import and evaluate the speaker identification and verification models pretrained on the ,(1 & 2) datasets  as described in the following papers (
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/models/vggvox_ident_net.mat,Model,"
", trained for identification on VoxCeleb1
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/models/vggvox_ver_net.mat,Model,"
", trained for verification on VoxCeleb1
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb2/ver_net.mat,Model,"
", trained for verification on VoxCeleb2 (this is a resnet based model)
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb,These models have been pretrained on the , (
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,1, (,&
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html,2,&,") datasets. VoxCeleb contains over 1 million utterances for 7,000+ celebrities, extracted from videos uploaded to YouTube. The speakers span a wide range of different ethnicities, accents, professions and ages. The dataset can be downloaded directly from "
9537,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,here,") datasets. VoxCeleb contains over 1 million utterances for 7,000+ celebrities, extracted from videos uploaded to YouTube. The speakers span a wide range of different ethnicities, accents, professions and ages. The dataset can be downloaded directly from ",.
9540,https://archive.ics.uci.edu/ml/datasets/isolet,Isolet,"
","
"
9541,http://cseweb.ucsd.edu/~yaq007/dataset.zip,here,A small subset of the BRATS dataset (after all the above data pre-processing) is provided , to run the preset examples.
9555,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford5k,"
", consists of 5062 images collected from Flickr by searching for particular Oxford landmarks.
9555,http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris6k,"
"," consists of 6412 images collected from Flickr by searching for particular Paris landmarks. In our experiments, we delete the 20 corrupted images and use the other "
9555,http://lear.inrialpes.fr/~jegou/data.php#holidays,Inria Holidays,"
", is a set of images which mainly contains some holidays photos.
9559,/data,Precomputed image features,"
",: ResNext-101
9559,data/nuswide100,NUS-WIDE100,"
",: An extra test set.
9559,data/coco-cn_ext.icap2020.txt,new annotations,2021-02-03: Release of ," (4,573 images and 4,712 manually written sentences) collected via our "
9567,https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py,this script,"The data is expected to be formatted in TFRecord format, as generated by ",.
9588,http://cocodataset.org/#download,official COCO dataset website,Please follow the , to download the dataset. After downloading the dataset you should have the following directory structure:
9595,https://medium.com/the-downlinq/building-extraction-with-yolt2-and-spacenet-data-a926f9ffac4f,Blog3: Building Extraction with YOLT2 and SpaceNet Data,"
","
"
9598,https://github.com/componavt/wcorpus/wiki/SQL#machine-readable-database-schema,WCorpus database schema,See , (tables and relations).
9598,https://github.com/componavt/wcorpus.py,wcorpus.py,"
", ‒ Python scripts for text corpus analysis which work with WCorpus database.
9598,https://meta.wikimedia.org/wiki/Community_Wishlist_Survey_2019/Wiktionary/Insert_attestation_exploiting_Wikisource_as_a_corpus,Insert attestation exploiting Wikisource as a corpus,"
", // Community Wishlist Survey 2019
9605,https://www.cs.ubc.ca/research/kmyi_data/files/2018/lf-net/pretrained.tar.gz,pretrained models,Download                             the                            , and the                                                                
9605,https://www.cs.ubc.ca/research/kmyi_data/files/2018/lf-net/sacre_coeur.tar.gz,scare_coeur sequence, and the                                                                ,. Extract them to the current folder so that they fall under 
9605,https://www.cs.ubc.ca/research/kmyi_data/files/2018/lf-net/lfnet-norotaug.tar.gz,pretrained model without rotation augmentation ,Download ,"
"
9610,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,For ,", first download the dataset using this "
9610,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,", first download the dataset using this "," provided on the official website, and then run the following command."
9610,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI2015,"For testing optical flow ground truths on KITTI, download ", dataset. You need to download 1) 
9610,https://www.cityscapes-dataset.com/,Cityscapes,For ,", download the following packages: 1) "
9610,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry,"For pose evaluation, you need to download ", dataset.
9614,https://quickdraw.withgoogle.com/data,quickdraw.withgoogle.com/data,". The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located. You can browse the recognized drawings on ",.
9614,#the-raw-moderated-dataset,The raw moderated dataset,"
","
"
9614,#preprocessed-dataset,Preprocessed dataset,"
","
"
9614,#get-the-data,Get the data,"
","
"
9614,#projects-using-the-dataset,Projects using the dataset,"
","
"
9614,https://github.com/googlecreativelab/quickdraw-dataset/issues/19#issuecomment-402247262,See here for code snippet used for generation,". These images were generated from the simplified data, but are aligned to the center of the drawing's bounding box rather than the top-left corner. ",.
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/,Cloud , files seperated by category. See the list of files in ,", or read more about "
9614,https://cloud.google.com/storage/docs/access-public-data,accessing public datasets,", or read more about "," using other methods. As an example, to easily download all simplified drawings, one way is to run the command "
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/raw,Raw files,"
", (
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/simplified,Simplified drawings files,"
", (
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/binary,Binary files,"
", (
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap,Numpy bitmap files,"
", (
9614,https://console.cloud.google.com/storage/browser/quickdraw_dataset/sketchrnn,Numpy .npz files,"
","
"
9614,https://github.com/gxercavins/dataflow-samples/tree/master/quick-draw,DataFlow processing,"
", by Guillem Xercavins
9618,http://buildingparser.stanford.edu/dataset.html#Download,S3DIS,Download and unzip the , instance segmentation data in your preferred location. (The data is provided by 
9619,http://dblp.uni-trier.de/faq/Under+what+license+is+the+data+from+dblp+released.html,Open Data Commons ODC-BY 1.0,. This data is available under the , license. The pickle files 
9661,https://sourceforge.net/projects/moa-datastream/files/MOA/2013%20August/moa-dev-13-11.jar,the new developer version of MOA,"
", | 
9661,https://sourceforge.net/projects/moa-datastream/files/MOA/2013%20August/samoa-moa.jar,samoa-moa.jar,"
","
"
9670,https://github.com/marcotcr/anchor/blob/master/notebooks/Anchor%20on%20tabular%20data.ipynb,Tabular data,"
","
"
9678,http://visionandlanguage.net/VIST/dataset.html,VIST homepage,"
","
"
9679,"https://twitter.com/intent/tweet?text=iNNvestigate%20neural%20networks!&url=https://github.com/albermax/innvestigate&hashtags=iNNvestigate,artificialintelligence,machinelearning,deeplearning,datascience","
Tweet
","
","
"
9680,www.newsreader-project.eu/results/data/the-ecb-corpus/,dataset,The ECB+ ,"
"
9697,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet test set,You can download test images from ,.
9704,https://github.com/OpenNMT/IntegrationTesting/tree/master/data,OpenNMT/IntegrationTesting,: An English-German translation model based on the 200k sentence dataset at ,. Perplexity: 20.
9724,http://software.seg.org/datasets/2D/2004_BP_Vel_Benchmark/,BP2004, etc.) and test on arbitrary image patches from , velocity dataset. We show that our method does not try to 
9730,http://www-personal.umich.edu/~timtu/Downloads/imagenet_npy/imagenet_test_data.npy,ImageNet Test images,"
","
"
9733,https://www.tensorflow.org/get_started/mnist/beginners#the_mnist_data,MNIST For ML Beginners,TFKeras is based on simplified ," and cnn.py. TFKeras.py, TFKeras.h5 & TFKpredict only uses Dense and so is less accurate than CNN. TFKpredict is a slimmed version of cnnPredict."
9741,https://ieee-dataport.org/competitions/day-ahead-electricity-demand-forecasting-post-covid-paradigm,DAY-AHEAD ELECTRICITY DEMAND FORECASTING: POST-COVID PARADIGM,(2021-05-11) Competition: ,"
"
9763,https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics-aida,AIDA project,"Development has been supported by The Royal Society, the EPSRC project LogMap, the EU FP7 projects SEALS and Optique, the ",", and the "
9769,acceptability_corpus/raw,acceptability_corpus/raw,Training and validation sets for CoLA are available under , with a tokenized version available under 
9769,acceptability_corpus/tokenized,tokenized, with a tokenized version available under ,. Test data (unlabeled) is available here: 
9787,https://www.math.colostate.edu/~king/software/Musisep-data.zip,https://www.math.colostate.edu/~king/software/Musisep-data.zip,The input data can be obtained from: ,.  It should be unzipped to the main directory.
9799,http://pandas.pydata.org/pandas-docs/stable/,Pandas,"
","
"
9799,https://seaborn.pydata.org/api.html,Seaborn,"
","
"
9817,http://www.slideshare.net/AmazonWebServices/open-data-innovation-building-on-open-data-sets-for-innovative-applications,NAIPs come from a requester pays bucket on S3 set up by Mapbox,The ,", and the OSM extracts come "
9817,https://github.com/developmentseed/skynet-data,Skynet Data,"
", - (spring 2016) - data pipeline for machine learning with OpenStreetMap
9826,http://www.mathieuramona.com/wp/data/jamendo/,Jamendo,"
"," with the same labeling, train/valid/test set split as described in the website."
9839,#prepare-datasets,that you have prepared, will contain example volume mounts for the datasets. You will need to edit the entries for datasets ,", and remove the others."
9839,http://gvv.mpi-inf.mpg.de/3dhp-dataset/,the original MPI-INF-3DHP dataset,Download ,.
9840,./data/downloadDataset.md,downloadDataset,Refer to , for data download instructions
9840,./data/downloadDataset.md,Data Dowload,Please first refer to ," and download all the data needed, Go to root of this project "
9847,/data/generate_trainingset_x234.m,Code for Data Generation,"the training data is generated with Matlab Bicubic Interpolation, please refer ", for creating training files.
9847,/data/Train_291,291, trained on ," images with data augmentation. The model can achieve a better performance with a smart optimization strategy. For the DRRN_B1U9 implementation, you can manually modify the number of recursive blocks "
9848,https://github.com/twtygqyy/pytorch-vdsr/tree/master/data,Code for Data Generation,"We provide a simple hdf5 format training sample in data folder with 'data' and 'label' keys, the training data is generated with Matlab Bicubic Interplotation, please refer ", for creating training files.
9849,https://github.com/adaa-polsl/GuideR/tree/master/datasets,datasets,"For convenience, we provide "," invesitgated in the GuideR paper, together with the corresponding "
9855,https://datatracker.ietf.org/doc/html/rfc9000,RFC9000,We added probe modules for IPv4 and IPv6 to detect QUIC capable hosts based on the Version negotiation as described in ,"
"
9856,http://www.h2database.com/,H2 Database Engine,An implementation of the , that includes a 
9864,https://eng.uber.com/modeling-censored-time-to-event-data-using-pyro/,time-to-event modeling, and , in Pyro.
9891,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,KITTI,. We ranked 1st place on both , and 
9891,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,KITTI,"
","
"
9895,http://sites.google.com/view/assistmentsdatamining/,2017 ASSISTments Data Mining competition\footnote,The ," aims to use data from a longitudinal study for predicting a brand-new outcome of students which had never been studied before by the educational data mining research community. Specifically, it facilitates research in developing predictive models that predict whether the first job of a student out of college belongs to a STEM (the acronym for science, technology, engineering, and mathematics) field. This is based on the student's learning history on "
9913,dataset/entity_typing,dataset/entity_typing,The dataset used in this experiment is contained in the , directory.
9927,https://sigsep.github.io/datasets/musdb.html,full MUSDB18 dataset,Download the ," and extract it into a folder of your choice. It should have two subfolders: ""test"" and ""train"" as well as a README.md file."
9929,http://corpus-texmex.irisa.fr/,BigANN,"To benchmark our method, we use the two standard benchmark datasets ", and 
9936,https://github.com/basveeling/pcam/blob/master/keras_pcam/dataset/pcam.py,General dataloader for keras,"
","
"
9940,http://nlp.stanford.edu/data/glove.840B.300d.zip,zipped GloVe file, (download will be skipped if , is manually placed in 
9943,https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/,Matterport3D,"The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released ", dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples 
9943,samples/coco/inspect_data.ipynb,inspect_data.ipynb,"
",. This notebook visualizes the different pre-processing steps to prepare the training data.
9943,samples/coco/inspect_data.ipynb,inspect_data.ipynb,"To help with debugging and understanding the model, there are 3 notebooks (",", "
9943,http://cocodataset.org/#home,MS COCO Dataset,"
","
"
9943,https://github.com/rbgirshick/py-faster-rcnn/blob/master/data/README.md,Faster R-CNN implementation, subsets. More details in the original ,.
9943,https://www.kaggle.com/c/data-science-bowl-2018,2018 Data Science Bowl,. Built for the ,"
"
9945,./download_dataset.sh,download_dataset.sh,see , for more datasets
9958,http://nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/metadata/26103042-50a1-4ee5-9c5b-857a2f8b7680,here,. Catalog info on the data set is ,. The ETL script is at 
9958,http://nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/metadata/1c0dcc64-91aa-4d44-a9e3-54355556f5e7?tab=relations,here," Web Feature Service. It uses a pre-configured spatial data set that already links functions to building, functions that are normally connected to functional sections within buildings - a building can have multiple functions of course. The catalog info on the data set is ",.
9967,http://www.cs.cmu.edu/~glai1/data/race/,RACE: Large-scale ReAding Comprehension Dataset From Examinations,"
","
"
9967,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe: Global Vectors for Word Representation,"
","
"
9983,#dataset-information,Dataset information,"
","
"
9985,#corpus-and-digital-resources,Corpus and digital resources,"
","
"
9985,http://www.corpus.unam.mx/axolotl/,"Nahuatl-Spanish, Axolot","
", Parallel Nahuatl - Spanish
9985,http://chana.inf.pucp.edu.pe/resources/parallel-corpus/,Shipibo-Konibo Spanish,"
", Parallel corpus.
9985,https://github.com/mingjund/mapudungun-corpus,Mapundung Speech and parallel corpus,"
","
"
9985,http://www.corpus.unam.mx/geco/portal/index/cplm,Mexican Languages Parallel Corpus,"
","
"
9988,#task-and-data,Task and data,"
","
"
9988,#data-format-for-sparkml,Data format for Spark.ML,"
","
"
9988,http://labs.criteo.com/2015/03/criteo-releases-its-new-dataset/,Criteo released,Our target application is prediction of click-through ratio (CTR) of banners in online advertising. , an industry-standard open dataset which represents banner impressions in online advertising during the timespan of 24 days. It is more than 1 terabyte in size and consists of more than 4 billion lines of data. Each line represents a banner impression and contains 40 columns separated by tabulation:
9988,https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science,one-hot-encoding,We tried to use ," of categorical features, but due to very large number of unique values it turned out to be very time and memory consuming, so for Spark.ML we decided to try the hashing trick. Spark.ML LogisticRegression was trained using this approach. We sticked to hashing space of 10⁵ hashes as it turned out to give about the same quality as VW on large samples. Taking less hashes usually leads to better quality on smaller data (because of less overfitting) and worse quality on bigger data (because some patterns in data are consumed by collisions in hashing space):"
9989,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#kdd2012,KDD 2012,"
","
"
9989,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#rcv1.binary,RCV1,"
","
"
9989,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#webspam,Webspam - Trigram,"
","
"
9989,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#criteo_tb,Criteo 1TB,"
","
"
9989,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#splice-site,Splice-Site 3.2TB,"
","
"
10001,sample_data,sample_data,You can refer the data format in ,.
10001,sample_data/train.cappos.bmes,train.cappos.bmes,"). In addition, handcrafted features have been proven important in sequence labeling tasks. NCRF++ allows users designing their own features such as Capitalization, POS tag or any other features (grey circles in above figure). Users can configure the self-defined features through configuration file (feature embedding size, pretrained feature embeddings .etc). The sample input data format is given at ",", which includes two human-defined features "
10007,https://numbbo.github.io/data-archive/bbob/,coco-algorithms, test suite at , and for the 
10007,https://numbbo.github.io/data-archive/bbob-biobj/,coco-algorithms-biobj, test suite at ,". For other test suites, please see the "
10007,https://numbbo.github.io/data-archive/,COCO data archive,". For other test suites, please see the ",.
10017,https://github.com/data61/PSL/,PSL and all that,"The main developer of this repository, Yutaka, has taken a full-time position at a private company, and he works on "," only in his spare time. Therefore, the progress of this project, unfortunately, will be slow for the foreseeable future. In case you find problems and requests about data61/PSL, contact Yutaka (email: united.reasoning+gmail.com (reaplace + with @), twitter: "
10024,data/,data/,"Generating binary data, please follow the script under ",", i have provide a "
10027,https://agreementdatabase.com,LexPredict Agreement Database,"OpenEDGAR is a comprehensive framework for building databases from EDGAR, and can automate the retrieval and parsing of EDGAR forms.  OpenEDGAR uses the same software that powers many of our data products, including the ",.
10029,https://www.cityscapes-dataset.com/file-handling/?packageID=1,gtFine_trainvaltest.zip,Annotations: , (241MB)
10029,https://www.cityscapes-dataset.com/file-handling/?packageID=3,leftImg8bit_trainvaltest.zip,Images: , (11GB)
10029,https://pantheon.corp.google.com/storage/browser/hpunet-data/lidc_crops,data link," can be downloaded as pngs, cropped to size 180 x 180 from Google Cloud Storage, see here: ",.
10030,/datasets,datasets,Benchmark problems for various solvers above can be found in ,.
10041,https://github.com/apmoore1/Bella/blob/master/notebooks/datasets.ipynb,dataset notebook, to state where the datasets are stored like we did but this is not a requirement as you can state where they are stored explictly in the code. For more details on the datasets and downloading them see the , The datasets used:
10041,http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools,SemEval 2014 Resturant dataset,"
",. We used Train dataset version 2 and the test dataset of which the gold standatd test can be found 
10041,http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-test-data-gold-annotations/b98d11cec18211e38229842b2b6a04d77591d40acd7542b7af823a54fb03a155/,here,. We used Train dataset version 2 and the test dataset of which the gold standatd test can be found ,.
10041,http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools,SemEval 2014 Laptop dataset,"
",. We used Train dataset version2 and the test dataset of which the gold standard test can be found 
10041,http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-test-data-gold-annotations/b98d11cec18211e38229842b2b6a04d77591d40acd7542b7af823a54fb03a155/,here,. We used Train dataset version2 and the test dataset of which the gold standard test can be found ,.
10041,https://figshare.com/articles/EACL_2017_-_Multi-target_UK_election_Twitter_sentiment_corpus/4479563/1,Election dataset,"
","
"
10041,https://github.com/bluemonk482/tdparse/tree/master/data/lidong,Twitter dataset,"
","
"
10041,https://github.com/apmoore1/Bella/blob/master/notebooks/Mitchel%20et%20al%20dataset%20splitting.ipynb,Mitchell, Before using Mitchell and YouTuBean datasets please go through these pre-processing notebooks: ,"
"
10041,https://github.com/apmoore1/Bella/blob/master/notebooks/YouTuBean%20dataset%20splitting.ipynb,YouTuBean,"
", for splitting their data and also in Mitchell case which train test split to use.
10041,./notebooks/datasets.ipynb,notebook,The best order to look at the notebooks is first look at the data with this ,. Then looking at the 
10041,./notebooks/datasets.ipynb,notebook,For the statistics of the datasets and where to find them see this ,"
"
10041,./notebooks/YouTuBean%20dataset%20splitting.ipynb,notebook,For the code on creating training and test splits for the YouTuBean dataset see this ,"
"
10041,./notebooks/Mitchel%20et%20al%20dataset%20splitting.ipynb,notebook, dataset see this ,"
"
10046,https://ai.baidu.com/broad/download?dataset=traffic,link, Code：umqd. Backup ,.
10046,src/dataloader.py,dataloader.py,"
",": Data processing and loading, subject to change due to data format if necessary"
10059,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,Download GTSRB from , and get the training 'Images and annotations' (
10061,pytorch/mean_teacher/data.py#L98,TwoStreamBatchSampler,It's useful to dedicate a portion of each minibatch for labeled examples. Then the supervised training signal is strong enough early on to train quickly and prevent getting stuck into uncertainty. In the PyTorch examples we have a quarter or a half of the minibatch for the labeled examples and the rest for the unlabeled. (See , in Pytorch code.)
10063,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove embeddings,"
","
"
10064,http://nlp.stanford.edu/data/glove.6B.zip,GloVe embeddings,Download the , and uncompress.
10079,https://pandas.pydata.org/,Pandas,"
","
"
10086,https://cschenck.github.io/SmoothParticleNets/docs/reorderdata,ReorderData,"
","
"
10090,https://fabienbaradel.github.io/masks_data/,Complementary Mask Data, | ,"
"
10090,https://fabienbaradel.github.io/masks_data/,website,Please visit the following , for downloading the mask predictions.
10106,#datasets,Datasets,"
","
"
10106,http://www.uco.es/grupos/ayrna/ucobigfiles/datasets-orreview.zip,datasets," folder includes different configuration files for running all the algorithms. In order to use these files, the ", used in the previously cited review paper are needed. To add your own method see 
10106,exampledata,example-data,The , folder includes partitions of several small ordinal datasets for code testing purposes. We have also collected 44 publicly available ordinal datasets from various sources. These can be downloaded from: 
10106,http://www.uco.es/grupos/ayrna/ucobigfiles/datasets-orreview.zip,datasets-OR-review, folder includes partitions of several small ordinal datasets for code testing purposes. We have also collected 44 publicly available ordinal datasets from various sources. These can be downloaded from: ,. The link also contains data partitions as used in different papers in the literature to ease experimental comparison. The characteristics of these datasets are the following:
10106,http://ntucsu.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances,libsvm-weights-3.12,"
",: framework used for Support Vector Machine algorithms. The version considered was 3.12.
10130,http://www.caida.org/data/as-relationships/,AS relationships,"
",". Both serial-1 and serial-2 can be used, but serial-2 is more complete and therefore preferred."
10130,http://www.caida.org/data/ixps/,IXPs Dataset,"
",", in particular the "
10137,https://github.com/Shen-Lab/DeepAffinity/tree/master/data_DeepRelations,data,We have not released the code but have already done so for the , labeled with both the affinity and the explanation of the affinity (binary residue-atom contacts).)
10137,https://github.com/Shen-Lab/DeepAffinity/blob/master/data/dataset/uniprot.human.scratch_outputs.w_sps.tab_corrected.zip,zip,"(Aug. 21, 2020) We are now providing SPS (Structure Property-annotated Sequence) for all human proteins! ",  (Credit: Dr. Tomas Babak at Queens University).  Columns: 1. Gene identifier 2. Protein FASTA  3. SS (Scratch)  4. SS8 (Scratch)  5. acc (Scratch)  6. acc20  7. SPS
10148,#data,Data,"
","
"
10148,http://gigadb.org/dataset/100439,GigaDB, files are now application free to download at ,. But still please contact the Camelyon16 organizers for data usage.
10168,http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html,Mall dataset,"
","
"
10168,https://engineering.purdue.edu/~sorghum/dataset-plant-centers-2016,Plant dataset,"
","
"
10168,#datasetformat,appropriate format,The dataset you used (including images and the CSV with groundtruth with the ,)
10177,https://github.com/zalandoresearch/disentangling_conditional_gans/blob/master/dataset_tool.py#L616,Loading color labels,"
","
"
10177,https://github.com/zalandoresearch/disentangling_conditional_gans/blob/master/dataset_tool.py#L622,Adding images and labels,"
","
"
10177,https://github.com/zalandoresearch/disentangling_conditional_gans/blob/master/dataset.py#L68,Real images,"
","
"
10177,https://github.com/zalandoresearch/disentangling_conditional_gans/blob/master/dataset.py#L78,Real masks,"
","
"
10177,https://github.com/zalandoresearch/disentangling_conditional_gans/blob/master/dataset.py#L89,Real color labels,"
","
"
10184,http://cs.jhu.edu/~xuchen/packages/jacana-qa-naacl2013-data-results.tar.bz2,availabe,"The addressed task is a popular answer sentence selection benchmark, where the goal is for each question to select relevant answer sentences. The dataset was first introduced by (Wang et al., 2007) and further elaborated by (Yao et al., 2013). It is freely ",.
10190,https://github.com/nmrksic/neural-belief-tracker/tree/master/data/woz,WOZ,"), goal-oriented dialogue (",", semantic parsing ("
10190,https://s3.amazonaws.com/research.metamind.io/decaNLP/data/schema.txt,MWSC,"), and commonsense reasoning (","). Each task is cast as question answering, which makes it possible to use our new Multitask Question Answering Network ("
10190,https://github.com/nmrksic/neural-belief-tracker/tree/master/data/woz,WOZ, | , | 
10190,https://s3.amazonaws.com/research.metamind.io/decaNLP/data/schema.txt,MWSC, | , | | --- | --- | --- | --- | --- | --- | --- | ---- | ---- | --- | --- |--- | | 
10194,https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones,UCI repository,The data can be downloaded from the ,.
10194,https://github.com/bhimmetoglu/seizure-forecast/blob/master/HAR/explore_data.ipynb,explore_data,Notebook | Description -------- | ------ , | Data exploration 
10208,http://pandas.pydata.org/,pandas,"
"," -- for data structuring and manipulation,"
10208,http://seaborn.pydata.org/,seaborn,"
", -- for plotting histograms.
10209,https://github.com/deepmind/dsprites-dataset,dSprites, replicates our results on ,", a 64x64 image dataset, which demonstrates that our method can still be used in high-dimensional settings where input features aren't individually meaningful."
10229,./data/,datasets,These are improved forms of prior datasets and a new dataset we developed. We have separate files describing the ,", "
10257,http://www.cs.sfu.ca/~colour/data/shi_gehler/,Gehler-Shi dataset,The ColorChecker RECommended dataset is an updated version of the original ,", which re-generates a new ""recommended"" ground-truth set. See "
10257,http://www.cs.sfu.ca/~colour/data/shi_gehler/,here,Download the raw images , and unzip it to 
10262,#input-data-format,Input data format,"
","
"
10262,#building-the-data-structures,Building the data structures,"
","
"
10262,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google format,-gram counts files follow the ,", i.e., one separate file for each distinct value of "
10266,https://www.kaggle.com/c/msk-redefining-cancer-treatment/data,kaggle's page,The input data can be found via , or 
10283,https://hyperledger-fabric.readthedocs.io/en/release-1.2/private-data/private-data.html,read the docs.,Information on Fabric private data can be found ,"
"
10298,https://github.com/FerranAlet/modular-metalearning/blob/master/create_functions_datasets.py,create_functions_datasets,First you have to generate the datasets using  ,. For instance to create the functions dataset:
10299,https://www.kaggle.com/c/amazon-employee-access-challenge/data,this kaggle competition, from , and specify its location via --data_dir.
10305,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe.840B.300d,", as we apply ", as the initialization of the word embeddings. We also provide a custom subset of GloVe embeddings at 
10320,https://github.com/RAMitchell/ml_dataset_loader/tree/ac520d8c34d1d3bd68819e49dffd97f4a3f671c6,ml_dataset_loader,Datasets are loaded using ,. Datasets are automatically downloaded and cached over subsequent runs. Allow time for these downloads on the first run.
10324,http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2016arXiv160309320M&data_type=BIBTEX&db_key=PRE&nocookieset=1,"
[BibTex]
",", abs/1603.09320. ","
"
10326,https://github.com/ja-thomas/OMLbots/blob/master/snapshot_database/readme_download,here,After having set up the SQL database (see ," for an example how to do it via a terminal in linux), the data can be extracted with this code: https://github.com/ja-thomas/OMLbots/blob/master/snapshot_database/database_extraction.R"
10338,http://imagenet.stanford.edu/internal/jcjohns/scene_graphs/sg_dataset.zip,"
VRD
", and , images.
10345,https://github.com/tedunderwood/genredistance/tree/master/select_data,"
select_data
","
"," contains a Jupyter notebook that documents the selection of fiction volumes for the experiment, and also the social ground truth about genre proximity used to test textual distances."
10345,https://github.com/tedunderwood/genredistance/tree/master/metadata,"
metadata
","
", The key file here is 
10345,https://github.com/tedunderwood/genredistance/tree/master/metadata,"
lda
","
", contains code that produced the topic model used in the article (see 
10348,https://sites.google.com/view/reside-dehaze-datasets,RESIDE,"The paper reviews the collective endeavors by the team of authors in exploring two interlinked important tasks, based on the recently released REalistic Single Image DEhazing (",") benchmark: i) single image dehazing as a low-level image restoration problem; ii) high-level visual understanding (e.g., object detection) from hazy images. For the first task, the authors investigated on a variety of loss functions, and found perception-driven loss to improve dehazing performance very notably. For the second task, the authors came up with multiple solutions including using more advanced modules in the dehazing-detection cascade, as well as domain-adaptive object detectors. In both tasks, our proposed solutions are verified to significantly advance the state-of-the-art performance."
10353,#data,Data,"
","
"
10353,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,KITTI Depth,Download the , Dataset from their website. Use the following scripts to extract corresponding RGB images from the raw dataset.
10358,https://github.com/wnzhang/make-ipinyou-data,make-ipinyou-data,This project is forked from ,", slightly changing the data format and feature alignment for future use."
10358,http://data.computational-advertising.org,data.computational-advertising.org,~~Go to , to download 
10358,http://bunwell.cs.ucl.ac.uk/ipinyou.contest.dataset.zip,here,) can be downloaded from ,.~~
10359,https://github.com/wnzhang/make-ipinyou-data,make-ipinyou-data,The feature engineering is contributed by @weinan zhang. On his benchmark ,", we re-organized the feature alignment and removed the "
10359,https://github.com/Atomu2014/make-ipinyou-data,make-ipinyou-data-refined, feature considering leaky problems ,.
10365,https://atm-data.s3.amazonaws.com/index.html,atm-data S3 Bucket in AWS,You can find a collection of demo datasets in the ,.
10365,https://atm-data.s3.amazonaws.com/pollution_1.csv,from here,"For this demo we will be using the pollution csv from the atm-data bucket, which you can download with your browser ",", or using the following command:"
10365,https://atm-data.s3.amazonaws.com/pollution_1.csv,pollution_1.csv,"For example, if we have previously downloaded the "," file inside our current working directory, we can call "
10368,http://crcv.ucf.edu/data/UCF101.php,UCF101,You must have downloaded the , (Action Recognition Data Set)
10377,https://data.world/jaredfern/googlenews-reduced-200-d,"
gnews_mod
",| embedding_name                                                                  | contributor   | embedding_type   |   dimension |    score | |:--------------------------------------------------------------------------------|:--------------|:-----------------|------------:|---------:| | ,            | jaredfern     | word2vec         |         200 | 0.491233 | | 
10377,https://data.world/jaredfern/gigaword-glove-embedding,"
glove_Gigaword100d
",            | jaredfern     | word2vec         |         200 | 0.491233 | | ,   | jaredfern     | glove            |         100 | 0.456143 | | 
10377,https://data.world/jaredfern/text-8-w-2-v,"
text8_emb
",   | jaredfern     | glove            |         100 | 0.456143 | | ,                        | jaredfern     | word2vec         |          50 | 0.37306  | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
books_40
",                        | jaredfern     | word2vec         |          50 | 0.37306  | | ,       | jaredfern     | word2vec         |         100 | 0.303337 | | 
10377,https://data.world/jaredfern/oanc-word-embeddings,"
OANC_Written
",       | jaredfern     | word2vec         |         100 | 0.303337 | | ,             | jaredfern     | word2vec         |         100 | 0.293891 | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
econ_40
",             | jaredfern     | word2vec         |         100 | 0.293891 | | ,        | jaredfern     | word2vec         |         100 | 0.290213 | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
agriculture_40
",        | jaredfern     | word2vec         |         100 | 0.290213 | | , | jaredfern     | word2vec         |         100 | 0.289704 | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
govt_40
", | jaredfern     | word2vec         |         100 | 0.289704 | | ,        | jaredfern     | word2vec         |         100 | 0.288382 | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
weather_40
",        | jaredfern     | word2vec         |         100 | 0.288382 | | ,     | jaredfern     | word2vec         |         100 | 0.277633 | | 
10377,https://data.world/jaredfern/new-york-times-word-embeddings,"
arts_40
",     | jaredfern     | word2vec         |         100 | 0.277633 | | ,        | jaredfern     | word2vec         |         100 | 0.266848 |
10377,https://data.world/settings/advanced,Settings > Advanced," library, configure the datadotworld library with your API token. Your token is obtainable on data.world under ","
"
10377,https://data.world/jaredfern/vecshare-large-indexer,https://data.world/jaredfern/vecshare-large-indexer,. Currently indexed embeddings are viewable at: ,.
10379,https://nlp.stanford.edu/data/glove.6B.zip,here,", and the glove.6b.50d vectors, which you can find ",.
10381,http://cocodataset.org,COCO image,"
",:
10389,https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/,https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/,Dataset: ,"
"
10389,lcas_simple_data.zip,lcas_simple_data.zip,"
", contains 172 consecutive frames (in .pcd file) with 2 fully annotated pedestrians.
10396,https://lfaidata.foundation,Linux Foundation AI & Data Foundation,Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security. ART is hosted by the ," (LF AI & Data). ART provides tools that enable developers and researchers to defend and evaluate Machine Learning models and applications against the adversarial threats of Evasion, Poisoning, Extraction, and Inference. ART supports all popular machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types (images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, speech recognition, generation, certification, etc.)."
10399,multi_categorical_gans/datasets,Datasets,"
","
"
10399,multi_categorical_gans/datasets/synthetic/,Synthetic data generation,"
","
"
10399,multi_categorical_gans/datasets/uscensus/,US Census 1990,"
","
"
10408,http://cocodataset.org/#home,MS COCO,For training on ,", run:"
10408,http://www.cvlibs.net/datasets/kitti/eval_object.php,KITTI,For training on ,", run:"
10408,https://github.com/fizyr/keras-retinanet-test-data/blob/master/config/config.ini,here, repository. To use the generated configuration check , for an example config file and then pass it to 
10408,https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9,NATO Innovation Challenge,"
",. The winning team of the NATO Innovation Challenge used keras-retinanet to detect cars in aerial images (
10408,https://towardsdatascience.com/how-i-monitor-and-track-my-machine-learning-experiments-from-anywhere-described-in-13-tweets-ec3d0870af99,comet.ml,"
",. Using keras-retinanet in combination with 
10408,https://github.com/fizyr/keras-retinanet-test-data/blob/master/config/config.ini,here," The train tool allows to pass a configuration file, where the anchor parameters can be adjusted. Check ", for an example config file.
10416,https://console.cloud.google.com/storage/slim-dataset,here,Raw data files referred to in this document are available to download ,.
10421,http://kaldi-asr.org/doc/data_prep.html,data preparation,"Do not use space in speaker folder name or utterance file name, using underscore instead. Make sure different speakers have different folder names (speaker ID) and different audio files have different file name (utt ID). Please refer to Kaldi's documentation on ",.
10428,data/ImageCLEF_Wikipedia/README.md,data/ImageCLEF_Wikipedia/README.md,"
","
"
10428,data/VOC2007/README.md,data/VOC2007/README.md,"
","
"
10437,https://pandas.pydata.org/,pandas,To aggregate the data and convert them to various formats we use a , data frame.
10451,http://buildingparser.stanford.edu/dataset.html#Download,Download,"
", the aligned version of the dataset. Now you should have a file called 'Stanford3dDataset_v1.2_Aligned_Version.zip'. Run
10478,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K dataset,Download DIV2K training data (800 training + 100 validtion images) from , or 
10484,https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/simplified,here,Then download Quick Draw (simplified). It's located on Google Cloud ,. You can install 
10520,https://s3.us-east-2.amazonaws.com/heer-data/dblp.zip,download,"): DBLP is a bibliographical network in the computer science domain. There are five types of nodes in the network: author, paper, key term, venue, and year. The edge types include authorship (aut.), term usage (term), publishing venue(ven.), and publishing year (year) of a paper, and the reference relationship from a paper to another (ref.). [",] [
10520,https://s3.us-east-2.amazonaws.com/heer-data/pretrained_dblp_emb.zip,pretrained LINE embeddings,] [,]
10520,https://s3.us-east-2.amazonaws.com/heer-data/yago.zip,download,"): YAGO is a large-scale knowledge graph derived from Wikipedia, WordNet, and GeoNames. There are seven types of nodes in the network: person, location, organization, piece of work, prize, position, and event. A total of 24 edge types exist in the network, with five being directed and others being undirected. [",] [
10520,https://s3.us-east-2.amazonaws.com/heer-data/pretrained_yago_emb.zip,pretrained LINE embeddings,] [,]
10530,https://www.wikidata.org/wiki/Q108072,Monterey,<,", "
10530,https://www.wikidata.org/wiki/Property:P131,isLocatedIn,", ",", "
10530,https://www.wikidata.org/wiki/Q99,California,", ",>
10530,https://www.wikidata.org/wiki/Q99,California,<,", "
10530,https://www.wikidata.org/wiki/Property:P150,hasCounties,", ",", ∃58>"
10544,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L3HN0K,Harvard Dataverse,". Given the low coverage of Dhaka, we dropped Dhaka. In all, we have images of 978 locations for Bangkok, 872 for Jakarta, 999 for Lagos, and 4,828 for Wayne. Each photo captures a small segment of the road. (All the photos are available on ",.)
10544,data/mturk/,data/mturk,"Next, we recruited workers on Amazon's Mechanical Turk (MTurk) to code the images for the condition of the roads. To ensure quality, we only recruited `master' workers. We asked them if the segment of the road in the image had any 1) cracks, and 2) potholes. We also asked them, ""if there are any road markings on the road, are they clear?"" Lastly, we asked them, if there any litter and if the sidewalks were paved. The final survey for Bangkok, Jakarta, and Wayne, MI was the same (see ","). (We initially got Jakarta's images coded using alternate instrumentation. But we were concerned that this would lead to incommensurability. So we did another round of data collection with the same instrument.) Lagos' survey differed in very minor ways from Bangkok, Jakarta, and Wayne's. We paid MTurkers 5 cents for answering the short survey for each image. To ensure quality, we also checked a few images at random to see if the coding was reasonable. We found one instance where one worker's judgments seemed really off and decided to reject those HITs."
10544,data/geo_sample_out/bangkok-roads-s1k.csv,Bangkok,"
","
"
10544,data/geo_sample_out/dhaka-roads-s1k.csv,Dhaka,"
","
"
10544,data/geo_sample_out/jakarta-roads-s1k.csv,Jakarta,"
","
"
10544,data/geo_sample_out/lagos-roads-s1k.csv,Lagos,"
","
"
10544,data/geo_sample_out/wayne2-roads-s5k.csv,Wayne,"
","
"
10544,data/google_street_view_metadata/,CSV with meta data,"
",": There are 4 columns ""url_img0"", ""url_img90"", ""url_img180"" and ""url_img270"" in the CSV file for each row corresponding to images taken at 4 different angles."
10544,data/mturk/bangkok_mturk_screenshot.png,Bangkok,"
","
"
10544,data/mturk/jakarta_mturk_screenshot.png,Jakarta,"
","
"
10544,data/mturk/lagos_mturk_screenshot.png,Lagos and Wayne,"
","
"
10544,data/mturk/bangkok_mturk_2018_06_02.csv,Bangkok,"
","
"
10544,data/mturk/jakarta_mturk_2018_06_02.csv,Jakarta,"
","
"
10544,data/mturk/lagos_mturk_2018_06_08.csv,Lagos,"
","
"
10544,data/mturk/wayne_mturk_2018_06_11_18.csv,Wayne,"
","
"
10546,https://www.kaggle.com/rishianand/devanagari-character-set/data,https://www.kaggle.com/rishianand/devanagari-character-set/data,Devanagari-Characters: ,"
"
10546,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Flowers 102: ,"
"
10546,http://ai.stanford.edu/~jkrause/cars/car_dataset.html,http://ai.stanford.edu/~jkrause/cars/car_dataset.html,Cars-196: ,"
"
10549,http://stackoverflow.com/questions/43702546/tensorboard-doesnt-show-all-data-points/,Stack Overflow question,). See this , for some more information.
10550,http://cocodataset.org,COCO Dataset,We have tested our method on ,"
"
10591,https://www.slideshare.net/SatoshiHara3/maximally-invariant-data-perturbation-as-explanation,[slide],". arXiv:1806.07004, 2018 ",.
10598,https://jsoup.org/cookbook/extracting-data/selector-syntax,extract data,find and ,", using DOM traversal or CSS selectors"
10598,https://jsoup.org/cookbook/modifying-data/set-html,HTML elements,manipulate the ,", attributes, and text"
10606,https://www.nlm.nih.gov/databases/download/pubmed_medline.html,PubMed® / Medline®,. The papers in the corpus were selected from those available from ,. Users are referred to that source for the most current and accurate version of the text for the corresponding papers.
10609,https://circleci.com/gh/ICIJ/datashare,"
CircleCI
",Datashare ,"
"
10609,https://crowdin.com/project/datashare,"
Crowdin
","
","
"
10634,https://github.com/paulgay/VGfM/tree/master/data_tools,data_tools,"Eventually, check the ", folder for details on how to pre-process the ScanNet data.
10642,http://cocodataset.org/#home,COCO," object detection CNNs on small real image datasets, after being pre-trained on huge and available datasets such as ",.
10642,http://soma.isr.ist.utl.pt/vislab_data/shapes2018/shapes2018.tar.gz,webpage,We provided the used real image dataset in our laboratory's ,. The used synthetic datasets were generated using an open-source Domain Randomization 
10642,http://pandas.pydata.org/,Pandas,"
","
"
10646,http://www.cp.jku.at/resources/2019_RLScoFo_TISMIR/data.tar.gz,here,If automatically downloading and preparing the data fails for any reason just download it manually from , and extract it to your desired data path 
10649,http://gigadb.org/dataset/100110,CORT,"This code contains the implementation specifically for the GAN. We will provide the code for the optimization in a later update. Note that the dataset used in the original paper cannot be shared publicly. This will be resolved in a later update, where we will provide a synthetic dataset. Alternatively, you can use a public dataset, such as ",", as long as you modify the dataloader appropriately."
10672,http://pandas.pydata.org/,pandas,"
", >= 0.18
10687,http://www.wf4ever-project.org/wiki/display/docs/Provenance+corpus,Provenance corpus in the Wf4Ever wiki,"For more information, read about the ",.
10698,http://www.nersc.gov/about/nersc-staff/data-analytics-services/wahid-bhimji/,Wahid Bhimji,"
","
"
10699,http://www.nersc.gov/about/nersc-staff/data-analytics-services/wahid-bhimji/,Wahid Bhimji,"
","
"
10700,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,VGGFace2 dataset,These models were trained on a training set from , using Softmax loss
10701,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,the IMDB-WIKI dataset,"This is a Keras implementation of a CNN for estimating age and gender from a face image [1, 2]. In training, ", is used.
10701,check_dataset.ipynb,check_dataset.ipynb, file. Please check , for the details of the dataset. The training data is created by:
10701,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,the IMDB-WIKI dataset,"This project is released under the MIT license. However, ", used in this project is originally provided under the following conditions.
10702,recognition/_datasets_,dataset,"The training data includes, but not limited to the cleaned MS1M, VGG2 and CASIA-Webface datasets, which were already packed in MXNet binary format. Please ", page for detail.
10704,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,IMDB-Wiki,"
","
"
10704,https://talhassner.github.io/home/projects/Adience/Adience-data.html,Adience,"
","
"
10704,http://yanweifu.github.io/FG_NET_data/index.html,FGNET,"
","
"
10706,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,IMDB-WIKI, and , (cropped face version) dataset.
10706,./prepare-data.ipynb,prepare-data.ipynb,Next run ,". This will split two datasets in to training parts and test parts. The IMDB-WIKI dataset we will split into two separate datasets, gender and age datasets. So we have 3 datasets: age gender age. Each file will run into MTCNN network to detect and crop face, resize to 48x48 and convert to gray, then all datasets are saved into pickle files."
10707,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_crop.tar,imdb,"In order to train your own models,you should first download ", or 
10707,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar,wiki, or ," dataset,and then extract it under "
10707,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,IMDB-WIKI – 500k+ face images with age and gender labels,"
","
"
10712,http://www.cs.columbia.edu/CAVE/databases/multispectral/,database, hyperspectral images are from the CAVE multispectral image ,.
10713,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_test_LR_bicubic_X4.zip,test_LR_bicubic_X4,"Other links for DIV2K, in case you can't find it : ",", "
10713,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip,train_HR,", ",", "
10713,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip,train_LR_bicubic_X4,", ",", "
10713,https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip,valid_HR,", ",", "
10713,https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip,valid_LR_bicubic_X4,", ",.
10735,http://www.gsma.com/personaldata/the-relationship-between-blockchain-and-digital-identity,article,An , by the GSMA about the relationship between blockchain and identity.
10735,http://intelligentsystemsmonitoring.com/community/blockchain-community/t-mobile-unveils-the-next-identity-platform-a-blockchain-powered-database/,press release," NEXT Identity Platform — In a company with millions of users with differing levels of service, permission management is a considerable bottleneck. Regulations around user security and privacy add a further layer of complexity to such a system.Next Identity combines the functionality of a traditional database with the benefits of a centralized ledger. The system handles user identities, accesses, and approvals while retaining the integrity of an immutable blockchain. (",)
10738,https://github.com/jinnovation/rainy-image-dataset,this paper," in MATLAB to generate the training or test data for the nine image operators mentioned in the paper, which includes L0 smoothing, RTV, WLS, RGF, WMF, shock filter, super resolution, denoising and deblocking. The training data for the derain task is collected from ",.
10742,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,"
VoxCeleb
","
", dataset and lists of 
10742,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt,dataset split, dataset and lists of , and 
10742,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt,verification evaluation pairs, and ,"
"
10742,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html,"
VoxCeleb2
"," was the largest publicly available dataset. However, not long after, much larger ", dataset with more speakers and more statistically sound evaluations was released and it would be really interesting to see how much improvement using suggested loss functions and augmentation would yield.
10750,readme/dataset.md,Dataset,"
","
"
10750,readme/dataset.md,Dataset Page,The dataset for the visual mesh is provided via TFRecord files. The keys that are used in the TFRecord files are determined by the flavour that is used. See the , for more information.
10775,scripts_data_processing/Readme.md#training-data,here,Follow instructions , to download and prepare the training data
10775,scripts_data_processing/Readme.md#test-data,here,"Also download the test data for descriptor matching (i.e. the 30,000 cluster pairs) by following the instructions ",". We monitor the false alarm rate at 95% recall, as the training loss is not very informative (The provided script evaluates on all of the test data which can be slow; you can change this behavior by modifying VAL_PROPORTION in train.py)"
10775,scripts_data_processing/Readme.md,scripts_data_processing/Readme.md,Refer to ,.
10781,http://www.cs.cornell.edu/~arb/data/tags-ask-ubuntu/index.html,here, directory. These files were downloaded directly from , and 
10781,http://www.cs.cornell.edu/~arb/data/DAWN/index.html,here, and ,". The ngrams dataset is available from https://www.ngrams.info/, but I cannot include the data directly due to the usage terms. The script "
10781,http://www.cs.cornell.edu/~arb/data/,here,"First, we need to compute all of the centralities. Here is an exmaple for the 4-uniform tags dataset. This code is designed to use the data format of the temporal higher-order networks available ",.
10805,#output-data,Output Data,"
","
"
10808,https://vision.cs.tum.edu/data/datasets/mono-dataset,dataset, provided by the ,.
10817,#open-source-speech-recognition-recipe-and-corpus-for-building-german-acoustic-models-with-kaldi,Open source speech recognition recipe and corpus for building German acoustic models with Kaldi,"
","
"
10817,#Get-LM-text-data,Get LM text data,"
","
"
10817,#getting-data-files-separately,Getting data files separately,"
","
"
10817,#speech-corpus,Speech corpus,"
","
"
10817,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,m-ailabs read speech data corpus,", containing about 285h of additional data and the German subset of ","
"
10817,https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/,(mirror),"
", (237h). Recently we also added the German Commonvoice corpus from Mozilla (https://commonvoice.mozilla.org/de) with 370h of data. We use the test/dev sets from Tuda-De for WER evaluations.
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_HCLG_s.fst.bz2,HCLG_s,". If you need an overall faster model, you can also replace the default HCLG with this much smaller one: ",.
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,download,A new pretrained model with a vocabulary of 400 thousand words is available: ,"
"
10817,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,m-ailabs speech data corpus,"We added more aligned speech data (630h total now), thanks to the ","
"
10817,https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/,(mirror),"
",. We also thank Pavel Denisov for sending us a Kaldi data preparation script for this new open source corpus.
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_350k_nnet3chain_tdnn1f_1024_sp_bi.tar.bz2,download,A new pretrained model with a vocabulary of 350 thousand words is available: ,"
"
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k.tar.bz2,full archive,The ivector extractor had been missing from the acoustic model binary archive. You can download it separately from  https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k_ivector_extractor.tar.bz2 or redownload the ,.
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv8_voc900k,| Acoustic model + FST | Training data | Tuda dev WER (FST) | Tuda test WER (FST) | | --- | --- | --- | --- | | , | 1700h (tuda+SWC+m-ailabs+cv8) | 9.30  | 10.17 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_const_arpa.tar.bz2,lm_v6_voc900k, | 1700h (tuda+SWC+m-ailabs+cv8) | 9.30  | 10.17 | | + , const arpa rescoring | 140 million sentences | 7.23 | 7.96  | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_rnnlm_lstm_4x.tar.bz2,rnn_lmv6_lstm4x_voc900k, const arpa rescoring | 140 million sentences | 7.23 | 7.96  | | + , rnnlm rescoring | 140 million sentences | 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_900k_HCLG_s.fst.bz2,HCLG_s,", e.g. drei und sechzig -> dreiundsechzig. If you need an overall faster model, you can also replace the default HCLG with this much smaller one: ",". Together with RNNLM rescoring, the WER result will only be minimally bigger."
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/tdnn_chain_cleaned_tuda_swc_voc126k.tar.bz2,tuda_swc_voc126k,| Acoustic model + FST | Training data | Tuda dev WER (FST) | Tuda test WER (FST) | | --- | --- | --- | --- | | , / 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_350k_nnet3chain_tdnn1f_1024_sp_bi.tar.bz2,tuda_swc_voc350k, | 375h (tuda+SWC) | 20.30 | 21.43 | | , / 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_voc400k, | 375h (tuda+SWC) | 15.32 | 16.49 | | , / 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_683k_nnet3chain_tdnn1f_2048_sp_bi_smaller_fst.tar.bz2,tuda_swc_mailabs_cv_voc683k_smaller_fst, | 630h (tuda+SWC+m-ailabs) | 14.78 | 15.87 | | , | 1000h (tuda+SWC+m-ailabs+cv) | 12.69 | 14.29 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/carpa_rescoring_language_model_v5_voc683k.tar.bz2,lm_v5_voc683k_smaller_fst, | 1000h (tuda+SWC+m-ailabs+cv) | 12.69 | 14.29 | | + , const arpa rescoring | 100 million sentences | 10.92 | 12.37 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_683k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv3_voc683k, | e.g. drei und sechzig -> dreiundsechzig |  8.94  | 10.26 | | , | 1000h (tuda+SWC+m-ailabs+cv3) | 12.26 | 13.79 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/carpa_rescoring_language_model_v5_voc683k.tar.bz2,lm_v5_voc683k, | 1000h (tuda+SWC+m-ailabs+cv3) | 12.26 | 13.79 | | + , const arpa rescoring | 100 million sentences | 10.47 | 11.85 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2,tuda_swc_mailabs_cv8_voc722k, | e.g. drei und sechzig -> dreiundsechzig | 8.61  | 9.85 | | , | 1700h (tuda+SWC+m-ailabs+cv8) | 10.94 | 12.09 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_const_arpa.tar.bz2,lm_v5_voc722k, | 1700h (tuda+SWC+m-ailabs+cv8) | 10.94 | 12.09 | | + , const arpa rescoring | 100 million sentences | 9.25 | 10.17 | | + 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_722k_rnnlm_lstm_2x.tar.bz2,rnn_lm_lstm2x_voc722k, | e.g. drei und sechzig -> dreiundsechzig | 7.51 | 8.53 | | + , rnnlm rescoring | 100 million sentences | 
10817,https://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/german-speechdata-package-v2.tar.gz,here,The corpus can be downloaded ,. The license is 
10817,http://kaldi.sourceforge.net/data_prep.html#data_prep_lang_creating,lexion_p.txt,export_lexicon.py will export such a serialised python dictionary into KALDIs ," format (this allows to model different phonetic realisations of the same word with probabilities). Stress markers in the phoneme set are grouped with their unstressed equivalents in KALDI using the extra_questions.txt file. It is also possible to generate a CMU Sphinx formated dictionary with the same data using the -spx option. The Sphinx format also allows pronunciation variants, but cannot model probabilities for these variants."
10822,https://github.com/pfnet-research/sngan_projection#preprocess-dataset,SNGAN-projection,Follow the , steps to download and pre-process data.
10836,mailto:dataset@buaamc2.net?subject=VQA-ODV&body=Anything%20to%20say,this hyperlink,Or you can use ," to send the email. After sending the email, you are supposed to receive the response "
10839,data/,Data,"
","
"
10839,data/YGOV1058_profile.csv,YG Profile Data (CSV),"
","
"
10839,data/YGOV1058_pwned.csv,HIBP Data on the people (CSV),"
","
"
10839,data/hibp_codebook.xlsx,HIBP codebook (xlsx),"
","
"
10839,data/breaches.json,HIBP Data on Breaches (JSON),"
","
"
10839,data/cps_2018.xlsx,Current Population Survey Data (xlsx),"
", and 
10839,data/cps_2018.csv,CSV, and ,"
"
10846,https://github.com/xiul-msr/e2e_dialog_challenge/data/,Data,[x] 07/28/2018: Restaurant and Taxi domains: , and 
10848,http://www.scidb.cn/en/detail?dataSetId=760619497378807808&code=5e05cb5d64a42fa9add9b7ae&tID=journalOne&dataSetType=journal&language=en_US,ScienceDB,"
","
"
10851,http://aws.amazon.com/de/datasets/apache-software-foundation-public-mail-archives/,Apache Software Foundation Public Mail Archives,"
",": all publicly available Apache Software Foundation mail archives as of July 11, 2011 (200 GB)"
10851,https://snap.stanford.edu/data/web-Amazon.html,Amazon Reviews,"
",: Stanford collection of 35 million amazon reviews. (11 GB)
10851,http://arxiv.org/help/bulk_data_s3,ArXiv,"
",: All the Papers on archive as fulltext (270 GB) + sourcefiles (190 GB).
10851,http://www.clips.uantwerpen.be/datasets/csi-corpus,CLiPS Stylometry Investigation (CSI) Corpus,"
",": a yearly expanded corpus of student texts in two genres: essays and reviews. The purpose of this corpus lies primarily in stylometric research, but other applications are possible. (on request)"
10851,http://aws.amazon.com/de/datasets/common-crawl-corpus/,Common Crawl Corpus,"
",: web crawl data composed of over 5 billion web pages (541 TB)
10851,http://nlp.stanford.edu/data/crosswikis-data.tar.bz2/,Crosswikis,"
",: English-phrase-to-associated-Wikipedia-article database. Paper. (11 GB)
10851,http://aws.amazon.com/de/datasets/dbpedia-3-5-1/?tag=datasets%23keywords%23encyclopedic,DBpedia,"
",: a community effort to extract structured information from Wikipedia and to make this information available on the Web (17 GB)
10851,https://go.umd.edu/diplomacy_data,Diplomacy,"
",": 17,000 conversational messages from 12 games of Diplomacy, annotated for truthfulness (3 MB)"
10851,https://data.mendeley.com/datasets/zm33cdndxs/2,Elsevier OA CC-BY Corpus,"
",": 40k (40,001) Open Access full-text scientific articles with complete metadata include subject classifications (963Mb)"
10851,http://aws.amazon.com/de/datasets/enron-email-data/,Enron Email Data,"
",": consists of 1,227,255 emails with 493,384 attachments covering 151 custodians (210 GB)"
10851,http://aws.amazon.com/de/datasets/federal-contracts-from-the-federal-procurement-data-center-usaspending-gov/,Federal Contracts from the Federal Procurement Data Center (USASpending.gov),"
",: data dump of all federal contracts from the Federal Procurement Data Center found at USASpending.gov (180 GB)
10851,http://aws.amazon.com/de/datasets/freebase-data-dump/,Freebase Data Dump,"
",: data dump of all the current facts and assertions in Freebase (26 GB)
10851,http://aws.amazon.com/de/datasets/freebase-simple-topic-dump/,Freebase Simple Topic Dump,"
",: data dump of the basic identifying facts about every topic in Freebase (5 GB)
10851,http://aws.amazon.com/de/datasets/freebase-quad-dump/,Freebase Quad Dump,"
",: data dump of all the current facts and assertions in Freebase (35 GB)
10851,https://www.kaggle.com/c/predict-wordpress-likes/data,GigaOM Wordpress Challenge [Kaggle],"
",": blog posts, meta data, user likes (1.5 GB)"
10851,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google Books Ngrams,"
",: available also in hadoop format on amazon s3 (2.2 TB)
10851,https://github.com/pgcorpus/gutenberg,Gutenberg Standardized Corpus,"
",": Standardized Project Gutenberg Corpus, 55905 books (3GB counts + 18GB tokens)"
10851,http://library.harvard.edu/open-metadata#Harvard-Library-Bibliographic-Dataset,Harvard Library,"
",": over 12 million bibliographic records for materials held by the Harvard Library, including books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials. (4 GB)"
10851,https://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz,Historical Newspapers Yearly N-grams and Entities Dataset,"
",": Yearly time series for the usage of the 1,000,000 most frequent 1-, 2-, and 3-grams from a subset of the British Newspaper Archive corpus, along with yearly time series for the 100,000 most frequent named entities linked to Wikipedia and a list of all articles and newspapers contained in the dataset (3.1 GB)"
10851,https://datadryad.org/resource/doi:10.5061/dryad.nh775,Historical Newspapers Daily Word Time Series Dataset,"
",": Time series of daily word usage for the 25,000 most frequent words in 87 years of UK and US historical newspapers between 1836 and 1922. (2.7GB)"
10851,https://www.kaggle.com/c/home-depot-product-search-relevance/data,Home Depot Product Search Relevance [Kaggle],"
",": contains a number of products and real customer search terms from Home Depot's website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. (65 MB)"
10851,https://www.crowdflower.com/data-for-everyone/,Identifying key phrases in text,"
",: Question/Answer pairs + context; context was judged if relevant to question/answer. (8 MB)
10851,http://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/,Jeopardy,"
",": archive of 216,930 past Jeopardy questions (53 MB)"
10851,https://github.com/taivop/joke-dataset,200k English plaintext jokes,"
",": archive of 208,000 plaintext jokes from various sources."
10851,http://aws.amazon.com/de/datasets/material-safety-data-sheets/,Material Safety Datasheets,"
",": 230,000 Material Safety Data Sheets. (3 GB)"
10851,https://datadryad.org/resource/doi:10.5061/dryad.p8s0j,Millions of News Article URLs,"
",: 2.3 million URLs for news articles from the frontpage of over 950 English-language news outlets in the six month period between October 2014 and April 2015. (101MB)
10851,https://www.kaggle.com/therohk/india-headlines-news-dataset,News Headlines of India - Times of India [Kaggle],"
",: 2.7 Million News Headlines with category published by Times of India from 2001 to 2017. (185 MB)
10851,https://www.crowdflower.com/data-for-everyone/,News article / Wikipedia page pairings,"
",: Contributors read a short article and were asked which of two Wikipedia articles it matched most closely. (6 MB)
10851,https://www.crowdflower.com/data-for-everyone/,Objective truths of sentences/concept pairs,"
",: Contributors read a sentence with two concepts. For example “a dog is a kind of animal” or “captain can have the same meaning as master.” They were then asked if the sentence could be true and ranked it on a 1-5 scale. (700 KB)
10851,http://www.clips.uantwerpen.be/datasets/personae-corpus,Personae Corpus,"
",: collected for experiments in Authorship Attribution and Personality Prediction. It consists of 145 Dutch-language essays by 145 different students. (on request)
10851,https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/,Reddit Comments,"
",: every publicly available reddit comment as of july 2015. 1.7 billion comments (250 GB)
10851,https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/,Reddit Submission Corpus,"
",": all publicly available Reddit submissions from January 2006 - August 31, 2015). (42 GB)"
10851,http://trec.nist.gov/data/reuters/reuters.html,Reuters Corpus,"
",": a large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as ""Reuters Corpus, Volume 1"" or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community. Need to sign agreement and sent per post to obtain. (2.5 GB)"
10851,http://data.stackexchange.com/,Stackoverflow,"
",: 7.3 million stackoverflow questions + other stackexchanges (query tool)
10851,https://www.crowdflower.com/data-for-everyone/,Twitter New England Patriots Deflategate sentiment,"
",": Before the 2015 Super Bowl, there was a great deal of chatter around deflated footballs and whether the Patriots cheated. This data set looks at Twitter sentiment on important days during the scandal to gauge public sentiment about the whole ordeal. (2 MB)"
10851,https://www.crowdflower.com/data-for-everyone/,Twitter Progressive issues sentiment analysis,"
",": tweets regarding a variety of left-leaning issues like legalization of abortion, feminism, Hillary Clinton, etc. classified if the tweets in question were for, against, or neutral on the issue (with an option for none of the above). (600 KB)"
10851,https://www.crowdflower.com/data-for-everyone/,Twitter sentiment analysis: Self-driving cars,"
",": contributors read tweets and classified them as very positive, slightly positive, neutral, slightly negative, or very negative. They were also prompted asked to mark if the tweet was not relevant to self-driving cars. (1 MB)"
10851,https://about.twitter.com/en_us/values/elections-integrity.html#data,Twitter Elections Integrity,"
",: All suspicious tweets and media from 2016 US election. (1.4 GB)
10851,http://followthehashtag.com/datasets/200000-tokyo-geolocated-tweets-free-twitter-dataset/,Twitter Tokyo Geolocated Tweets,"
",: 200K tweets from Tokyo. (47 MB)
10851,http://followthehashtag.com/datasets/170000-uk-geolocated-tweets-free-twitter-dataset/,Twitter UK Geolocated Tweets,"
",: 170K tweets from UK. (47 MB)
10851,http://followthehashtag.com/datasets/free-twitter-dataset-usa-200000-free-usa-tweets/,Twitter USA Geolocated Tweets,"
",: 200k tweets from the US (45MB)
10851,https://www.crowdflower.com/data-for-everyone/,U.S. economic performance based on news articles,"
",: News articles headlines and excerpts ranked as whether relevant to U.S. economy. (5 MB)
10851,https://www.kaggle.com/therohk/urban-dictionary-words-dataset,Urban Dictionary Words and Definitions [Kaggle],"
",": Cleaned CSV corpus of 2.6 Million of all Urban Dictionary words, definitions, authors, votes as of May 2016. (238 MB)"
10851,http://aws.amazon.com/de/datasets/the-westburylab-usenet-corpus/,Wesbury Lab Usenet Corpus,"
",": anonymized compilation of postings from 47,860 English-language newsgroups from 2005-2010 (40 GB)"
10851,http://aws.amazon.com/de/datasets/wikipedia-extraction-wex/,Wikipedia Extraction (WEX),"
",: a processed dump of english language wikipedia (66 GB)
10851,http://aws.amazon.com/de/datasets/wikipedia-xml-data/,Wikipedia XML Data,"
",": complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. (500 GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Answers Comprehensive Questions and Answers,"
",": Yahoo! Answers corpus as of 10/25/2007. Contains 4,483,032 questions and their answers. (3.6 GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Answers consisting of questions asked in French,"
",": Subset of the Yahoo! Answers corpus from 2006 to 2015 consisting of 1.7 million questions posed in French, and their corresponding answers. (3.8 GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Answers Manner Questions,"
",": subset of the Yahoo! Answers corpus from a 10/25/2007 dump, selected for their linguistic properties. Contains 142,627 questions and their answers. (104 MB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! HTML Forms Extracted from Publicly Available Webpages,"
",": contains a small sample of pages that contain complex HTML forms, contains 2.67 million complex forms. (50+ GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Metadata Extracted from Publicly Available Web Pages,"
",: 100 million triples of RDF data (2 GB)
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo N-Gram Representations,"
",": This dataset contains n-gram representations. The data may serve as a testbed for query rewriting task, a common problem in IR research as well as to word and sentence similarity task, which is common in NLP research. (2.6 GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,"Yahoo! N-Grams, version 2.0","
",": n-grams (n = 1 to 5), extracted from a corpus of 14.6 million documents (126 million unique sentences, 3.4 billion running words) crawled from over 12000 news-oriented sites (12 GB)"
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Search Logs with Relevance Judgments,"
",: Annonymized Yahoo! Search Logs with Relevance Judgments (1.3 GB)
10851,http://webscope.sandbox.yahoo.com/catalog.php?datatype=l,Yahoo! Semantically Annotated Snapshot of the English Wikipedia,"
",": English Wikipedia dated from 2006-11-04 processed with a number of publicly-available NLP tools. 1,490,688 entries. (6 GB)"
10851,https://www.yelp.com/academic_dataset,Yelp,"
",: including restaurant rankings and 2.2M reviews (on request)
10851,https://www.reddit.com/r/datasets/comments/3gegdz/17_millions_youtube_videos_description/,Youtube,"
",: 1.7 million youtube videos descriptions (torrent)
10851,https://github.com/caesar0301/awesome-public-datasets#natural-language,Awesome public datasets/NLP,"
", (includes more lists)
10851,http://aws.amazon.com/de/datasets/,AWS Public Datasets,"
","
"
10851,https://www.crowdflower.com/data-for-everyone/,CrowdFlower: Data for Everyone,"
", (lots of little surveys they conducted and data obtained by crowdsourcing for a specific task)
10851,https://www.kaggle.com/datasets,Kaggle 1,"
",", "
10851,https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus,Quora,"
", (mainly annotated corpora)
10851,https://www.reddit.com/r/datasets,/r/datasets,"
"," (endless list of datasets, most is scraped by amateurs though and not properly documented or licensed)"
10851,http://rs.io/100-interesting-data-sets-for-statistics/,rs.io,"
", (another big list)
10851,http://opendata.stackexchange.com/,Stackexchange: Opendata,"
","
"
10851,https://www.kaggle.com/gentrexha/kosovo-news-articles-dataset,Albanian News Articles Dataset,"
",": Over 3 million Albanian news articles alongwith metadata, extracted from various albanian news sources (see list in link)."
10851,http://purl.org/corpus/german-speeches,German Political Speeches Corpus,"
",": collection of recent speeches held by top German representatives (25 MB, 11 MTokens)"
10851,http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html,NEGRA,"
",: A Syntactically Annotated Corpus of German Newspaper Texts. Available for free for all Universities and non-profit organizations. Need to sign and send form to obtain. (on request)
10851,http://openlegaldata.io/research/2019/02/19/court-decision-dataset.html,100k German Court Decisions,"
",": Open Legal Data releases a dataset of 100,000 German court decisions and 444,000 citations (772 MB)"
10852,https://github.com/Jeffrey-Ede/datasets/wiki,here,Our training dataset of 17267 2048x2048 micrographs with mean electron counts of at least 2500 ppx is available ,.
10854,https://www.tensorflow.org/api_docs/python/tf/data/Dataset,here," objects, which are documented ",". Optionally, also numpy blobs can be produced (be careful with memory usage). All data interfaces inherit from "
10860,http://mscoco.org/dataset/#overview,COCO, and , joint workshop at 
10860,http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar,here,. Download the Places365 standard easyformat split at ,. Untar it to some folder. Then run the following:
10864,https://github.com/JinkyuKimUCB/BDD-X-dataset.git,BDD-X,BDD-X dataset is available at ,.
10867,#data-download,Data Download,"
","
"
10867,#data-statistics,Data Statistics,"
","
"
10874,#datasetconstruction,Creation of a dataset of Kotlin applications,"
","
"
10874,https://cloud.google.com/bigquery/public-data/github,BigQuery, applications and it is based on publicly-available GitHub mirror available in ," dated on 18th October 2018. Using the Neo4j database is possible to retrieve for each application its source-code repository link and the application's package name. However, this dataset does not provide any apks from its applications. For this reason, we decided to mine the missing apks on AndroZoo dataset."
10874,https://androidtimemachine.github.io/dataset,dataset," applications and it has been executed in October 2017. As their infrastructure is publicly available,", and 
10874,docs/final_dataset.md,here,The table below shows some of these applications. To see the complete list click ,.
10874,docs/final_kotlin_dataset.md,here,) but 7 repositories were not available. Then we analyzed 297 repositories and we found 244 repositories that contains Kotlin code. To see the complete list click ,. It is important to mentioned that two repositories appear twice.
10876,https://cloud.google.com/bigquery/public-data/github,Google BigQuery,"Some data preparation is necessary for commands in this script to run. An initial list of package names and GitHub repositories is required because GitHub API limitations prevent retrieving all search results, even if stratified by byte-granular filesize. A good way to get such data is ",. All steps of the process are detailed in 
10877,http://reclab.idi.ntnu.no/dataset,Adressa, and ,". Furthermore, a recent baseline using Graph Neural Networks (SR-GNN) was included and some additional instantiations of the ACR module were implemented, using GRUs for both supervised and unsupervised training (Sequence Denoising Autoencoder) of the Article Content Embeddings."
10877,https://www.tensorflow.org/guide/datasets,Datasets, and ,.
10877,http://reclab.idi.ntnu.no/dataset,SmartMedia Adressa dataset,"
"," - This dataset contains approximately 20 million page visits from a Norwegian news portal [91]. In our experiments we used 16 days of the full dataset, which is available upon request, and includes article text and click events of about 2 million users and 13,000 articles."
10877,http://reclab.idi.ntnu.no/dataset,Adressa dataset,The creators of the ," can make available the full textual content of articles upon request. After download their data, you must follow this steps:"
10877,https://github.com/gabrielspmoreira/chameleon_recsys/blob/master/acr_module/scripts/dataproc_preprocessing/create_cluster.sh,dataproc_preprocessing/create_cluster.sh,Create a Spark cluster (,) on GCP Dataproc
10877,https://github.com/gabrielspmoreira/chameleon_recsys/blob/master/acr_module/scripts/dataproc_preprocessing/browse_cluster.sh,dataproc_preprocessing/browse_cluster.sh,Open a Jupyter session (,)
10877,https://github.com/gabrielspmoreira/chameleon_recsys/blob/master/acr_module/scripts/dataproc_preprocessing/nar_preprocessing_addressa_01_dataproc.ipynb,dataproc_preprocessing/nar_preprocessing_addressa_01_dataproc.ipynb,Upload the preprocessing notebook (,"), adjust the GCS path where the Adressa dataset was uploaded and run the notebook."
10877,https://github.com/gabrielspmoreira/chameleon_recsys/blob/master/acr_module/scripts/dataproc_preprocessing/destroy_cluster.sh,dataproc_preprocessing/destroy_cluster.sh,Destroy the Spark cluster (,)
10889,https://pandas.pydata.org/,Pandas,", ",", "
10914,https://docs.quiltdata.com/,"
docs on_gitbook
","
","
"
10914,https://slack.quiltdata.com/,"
chat on_slack
","
","
"
10914,https://codecov.io/gh/quiltdata/quilt,"
codecov
","
","
"
10914,https://docs.quiltdata.com/installation,Quilt docs,"If you have Python and an S3 bucket, you're ready to create versioned datasets with Quilt. Visit the "," for installation instructions, a quick start, and more."
10914,https://open.quiltdata.com/,open.quiltdata.com,"
", is a petabyte-scale open data portal that runs on Quilt
10914,https://quiltdata.com,quiltdata.com,"
"," includes case studies, use cases, videos, and instructions on how to run a private Quilt instance"
10914,https://medium.com/pytorch/how-to-iterate-faster-in-machine-learning-by-versioning-data-and-models-featuring-detectron2-4fd2f9338df5,Versioning data and models for rapid experimentation in machine learning,"
", shows how to use Quilt for real world projects
10914,https://quiltdata.com,quiltdata.com,The backend services are available under a paid license on ,.
10916,https://github.com/cocodataset/cocoapi,COCO API,"
","
"
10924,https://github.com/rbgirshick/py-faster-rcnn/tree/master/data,Faster R-CNN, (originally from ,)
10924,http://cocodataset.org/#download,here,"Download the images (2014 Train, 2014 Val, 2017 Test) from ","
"
10928,http://jmcauley.ucsd.edu/data/amazon/,amazon-review dataset," language model for unsupervised modeling of large text datasets, such as the ",", is implemented in PyTorch. We also support other tokenization methods, such as character or sentencepiece tokenization, and language models using various recurrent architectures."
10928,#data-downloads,Data Downloads,"
","
"
10928,./analysis/unsupervised.md#data-robustness,Data Robustness,"
","
"
10928,http://ai.stanford.edu/~amaas/data/sentiment/,IMDB Movie Review,", ",", and the "
10928,http://jmcauley.ucsd.edu/data/amazon/,site," datasets as part of this repository. In order to train on the amazon dataset please download the ""aggressively deduplicated data"" version from Julian McAuley's original ",. Access requests to the dataset should be approved instantly. While using the dataset make sure to load it with the 
10928,./analysis/unsupervised.md#data-robustness,Data Robustness,"
","
"
10928,http://jmcauley.ucsd.edu/data/amazon/,amazon review dataset,This project uses the , collected by J. McAuley
10929,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2, and indoor Realistic Dataset comes from ,. The outdooe Synthetic Dataset is 
10929,http://www.cvlibs.net/datasets/kitti/,KITTI, and outdoor Realistic dataset is ,"
"
10939,https://github.com/jindongwang/transferlearning/blob/master/data,HERE,Please see , for the popular transfer learning 
10939,https://github.com/jindongwang/transferlearning/blob/master/data,这里,"
",整理了常用的公开数据集和一些已发表的文章在这些数据集上的实验结果。
10942,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
10950,./data,data,"
",": all data are stored in this subdirectory, includes:"
10952,https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions,link,. The HAPT data can be obtained from this ,.
10958,#datasets,Datasets,"
","
"
10959,http://speech.ee.ntu.edu.tw/~chiahsuan/ODSQA/audio_data.zip,"
Audio Download Link
", | , | 
10965,http://www.cs.ucr.edu/~eamonn/time_series_data/,UCR archive,The data used in this project comes from the ,", which contains the 85 univariate time series datasets we used in our experiements."
10965,https://pandas.pydata.org/,pandas,"
","
"
10966,#dataset,Dataset,"
","
"
10966,https://obj.umiacs.umd.edu/private_datasets/SketchyScene-7k.7z,SketchyScene-7k,"
", (750.3 MB)
10966,https://obj.umiacs.umd.edu/private_datasets/SketchyScene-Selected3.7z,SketchyScene-Selected3,"
", (1.6 GB)
10966,https://obj.umiacs.umd.edu/private_datasets/SketchyScene-Synthesized30.7z,SketchyScene-Synthesized30,"
", (16.1 GB)
10973,#data-format,Data Format,"
","
"
10973,data,data,Main folder: ,"
"
10973,data/task1/English,/task1/English/,Subfolder ,"
"
10973,data/task1/English/Task1-English-1st-Presidential.txt,Task1-English-1st-Presidential.txt,"
","
"
10973,data/task1/English/Task1-English-2nd-Presidential.txt,Task1-English-2nd-Presidential.txt,"
","
"
10973,data/task1/English/Task1-English-Vice-Presidential.txt,Task1-English-Vice-Presidential.txt,"
","
"
10973,data/task1/Arabic,/task1/Arabic/,Subfolder ,"
"
10973,data/task1/Arabic/Task1-Arabic-1st-Presidential.txt,Task1-Arabic-1st-Presidential.txt,"
","
"
10973,data/task1/Arabic/Task1-Arabic-2nd-Presidential.txt,Task1-Arabic-2nd-Presidential.txt,"
","
"
10973,data/task1/Arabic/Task1-Arabic-Vice-Presidential.txt,Task1-Arabic-Vice-Presidential.txt,"
","
"
10973,data/task2/English,/task2/English/,Subfolder ,"
"
10973,data/task2/English/Task2-English-1st-Presidential.txt,Task2-English-1st-Presidential.txt,"
","
"
10973,data/task2/English/Task2-English-2nd-Presidential.txt,Task2-English-2nd-Presidential.txt,"
","
"
10973,data/task2/English/Task2-English-Vice-Presidential.txt,Task2-English-Vice-Presidential.txt,"
","
"
10973,data/task2/Arabic,/task2/Arabic/,Subfolder ,"
"
10973,data/task2/Arabic/Task2-Arabic-1st-Presidential.txt,Task2-Arabic-1st-Presidential.txt,"
","
"
10973,data/task2/Arabic/Task2-Arabic-2nd-Presidential.txt,Task2-Arabic-2nd-Presidential.txt,"
","
"
10973,data/task2/Arabic/Task2-Arabic-Vice-Presidential.txt,Task2-Arabic-Vice-Presidential.txt,"
","
"
10982,http://adni.loni.usc.edu/data-samples/access-data/,here,". The data is free but you need to apply for access on http://adni.loni.usc.edu/. Once you have an account, go ", and log in.
10997,http://papers.nips.cc/paper/4548-a-simple-and-practical-algorithm-for-differentially-private-data-release,"""A Simple and Practical Algorithm for Differentially Private Data Release.""",.  The algorithm in this case is MWEM (Hardt et al. , NIPS 2012).  Note this example 
11008,https://towardsdatascience.com/introduction-to-sequence-models-rnn-bidirectional-rnn-lstm-gru-73927ec9df15,bidirectional RNN (BRNN), and processes them through a ,.
11008,data/sample-cs-cltt-ud-test.txt,data/sample-cs-cltt-ud-test.txt," or LemmaTag format. The LemmaTag format has 3 tab-separated columns: the word form, its lemma, and its part-of-speech tag. Sentences are split by empty lines. See ", for an example of a small Czech dataset.
11016,http://apex.sjtu.edu.cn/datasets/13,here,The full dataset for this project has been published ,.
11016,http://ailab.criteo.com/criteo-attribution-modeling-bidding-dataset/,this page,Our raw data is Criteo Attribution Modeling for Bidding Dataset . You can download it and read its description on ,.
11025,./data/netflix,./data/netflix," from your brower, extract and put the extracted files in ", directly.
11025,https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html,SparkALS,(2) Multi GPU support. We have tested on very large data sets such as ," and HugeWiki, on multiple GPUs on one server. We will make our multi GPU support code available soon."
11037,https://www.cs.ucr.edu/~eamonn/time_series_data_2018/,UCR Time Series Classification Archive,. The site also hosts the popular ,.
11037,https://www.cs.ucr.edu/~eamonn/time_series_data_2018/,UCR Time Series Classification Archive,"We want to thank the many researchers that made available datasets and open source code for time series classification. In particular, we want to thank Eamonn Keogh and his team for preparing the ",", and Tony Bagnall and his team for preparing the "
11048,https://archive.ics.uci.edu/ml/datasets/covertype,Covertype dataset,To distributedly train a multi-class logistic regression classifier on the ," as in the paper,  run the following command first"
11050,https://public.ukp.informatik.tu-darmstadt.de/coling2018-graph-neural-networks-question-answering/wikidata-access-master.zip,zip, (,) for sending queries to a local Wikidata endpoint
11050,WikidataHowTo.md,here,A local copy of the Wikidata knowledge base in RDF format. See , for more info on the Wikidata installation. (This step takes a lot of time!)
11050,questionanswering/preprocessing/map_dataset_to_wikidata.py,script for mapping to Wikidata, for the data set and the , for more details.
11050,https://public.ukp.informatik.tu-darmstadt.de/coling2018-graph-neural-networks-question-answering/qald.examples.test.wikidata.json,here,We have used the 80 open-questions from the QALD-7 train set for evaluation. You can this subset in the format accepted by our evaluation script ,.
11063,http://nlp.stanford.edu/data/glove.6B.zip,here,", which can be downloaded from ",.
11076,https://github.com/Ha0Tang/GestureGAN/tree/master/datasets/samples,code repo,"For hand gesture-to-gesture translation tasks, we use NTU Hand Digit and Creative Senz3D datasets. For cross-view image translation task, we use Dayton and CVUSA datasets. These datasets must be downloaded beforehand. Please download them on the respective webpages. In addition, we put a few sample images in this ",. Please cite their papers if you use the data.
11076,https://github.com/Ha0Tang/GestureGAN/tree/master/datasets/ntu_split,here, to generate hand skeletons and use them as training and testing data in our experiments. Note that we filter out failure cases in hand gesture estimation for training and testing. Please cite their papers if you use this dataset. Train/Test splits for Creative Senz3D dataset can be downloaded from ,. Download images and the crossponding extracted hand skeletons of this dataset:
11076,https://github.com/Ha0Tang/GestureGAN/tree/master/datasets/senz3d_split,here, to generate hand skeletons and use them as training data in our experiments. Note that we filter out failure cases in hand gesture estimation for training and testing. Please cite their papers if you use this dataset. Train/Test splits for Creative Senz3D dataset can be downloaded from ,. Download images and the crossponding extracted hand skeletons of this dataset:
11076,https://github.com/Ha0Tang/SelectionGAN/tree/master/datasets/dayton_split,here, trained on CityScapes dataset for generating semantic maps and use them as training data in our experiments. Please cite their papers if you use this dataset. Train/Test splits for Dayton dataset can be downloaded from ,.
11076,http://cs.uky.edu/~jacobs/datasets/cvusa/,page,", which is from the ",". After unzipping the dataset, prepare the training and testing data as discussed in "
11076,https://github.com/Ha0Tang/SelectionGAN/tree/master/selectiongan_v1#dataset-preparation,here,Or you can directly download the prepared Dayton and CVUSA data from ,.
11076,https://github.com/Ha0Tang/SelectionGAN/tree/master/selectiongan_v1/scripts/evaluation/KL_model_data.py,KL score,"
",", need install "
11077,https://github.com/MKLab-ITI/image-verification-corpus,VMU 2015 dataset,"For CCMR Twitter, each tweet is saved as a json object with keys ""tweet_id"", ""content"", ""image_id"", ""event"", and ""timestamp"". For CCMR Google and Baidu, each webpage is saved as a json object with keys ""url"", ""title"", ""image_id"", and ""event"". The values of ""image_id"" are lists of image or video names from ",". All of those image files and video URLs are available in ""images.zip""."
11077,http://nlp2ct.cis.umac.mo/um-corpus/index.html,UM-Corpus,Download parallel English and Mandarin sentence of news and microblogs from , and save them in a folder named 'UM_Corpus'.
11084,http://cs.uky.edu/~jacobs/datasets/cvusa/,CVUSA,"
","
"
11084,https://github.com/kregmi/cross-view-image-synthesis/tree/master/datasets/dayton_split,Dayton,Train/Test splits for Dayton Dataset can be downloaded from here ,.
11086,https://huggingface.co/docs/transformers/model_doc/data2vec,Data2Vec,"
","
"
11091,http://opendatacommons.org/licenses/odbl/summary/,Open Database License,Available under the ,"
"
11101,https://github.com/IITDBGroup/gprom/wiki/datalog_prov,Provenance Graphs for Datalog,"
","
"
11109,https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names,Other research,"Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. "," on the activity recognition dataset can use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much was the data preprocessed."
11109,https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones,dataset,The , can be found on the UCI Machine Learning Repository:
11110,http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d313030,that list,"The final accuracy is of 67.61% in average on the 100 fine labels, and is of 77.31% in average on the 20 coarse labels. My results are comparable to the ones in the middle of ",", under the CIFAR-100 section. The only image preprocessing that I do is a random flip left-right."
11112,#some-datasets,Some Datasets,"
","
"
11112,"https://www.google.ca/trends/explore?date=all&q=machine%20learning,deep%20learning,data%20science,computer%20programming",Google Trends,Here are the all-time ,", from 2004 up to now, September 2017:"
11112,https://books.google.ca/books?hl=en&as_coll=4&num=100&uid=103409002069648430166&source=gbs_slider_cls_metadata_4_mylibrary_title,Some other books I have read,"
", - Some books listed here are less related to deep learning but are still somehow relevant to this list.
11112,http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/,Discover structure behind data with decision trees,"
"," - Grow decision trees and visualize them, infer the hidden logic behind data."
11112,https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/,Why do 87% of data science projects never make it into production?,"
"," - Data is not to be overlooked, and communication between teams and data scientists is important to integrate solutions properly."
11112,https://towardsdatascience.com/what-is-the-main-reason-most-ml-projects-fail-515d409a161f,The real reason most ML projects fail,"
"," - Focus on clear business objectives, avoid pivots of algorithms unless you have really clean code, and be able to know when what you coded is ""good enough""."
11112,https://archive.ics.uci.edu/ml/datasets.html,UCI Machine Learning Repository,"
", - TONS of datasets for ML.
11112,https://github.com/caesar0301/awesome-public-datasets,Awesome Public Datasets,"
", - An awesome list of public datasets.
11112,http://www.datatau.com/,DataTau,"
"," - This is a hub similar to Hacker News, but specific to data science."
11113,http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d313030,that list,"The final accuracy is of 67.61% in average on the 100 fine labels, and is of 77.31% in average on the 20 coarse labels. My results are comparable to the ones in the middle of ",", under the CIFAR-100 section. The only image preprocessing that I do is a random flip left-right."
11115,https://github.com/Pascalson/chatbot-data,"our dataset repository (OpenSubtitles, Counting)",The dataset can be downloaded from ,". Feel free to cope with any dataset you prepare, but please make sure the format is consistent with our assigned format."
11116,http://ltdata1.informatik.uni-hamburg.de/path2vec/embeddings/jcn-semcor_embeddings.vec.gz,Jiang-Conrath (SemCor),"
","
"
11116,http://ltdata1.informatik.uni-hamburg.de/path2vec/embeddings/lch_embeddings.vec.gz,Leacock-Chodorow,"
","
"
11116,http://ltdata1.informatik.uni-hamburg.de/path2vec/embeddings/shp_embeddings.vec.gz,Shortest path,"
","
"
11116,http://ltdata1.informatik.uni-hamburg.de/path2vec/embeddings/wup_embeddings.vec.gz,Wu-Palmer,"
","
"
11116,https://ltnas1.informatik.uni-hamburg.de:8081/owncloud/index.php/s/lhcJQNxaGBLjL8o?path=%2Fdatasets,datasets,"
","
"
11129,code/data/get_otb_data.sh,script,You must download OTB2015 dataset (download ,) at first.
11132,http://data.crowdtruth.org/,Data,"
", collected with CrowdTruth
11141,https://s3.eu-west-2.amazonaws.com/nesta-open-data/arxiv_ai/corex_matched_noOAG.json,"Matched academic data, with CorEx topics","
", (the output of 
11144,https://gombru.github.io/2018/08/01/learning_from_web_data/,here,A blog post explaining the work is available ,.
11151,https://www.packtpub.com/big-data-and-business-intelligence/mastering-python-data-analysis,(Book!) Mastering Python Data Analysis,"
","
"
11164,data/lam/data.tar.gz,Full data,"
", (compressed) for reconstructing the model.
11164,data/lam/toy_data_with_noisy_user_ans.pkl,Toy data,"
"," that contains a subset of the full training data, for quick model test."
11171,#process-of-imagenet-dataset,Process of ImageNet dataset,"
","
"
11171,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,Facebook process of ImageNet,We follow the ,". Two subfolders (""train"" and ""val"") are included in the ""/path/to/ImageNet2012"". The correspding code is "
11174,http://visionandlanguage.net/VIST/dataset.html,visual storytelling dataset,"
","
"
11174,https://github.com/google-research-datasets/sentence-compression/tree/master/data,compression dataset,"
","
"
11180,http://datasets.d2.mpi-inf.mpg.de/eth80/eth80-cropped256.tgz,eth80,", ",).
11184,https://medium.com/jigsaw/identifying-machine-learning-bias-with-updated-data-sets-7c36d6063a2c,blog post,. See our accompanying , to learn more about how we created these datasets.
11190,http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html,iLIDS-VIDS," [3], "," [4], "
11191,https://www.yelp.com/dataset/challenge,yelp,"
","
"
11216,https://github.com/phohenecker/rel-data,here," format, which is specified in detail ",.
11216,/run-data-gen.sh,run-data-gen.sh,Running the data generator is as easy as cloning this repository and launching the shell script ,". Notice, however, that the application depends on numerous Python packages that need to be installed in order to run the same. For a complete list of dependencies, confer "
11217,http://opendatacommons.org/licenses/odbl/1.0/,Open Database License,. This has been published under the ,", which you should familiarize yourself with before using the data generator."
11217,https://github.com/phohenecker/rel-data,here," format, which is specified in detail ",.
11217,/run-data-gen.sh,run-data-gen.sh,Running the data generator is as easy as cloning this repository and launching the shell script ,". Notice, however, that the application depends on numerous Python packages that need to be installed in order to run the same. For a complete list of dependencies, confer "
11225,https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/,"
OpenPose foot dataset
","
","
"
11241,http://data.csail.mit.edu/places/places365/val_256.tar,raw dataset,Download the , from the Places365 website to 
11252,https://github.com/thunlp/MMDW/tree/master/data,Wiki,"
", (Wiki dataset is provided by 
11252,https://linqs.soe.ucsc.edu/data,Cora,"
",": 2708 nodes, 5429 edges, 7 labels, directed:"
11253,create_wn18_data.py,"
create_wn18_data.py
","Create a pickled WordNet prediction dataset in sparse matrix format, using ",". To use our exact dataset, obtain the distibution of WN18RR "
11253,https://github.com/villmow/datasets_knowledge_embedding/tree/master/WN18RR,here,". To use our exact dataset, obtain the distibution of WN18RR ", and point the script at the text version.
11256,http://implicitemotions.wassa2018.com/data/,website,"Alternatively, you could download the tweets according to their IDs, already published in the official ",", and not requiring any credentials. However, the organizers haven't published the code they used for replacing username mentions, newlines, urls, and trigger-words, so you might not end up with the same dataset that was used during the shared task."
11256,data/results/,"
data/results/
", of the current run in ,. See 
11256,data/results/,"
data/results/
"," by the hash of the model you wish to test, corresponding to the name of its directory located in ",.
11264,http://cocodataset.org/,COCO," team for providing online evaluation, ", team and 
11265,http://nlp.stanford.edu/data/glove.840B.300d.zip,pretrained GloVe model,Download ," and extract them into a folder called ""glove"". If the embeddings are stored elsewhere, the PRETRAINED_EMBEDDINGS_PATH variable in the config file needs to be changed."
11266,xsum-human-evaluation-data.tar.gz,Human Evaluation Data,"
","
"
11269,http://data.allenai.org/scitail/,SciTail,"
","
"
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-de.vec.gz,de,||||||| |---|---|---|---|---|---| | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-en.vec.gz,en, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-es.vec.gz,es, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-fr.vec.gz,fr, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-ru.vec.gz,ru, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/muse/muse-zh.vec.gz,zh, | , |
11270,https://github.com/artetxem/vecmap/blob/master/get_data.sh,VecMap dataset,These embeddings have been trained jointly using en-XX bilingual dictionaries and embeddings from the ,.
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/vecmap/vecmap-de.vec.gz,de,|||||| |---|---|---|---|---| | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/vecmap/vecmap-en.vec.gz,en, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/vecmap/vecmap-es.vec.gz,es, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/vecmap/vecmap-fi.vec.gz,fi, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/vecmap/vecmap-it.vec.gz,it, | , |
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/cc/en-hi.tgz,en-hi,|||| |---|---|---| | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/cc/en-bn.tgz,en-bn, | , | 
11270,https://akpublicdata.blob.core.windows.net/publicdata/geomm/cc/en-ta.tgz,en-ta, | , |
11280,https://github.com/facebookresearch/MUSE/blob/master/data/get_evaluation.sh#L99-L100,here,"Note: Requires bash 4. The download of Europarl is disabled by default (slow), you can enable it ",.
11285,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/train.txt,train,"
", / 
11285,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/valid.txt,valid, / , / 
11285,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/test.txt,test, / , / 
11285,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/vocab.txt,vocab, / ,"
"
11286,https://sites.cs.ucsb.edu/~xwhan/datasets/wiki.tar.gz,Wiki-One,Download datasets , and 
11286,https://sites.cs.ucsb.edu/~xwhan/datasets/nell.tar.gz,NELL-One, and ,"
"
11288,datasets/README.md,datasets/README.md, folder. Follow the specific instructions provided in: ,"
"
11288,http://datasets.cvc.uab.es/rrc/SynthText_90KDict.tar,here,The SynthText_90KDict dataset can be downloaded , (41Gb).
11291,https://github.com/freesunshine0316/nary-grn/blob/master/gs_lstm/G2S_data_stream.py#L26,here,"The value can be either 5, which corresponds to the normal binary classification setting (Table 3 and 5 in our paper), or the multi-label classification setting (Table 6 in our paper). The original relation set has 5 relations: 'resistance or non-response', 'sensitivity', 'response', 'resistance', 'None'. For binary setting, we follow Peng et al., (2017) to group the first four relations into one relation. More details can be found ","
"
11291,./gs_lstm/data,./gs_lstm/data,"We used 5-fold cross validation to conduct our experiment. If your dataset has a training/dev/test separation, just ignore the words below. To make things a little bit easier, we use file-of-file, where the first-level files store the locations of the data. One example is ""train_list_0"" and ""test_list_0"" in ",", where each line points to a file address. Our data has been segmented into 5 folds by Peng et al., thus we simply follow it. You need to modify both ""train_list_0"" and ""test_list_0"" and make the rest, such as ""train_list_1"" and ""test_list_1"""
11291,./gs_lstm/data,./gs_lstm/data,Other scripts within , is for extracting pretrained word embeddings. We use Glove-100d pretrained embeddings.
11291,./peng_data,this repository,"We put the data by Peng et al., (2017) inside ", for easy access for others.
11293,data/bibtex/sna.txt,Citation,. [,]
11293,data/bibtex/kth.txt,Citation,. [,]
11293,video_prediction/datasets/kth_dataset.py,"
kth_dataset.py
","To use a different dataset, preprocess it into TFRecords files and define a class for it. See ", for an example where the original dataset is given as videos.
11294,http://www.nltk.org/howto/corpus.html,NLTK corpus,We provide the pre-trained word vector file we used in the paper and a small subset of Penn Treebank data for testing the tagging code. This dataset contains 10% samples of Penn Treebank and is public in ,. Full Penn Treebank dataset requires a LDC license.
11295,https://github.com/fastai/fastai/blob/master/nbs/examples/dataloader_spawn.py,this example,See , to fully leverage the fastai API on Windows.
11296,http://ltdata1.informatik.uni-hamburg.de/sensegram/,"pre-trained models for English, German, and Russian",You can downlooad ,. Note that to run examples from the QuickStart you only need files with extensions 
11301,sample-data/single-formula.xml,example formula,Executing the following command derives a series of four increasingly general formulae from an ,:
11305,make_dataset.py,make_dataset.py,", which can use tagging datasets created using the ", script. Note that 
11306,https://github.com/taolei87/text_convnet/tree/master/data,this directory,We use the Stanford Sentiment Treebank (SST) datasets processed by Lei et al. (2015). Please put all the files of , into the 
11306,data/sst_text_convnet,data/sst_text_convnet, into the , folder.
11306,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe vectors,Please download the pre-trained , and unzip it into the 
11306,data,data, and unzip it into the , folder.
11312,https://snap.stanford.edu/data/web-BeerAdvocate.html,beer review,The original raw dataset can be found at: ,", "
11312,http://www.cs.virginia.edu/~hw5x/dataset.html,hotel review,", ",.
11312,https://people.csail.mit.edu/yujia/files/r2a/data.zip,data.zip,We provide the processed data (together with the machine-generated rationales) that we used for all our experiments at ,. 
11312,https://people.csail.mit.edu/yujia/files/r2a/data.zip,data.zip,Unzip , to the root directory of this repo.
11319,#data,Data,"
","
"
11331,http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,The Paris Dataset,. You can also use , to train your model
11334,https://datashare.is.ed.ac.uk/handle/10283/2211,download page,Voice Conversion Challenge 2016 (VCC2016): ,"
"
11335,data/README.md,instruction,. You may process your own data following our ,. Please 
11335,data/README.md,instruction,. Please refer to our examples and , for more details.
11337,#data,Data,"
","
"
11338,https://duc.nist.gov/data.html,the DUC website,Download the DUC01/02/04 data from , and extract the data to folder 'data/'.
11354,https://github.com/pytorch/examples/tree/75e435f98ab7aaa7f82632d4e633e8e03070e8ac/word_language_model/data/penn,Penn Treebank corpus,"Download PTB data. Note that the two tasks, i.e., language modeling and unsupervised parsing share the same model strucutre but require different formats of the PTB data. For language modeling we need the standard 10,000 word "," data, and for parsing we need "
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/AutomotivePartitioned.npy,Amazon Automotive,"
","
"
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/BeautyPartitioned.npy,Amazon Beauty,"
","
"
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/ClothingPartitioned.npy,Amazon Clothing,"
","
"
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/Toys_and_GamesPartitioned.npy,Amazon Toys,"
","
"
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/Video_GamesPartitioned.npy,Amazon Games,"
","
"
11359,http://cseweb.ucsd.edu/~wckang/MoHR/data/GooglePartitioned.npy,Google Local,"
","
"
11368,https://github.com/harvardnlp/neural-template-gen/blob/master/data/e2e_aligned.tar.gz,data/e2e_aligned.tar.gz,", and the preprocessed version of the data used for training is at ",. This preprocessed data uses the same database record preprocessing scheme applied by Sebastian Gehrmann in his 
11368,https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_e2e_labedata.py,data/make_e2e_labedata.py,", and also annotates text spans that occur in the corresponding database. Code for annotating the data in this way is at ",.
11368,https://github.com/DavidGrangier/wikipedia-biography-dataset,here,The WikiBio data is available ,", and the preprocessed version of the target-side data used for training is at "
11368,https://github.com/harvardnlp/neural-template-gen/blob/master/data/wb_aligned.tar.gz,data/wb_aligned.tar.gz,", and the preprocessed version of the target-side data used for training is at ",. This target-side data is again preprocessed to annotate spans appearing in the corresponding database. Code for this annotation is at 
11368,https://github.com/harvardnlp/neural-template-gen/blob/master/data/make_wikibio_labedata.py,data/make_wikibio_labedata.py,. This target-side data is again preprocessed to annotate spans appearing in the corresponding database. Code for this annotation is at ,. The source-side data can be downloaded directly from the 
11368,https://github.com/DavidGrangier/wikipedia-biography-dataset,WikiBio repo,. The source-side data can be downloaded directly from the ,", and we used it unchanged; in particular the "
11372,./data,./data, file in ," directory. While creating the pickle file, you will need to create a python dictionary with the following structure: "
11372,./data,./data, file in , directory. Please double check if you have generated this file successfully before you procede.
11377,data/responses/data/messages.csv,here,-formatted data can be found ,. For details regarding our annotation methodology please refer to the paper.
11412,https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology-test-data,here,An update on how to run the experiments from the paper is coming shortly. All test data reported in the paper can be found ,.
11421,https://github.com/facebookresearch/MUSE/blob/master/data/get_evaluation.sh#L99-L100,here,"Note: Requires bash 4. The download of Europarl is disabled by default (slow), you can enable it ",.
11422,[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).,End-to-end training with on-the-fly data processing,"
","
"
11432,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K, PSNR_oriented model trained with DF2K dataset (a merged dataset with , and 
11432,https://github.com/xinntao/BasicSR#datasets,BasicSR-Datasets,HR images can be downloaed from ,.
11432,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K, |:heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :o: | | urban100 | :heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :o: | | ,"
"
11438,http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,CUB-200-2011,Download the , datasets and put it in the root directory named 
11455,/data,Data,All the files with the Claim information are in the , folder. The files 
11455,/data/train.jsonl,train.jsonl, folder. The files ,", "
11455,/data/dev.jsonl,dev.jsonl,", ", and 
11455,/data/test.jsonl,test.jsonl, and , are the files extracted from the FEVER database.
11455,subsample_training_data.py,subsample_training_data.py,We also created a train subsample using the script ,.
11455,/data/subsample_train_relevant_docs.jsonl,subsample_train_relevant_docs.jsonl,The files ,", "
11455,/data/shared_task_dev_public_relevant_docs.jsonl,shared_task_dev_public_relevant_docs.jsonl,", ", and 
11455,/data/shared_task_test_relevant_docs.jsonl,shared_task_test_relevant_docs.jsonl, and , contain the information from the TF-IDF part of Document Retrieval (
11455,/fever-baselines#data-preparation,database," folder. First, download the "," and than, run the "
11455,/data,data, part. The files are already generated and can be found in the , folder.
11455,/data/subsample_train.jsonl,subsample_train.jsonl, contains already calculated probabilities for our ,. A file is genereted ready to be submitted.
11457,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,[glove.840B.300d.zip],Download the pretrained Glove vectors ,. Decompress the zip file and put the txt file in the root directory.
11462,https://github.com/ratishsp/data2text-macro-plan-py,macro planning repository," For a model with better relation generation precision (RG P%) and other metrics, please see the ", and the corresponding 
11462,https://github.com/harvardnlp/boxscore-data,boxscore-data repo,The boxscore-data json files can be downloaded from the ,.
11464,https://bitbucket.org/tkowark/data-rover,DataRover,We used the , project by 
11467,https://github.com/hunterhector/uima-base-tools/blob/master/corpus-reader/src/main/java/edu/cmu/cs/lti/collection_reader/AnnotatedNytReader.java,AnnotatedNytReader,"
","
"
11473,#download-dataset,Download Dataset,"
","
"
11473,#dataset-bugs,Report a bug,"
","
"
11478,http://curtis.ml.cmu.edu/datasets/graftnet/data_wikimovie.zip,WikiMovies,"
","
"
11478,http://curtis.ml.cmu.edu/datasets/graftnet/data_webqsp.zip,WebQuestionsSP,"
","
"
11478,http://curtis.ml.cmu.edu/datasets/graftnet/model_wikimovie.zip,WikiMovies,"
","
"
11478,http://curtis.ml.cmu.edu/datasets/graftnet/model_webqsp.zip,WebQuestionsSP,"
","
"
11479,https://sheffieldnlp.github.io/fever/data.html,the website of the FEVER share task,Download the FEVER dataset from , into the data directory
11482,https://www.cityscapes-dataset.com/method-details/?submissionID=7836,85.4%," have been accepted by the ECCV-2020. Notably, the reseachers from Nvidia set a new state-of-the-art performance on Cityscapes leaderboard: ", via combining our HRNet + OCR with a new 
11485,http://ai.stanford.edu/~amaas/data/sentiment/,Large movie review dataset,The dataset construction is based on the method noted in ," from Maas et al., 2011."
11488,http://rpg.ifi.uzh.ch/datasets/netvlad/vd16_pitts30k_conv5_3_vlad_preL2_intra_white.zip,weights of the original NetVLAD model,. The , are provided by 
11494,http://mscoco.org/dataset/#download,here,". As for COCO dataset, we use COCO 2014, which can be downloaded ",". And in case of COCO changes in the future, we also provide a download link "
11494,./data/imagenet,./data/imagenet," on google drive. After downloading, you need to move the imagenet.tar.gz to ", and extract the file there.
11494,./data/nuswide_81,./data/nuswide_81,"Also, for NUS-WIDE, you need to move the nus_wide.tar.gz to ", and extract the file there.
11494,./data/coco,./data/coco,"For COCO dataset, you need to extract both train and val archive for COCO in ",. If you download from 
11494,http://mscoco.org/dataset/#download,COCO download page,. If you download from ,","
11498,https://github.com/OpenNMT/IntegrationTesting/tree/master/data,OpenNMT/IntegrationTesting,: An English-German translation model based on the 200k sentence dataset at ,. Perplexity: 20.
11499,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove.840B.300d.zip,"
","
"
11519,https://www.robots.ox.ac.uk/~vgg/data/dtd/,Describable Textures Dataset, and ,.
11519,https://www.robots.ox.ac.uk/~vgg/data/dtd/,Describable Textures Dataset, and ,.
11519,http://mscoco.org/dataset/#download,MS COCO images,Download , for content data.
11550,https://github.com/PyThaiNLP/pythainlp/blob/dev/pythainlp/corpus/corpus_license.md,Corpus License,  | | Other corpora and models that may included with PyThaiNLP | See , |
11562,datasets.py,"
datasets.py
", | string | yes | | The type of the dataset to be used corresponding to one of the classes defined in ,", but without the "
11570,#dataset--preparation,Dataset Preparation,"
","
"
11570,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp,C++ code for Oxford Building,"For mAP calculation, you also can refer to the ",. We use the triangle mAP calculation (consistent with the Market1501 original code).
11579,https://dumps.wikimedia.org/wikidatawiki/entities/,WikiData,The importer takes JSON dumps from ," and imports entities / properties, then generates relations between each other and claims."
11579,https://dumps.wikimedia.org/wikidatawiki/entities/,WikiData JSON File,"
","
"
11579,https://www.wikidata.org/wiki/Special:ListDatatypes,here,A list of properties can be found ,"
"
11579,https://github.com/findie/wikidata-neo4j-importer/blob/master/LICENSE.MD,here,License can be found ,"
"
11585,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove.840B.300d,"GloVe vectors are required, please download ", first. run 
11592,data/test_problems.txt,data/test_problems.txt,| File                                                         | Description                         | | -------------------------------------------------------------| ------------------------------------| | ,             | held out tree manipulation problems | | 
11592,data/repeat_test.txt,data/repeat_test.txt,             | held out tree manipulation problems | | ,                 | generalization: repeat(N)           | | 
11592,data/drop_last_test.txt,data/drop_last_test.txt,                 | generalization: repeat(N)           | | ,           | generalization: dropLast(N)         | | 
11592,data/bring_to_front_test.txt,data/bring_to_front_test.txt,           | generalization: dropLast(N)         | | , | generalization: bringToFront(N)     |
11594,./modules/datasets/,"
datasets
","
", : Provides dataset parsing and evaluation utilities with precaching and async processing support.
11599,https://www.cityscapes-dataset.com/method-details/?submissionID=4792,"
82.9%
",2020/8：The new TNNLS version DRANet achieves ," on Cityscapes test set (submit the result on August, 2019), which is a new state-of-the-arts performance with only using fine annotated dataset and Resnet-101. The code will be released in "
11599,https://www.cityscapes-dataset.com/,Cityscapes,Download the , dataset and convert the dataset to 
11609,https://github.com/utiasSTARS/certifiable-calibration/tree/master/data,"
data/
",. The , directory contains files with experimental results from our paper.
11628,https://github.com/OpenNMT/IntegrationTesting/tree/master/data,OpenNMT/IntegrationTesting,: An English-German translation model based on the 200k sentence dataset at ,. Perplexity: 21.
11637,http://mokk.bme.hu/resources/hunglishcorpus,Hunglish Corpus,hunalign was developed under the Hunglish Project to build the ,.
11638,#datasets,Datasets,"
","
"
11638,data/squad_japanese_test.json,japanese_squad.json,| | Multilingual SQuAD Datasets       | | ------------- |:-------------:| | Japanese    | , | | French | 
11638,data/squad_french_test.json,french_squad.json, | | French | , |
11638,data/questions_jaen.ja,questions.ja,| | question sentences        | | ------------- |:-------------:| | Japanese     | ,", "
11638,data/questions_jaen.en,questions.en,", ", | | French  | 
11638,data/questions_fren.fr,questions.fr, | | French  | ,", "
11638,data/questions_jaen.en,questions.en,", ", |
11645,https://github.com/ir-nlp-csui/idn-tagged-corpus-CSUI,repository,please refer to the IR-NLP Lab's , for official updates and future versions of this work
11646,https://github.com/famrashel/idn-tagged-corpus,IDN Tagged Corpus," replaced with the fold number. Each file contains the indices of the sentences in the original corpus. To obtain the sentences, you must first download the ",". Then, run"
11668,#data,Data,"
","
"
11668,#data-1,Data,"
","
"
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Train_mscoco.zip,training question files,"
","
"
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Questions_Val_mscoco.zip,validation question files,"
","
"
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Questions_Train_mscoco.zip,here,"Question files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Train_mscoco.zip,training annotation files,"
","
"
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.9/Annotations_Val_mscoco.zip,validation annotation files,"
","
"
11693,http://visualqa.org/data/mscoco/prev_rel/Beta_v0.1/Annotations_Train_mscoco.zip,here,"Annotation files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found ",.
11693,http://mscoco.org/dataset/#download,MS COCO website,"For real, create a directory with name mscoco inside this directory. For each of train, val and test, create directories with names train2014, val2014 and test2015 respectively inside mscoco directory, download respective images from ", and place them in respective folders.
11694,https://www.cs.ucr.edu/~eamonn/time_series_data_2018/,here,The password can be found ,.
11694,http://www.mustafabaydogan.com/files/viewcategory/20-data-sets.html,MTS archive,The ,", which contains the 13 multivariate time series datasets."
11694,https://pandas.pydata.org/,pandas,"
","
"
11694,https://www.cs.ucr.edu/~eamonn/time_series_data_2018/,UCR archive 2018, on the 128 datasets from the ,. Our 
11709,docs/new.md#dataset,here,A new dataset for evaluation ,.
11713,www.cs.ucr.edu/~eamonn/time_series_data/,The UCR Time Series Classification Archive,"C. Yanping, K. Eamonn, H. Bing, B. Nurjahan, B. Anthony, M. Abdullah and B. Gustavo. ",", 2015."
11729,http://mscoco.org/dataset/#download,link,Download the coco images from ,. We need 2014 training images and 2014 val. images. You should put the 
11733,http://www.cs.ucr.edu/~eamonn/time_series_data/,Dr. Eamonn Keogh at University of California Riverside,The work of , has shown that a good way to classify time series is with a k-NN algorithm using a dynamic time warping similarity measure.
11739,http://crcns.org/data-sets/hc/hc-1/about,"
HC1
","
","
"
11739,http://data.cortexlab.net/singlePhase3/,"
Neuropixels
","
", (requires permission from 
11750,http://nlp2ct.cis.umac.mo/um-corpus/,UM-Corpus,Get training dataset for English-Chinese parallel corpus: ,.
11750,http://nlp2ct.cis.umac.mo/um-corpus/,UM-Corpus,will generate all the training and evaluation files for the Engilsh-Chinese language pair. Note that there are several domains in ,", and we simply concatenate all the files."
11757,http://aif360.mybluemix.net/data,AI Fairness 360 interactive experience,The , provides a gentle introduction to the concepts and capabilities. The 
11757,aif360/data/README.md,aif360/data/README.md,"If you'd like to run the examples, download the datasets now and place them in their respective folders as described in ",.
11757,aif360/data/README.md,aif360/data/README.md,"Finally, if you did not already, download the datasets as described in ",.
11769,https://github.com/fieldsend/multiobjective_data_structures,Data Structures for Non-Dominated Sets by Jonathan Fieldsend,"
","
"
11774,data/data_generators/cipher_generator.py,"
cipher_generator
",We make use of data generators to generate the TFRecords that are used for training. Of particular note is ,", which may be used to generate data for the shift and Vigenère ciphers that were tested in the paper."
11780,http://www.cs.ecu.edu/hillsma/experiments/corpus-icse13.tgz,first part of the corpus,. The ," is used in both papers, while "
11780,http://www.cs.ecu.edu/hillsma/experiments/corpus-includes-extension.tgz,the second," is used in both papers, while ", is used just in the 
11780,http://www.cs.ecu.edu/hillsma/experiments/corpus-icse13.tgz,base corpus," is not installed, click on the "," link, save this to the "
11786,DriftDetection.md#data-drift-detection,Data drift detection,"
", (
11786,DriftDetection.md#data-drift-detection,Data drift detection and model update with streaming data,"
","
"
11786,#data-drift-detection,below, (Default): Replace trees based on KL-divergence. Further details are ,". For this mode, set "
11790,https://www.ibm.com/academic/technology/data-science,IBM Academic Initiative,Register for ,"
"
11812,https://www.globalbioticinteractions.org/?interactionType=ecologicallyRelatedTo&accordingTo=globi:hurlbertlab/dietdatabase&refutes=true&refutes=false,"
GloBI Index Status
","
","
"
11812,https://depot.globalbioticinteractions.org/reviews/hurlbertlab/dietdatabase/README.txt,"
GloBI Review
","
","
"
11833,https://zenodo.org/record/804392/files/data.tgz,here,", which can be downloaded ",.
11848,https://www.cityscapes-dataset.com/,Cityscapes,These images are from the , and the 
11848,https://svsdataset.github.io/,SVS, and the , datasets.
11848,https://www.cityscapes-dataset.com/,Cityscapes dataset,"For better adaptation to real world images, we have used the ",.
11860,data/task1/raw,data/task1/raw,Raw files under ,"
"
11860,data/task1/tok,data/task1/tok,Tokenized files under ,. These files were produced with the preprocessing script 
11878,#training-data,Training data,"
","
"
11878,#fine-tuning-the-pre-trained-model-on-your-data,Fine-tuning the pre-trained model on your data,"
","
"
11878,data/corpora_processed/train_processed_dialogs.txt,dummy train dataset,"The training data should be a txt file, where each line is a valid json object, representing a list of dialog utterances. Refer to our ", to see the necessary file structure. Replace this dummy corpus with your data before training.
11878,data/corpora_processed/train_processed_dialogs.txt,"
data/corpora_processed/train_processed_dialogs.txt
",Put your training text corpus to ,". Make sure that your dataset is large enough, otherwise your model risks to overfit the data and the results will be poor."
11878,data/corpora_processed/train_processed_dialogs.txt,"
data/corpora_processed/train_processed_dialogs.txt
",Put your training text corpus to ,.
11878,data/corpora_processed/val_processed_dialogs.txt,"
data/corpora_processed/val_processed_dialogs.txt
","
","(dummy example, replace with your data) – for the context-sensitive dataset"
11878,data/quality/context_free_validation_set.txt,"
data/quality/context_free_validation_set.txt
","
", – for the context-free validation dataset
11878,data/quality/context_free_questions.txt,"
data/quality/context_free_questions.txt
","
", – is used for generating responses for logging and computing distinct-metrics
11878,data/quality/context_free_test_set.txt,"
data/quality/context_free_test_set.txt
","
"," – is used for computing metrics of the trained model, e.g. ranking metrics"
11878,data/corpora_processed/test_processed_dialogs.txt,test data,You can run the following tools to evaluate your trained model on ,"(dummy example, replace with your data):"
11878,data/corpora_processed/train_processed_dialogs.txt,training set,". Just set the ""condition"" field in the ", to one of the following: 
11894,utils/datasetcollections.py,dataset collections,This runs all experiments included in the results section of our paper. It is up to the user to download the datasets and link to them in the , file. The output folders including the folder where the data will be stored (in this case 
11901,https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741,solution description on Kaggle,See also ,"
"
11916,https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/software-and-datasets/,official website,Download the BRAINWASH dataset from the ,. Unzip it and store the dataset in the 
11925,https://github.com/idsia-robotics/proximity-quadrotor-learning/tree/master/dataset,here,A Jupyter notebook implementing dataset extraction from rosbag files can be found ,.
11941,http://groups.google.com/group/googleclusterdata-discuss,discussion group,Please join our (low volume) ,", so we can send you announcements, and you can let us know about any issues, insights, or papers you publish using these traces. "
11941,mailto:googleclusterdata-discuss@googlegroups.com,googleclusterdata-discuss@googlegroups.com,". Once you are a member, you can send email to ", to:
11941,mailto:googleclusterdata-discuss@googlegroups.com,googleclusterdata-discuss@googlegroups.com," of papers that have used and/or analyzed the traces, and encourage anybody who publishes one to add it to the bibliography using a github pull request [preferred], or by emailing the bibtex entry to ",".  In either case, please mimic the existing format "
11945,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data,data,"All data, including Sequential Data, Industry Relation, and Wiki Relation, are under the ", folder.
11945,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data/google_finance,google_finance,Raw data: files under the ," folder are the historical (30 years) End-of-day data (i.e., open, high, low, close prices and trading volume) of more than 8,000 stocks traded in US stock market collected from Google Finance."
11945,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data/2013-01-01,2013-01-01,Processed data: , is the dataset used to conducted experiments in our paper.
11948,https://download.visinf.tu-darmstadt.de/data/from_games/,The GTA5 Dataset,Download ,"
"
11948,http://synthia-dataset.net/download-2/,The SYNTHIA Dataset,Download ,"
"
11948,https://www.cityscapes-dataset.com/,The Cityscapes Dataset,Download ,"
"
11954,https://www.visuallocalization.net/datasets/,HERE,"Because of sesitivity to instrinsic camera characteristics, testing should ideally be on the same Oxford dataset photos (and same Grasshopper camera) found conveniently preprocessed and ready-to-use ",.
11957,http://bollin.inf.ed.ac.uk/public/direct/Refresh-NAACL18-preprocessed-input-data.tar.gz,Preprocessed CNN and DailyMail data,"
",": Articles are tokenized/segmented with the original case. Then, words are replaced with word ids in the word embedding file with (PAD_ID = 0, UNK_ID = 1). (1.9GB)"
11957,http://bollin.inf.ed.ac.uk/public/direct/Refresh-NAACL18-baseline-gold-data.tar.gz,Gold Test and Validation highlights,"
",: These files are used to estimate ROUGE scores. (11MB)
11959,https://www.ilexir.co.uk/datasets/index.html,here,You will need to acquire source data in order to run these experiments. You can request for the original FCE dataset from ,", and you can request the original authors of the respective papers for the "
11961,#24-supported-datasets,Supported datasets,"
","
"
11961,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,Stanford Cars,: Interface to the , dataset with an input image size of 448x448.
11961,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html,Oxford Flowers-102,: Interface to the , dataset with an input image size of 448x448.
11961,datasets/,"
datasets
",Own dataset interfaces can be defined by creating a new module in the ," package, defining a class derived from "
11961,datasets/common.py,"
FileDatasetGenerator
"," package, defining a class derived from ",", importing it in "
11961,datasets/__init__.py,"
datasets/__init__.py
",", importing it in ",", and adding a branch for it in the "
11962,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,See https://github.com/JakobEngel/dso for how to run on a dataset. Run on a dataset from , using:
11971,https://github.com/big-data-lab-team/repro-tools,repro-tools, from , version 0.1.1.
11971,https://github.com/big-data-lab-team/repro-tools,repro-tools, from , version 0.1.1.
11987,https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data,Raw SC09,Techsorflow Challenge ,"
"
11991,#datasets,Datasets,"
","
"
12000,https://github.com/nicola-decao/MolGAN/tree/master/data,data,"
",: should contain your datasets. If you run 
12005,https://aws.amazon.com/public-data-sets/spacenet,this link,"Note that the sample images bundled together with the visualizer tool are not actual satellite images, these are only added for demonstration purposes. The 3-band images are created from aerial photography, the 8-band images are created by image processing manipulation of the 3-band images. (The Red, Green, Blue channels are correct but the Coastal, Near-IR1, etc. channels are fake.) You need to download the training data set of the contest to obtain real, ground-truthed satellite imagery. See the problem statement and ", for details on how to access real data.
12025,https://github.com/budzianowski/multiwoz/blob/master/data/MultiWOZ_2.2,MultiWOZ_2.2,"The newest, corrected version of the dataset is available at ", thanks to 
12025,https://github.com/budzianowski/multiwoz/blob/master/data/MultiWOZ_2.1.zip,MultiWOZ_2.1,"The new, corrected version of the dataset is available at ", thanks to 
12025,https://github.com/budzianowski/multiwoz/blob/master/data/MultiWOZ_2.0.zip,MultiWOZ_2.0,The dataset used in the EMNLP publication can be accessed at: ,"
"
12025,https://github.com/budzianowski/multiwoz/blob/master/data/MultiWOZ_1.0.zip,MultiWOZ_1.0,The dataset used in the ACL publication can be accessed at: ,"
"
12045,https://paperdatasets.s3.amazonaws.com/tcga.db,tcga.db,You can download the raw data under these links: , and 
12045,https://paperdatasets.s3.amazonaws.com/news.db,news.db, and ,.
12049,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUM RGB-D benchmark,The program supports datasets in the format of the , with two small additions:
12056,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry sequences,We use , and 
12056,http://robotcar-dataset.robots.ox.ac.uk/datasets/,Oxford Robotcar dataset, and , to evaluate the robustness of our method.
12076,https://www.cityscapes-dataset.com/,Cityscapes," (bicubically resized to 128×128 px), and ", (bicubically resized to 128×128 px). For MovingMNIST and KTH Action the networks were trained to predict 6 frames conditioned on 6 input frames. For Cityscapes they were trained to predict 5 frames based on 5 input frames. 
12078,https://ai.googleblog.com/2019/03/reducing-need-for-labeled-data-in.html,"
<font color=""green"">[Blog Post]</font>
","
","
"
12078,https://www.tensorflow.org/datasets,TensorFlow Datasets,Compare GAN uses , and it will automatically download and prepare the data. For ImageNet you will need to download the archive yourself. For CelebAHq you need to download and prepare the images on your own. If you are using TPUs make sure to point the training script to your Google Storage Bucket (
12097,#data-loading,used with PyTorch data loaders,CINIC-10 is saved to be ,. The following folder structure is used:
12097,https://datashare.is.ed.ac.uk/handle/10283/3192,use this link,To download either , or the following wget command:
12102,http://cocodataset.org/#home,"
Objects
",", ",", "
12107,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,"For training, download ", dataset and place the folder into prepare. Then run the 
12110,#input-data,general input requirements,"To get started, preprocess your input data. Input data has slightly different requirements depending on whether you are using Aequitas via the webapp, CLI or Python package. See ", and specific requirements for the 
12110,#input-data-for-webapp,web app, and specific requirements for the ,", "
12110,#input-data-for-cli,CLI,", ",", and "
12110,#input-data-for-python-api,Python API,", and ", in the section immediately below.
12122,doc/dataset.md,this document,Instructions for preparing datasets for pretraining and transfer to VQA is described in ,.
12125,datasets/bibtex/facades.tex,Citation,. [,]
12125,https://www.cityscapes-dataset.com/,Cityscapes training set,: 2975 images from the ,. [
12125,datasets/bibtex/cityscapes.tex,Citation,. [,"]. Note: Due to license issue, we do not host the dataset on our repo. Please download the dataset directly from the Cityscapes webpage. Please refer to "
12133,https://github.com/google-research/episodic-curiosity/blob/master/episodic_curiosity/generate_r_training_data.py,generate_r_training_data.py,", it first launches "," to accumulate training data for the R-network using a random policy, then launches "
12133,https://cloud.google.com/compute/docs/storing-retrieving-metadata,Instance Metadata," launches one VM for each (scenario, method, run_number) tuple. The VMs use startup scripts to launch training, and retrieve the parameters of the run through ",.
12138,https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet,torchvision,The ImageNet dataset (which can be automatically downloaded by recent version of ,)
12151,https://figshare.com/articles/antithetic-vae-datasets/7531013,here,"We use many classic datasets for density estimation: MNIST, OMNIGLOT, FashionMNIST, Histopathology, Caltech101, and FreyFaces. Many of these are available in torchvision, but for the others, I've included a frozen version that should be downloaded ",.
12163,https://www.dropbox.com/s/eq1iuu4trkjkopt/entity-tracking-data.zip?dl=0,here,The relevant datasets can be downloaded from ,", which include:"
12165,data/GetData.md,GetData.md,Copy the sample data into the Docker container. Refer to , to obtain datasets needed for notebooks.
12165,data/GetData.md,GetData.md,Copy the sample data into the mli-resources repo directory. Refer to , to obtain datasets needed for notebooks.
12165,http://www.oreilly.com/data/free/an-introduction-to-machine-learning-interpretability.csp,An Introduction to Machine Learning Interpretability,"
","
"
12165,https://www.slideshare.net/0xdata/interpretable-machine-learning,Ideas on Interpreting Machine Learning - SlideShare,"
","
"
12166,https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476,The Importance of Human Interpretable Machine Learning,"
","
"
12166,https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739,Model Interpretation Strategies,"
","
"
12166,https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608,Hands-on Machine Learning Model Interpretation,"
","
"
12166,https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608,Interpreting Deep Learning Models for Computer Vision,"
","
"
12166,https://github.com/datascienceinc/Skater,Skater,"
","
"
12166,https://github.com/datamllab/xdeep,xdeep,"
","
"
12166,https://github.com/google/ml-metadata,mlmd,"
","
"
12166,https://mitpress.mit.edu/books/data-feminism,Data Feminism,"
","
"
12166,https://www.scu.edu/ethics/focus-areas/technology-ethics/resources/an-introduction-to-data-ethics/,An Introduction to Data Ethics,"
","
"
12166,https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2017/07/royal-free-google-deepmind-trial-failed-to-comply-with-data-protection-law/,Jul 2017 - Royal Free - Google DeepMind trial failed to comply with data protection law,"
","
"
12166,https://www.cnbc.com/2018/04/05/facebook-building-8-explored-data-sharing-agreement-with-hospitals.html,Jun 2018 - Facebook sent a doctor on a secret mission to ask hospitals to share patient data,"
","
"
12166,https://techcrunch.com/2019/01/09/cambridge-analyticas-parent-pleads-guilty-to-breaking-uk-data-law/,Jan 2019 - Cambridge Analytica’s parent pleads guilty to breaking UK data law,"
","
"
12166,https://www.nytimes.com/2019/06/26/technology/google-university-chicago-data-sharing-lawsuit.html,Jun 2019 - Google and the University of Chicago Are Sued Over Data Sharing,"
","
"
12166,https://www.biometricupdate.com/202007/facial-biometrics-training-dataset-leads-to-bipa-lawsuits-against-amazon-alphabet-and-microsoft,"Jul 2020 - Facial biometrics training dataset leads to BIPA lawsuits against Amazon, Alphabet and Microsoft","
","
"
12166,https://theintercept.com/2020/07/09/twitter-dataminr-police-spy-surveillance-black-lives-matter-protests/,Jul 2020 - POLICE SURVEILLED GEORGE FLOYD PROTESTS WITH HELP FROM TWITTER-AFFILIATED STARTUP DATAMINR,"
","
"
12166,https://www.zdnet.com/article/when-algorithms-define-kids-by-postcode-uk-exam-results-chaos-reveal-too-much-reliance-on-data-analytics/,Aug 2020 - When algorithms define kids by postcode: UK exam results chaos reveal too much reliance on data analytics,"
","
"
12166,https://www.tampabay.com/news/pasco/2020/09/03/pascos-sheriff-uses-data-to-guess-who-will-commit-crime-then-deputies-hunt-down-and-harass-them/,Sep 2020 - Pasco’s sheriff uses data to guess who will commit crime. Then deputies ‘hunt down’ and harass them,"
","
"
12166,https://techcrunch.com/2020/09/10/ola-is-facing-a-drivers-legal-challenge-over-data-access-rights-and-algorithmic-management/,Sep 2020 - Ola is facing a drivers’ legal challenge over data access rights and algorithmic management,"
","
"
12166,https://www.theaustralian.com.au/business/technology/australian-researchers-at-data61-show-you-could-become-invisible-to-a-security-camera/news-story/491b70e05c8fbdd566c1b2fd30b6d427,Oct 2020 - Australian researchers have shown how you could become invisible to security cameras,"
","
"
12166,https://www.dailymail.co.uk/news/article-9100755/Girl-12-suing-TikTok-alleged-misuse-personal-information-data-protection-law-breaches.html,"Dec 2020 - Girl, 12, is suing social media giant TikTok for alleged misuse of personal information and breaches of data protection laws","
","
"
12166,https://techcrunch.com/2021/02/12/swedens-data-watchdog-slaps-police-for-unlawful-use-of-clearview-ai/,Feb 2021 - Sweden’s data watchdog slaps police for unlawful use of Clearview AI,"
","
"
12166,https://www.datacenterknowledge.com/security/ai-wielding-hackers-are-here,Feb 2021 - AI-Wielding Hackers are Here,"
","
"
12166,https://ant.isi.edu/datasets/addelivery/,Research Outputs from Auditing for Discrimination in Job Ad Delivery, (see also , on the USC Information Sciences Institute web site)
12166,https://www.law360.com/ip/articles/1375537/facebook-princeton-must-face-ai-data-theft-claims,"Apr 2021 - Facebook, Princeton Must Face AI Data Theft Claims","
","
"
12166,https://venturebeat.com/2021/05/06/deepfake-detectors-and-datasets-exhibit-racial-and-gender-bias-usc-study-shows/,"May 2021 - Deepfake detectors and datasets exhibit racial and gender bias, USC study shows","
","
"
12173,data/meetup-normalised-comembership.edges,meetup-normalised-comembership.edges," that allows access to data from its platform. In September 2018 we used this to collect information about all meetups in Dublin, Ireland. This data was used to construct a weighted network, where each unique meetup is represented as a node, and a weighted edge between two nodes represents an association between the two meetups represented by its endpoint. The file "," contains this network, in edge list format: (i.e. "
12173,data/meetup-metadata.csv,meetup-metadata.csv,). The corresponding file , contains the corresponding metadata for each node.
12179,https://echr-opendata.eu,ECHR-OD,The datasets are not provided with the library and you must manually download them. See ,. The library will try to access the data in three different ways:
12181,https://echr-opendata.eu,ECHR-OD project,Download all datasets from the , and place them in 
12184,http://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,the flying chair dataset,"The code provides a training example, using "," , with data augmentation. An implementation for "
12184,http://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow Datasets," , with data augmentation. An implementation for ", may be added in the future.
12184,http://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,the flying chair dataset,"First, you need to download the ", . It is ~64GB big and we recommend you put it in a SSD Drive.
12185,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,For ,", first download the dataset using this "
12185,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,", first download the dataset using this "," provided on the official website, and then run the following command. The "
12185,https://www.cityscapes-dataset.com/,Cityscapes,For ,", download the following packages: 1) "
12185,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,Odometry dataset,Pose evaluation is also available on ,. Be sure to download both color images and pose !
12201,https://webis.de/data/webis-cls-10.html,Amazon, and ," datasets, as well as the multilingual word embeddings ("
12208,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI raw data,You would need to download all of the , and calibration files to train the model. You would also need the training files of 
12208,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo,KITTI 2012, and calibration files to train the model. You would also need the training files of , and 
12208,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI 2015, and , for validating the models.
12234,https://github.com/AKASH2907/bird-species-classification/blob/master/modify_data.py,modify_data.py,"
", - This code is used to rename the files. 
12234,https://github.com/AKASH2907/bird-species-classification/blob/master/data_augmentation/data_augmentation.py,data_augmentation.py,"
"," - Various types of data augmentation used to counter the challenge of large scale variation in illumination,scale, etc. and class imbalance."
12234,https://github.com/AKASH2907/bird-species-classification/blob/master/gen_train_data_test_data.py,gen_train_data_test_data.py,"
"," - Generates X_train, Y_train, X_validation, Y_validation, X_test, Y_test"
12234,https://github.com/AKASH2907/bird-species-classification/tree/master/data_augmentation,data_augmentation folder,.Table for data Augmentation done for different species is shared in ,.
12241,https://www.tensorflow.org/datasets,TensorFlow Datasets,"Alternatively, you can access the DeepWeeds dataset with ",", TensorFlow's official collection of ready-to-use datasets. "
12241,https://www.tensorflow.org/datasets/catalog/deep_weeds,DeepWeeds,", TensorFlow's official collection of ready-to-use datasets. ", was officially added to the TensorFlow Datasets catalog in August 2019.
12247,https://github.com/m4nh/cables_dataset,Cable Dataset,Check also for , ...
12252,https://github.com/warnikchow/sae4k/blob/master/data/sae4k_v1.txt,sae4k_v1.txt,"
",: Original Corpus
12252,https://github.com/warnikchow/sae4k/blob/master/data/sae4k_v2.txt,sae4k_v2.txt,"
",: Augmented Corpus
12253,https://medium.com/balabit-unsupervised/releasing-the-balabit-mouse-dynamics-challenge-data-set-a15a016fba6c,Balabit Mouse Dynamics Challenge,"
","
"
12253,https://datapallet.io,datapallet.io, data set which includes timing and positioning information of mouse pointers. It can be used for evaluating the performance of behavioral biometric algorithms based on mouse dynamics for user authentication/identification purposes. We make the data set accessible to researchers and experts in the fields of IT security and data science with the hope of contributing to research and providing a benchmark data set. The data set was first made public during a data science competition on ,. The files and descriptions in this repository originate from the challenge itself.
12254,https://github.com/OpenNMT/IntegrationTesting/tree/master/data,OpenNMT/IntegrationTesting,: An English-German translation model based on the 200k sentence dataset at ,. Perplexity: 20.
12264,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html,VoxCeleb2,This model was trained with ," dataset, where utterances are randomly fit to time length [70, 90] frames. Tests are done with window 80 / hop 40 and have shown equal error rate about 1%. Data used for test were selected from first 8 speakers of "
12264,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,VoxCeleb1," dataset, where utterances are randomly fit to time length [70, 90] frames. Tests are done with window 80 / hop 40 and have shown equal error rate about 1%. Data used for test were selected from first 8 speakers of "," test dataset, where 10 utterances per each speakers are randomly selected."
12273,https://grouplens.org/datasets/movielens/,MovieLens, and it deals with a CSV file saved in a format similar to the one of ,;
12273,http://pandas.pydata.org/,pandas,", ",", "
12273,http://web.archive.org/web/20170629232107/https://www.cs.cornell.edu/~shuochen/lme/data_page.html,Yes.com, contains a random sample of the ," dataset, reduced 10 times its original size. Please note that the Yes.com dataset was originally released under the terms of the "
12274,http://datascience.ismb.it/reclab/,http://datascience.ismb.it/reclab/,A demo of RecLab is available at ,.
12293,datasets/mnist.yaml,"
datasets/mnist.yaml
"," for an example of a neural network model description, "," of the MNIST dataset, and "
12293,http://cocodataset.org,COCO," classification, but also object detection with ",", or even customized tasks."
12307,http://mscoco.org/dataset/#download,download,Visit MS COCO , page for more details.
12307,http://mscoco.org/dataset/#format,format,Visit MS COCO , page for more details.
12317,https://github.com/oxwhirl/smac/releases/download/v1/smac_run_data.json,here,Data from the runs used in the paper is included ,. 
12332,https://timelydataflow.github.io/timely-dataflow/,long-form text,". It is a work in progress, but mostly improving. There is more ", in 
12332,https://github.com/timelydataflow/timely-dataflow/blob/master/timely/examples/simple.rs,timely/examples/simple.rs,", which should allow you to start writing timely dataflow programs like this one (also available in ",):
12332,https://github.com/timelydataflow/timely-dataflow/blob/master/examples/hello.rs,examples/hello.rs,"For a more involved example, consider the very similar (but more explicit) ",", which creates and drives the dataflow separately:"
12332,https://docs.rs/timely/0.6.0/timely/dataflow/operators/index.html,"
Timely dataflow
","
",": Timely dataflow includes several primitive operators, including standard operators like "
12332,https://github.com/timelydataflow/differential-dataflow,"
Differential dataflow
","
",": A higher-level language built on timely dataflow, differential dataflow includes operators like "
12332,https://github.com/frankmcsherry/dataflow_join,a streaming worst-case optimal join implementation,"There are also a few applications built on timely dataflow, including ", and a 
12332,https://github.com/timelydataflow/timely-dataflow/issues,issue tracker,"If you like the idea of getting your hands dirty in timely dataflow, the ", has a variety of issues that touch on different levels of the stack. For example:
12332,https://github.com/timelydataflow/timely-dataflow/issues/111,does more copies of data than it must,Timely currently ,", in the interest of appeasing Rust's ownership discipline most directly. Several of these copies could be elided with some more care in the resource management (for example, using shared regions of one "
12332,https://github.com/timelydataflow/timely-dataflow/issues/114,a list of nice to have features,"We recently landed a bunch of logging changes, but there is still "," that haven't made it yet. If you are interested in teasing out how timely works in part by poking around at the infrastructure that records what it does, this could be a good fit! It has the added benefit that the logs are timely streams themselves, so you can even do some log processing on timely. Whoa..."
12332,https://github.com/timelydataflow/timely-dataflow/issues/77,integrating Rust ownership idioms into timely dataflow,There is an open issue on ,". Right now, timely streams are of cloneable objects, and when a stream is re-used, items will be cloned. We could make that more explicit, and require calling a "
12333,http://www.vision.ee.ethz.ch/en/datasets/,ETH Dataset," ETH or UCY datasets, you can find them here: ", and 
12333,https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data,UCY Dataset, and ,.
12342,https://www.dropbox.com/s/hs1nizadbf5sz2g/data.zip?dl=0,here,Download the training data folder from , and place it in the root folder.
12361,https://github.com/danping/structvio/blob/master/structvio_data.yaml,structvio_data.yaml,Here , and 
12361,https://github.com/danping/structvio/blob/master/euroc_data.yaml,euroc_data.yaml, and , are the default configurations for StructVIO datasets and Euroc datasets. You can type
12361,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,Euroc,To run StructVIO on the ," datasets, download the configuration file  "
12361,https://github.com/danping/structvio/blob/master/euroc_data.yaml,euroc_data.yaml," datasets, download the configuration file  ", and type
12366,https://www.keeleveeb.ee/dict/corpus/shared/categories.html,GT-categories,"
","
"
12377,https://data.stackexchange.com/,StackExchange,This project is a collection of three corpora which can be used for evaluating chatbots or other conversational interfaces. Two of the corpora were extracted from ,", one from a Telegram chatbot."
12383,http://www.cvlibs.net/datasets/kitti/,KITTI,Code is included for training the PredNet on the raw ," dataset. We include code for downloading and processing the data, as well as training and evaluating the model. The preprocessed data and can also be downloaded directly using "
12388,https://github.com/huggingface/datasets,HuggingFace datasets library,You can also access this dataset as part of the , library as follow:
12414,https://dataserv.ub.tum.de/s/m1470791/download?path=%2F&files=data_6k.tar.gz,https://dataserv.ub.tum.de/s/m1470791/download?path=%2F&files=data_6k.tar.gz,Reduced data set with 6.4k samples plus test data (1.2GB): , (or via mediaTUM 
12414,https://dataserv.ub.tum.de/index.php/s/m1470791,https://dataserv.ub.tum.de/index.php/s/m1470791, (or via mediaTUM ,)
12414,https://dataserv.ub.tum.de/s/m1459172/download?path=%2F&files=data_full.tar.gz,https://dataserv.ub.tum.de/s/m1459172/download?path=%2F&files=data_full.tar.gz,Full data set with 53.8k samples plus test data (10GB): , (or via mediaTUM 
12414,https://dataserv.ub.tum.de/index.php/s/m1459172,https://dataserv.ub.tum.de/index.php/s/m1459172, (or via mediaTUM ,)
12414,https://ge.in.tum.de/download/2019-deepFlowPred/model_data05_exp50,https://ge.in.tum.de/download/2019-deepFlowPred/model_data05_exp50,"A smaller model (1.9m weights, i.e exp=5) trained with 1.6k regular samples: ","
"
12448,https://github.com/tensorflow/tensorflow/blob/7c36309c37b04843030664cdc64aca2bb7d6ecaa/tensorflow/contrib/learn/python/learn/datasets/mnist.py,mnist training data,Like in the ," as provided by TensorFlow, the newly created "
12452,http://cocodataset.org/#format-data,COCO panoptic segmentation format,: Convert annotations in standard png format to ,.
12452,www.cityscapes-dataset.com/submit,Submission Page,"Once you want to test your method on the test set, please run your approach on the provided test images and submit your results: ","
"
12452,https://www.cityscapes-dataset.com/license/,terms and conditions,The dataset itself is released under custom ,.
12453,http://www.cvlibs.net/datasets/kitti/eval_object.php,Kitti Object Detection Dataset,KittiBox is a collection of scripts to train out model FastBox on the ,. A detailed description of Fastbox can be found in our 
12453,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,Retrieve Kitti data url here: ,"
"
12453,data/demo.png,demo.png, to obtain a prediction using , as input.
12455,docs/python/index.md#data-types,data types,"The Ocean Tensor Package provides support for various integer, floating-point, and complex "," and supports non-aligned and byteswapped memory layouts. Automatic conversion between data types and devices, as well as dimension broadcasting is supported and can be configured to provide low-level control over all operations. On the GPU, high levels of asynchronicity are enabled by consistent usage of streams and the usage of special intermediate tensors."
12460,https://www.dropbox.com/s/yz31j1zd3vvjwrw/data.zip?dl=0,link,Alternatively you can also download the data for context of 2 and 5 (as well as pickled Shah et al. data) that we used for training from this ,.
12460,https://www.dropbox.com/s/s60owkgdv6glmuz/mmd_data.tar.gz?dl=0,here,"Update: we are also providing the raw catalog, chat transcripts and metadata provided by Shah et al. ", All copyrights regarding the catalog and chat transcripts rests with them. Please contact them for further information.
12460,https://github.com/Maluuba/nlg-eval#functional-api-for-the-entire-corpus,functional api,"In particular, we used their ", for getting evaluation metrics. Install the dependencies in the same or different conda environment.
12463,https://s3.amazonaws.com/fairseq-py/data/wmt14.v2.en-fr.newstest2014.tar.bz2,download (.tar.bz2),"
","
"
12463,https://s3.amazonaws.com/fairseq-py/data/wmt14.v2.en-fr.ntst1213.tar.bz2,download (.tar.bz2),"
", Convolutional 
12463,https://s3.amazonaws.com/fairseq-py/data/wmt14.v2.en-de.newstest2014.tar.bz2,download (.tar.bz2),"
", Transformer 
12463,https://s3.amazonaws.com/fairseq-py/data/wmt14.en-fr.joined-dict.newstest2014.tar.bz2,download (.tar.bz2),"
", Transformer 
12463,https://s3.amazonaws.com/fairseq-py/data/wmt16.en-de.joined-dict.newstest2014.tar.bz2,download (.tar.bz2),"
","
"
12463,https://s3.amazonaws.com/fairseq-py/data/gbw_test_lm.tar.bz2,download (.tar.bz2), | , Convolutional 
12463,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-103,) | , | 
12463,https://s3.amazonaws.com/fairseq-py/data/wiki103_test_lm.tar.bz2,download (.tar.bz2), | ,"
"
12463,https://s3.amazonaws.com/fairseq-py/data/stories_test.tar.bz2,download (.tar.bz2), | ,"
"
12473,http://islpc21.is.cs.cmu.edu/yunwang/git/cmu-thesis/data/dcase.tgz,dcase.tgz,"
", (4.9 GB)
12473,http://islpc21.is.cs.cmu.edu/yunwang/git/cmu-thesis/data/audioset.tgz,audioset.tgz,"
", (341 GB)
12473,http://islpc21.is.cs.cmu.edu/yunwang/git/cmu-thesis/data/sequential.tgz,sequential.tgz,"
", (63 GB)
12475,https://github.com/Ideamaxwu/biggy/tree/master/beta_src/biggy/src/main/java/edu/helpal/datar/biggy/examples,here,See beta-version guide ,"
"
12479,https://github.com/pytorch/examples/tree/75e435f98ab7aaa7f82632d4e633e8e03070e8ac/word_language_model/data/penn,Penn Treebank corpus,"Download PTB data. Note that the two tasks, i.e., language modeling and unsupervised parsing share the same model strucutre but require different formats of the PTB data. For language modeling we need the standard 10,000 word "," data, and for parsing we need "
12488,https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192,Interpretable or Accurate? Why Not Both?,"
","
"
12488,https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb,"The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.","
","
"
12488,https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8,InterpretML: Another Way to Explain Your Model,"
","
"
12488,https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254,The right way to compute your Shapley Values,"
","
"
12488,https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727,The Art of Sprezzatura for Machine Learning,"
","
"
12488,https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95,Mixing Art into the Science of Model Explainability,"
","
"
12504,http://ana.cachopo.org/datasets-for-single-label-text-categorization,here, datasets can be found ,.
12513,http://data.nytimes.com/,New York Times, |your.system.ip.address:8896/sparql |http://www.linkedmdb.org/sparql | | , |
12513,http://data.semanticweb.org/,Semantic Web Dog Food, |your.system.ip.address:8897/sparql | - | | , |
12516,https://magenta.tensorflow.org/datasets/nsynth,NSynth dataset,SING is a deep learning based music notes synthetizer that can be trained on the ,". Despite being 32 times faster to train and 2,500 faster for inference, SING produces audio with significantly improved perceptual quality compared to the NSynth wavenet-like autoencoder "
12567,https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html,ATIS (Airline Travel Information Systems) dataset,There is example code available using Magnitude to build an intent classification model for the , (
12567,http://magnitude.plasticity.ai/data/atis/atis-intent-train.txt,Train, (,/
12567,http://magnitude.plasticity.ai/data/atis/atis-intent-test.txt,Test,/,"), used for chatbots or conversational interfaces, in a few popular machine learning libraries below."
12568,https://github.com/RaRe-Technologies/gensim-data,Gensim-data, (no images or audio). This , repository serves as that storage.
12568,https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/,New Download API for Pretrained NLP Models and Datasets,Read more about the project rationale and design decisions in this article: ,.
12568,https://github.com/RaRe-Technologies/gensim-data/releases,release attachments,"Technically, the actual (sometimes large) corpora and model files are being stored as "," here on Github. Each dataset (and each new version of each dataset) gets its own release, forever immutable."
12568,https://github.com/RaRe-Technologies/gensim-data/releases/tag/patent-2017,Corpus of USPTO Patents from 2017,"Each release is accompanied by a usage example and release notes, for example: ",; 
12568,https://github.com/RaRe-Technologies/gensim-data/releases/tag/wiki-english-20171001,English Wikipedia from 2017 with plaintext section,; ,.
12568,https://github.com/RaRe-Technologies/gensim-data/blob/master/generate_table.py,generate_table.py,(generated by , based on 
12568,https://github.com/RaRe-Technologies/gensim-data/blob/master/list.json,list.json, based on ,)
12568,https://github.com/RaRe-Technologies/gensim-data/issues,new issue,Create a , and give us the dataset link. Add a 
12568,https://github.com/rare-technologies/gensim-data/blob/master/LICENSE,LGPL 2.1 license, is open source software released under the ,.
12575,http://www.cvlibs.net/datasets/kitti/eval_stereo.php,KITTI Stereo,"
","
"
12575,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow,"
","
"
12585,#supplementary-data,Supplementary data,"
","
"
12585,https://fasttext.cc/docs/en/dataset.html#content,YFCC100M data,The preprocessed , used in [2].
12590,#dataset,Dataset,"
","
"
12590,#download-dataset,Download dataset,"
","
"
12590,https://motchallenge.net/data/MOT17/,MOT17,Our method can be evaluated on ,", "
12590,https://motchallenge.net/data/MOT15/,MOT15,", ", and 
12590,https://motchallenge.net/data/MOT17.zip,mot 17 dataset 5.5 GB,Download the , and 
12590,https://motchallenge.net/data/devkit.zip,development kit 0.5 MB, and ,.
12597,https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/socialRec/README.md#douban-data,here,"We contribute a new large-scale dataset, which is collected from a popular movie/music/book review website Douban (www.douban.com). The data set could be useful for researches on sequential recommendation, social recommendation and multi-domain recommendation. See details ",.
12612,https://datascience.ch/deepsphere-a-neural-network-architecture-for-spherical-data,blog,", ",", "
12613,https://github.com/francesclluis/source-separation-wavenet#dataset,below,Download the dataset as described ,"
"
12613,https://sigsep.github.io/datasets/musdb.html#download,Download here,"
","
"
12619,https://wagda.lib.washington.edu/data/type/elevation/lidar/st_helens/,data,We apply our method to rough terrain reconsutrction of Mount St. Helens from lidar ,. Run 
12619,http://graphics.stanford.edu/data/3Dscanrep/,Stanford bunny,We apply our method to implicit surface reconstruction. We reconstruct the ," from noisy surface normals (i.e. gradients), which is difficult to do using traditional spline methods. Note that, as we are using roughly 40 thousand points and gradients, reconstruction will take a bit of time. Run "
12628,https://sites.google.com/view/calgary-campinas-dataset/home,Calgary-Campinas dataset,The MR raw-data is publicly available as part of the , for benhcmarking purposes.
12633,http://cocodataset.org/#download,here,Download the MSCOCO2014 dataset ,.
12633,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,here,Download Karpathy's split ,", and put it in the folder "
12640,http://pmb.let.rug.nl/data.php,PMB release, all data you need for running basic DRS parsing experiments you can now find in data/. You can potentially still download a ," and construct the training data yourself, but I'd advise not to unless you want to exploit more resources."
12650,http://groups.csail.mit.edu/vision/datasets/ADE20K/,ADE20K,: We provide , as an example.
12657,https://www.kaggle.com/c/freesound-audio-tagging/data,Freesound Audio Tagging,Date could be downloaded from Kaggle competition ,.
12669,http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz,"
Download
",Speech Commands Dataset v0.02: ,"
"
12672,http://nlp.stanford.edu/data/glove.twitter.27B.zip,GloVe Twitter 27B pretrained,English: ,"
"
12676,https://dziganto.github.io/data%20science/python/anaconda/Creating-Conda-Environments/,How to create an anaconda virtual environment,Follow the instructions from ,"
"
12687,https://github.com/nlpdata/mrc_bert_baseline,this repository,You can refer to , for a finetuned transformer baseline based on BERT.
12694,https://github.com/SullyChen/driving-datasets,dataset,Download the , and extract into the repository folder
12700,mailto:azurepublicdataset@service.microsoft.com,mailing list,"We provide the traces as they are, but are willing to help researchers understand and use them. So, please let us know of any issues or questions by sending email to our  ",.
12700,mailto:azurepublicdataset@service.microsoft.com,mailing list,Please let us know of any issues or questions by sending email to our ,.
12705,https://github.com/NorThanapon/dict-definition/tree/master/data,Data,"For detail of the data, see ","
"
12713,https://github.com/declare-lab/M2H2-dataset,M2H2,| Date 	| Announcements 	| |-	|-	| | 03/08/2021  | 🎆 🎆 We have released a new dataset M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations. Check it out: ,. The baselines for the M2H2 dataset are created based on DialogueRNN and bcLSTM. | | 18/05/2021  | 🎆 🎆 We have released a new repo containing models to solve the problem of emotion cause recognition in conversations. Check it out: 
12713,#data-format,Data Format,"
","
"
12713,#ecpe-2d-on-reccon-dataset,ECPE-2D on RECCON dataset,"
","
"
12713,#rank-emotion-cause-on-reccon-dataset,Rank-Emotion-Cause on RECCON dataset,"
","
"
12713,#ecpe-mll-on-reccon-dataset,ECPE-MLL on RECCON dataset,"
","
"
12713,#roberta-and-spanbert-baselines-on-reccon-dataset,RoBERTa and SpanBERT Baselines on RECCON dataset,"
","
"
12713,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove,Set , path in the preprocessing files.
12719,http://mulan.sourceforge.net/datasets-mlc.html,Bibtex/Bookmarks,"
","
"
12720,https://github.com/facebookresearch/clevr-dataset-gen,"CLEVR (Johnson et al., 2017)","Questions for EmbodiedQA are generated programmatically, in a manner similar to ",.
12720,https://embodiedqa.org/data,here,NOTE: Pre-generated EQA v1 questions are available for download ,.
12720,https://embodiedqa.org/data,EQA v1,Download , and shortest path navigations:
12720,https://github.com/facebookresearch/EmbodiedQA/blob/master/data/shortest-path-gen/generate-paths-a-star.py,here,We also updated the shortest paths to fix an issue with the shortest path algorithm we initially used.  Code to generate shortest paths is ,.
12736,https://www.cs.ucsb.edu/~william/data/liar_dataset.zip,LIAR, is based on the publicly available data from PolitiFact.com and the benchmark data set ," (Wang 2017). Given a speaker's name, job title, party affiliation and home state one can look up their corresponding credibility vector."
12738,https://pandas.pydata.org,"
pandas
","
","
"
12745,http://lcl.uniroma1.it/wsdeval/data/WSD_Unified_Evaluation_Datasets.zip,http://lcl.uniroma1.it/wsdeval/data/WSD_Unified_Evaluation_Datasets.zip,"'s versions of SensEval/SemEval corpora (6 separate corpora, original data: ",)
12745,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishLS/EnglishLS.train.tar.gz,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishLS/EnglishLS.train.tar.gz,"Training and testing data of SensEval 3 Task 6 (lexical sample) (2 separate corpora, original data: ", and 
12745,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishLS/EnglishLS.test.tar.gz,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishLS/EnglishLS.test.tar.gz, and ,)
12745,http://trainomatic.org/data/train-o-matic-data.zip,http://trainomatic.org/data/train-o-matic-data.zip,Train-O-Matic (original data: ,)
12745,https://github.com/google-research-datasets/word_sense_disambigation_corpora,https://github.com/google-research-datasets/word_sense_disambigation_corpora,MASC (original data: ,)
12745,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishAW/EnglishAW.test.tar.gz,http://web.eecs.umich.edu/~mihalcea/senseval/senseval3/data/EnglishAW/EnglishAW.test.tar.gz,SensEval 3 (original data: ,)
12745,http://nlp.cs.swarthmore.edu/semeval/tasks/task07/data.shtml,http://nlp.cs.swarthmore.edu/semeval/tasks/task07/data.shtml,SemEval 2007 task 07 (original data: ,)
12745,http://nlp.cs.swarthmore.edu/semeval/tasks/task17/data.shtml,http://nlp.cs.swarthmore.edu/semeval/tasks/task17/data.shtml,SemEval 2007 task 17 (original data: ,)
12745,https://www.cs.york.ac.uk/semeval-2013/task12/index.php%3Fid=data.html,https://www.cs.york.ac.uk/semeval-2013/task12/index.php%3Fid=data.html,SemEval 2013 task 12 (original data: ,)
12745,http://alt.qcri.org/semeval2015/task13/index.php?id=data-and-tools,http://alt.qcri.org/semeval2015/task13/index.php?id=data-and-tools,SemEval 2015 task 13 (original data: ,)
12746,#preparing-data,Preparing data,"
","
"
12776,https://github.com/harvardnlp/sent-conv-torch/tree/master/data,https://github.com/harvardnlp/sent-conv-torch/tree/master/data,"For most dataset, it could be downloaded from ","
"
12777,https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics-aida,Artificial Intelligence for Data Analytics,The ," (AIDA) project aims at applying new advances in AI and machine learning to address data wrangling issues, and help to automate the data analytics process. Semantic Web technologies have the potential of contributing in the Data Science pipeline by providing a more complete (semantic) understanding of the data."
12777,https://github.com/ernestojimenezruiz/tabular-data-semantics,(gihub-java),"Tabular Data Semantics.  Auxiliary classes to access DBpedia, Wikidata and Google's KG for Web table matching. ","
"
12777,https://github.com/ernestojimenezruiz/tabular-data-semantics-py,(github-python),"
","
"
12777,https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics-aida,AIDA project,This work is supported by the , (UK Government's Defence & Security Programme in support of the 
12778,http://research.signalmedia.co/newsir16/signal-dataset.html,Signal Media 1M,See , to download the Signal Media dataset.
12789,https://github.com/tensorflow/datasets/commit/211cb6f082c5cc3c482e37d70234142a8fda2db3,commit 211cb6f, dataset was removed from TensorFlow Datasets in ,", so in order to train a model using this dataset you need to install a version of TFDS before this commit. Then, you can decode the WMT en-de development set and evaluate it using "
12793,http://cci.drexel.edu/bigdata/bigdata2018/index.html,IEEE International Conference on Big Data 2018," titled ""Transfer learning for time series classification"" accepted as a regular paper at ", also available on 
12793,https://www.cs.ucr.edu/~eamonn/time_series_data/,UCR archive,The software is developed using Python 3.5. We trained the models on a cluster of more than 60 GPUs. You will need the , to re-run the experiments of the paper.
12793,https://github.com/hfawaz/bigdata18/blob/master/utils/build-cython.sh,build-cython.sh,"If you encouter problems with cython, you can re-generate the ""c"" files using the ", script.
12793,http://germain-forestier.info/src/bigdata2018/,web page,You can download from the companion , all pre-trained and fine-tuned models you would need to re-produce the experiments. Feel free to fine-tune on your own datasets !!!
12793,https://github.com/hfawaz/bigdata18/blob/master/utils/pip-requirements.txt,pip-requirements.txt,All python packages needed are listed in , file and can be installed simply using the pip command.
12793,https://pandas.pydata.org/,pandas,"
","
"
12793,https://github.com/hfawaz/bigdata18/blob/master/results/df_transfer.csv,here,You can download , the accuracy variation matrix which corresponds to the raw results of the transfer matrix in the paper.
12793,https://github.com/hfawaz/bigdata18/blob/master/results/df_transfer_acc.csv,here,You can download , the raw results for the accuracy matrix instead of the variation.
12793,https://github.com/hfawaz/bigdata18/blob/master/results/similar_datasets.csv,here,You can download ," the result of the applying nearest neighbor algorithm on the inter-datasets similarity matrix. You will find for each dataset in the archive, the 84 most similar datasets. The steps for computing the similarity matrix are presented in Algorithm 1 in our paper."
12800,#the-nyu-depth-v2-dataset,The NYU-Depth-v2 dataset,"
","
"
12801,data,Data,"
",: A collection of scripts to utility functions to handle and analyse the formated data
12811,https://www.cs.washington.edu/ai/gated_instructions/naacl_data.zip,UW, dataset and the , as mentioned in the paper.
12822,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe 300d pretrained vector,"Before training the model, you need to first prepare data. First of all, you need to download the "," as we use it for initialization in all experiments. After unzipping it, you need to convert the txt file to pickle file by"
12825,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe 300d pretrained vector,Please download the ,", which is used for word embeddding initialization in all experiments."
12837,https://github.com/udacity/self-driving-car/tree/master/datasets,"
Driving Datasets
","
"," – Over 10 hours of driving data (LIDAR, camera frames and more)"
12843,https://www.cs.toronto.edu/~vmnih/data/,Massachusetts road & building dataset,This is a state-of-the-art project for building extraction in high resolution remote sensing image using dataset ," . And, our approach was published in ACCV 2016, clik here to download "
12850,https://github.com/alabatie/moments-dnns/blob/master/notebooks/Complements%20on%20width%2C%20boundary%20conditions%2C%20dataset%2C%20epsilon.ipynb,"Complements on width, boundary conditions, dataset, epsilon.ipynb","
"," discusses the effect of changing the width, boundary conditions of convolutional layers, input dataset and batch normalization fuzz factor"
12855,data,data,Pre-trained models as well as data used for training and evaluating attacks can be found here: https://github.com/ftramer/ad-versarial/releases The data is expected to be placed under , and the pre-trained models under 
12887,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,new M-AILABS speech dataset,We are also running current tests on the , which contains more than 700h of speech (more than 80 Gb of data) for more than 10 languages.
12900,http://numba.pydata.org/,numba,", ",", "
12905,https://github.com/Vastlab/ObjectoSphere/blob/master/MNIST/data_prep.py,MNIST/data_prep.py,All datasets and their preprocessing is provided in ,". The preprocessing is largely limited to normalization of pixel values except for MNIST where we negate the pixel values before normalization, in order to achieve a consistent black on white background images for all datasets. The datasets used are:"
12905,https://github.com/Vastlab/ObjectoSphere/blob/master/MNIST/data_prep.py,MNIST/data_prep.py,You may want to update the dataset locations in ,.
12916,https://www.robots.ox.ac.uk/~vgg/data/scenetext/,"""Synthetic Data for Text Localisation in Natural Images"", Ankush Gupta, Andrea Vedaldi, Andrew Zisserman, CVPR 2016",Code for generating synthetic text images as described in ,.
12916,https://www.robots.ox.ac.uk/~vgg/data/scenetext/,here,A dataset with approximately 800000 synthetic scene-text images generated with this code can be found ,.
12917,https://github.com/google-research-datasets/hiertext,HierText (2022),| Dataset (Year) | Image Num (train/val/test) | Text Num (train/test) | Orientation| Language| Characteristics | Detec/Recog Task | |:------:|:------:|:------:|:------:|:------:|:------:|:------:| |End2End|====|====|====|====|====|====| | , | 11639 (8281/1724/1634) | 103.8 per image | All | Latin | Word/line/paragraph annotations | ✓/✓ | | 
12917,http://dagdata.cvc.uab.es/icdar2013competition/,ICDAR13 Scene Text(2013), | 509 (258/0/251) | 2276 (1110/1156) | Horizontal | En | - | ✓/✓ | | , | 462 (229/0/233) | - (848/1095) | Horizontal | En | - | ✓/✓ | | 
12917,http://rctw.vlrlab.net/dataset/,ICDAR17 / RCTW (2017)," | 1500 (1000/0/500) | - (-/-) | Multi-Oriented | En |  Blur, Small, Defocused | ✓/✓ | | ", | 12263 (8034/0/4229) | - (-/-) | Multi-Oriented | Cn | - | ✓/✓ | | 
12917,http://cs-chan.com/downloads_CUTE80_dataset.html,CUTE (2014), | 659 (-/-/-) | 5238 (-/-) | Multi-oriented| 8 langs| - | ✓/✓ | | , or 
12917,https://ctwdataset.github.io,CTW (2017), | 80 (-/0/80) | - (-/-) | Curved | En | - | ✓/✓ | | , |  32K ( 25K/0/6K) |  1M ( 812K/205K) | Multi-Oriented | Cn |  Fine-grained annotation | ✓/✓ | | 
12917,https://github.com/Jyouhou/SceneTextPapers/blob/master/datasets/CASIA-10K.md,CASIA-10K (2018), |  32K ( 25K/0/6K) |  1M ( 812K/205K) | Multi-Oriented | Cn |  Fine-grained annotation | ✓/✓ | | , | 10K (7K/0/3K) | - (-/-) | Multi-Oriented | Cn |  | ✓/✓ | |Detection Only|====|====|====|====|====|====| | 
12917,http://mclab.eic.hust.edu.cn/UpLoadFiles/dataset/HUST-TR400.zip,HUST-TR400 (2014)," | 500 (300/0/200) | 1719 (1068/651) |  Multi-Oriented | En, Cn |  Long text | ✓/- | | "," | 400 (400/0/-) | - (-/-) |  Multi-Oriented | En, Cn |  Long text | ✓/- | | "
12917,https://github.com/Jyouhou/SceneTextPapers/blob/master/datasets/svt-p.zip,SVTP (2013), | - (-/-/-) | 600000 (-/-) | Horizontal| -| House number digits | -/✓ | | , | 639 (-/-/639) | - (-/-) |  | En | Distorted | -/✓ |
12962,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,images,Download KITTI object detection dataset: , and 
12962,http://www.cvlibs.net/download.php?file=data_object_label_2.zip,labels, and ,. Put them under 
12979,https://github.com/lixin4ever/BERT-E2E-ABSA/tree/master/data,project,": restaurant reviews from SemEval 2014 (task 4), SemEval 2015 (task 12) and SemEval 2016 (task 5) respectively. We have prepared data files with train/dev/test split in our another ",", check it out if needed."
12994,http://corpora.uni-leipzig.de/en?corpusId=zul_mixed_2016,Leipzig Zulu 100K Corpus,"
","
"
13000,https://towardsdatascience.com/deep-learning-in-your-browser-a-brisk-guide-ca06c2198846,tutorial,"To convert the model to JavaScript, we followed the following ",.
13003,#datasets,datasets,. Check the , section for details.
13004,documentation/database-structure.md,Database Structure,"
","
"
13005,mailto:alibaba-clusterdata@list.alibaba-inc.com,alibaba-clusterdata,"We encourage anyone to use the traces for study or research purposes, and if you had any question when using the trace, please contact us via email: ",", or file an issue on Github. Filing an issue is recommanded as the discussion would help all the community. Note that the more clearly you ask the question, the more likely you would get a clear answer."
13005,https://github.com/alibaba/clusterdata/wiki/About-Alibaba-cluster-and-why-we-open-the-data,the challenges Alibaba face,"From our perspective, the data is provided to address ", in IDC's where online services and batch jobs are collocated.  We distill the challenges as the following topics:
13005,mailto:alibaba-clusterdata@list.alibaba-inc.com,aliababa-clusterdata,"Last but not least, we are always open to work together with researchers to improve the efficiency of our clusters, and there are positions open for research interns. If you had any idea in your mind, please contact us via ", or 
13005,mailto:alibaba-clusterdata@list.alibaba-inc.com,aliababa-clusterdata, (,).
13006,#define-a-customized-dataset,Define a customized dataset,"
","
"
13006,#data,Data,"
","
"
13006,http://conda.pydata.org/,conda,. After installing ,", run the following command to create a new "
13006,http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#sphx-glr-beginner-data-loading-tutorial-py,Tutorial,". If you want a more flexible way to input crystal structures, PyTorch has a great ", for writing your own dataset class.
13006,#define-a-customized-dataset,Define a customized dataset,"
", at 
13006,#define-a-customized-dataset,Define a customized dataset,"
", at 
13006,data/material-data,instruction,"To reproduce our paper, you can download the corresponding datasets following the ",.
13007,#define-a-customized-dataset,Define a customized dataset,"
","
"
13048,https://github.com/arangesh/Ground-Plane-Polling/tree/master/road_planes_database,here, file is an (N x 4) array of ground planes for the KITTI dataset. Different versions of this file can be found ,.
13048,https://github.com/arangesh/Ground-Plane-Polling/blob/master/prepare_kitti_data.py,this Python script,3. Create train-val split using , as follows:
13048,https://github.com/arangesh/Ground-Plane-Polling/tree/master/road_planes_database,here, file is an (N x 4) array of ground planes for the KITTI dataset. Different versions of this file can be found ,.
13050,http://cocodataset.org/#download,MS COCO 2017,", ", and 
13052,https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz,archive,Download the Speech command v0.01 ,.
13052,https://github.com/TomVeniat/AdaptiveSequenceClassification/blob/master/src/commons/pytorch/data/collate.py#L20,PadCollate,It is possible to use the , class in the dataloader to pad each sequence to the length of the longest one in the sampled batch.
13054,https://datamol.io/,Datamol,"
", (
13054,https://doc.datamol.io/stable/,docs, (,", "
13054,https://github.com/datamol-org/datamol/,repo,", ",) - A Python library to intuitively manipulate molecules.
13055,https://share.phys.ethz.ch/~gsg/3DSmoothNet/training_data/trainingData.rar,here (145GB),Training data created using the RGB-D data from 3DMatch data set can be downloaded from ,. It consists of a 
13055,https://share.phys.ethz.ch/~gsg/3DSmoothNet/data/3DMatch.rar,here (0.27GB), data set can be downloaded from ,"
"
13055,https://share.phys.ethz.ch/~gsg/3DSmoothNet/data/3DSparseMatch.rar,here (0.83GB), data set can be downloaded from ,"
"
13055,https://share.phys.ethz.ch/~gsg/3DSmoothNet/data/ETH.rar,here (0.11GB), data set can be downloaded from ,"
"
13055,https://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration,ETH,If you use these data please consider also citing the authors of the original data set ,.
13059,https://www.h2database.com/html/main.html,instructions, databases. Please follow their , for setup and maybe consider our helping 
13059,evaluation/database_resources/database_notes.txt,notes, for setup and maybe consider our helping ,. Our fuzzing drivers for the 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,website,VGGFace2 Dataset for Face Recognition (,)
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/resnet50_scratch_caffe.tar.gz,Caffe,| Architecture   | Feat dim | Pretrain | TAR@FAR = 0.001 | TAR@FAR = 0.01 | Model Link | |:-:|:-:|:-:|:-:|:-:|:-:| |   ResNet-50    | 2048 |  N  | 0.878 | 0.938 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/resnet50_scratch_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/resnet50_scratch_pytorch.tar.gz,PyTorch,", ", | |   ResNet-50    | 2048 |  Y  | 0.891 | 0.947 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/resnet50_ft_caffe.tar.gz,Caffe, | |   ResNet-50    | 2048 |  Y  | 0.891 | 0.947 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/resnet50_ft_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/resnet50_ft_pytorch.tar.gz,PyTorch,", ", | | SE-ResNet-50   | 2048 |  N  | 0.888 | 0.949 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/senet50_scratch_caffe.tar.gz,Caffe, | | SE-ResNet-50   | 2048 |  N  | 0.888 | 0.949 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/senet50_scratch_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/senet50_scratch_pytorch.tar.gz,PyTorch,", ",| | SE-ResNet-50   | 2048 |  Y  | 0.908 | 0.956 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/senet50_ft_caffe.tar.gz,Caffe,| | SE-ResNet-50   | 2048 |  Y  | 0.908 | 0.956 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/senet50_ft_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/senet50_ft_pytorch.tar.gz,PyTorch,", ",| | ResNet-50-256D | 256  |  Y  | 0.898 | 0.956 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/resnet50_256_caffe.tar.gz,Caffe,| | ResNet-50-256D | 256  |  Y  | 0.898 | 0.956 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/resnet50_256_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/resnet50_256_pytorch.tar.gz,PyTorch,", ",| | ResNet-50-128D | 128  |  Y  | 0.904 | 0.956 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/resnet50_128_caffe.tar.gz,Caffe,| | ResNet-50-128D | 128  |  Y  | 0.904 | 0.956 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/resnet50_128_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/resnet50_128_pytorch.tar.gz,PyTorch,", ", | | SE-ResNet-50-256D|  256    |    Y   | 0.912 | 0.965 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/senet50_256_caffe.tar.gz,Caffe, | | SE-ResNet-50-256D|  256    |    Y   | 0.912 | 0.965 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/senet50_256_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/senet50_256_pytorch.tar.gz,PyTorch,", ",| | SE-ResNet-50-128D|  128    |    Y   | 0.910 | 0.959 | 
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/caffe/senet50_128_caffe.tar.gz,Caffe,| | SE-ResNet-50-128D|  128    |    Y   | 0.910 | 0.959 | ,", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/matconvnet/senet50_128_mat.tar.gz,MatConvNet,", ",", "
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/models/pytorch/senet50_128_pytorch.tar.gz,PyTorch,", ", |
13069,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,here," for face detection. This bounding box is then extended by a factor 0.3 (except the extension outside image) to include the whole head, which is used as network input (Please note that the released faces are based on a larger extension ratio 1.0. The coordinates of bounding boxes and 5 facial keypoints referring to the loosely cropped faces can be found ",.
13096,https://github.com/mshaikh2/HDL_Forensics/tree/master/BMVC_XAI/dataset,dataset,Please try out our , to try your algorithms and kindly cite the below research papers:
13102,https://www.tensorflow.org/tutorials/load_data/tf_records,TFRecords,. Relevant files for The NECST model operates over Tensorflow ,. A few points to note:
13103,https://github.com/lberrada/InferSent/tree/dfw#download-datasets,preparation instructions,To reproduce the SNLI experiments: follow the , and run  
13106,pytracking/evaluation/avistdataset.py,avistdataset.py,", the integration ", and the evaluation code 
13109,https://www5.cs.fau.de/research/data/fundus-images/,HRF, and , and place them in the following directory tree structure:
13115,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,here,Graph classification datasets were downloaded ,.
13115,https://linqs.soe.ucsc.edu/data,here,Node classification datasets can be found ,", but we use the specific splits provided "
13115,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,here," files are quite large, please download it from ",.
13128,https://www.cityscapes-dataset.com/,Cityscapes,Download the , dataset and convert the dataset to 
13147,#data-preparation,Data Preparation,"
","
"
13147,https://deepmind.com/research/open-source/open-source-datasets/kinetics/,Kinetics,We have successfully trained on ,", "
13147,http://crcv.ucf.edu/data/UCF101.php,UCF101,", ",", "
13147,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB51,", ",", "
13147,https://20bn.com/datasets/something-something/v1,Something-Something-V1,", ", and 
13147,https://20bn.com/datasets/something-something/v2,V2, and ,", "
13147,https://20bn.com/datasets/jester,Jester,", "," datasets with this codebase. Basically, the processing of video data can be summarized into 3 steps:"
13147,ops/dataset_configs.py,ops/dataset_configs.py,Add the information to ,"
"
13147,https://20bn.com/datasets/something-something/v1,V1,Something-Something ,&
13147,https://20bn.com/datasets/something-something,V2,&, datasets are highly temporal-related. TSM achieves state-of-the-art performnace on the datasets: TSM achieves the 
13170,http://cocodataset.org/#download,Link, inside the folder Dataset. These files are downloadable from ,. Make sure the pre-trained model is present inside the Model folder ('Model/resnet50_csv_50_focal_seen_w2v.h5'). This pre-trained model is trained by focal loss on 65 seen classes without considering any vocabulary metric. This model is available to download from (
13170,http://cocodataset.org/#download,Link,. These zipped archives are downloadable from MSCOCO website (,"). Please find the exact list of images (with annotations) used for ""training"" in "
13183,http://www.nielsen.com/us/en/insights/news/2010/nielsen-provides-topline-u-s-web-data-for-march-2010.html,reported,"For this application, the number of possible calls is the number of domains that a user might visit (per day), and the number of calls is the number of visits made. Nielson "," in 2010 that the average person visits 89 domains per month. To be extremely conservative in (over)estimating the number of noise calls necessary to obscure this browsing data, assume that the average user visits "
13221,http://cocodataset.org/#download,COCO2017,1.3 Download ,.
13222,https://travis-ci.org/big-data-lab-team/stream-summarization,"
Build Status
","
","
"
13222,https://github.com/big-data-lab-team/paper-multidimensional-ltc.git,A multi-dimensional extension of the LTC method,"This capture shows how to measure Compression Ratio and Max Error, that are mentioned in ",.
13225,http://crcv.ucf.edu/data/UCF101.php,UCF101,Download , and 
13225,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB, and , and organize the image files (from the videos) as follows:
13225,https://deepmind.com/research/open-source/open-source-datasets/kinetics/,official website,Download Kinetics-400 from the , or from the copy of 
13225,https://www.cityscapes-dataset.com/downloads/,Cityscapes,Download data from ,", organize the image files and annotation json files as follows:"
13230,http://isis-data.science.uva.nl/zhenyang/cvpr17-langtracker/data/OTB_sentences.zip,Sentences,Lingual Lingual OTB99 ,"
"
13230,http://isis-data.science.uva.nl/zhenyang/cvpr17-langtracker/data/ImageNet_sentences.zip,Sentences,Lingual ImageNet ,"
"
13230,http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html,OTB100,Please note that we use all the frames from original ," dataset in our OTB99 videos, while for "
13230,http://isis-data.science.uva.nl/zhenyang/cvpr17-langtracker/code/pretrain-models/snapshots/lang_high_res_seg/_iter_25000.caffemodel,caffemodel,Download natural language segmentation model , and copy to 
13230,http://isis-data.science.uva.nl/zhenyang/cvpr17-langtracker/code/pretrain-models/VGG16.v2.caffemodel,caffemodel,Download tracking model , and copy to 
13233,https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-virtual-machine-overview,Azure Data Science VM,"It is recommended to prepare a machine with a GPU if you wish to train the 2D/3D deep learning models in the examples. If you don't have one at hand, ", with 
13237,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=Cross-modality+image+synthesis+from+unpaired+data+using+CycleGAN%3A+Effects+of+gradient+consistency+loss+and+training+data+size&btnG=,[scholar],[Cross-modality image synthesis from unpaired data using CycleGAN: Effects of gradient consistency loss and training data size] ,"
"
13237,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=Chest+x-ray+generation+and+data+augmentation+for+cardiovascular+abnormality+classification&btnG=,[scholar],[Chest x-ray generation and data augmentation for cardiovascular abnormality classification] ,"
"
13237,https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10574/105741M/Chest-x-ray-generation-and-data-augmentation-for-cardiovascular-abnormality/10.1117/12.2293971.short?SSO=1,[SPIE MI2018],"
","
"
13237,https://scholar.google.com/scholar?q=Red%20blood%20cell%20image%20generation%20for%20data%20augmentation%20using%20Conditional%20Generative%20Adversarial%20Networks%20Bailo%202019,[scholar],[Red blood cell image generation for data augmentation using Conditional Generative Adversarial Networks] ,"
"
13237,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=CT-realistic+data+augmentation+using+generative+adversarial+network+for+robust+lymph+node+segmentation&btnG=,[scholar],[CT-realistic data augmentation using generative adversarial network for robust lymph node segmentation] ,"
"
13237,https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10950/109503V/CT-realistic-data-augmentation-using-generative-adversarial-network-for-robust/10.1117/12.2512004.short,[SPIE MI2019],"
","
"
13237,https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Synthesis+of+brain+tumor+multicontrast+MR+images+for+improved+data+augmentation&btnG=,[scholar],[Synthesis of brain tumor multicontrast MR images for improved data augmentation] ,"
"
13237,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=Article+%7C+OPEN+%7C+Published%3A+18+October+2018++Deep+echocardiography%3A+data-efficient+supervised+and+semi-supervised+deep+learning+towards+automated+diagnosis+of+cardiac+disease&btnG=,[scholar],[Deep echocardiography: data-efficient supervised and semi-supervised deep learning towards automated diagnosis of cardiac disease] ,"
"
13237,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=Exploiting+the+potential+of+unlabeled+endoscopic+video+data+with+self-supervised+learning&btnG=,[scholar],[Exploiting the potential of unlabeled endoscopic video data with self-supervised learning] ,"
"
13245,https://nvlabs.github.io/ffhq-dataset/search/,click this link,"To find out whether your photo is included in the Flickr-Faces-HQ dataset, please ", to search the dataset with your Flickr username.
13253,https://github.com/Yale-LILY/LectureBank/blob/master/alldata.tsv,"
alldata.tsv
",": we combined all five batches of LectureBank, and remove duplicates and invlaid urls. All data can be found in ", with a total number to be 
13256,http://pan.webis.de/data.html,datasets can be found there,. A number of ,", and all of them are formatted as follows."
13261,http://documents.epfl.ch/groups/c/cv/cvlab-unit/www/data/multiview/denseMVS.html,fountain-P11,"Suppose you have a folder containing the images of the same scene, take "," as an example (or any other image folder by your hand), generate the image list by "
13261,https://www.kaggle.com/google/google-landmarks-dataset,Google-Landmarks-Dataset,We have additionally trained a ResNet-50 model not documented in the original paper. By refined from a base model trained on ,", it achieves better performance than GoogleNet on object retrieval datasets (Oxford5K and Paris6K)."
13261,https://s3-ap-southeast-1.amazonaws.com/awsiostest-deployments-mobilehub-806196172/GL3D/eval_data.zip,GL3D test images,Download the , of size 
13261,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford5K,To run the model on , or 
13261,http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris6K, or ,", there is one additional step. Suppose you are in the root directory of this repo, you need to first compile the C++ program "
13261,http://lear.inrialpes.fr/~jegou/data.php,official website,Downloads INRIA Holidays dataset at ,", prepare the required input lists and use the commands in "
13261,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,website,We've made the benchmark of Oxford (or Paris) simply. You only need to prepare the oxford dataset images (download from the official ,") and run `pipeline.sh', see ""Note on Oxford5K or Paris6K"" for details. Using the default settings in the script, you can easily achieve "
13264,https://oscar-corpus.com/,OSCAR Corpus (Common Crawl extract),"
","
"
13264,http://data.statmt.org/cc-100/,CC-100 Web Crawl Data (Common Crawl extract),"
","
"
13264,https://huggingface.co/datasets/allenai/c4,C4 and mC4 corpora (contains ~180GB of compressed Polish text),"
","
"
13264,https://huggingface.co/datasets/allenai/nllb,NLLB parallel corpus (1613 language pairs of which 43 include Polish),"
","
"
13271,https://ieee-dataport.org/open-access/clear-dataset-compositional-language-and-elementary-acoustic-reasoning,IEEE Dataport, The first version of the dataset can be downloaded via ,.
13271,https://github.com/facebookresearch/clevr-dataset-gen,code, question generation ,.
13274,https://ailb-web.ing.unimore.it/publicfiles/drive/show-control-and-tell/dataset_coco.tgz,dataset_coco.tgz,Download the annotations and metadata file , (~85.6 MB) and extract it in the code folder using 
13274,https://ailb-web.ing.unimore.it/publicfiles/drive/show-control-and-tell/dataset_flickr.tgz,dataset_flickr.tgz,"As before, download the annotations and metadata file ", (~32.8 MB) and extract it in the code folder using 
13276,https://ailb-web.ing.unimore.it/publicfiles/drive/CVPR%202019%20-%20Art2Real/datasets/monet2photo.zip,[Dataset],"
","
"
13276,https://ailb-web.ing.unimore.it/publicfiles/drive/CVPR%202019%20-%20Art2Real/datasets/landscape2photo.zip,[Dataset],"
","
"
13276,https://ailb-web.ing.unimore.it/publicfiles/drive/CVPR%202019%20-%20Art2Real/datasets/portrait2photo.zip,[Dataset],"
","
"
13276,https://ailb-web.ing.unimore.it/publicfiles/drive/CVPR%202019%20-%20Art2Real/data_for_patch_retrieval.zip,[data for patch retrieval],"
","
"
13277,https://github.com/uzh-rpg/imips_open_deps/tree/master/rpg_datasets_py,these instructions,Follow ," to link up KITTI. To speed things up, you can download http://rpg.ifi.uzh.ch/datasets/imips/tracked_indices.zip and extract the contained files to "
13277,https://github.com/uzh-rpg/imips_open_deps/tree/master/rpg_datasets_py,these instructions,(Re)move the previously downloaded checkpoints. Follow ," to link up TUM mono. Then, run:"
13304,https://github.com/fwilliams/deep-geometric-prior#surface-reconstruction-benchmark-data,above, (See , section for details).
13310,http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/,M-AILABS speech database,"
",: 
13321,https://www.cityscapes-dataset.com/anonymous-results/?id=b2cc8f49fc3267c73e6bb686425016cb152c8bc34fc09ac207c81749f329dc8d,https://www.cityscapes-dataset.com/anonymous-results/?id=b2cc8f49fc3267c73e6bb686425016cb152c8bc34fc09ac207c81749f329dc8d,ShelfNet18-lw real-time: ,"
"
13321,https://www.cityscapes-dataset.com/anonymous-results/?id=c0a7c8a4b64a880a715632c6a28b116d239096b63b5d14f5042c8b3280a7169d,https://www.cityscapes-dataset.com/anonymous-results/?id=c0a7c8a4b64a880a715632c6a28b116d239096b63b5d14f5042c8b3280a7169d, ShelfNet34-lw non real-time: ,"
"
13323,https://www.cityscapes-dataset.com/anonymous-results/?id=2267c613d55dd75d5301850c913b1507bf2f10586ca73eb8ebcf357cdcf3e036,66.15, and ," on the PASCAL VOC and the Cityscapes test sets, respectively."
13329,http://cocodataset.org/#home,MS COCO,The model requires , and the 
13335,https://maven-badges.herokuapp.com/maven-central/io.cloudslang/data,"
Maven Central
", | data | , | score-data-api | 
13335,https://maven-badges.herokuapp.com/maven-central/io.cloudslang/score-data-api,"
Maven Central
", | score-data-api | , | score-api | 
13335,https://maven-badges.herokuapp.com/maven-central/io.cloudslang/score-data-impl,"
Maven Central
", | score-data-impl | , | worker | 
13349,http://www.cvlibs.net/datasets/kitti/eval_semantics.php,KITTI Semantic Dataset, Bayesian SegNet trained on the ,; these weights were first trained using the 
13349,https://www.cityscapes-dataset.com,Cityscapes Dataset,; these weights were first trained using the ,", and were then fine tuned. All weights have the batch normalization layer merged with the preceding convolutional layer in order to speed up inference."
13349,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,here,Download the dataset (colour images) from ,.
13349,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI dataset, cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the ," as stereo or monocular, in the "
13349,http://vision.in.tum.de/data/datasets/rgbd-dataset,TUM dataset," as stereo or monocular, in the "," as RGB-D or monocular, and in the "
13349,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC dataset," as RGB-D or monocular, and in the "," as stereo or monocular. ORB_SLAM2 also contains a ROS node to process live monocular, stereo or RGB-D streams. "
13350,https://www.cityscapes-dataset.com/,here,"Cityscapes is a dataset that can be used to train SegNet/Bayesian SegNet, but a few steps must be done first. You can download the dataset ", and the Cityscape scripts repo 
13360,https://datatracker.ietf.org/doc/draft-ietf-tls-tls13/,TLS 1.3, for its , implementation. and 
13361,https://datatracker.ietf.org/doc/rfc9000/,RFC 9000,"The current version is aligned with version 1, ",". All big features are supported, including the interface between QUIC and TLS, 0-RTT, migration and key rollover. The state of development is tracked in the list of issues in this repository. The code also supports several features that are not yet standardized, including "
13361,https://datatracker.ietf.org/doc/draft-ietf-quic-datagram/,Datagrams,". All big features are supported, including the interface between QUIC and TLS, 0-RTT, migration and key rollover. The state of development is tracked in the list of issues in this repository. The code also supports several features that are not yet standardized, including ",", "
13361,https://datatracker.ietf.org/doc/draft-ietf-quic-ack-frequency/,ACK Frequency,", ",", "
13361,https://datatracker.ietf.org/doc/draft-ietf-quic-version-negotiation/,Compatible Version Negotiation,", ",", and "
13361,https://datatracker.ietf.org/doc/draft-ietf-quic-multipath/,Multipath,", and ",". The code includes specific tuning for geostationary satellite links and long delay links, including support for the "
13361,https://datatracker.ietf.org/doc/draft-kuhn-quic-bdpframe-extension/,BDP Frame Extension,". The code includes specific tuning for geostationary satellite links and long delay links, including support for the ",.
13364,https://github.com/sungjinl/icassp2019-ood-dataset,Repo with OOD-augmented data,"
","
"
13368,https://www.kaggle.com/c/painter-by-numbers/data,Kaggle's painter-by-numbers dataset, from ,; extract the content (paintings) into this new directory: 
13368,https://github.com/bethgelab/stylize-datasets,https://github.com/bethgelab/stylize-datasets,"This repository is tailored to creating a stylized version of ImageNet. Should you be interested in stylizing a different dataset, I recommend using this code: ", which stylizes arbitrary image datasets.
13374,https://storage.googleapis.com/disentanglement_dataset/data_npz/sim_toy_64x_ordered_without_heldout_factors.npz,mpi3d_toy,Simplistic rendered images (,)
13374,https://github.com/rr-learning/disentanglement_dataset,here,. More information about the dataset can be found , or in the arXiv preprint 
13379,https://data.vision.ee.ethz.ch/cvl/DIV2K/,official website,The original BSRN model is trained with the DIV2K dataset. You can download the images from its ,". After downloading and extracting the images, the directory structure may be looked like this:"
13382,http://cocodataset.org/#download,MSCOCO2017,(using original , images)
13382,https://github.com/soeaver/Parsing-R-CNN/blob/master/INSTALL.md#data-and-pre-train-weights,data structure,And following , to train or evaluate Parsing R-CNN models.
13386,https://media.githubusercontent.com/media/molecularsets/moses/master/data/dataset_v1.csv,a benchmarking dataset,We propose , refined from the ZINC database.
13386,https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader,DataLoader,You can use a standard torch , in your models. We provide a simple 
13390,https://www.ipf.kit.edu/lafida_datasets.php,Lafida dataset, to utilize the large FoV of the fisheye camera without introducing the distortion. The system is able to compute the camera trajectory and recover a sparse structure of the environment. It is also able to detect loops and relocalize the camera in real time. We provide examples on the ," and the self-collected data sequences. Like ORB-SLAM, we also provide a GUI to change between a "
13404,http://data.vision.ee.ethz.ch/mentzerf/l3c_models_v3/L3C.tar.gz,L3C.tar.gz, | , | | Baseline | RGB Shared | Open Images | 
13404,http://data.vision.ee.ethz.ch/mentzerf/l3c_models_v3/RGB_Shared.tar.gz,RGB_Shared.tar.gz, | , | | Baseline | RGB | Open Images | 
13404,http://data.vision.ee.ethz.ch/mentzerf/l3c_models_v3/RGB.tar.gz,RGB.tar.gz, | , | | Main Model | L3C | 
13404,http://data.vision.ee.ethz.ch/mentzerf/l3c_models/L3C_inet32.tar.gz,L3C_inet32.tar.gz, | , | | Main Model | L3C | 
13404,http://data.vision.ee.ethz.ch/mentzerf/l3c_models/L3C_inet64.tar.gz,L3C_inet64.tar.gz, | , |
13404,http://data.vision.ee.ethz.ch/mentzerf/validation_sets_lossless/val_oi_500_r.tar.gz,download Open Images evaluation set here,"We evaluated our model on 500 images randomly selected from the Open Images validation set, and preprocessed like the training data. To compare, please ",.
13404,https://marknelson.us/posts/2014/10/19/data-compression-with-arithmetic-coding.html,this blog post,The implementation is based on ,", meaning that we implement "
13405,docs/howto/add-dataloader-to-lipizzaner.md,this tutorial,"If you want to add your own data to Lipizzaner, refer to ",". For guidelines about how to compile and run the Lipizzaner dashboard, check its "
13425,https://captain-whu.github.io/DOTA/dataset.html,DOTA,1、please download ," 2、crop data, reference:"
13427,http://mscoco.org/dataset/#download,link,Download the images from this ,. We need the 2014 training images and 2014 val images.
13434,https://github.com/Spirals-Team/npe-dataset,NPE-dataset, on the dataset ,.
13444,http://www-nlp.stanford.edu/data/glove.840B.300d.zip,Glove embeddings,Step 3. Download the , and train word embeddings with Word2Vec. Then combine these two embedding files as proposed in the paper by running the following commands. 
13454,https://prior-datasets.s3.us-east-2.amazonaws.com/savn/pretrained_models.tar.gz,pretrained models,Download the , and 
13454,https://prior-datasets.s3.us-east-2.amazonaws.com/savn/data.tar.gz,data, and , to the 
13454,https://prior-datasets.s3.us-east-2.amazonaws.com/savn/offline_data_with_images.tar.gz,thor_offlline_data_with_images, with ,". If you wish to run your model on the image files, add the command line argument "
13461,https://gluon-cv.mxnet.io/api/data.datasets.html#,API Notes,"For experienced users, check out our ",.
13464,download_data.py,"
python download_data.py
",🌟 You can run , to interactively select and download any of these datasets!
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz,train-images-idx3-ubyte.gz,"| File            | Examples | Download (MNIST format)    | Download (NumPy format)      | |-----------------|--------------------|----------------------------|------------------------------| | Training images | 60,000             | ", (18MB) | 
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz,kmnist-train-imgs.npz, (18MB) | ," (18MB)   | | Training labels | 60,000             | "
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz,train-labels-idx1-ubyte.gz," (18MB)   | | Training labels | 60,000             | ", (30KB) | 
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz,kmnist-train-labels.npz, (30KB) | ," (30KB)  | | Testing images  | 10,000             | "
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz,t10k-images-idx3-ubyte.gz," (30KB)  | | Testing images  | 10,000             | ", (3MB) | 
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz,kmnist-test-imgs.npz, (3MB) | ," (3MB)   | | Testing labels  | 10,000             | "
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz,t10k-labels-idx1-ubyte.gz," (3MB)   | | Testing labels  | 10,000             | ", (5KB)  | 
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz,kmnist-test-labels.npz, (5KB)  | , (5KB) |
13464,http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist_classmap.csv,kmnist_classmap.csv,Mapping from class indices to characters: , (1KB)
13464,http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz,k49-train-imgs.npz,"| File            | Examples |  Download (NumPy format)      | |-----------------|--------------------|----------------------------| | Training images | 232,365            | "," (63MB)   | | Training labels | 232,365            | "
13464,http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz,k49-train-labels.npz," (63MB)   | | Training labels | 232,365            | "," (200KB)  | | Testing images  | 38,547             | "
13464,http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz,k49-test-imgs.npz," (200KB)  | | Testing images  | 38,547             | "," (11MB)   | | Testing labels  | 38,547             | "
13464,http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz,k49-test-labels.npz," (11MB)   | | Testing labels  | 38,547             | ", (50KB) |
13464,http://codh.rois.ac.jp/kmnist/dataset/k49/k49_classmap.csv,k49_classmap.csv,Mapping from class indices to characters: , (1KB)
13464,http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar,here,The full dataset is available for download , (310MB). We plan to release a train/test split version as a low-shot learning dataset very soon.
13466,#data-augmentation,Data augmentation,"
","
"
13476,https://data.csail.mit.edu/graphics/fivek/,https://data.csail.mit.edu/graphics/fivek/,MIT-5k - ,"
"
13494,https://github.com/facebookresearch/clevr-dataset-gen,CLEVR dataset,This is a repository modified from the , for generating realistic visualizations of 
13495,https://travis-ci.org/dataplayer12,"
Build status
","
","
"
13499,https://github.com/YuanKQ/MedSim-antibiotics-labeled-dataset/blob/master/antibioitc_1326_sim.csv,antibioitc_1326_sim.csv,Each row represents an antibiotic pair and the simiality score annotated by doctors in the dataset(,).
13505,http://www.eecs.qmul.ac.uk/~kz303/vsumm-reinforce/datasets.tar.gz,http://www.eecs.qmul.ac.uk/~kz303/vsumm-reinforce/datasets.tar.gz,Original version of the datasets can be downloaded from , or 
13505,https://www.dropbox.com/s/ynl4jsa2mxohs16/data.zip?dl=0,https://www.dropbox.com/s/ynl4jsa2mxohs16/data.zip?dl=0, or ,.
13530,https://datatracker.ietf.org/doc/draft-irtf-cfrg-hash-to-curve/07/,hash-to-curve-draft-07, is added for ,.
13531,#datasets,Datasets,"
","
"
13538,https://www.cityscapes-dataset.com/downloads/,here,Download the datasets from ,". Specifically, we will use "
13541,https://visualdialog.org/data,here,Download the VisDial v1.0 dialog json files from , and keep it under 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json,here,Get the word counts for VisDial v1.0 train split ,. They are used to build the vocabulary.
13541,https://visualdialog.org/data,here," provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VisDial v1.0 images from "," instead. Extracted features for v1.0 train, val and test are available for download at these links. Note that these files do not contain the bounding box information."
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_train.h5,"
features_faster_rcnn_x101_train.h5
","
",: Bottom-up features of 36 proposals from images of 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_val.h5,"
features_faster_rcnn_x101_val.h5
","
",: Bottom-up features of 36 proposals from images of 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_test.h5,"
features_faster_rcnn_x101_test.h5
","
",: Bottom-up features of 36 proposals from images of 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_train.h5,"
features_vgg16_fc7_train.h5
","
",: VGG16 FC7 features from images of 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_val.h5,"
features_vgg16_fc7_val.h5
","
",: VGG16 FC7 features from images of 
13541,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_test.h5,"
features_vgg16_fc7_test.h5
","
",: VGG16 FC7 features from images of 
13541,http://nlp.stanford.edu/data/glove.6B.zip,here,Download the GloVe pretrained word vectors from ,", and keep "
13541,https://www.github.com/cocodataset/cocoapi,cocoapi,"Set up opencv, ", and 
13541,http://cocodataset.org/#download,MSCOCO,Prepare the , and 
13541,https://visualdialog.org/data,Flickr, and , images.
13546,http://www.fki.inf.unibe.ch/databases/iam-handwriting-database,this website,"To download the IAM datasets, go to "," and register. Download and unpack the data by running these commands, but with your own username and passwords"
13546,https://www.prhlt.upv.es/contests/icfhr2016-kws/data.html,botany or konzilsprotokolle datasets,For the , run
13559,http://mulan.sourceforge.net/datasets-mlc.html,Mulan,scene (downloaded from ,)
13583,http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools,here,You can find the dataset in the semeval 2014 website ,. Copy the dataset in the directory 'dataset'
13584,https://github.com/tensorpack/dataflow,"
tensorpack.dataflow
",Squeeze the best data loading performance of Python with ,.
13584,https://tensorpack.readthedocs.io/tutorial/philosophy/dataflow.html#alternative-data-loading-solutions,does not,) , offer the data processing flexibility needed in research. Tensorpack squeezes the most performance out of 
13596,https://projects.asl.ethz.ch/datasets/doku.php?id=cvpr2019hfnet,trained weights for HF-Net and reconstructed SfM 3D models, using this codebase. We also provide ,.
13596,doc/datasets.md,dataset documentation,Refer to our , for an overview of the supported datasets and their expected directory structure.
13596,https://projects.asl.ethz.ch/datasets/doku.php?id=cvpr2019hfnet,here,. Download the trained model , and unpack it in 
13596,doc/datasets.md,dataset documentation,Download the datasets as indicated in the ,". SfM models of Aachen, RobotCar, CMU, and Extended CMU, built SuperPoint and usable with HF-Net, are provided "
13596,https://projects.asl.ethz.ch/datasets/doku.php?id=cvpr2019hfnet,here,". SfM models of Aachen, RobotCar, CMU, and Extended CMU, built SuperPoint and usable with HF-Net, are provided ",. Download and unpack the HF-Net weights in 
13596,http://rpg.ifi.uzh.ch/datasets/netvlad/vd16_pitts30k_conv5_3_vlad_preL2_intra_white.zip,NetVLAD,". To localize with NV+SP, download the network weights of ", and 
13597,http://cocodataset.org/#home,MS COCO 2017,", and ","
"
13597,https://github.com/cocodataset/cocoapi,COCO API,"
","
"
13597,http://cocodataset.org/#format-data,MS COCO format,"If you want to add your own dataset, you have to convert it to ",.
13597,http://cocodataset.org/#format-results,MS COCO format, should be follow ,. To test on the 
13597,https://github.com/cocodataset/cocoapi,cocoapi,"As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use ", or 
13598,http://cocodataset.org/#home,MS COCO 2017,", and ","
"
13598,https://github.com/cocodataset/cocoapi,COCO API,"
","
"
13598,http://cocodataset.org/#format-results,MS COCO format, is used in testing stage which should be prepared before testing and follow ,.
13598,http://cocodataset.org/#format-data,MS COCO format,"If you want to add your own dataset, you have to convert it to ",.
13598,https://github.com/cocodataset/cocoapi,cocoapi,"As this repo outputs compatible output files for MS COCO and PoseTrack, you can directly use ", or 
13599,https://github.com/Sha-Lab/FEAT/blob/master/data/README.md,this,Check , for details of data downloading and preprocessing.
13604,https://pandas.pydata.org/,pandas, files in the corresponding generation folder which can be processed using ,.
13612,http://robotics.ethz.ch/~asl-datasets/2018_mav_voxblox_planning/mav_voxblox_planning_maps.zip,"
here
",We've prepared a number of maps for you to try out our planning on. The archive is 260 MB big and available ,.
13617,slowfast/datasets/DATASET.md,DATASET.md,. You may follow the instructions in , to prepare the datasets.
13619,https://github.com/debadeepta/learningtoask/tree/master/data,Download data,"
",.
13628,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,leaderboard, for 3D object detection by using only the raw point cloud as input. PointRCNN is evaluated on the KITTI dataset and achieves state-of-the-art performance on the KITTI 3D object detection , among all published works at the time of submission.
13628,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI 3D object detection,Please download the official , dataset and organize the downloaded files as follows:
13632,https://pandas.pydata.org/,pandas,"
","
"
13632,https://bokeh.pydata.org/en/latest/,bokeh,"
","
"
13635,https://www.wikidata.org,Wikidata,We first downloaded from , the URIs of all the cities and the associated countries available on 
13635,https://sites.google.com/site/yangdingqi/home/foursquare-dataset,Global-Scale Check-in Dataset,", in the same format of the ",", and the previously mentioned "
13654,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2,"
",:
13654,http://www.cvlibs.net/datasets/kitti/,KITTI,"
",:
13654,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU-Depth,Try it out on ,", "
13654,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUM-RGBD,", ",", or "
13654,http://www.cvlibs.net/datasets/kitti/,KITTI,", or ",. Using more keyframes 
13654,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2,"
",:
13654,http://www.cvlibs.net/datasets/kitti/,KITTI,"
",:
13654,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,First download the dataset using this , provided on the official website. Then run the evaluation script where KITTI_PATH is the location of where the dataset was downloaded
13654,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2,"
",:
13654,http://www.cvlibs.net/datasets/kitti/,KITTI,"
",:
13654,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,First download the dataset using this ," provided on the official website. Once the dataset has been downloaded, write the training sequences to a tfrecords file"
13655,https://www.robots.ox.ac.uk/~vgg/data/dtd/,Textures,", ",", "
13661,https://github.com/naoto0804/cross-domain-detection/tree/master/datasets,Cross Domain Detection ,: Dataset preparation instruction link ,. Images translated by Cyclegan are available in the website.
13661,https://fcav.engin.umich.edu/sim-dataset/,Sim10k,: Website ,"
"
13661,https://www.cityscapes-dataset.com/,Cityscape,: Download website ,", see dataset preparation code in "
13661,https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data,DA-Faster RCNN,", see dataset preparation code in ","
"
13661,https://fcav.engin.umich.edu/sim-dataset/,Sim10k,"All codes are written to fit for the format of PASCAL_VOC. For example, the dataset ", is stored as follows.
13675,https://sjtueducn-my.sharepoint.com/personal/lilei_sky_sjtu_edu_cn/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly9zanR1ZWR1Y24tbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvbGlsZWlfc2t5X3NqdHVfZWR1X2NuL0VqV0tzYTAtTnRSTWlMM0c5bDgtVEFjQkRvMUY0UG0xTnhQb3NwX1UydUJNUEE%5FcnRpbWU9TFE5ZTlqbFAyRWc&id=%2Fpersonal%2Flilei%5Fsky%5Fsjtu%5Fedu%5Fcn%2FDocuments%2F2017%5FMMWHS%2FEvaluate%2Ftools%2FMMWHS%5Fevaluation%5Ftestdata%5Flabel%5Fencrypt%5F1mm%5Fforpublic%2Fnii,here, the ground-truth label for test images are now available ,.
13678,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ dataset, | Raw data for the ,. | └  
13678,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ repository,"For license information regarding the FFHQ dataset, please refer to the ",.
13678,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ repository,"), please refer to the ",.
13678,./dataset_tool.py,dataset_tool.py,"To obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided ",:
13711,http://www.cvlibs.net/datasets/kitti/index.php,KITTI,Training on ,"
"
13713,http://bnci-horizon-2020.eu/database/data-sets,BCI competition IV-2a,". For the 4-class dataset, download the dataset ""Four class motor imagery (001-2014)"" of the ",. Put all files of the dataset (A01T.mat-A09E.mat) into a subfolder within the project called 'dataset/IV2a' or change DATA_PATH in run_hd.py
13716,https://cddis.nasa.gov/Data_and_Derived_Products/GNSS/GNSS_data_and_product_archive.html,CDDIS,GNSS processing requires getting data from the internet from various analysis groups such as NASA's ,. AstroDog downloads files from FTP servers from these groups when it needs them. Downloading and parsing all this data can be slow. AstroDog caches all downloaded files locally to avoid re-downloading.
13716,https://urs.earthdata.nasa.gov/,here,It is no longer possible to download GNSS data from NASA servers without an account. You can make an account ,. Then create a .netrc file in the laika folder with content:
13721,https://github.com/phillipi/pix2pix/blob/master/datasets/download_dataset.sh,torch code,Download the dataset (script borrowed from ,):
13726,https://github.com/taverna/databundle,Data bundle API,"The remaining text of this section describes the content of the RO bundle, as if it was unpacked to a folder. Note that many programming frameworks include support for working with ZIP files, and so complete unpacking might not be necessary for your application. For Java, the ", gives a programmating way to inspect and generate data bundles.
13742,http://im2recipe.csail.mit.edu/dataset/download,Recipe1M,Download , (registration required)
13746,https://github.com/v-m/PropagationAnalysis-dataset,here,Dataset generated for those papers can be found ,.
13754,https://www.jstor.org/stable/2334380?seq=1#metadata_info_tab_contents,"Kettenring, 1971",; ,"] to the task of visual dialogue - a sequential question-answering task where the questions and answers are related to an image. With CCA, we learn mappings for questions and answers (and images) to a joint embedding space, within which we measure correlation between test set questions and their corresponding candidate answers in order to rank the answers. We show comparable performance in mean rank (MR), one of the established metrics on the "
13754,http://www.mscoco.org/dataset,Microsoft COCO,This will download ,"
"
13755,http://datasets.d2.mpi-inf.mpg.de/xian/xlsa17.zip,NS (PS),Download the resnet features and class splits from Yongqin Xian's website: , and 
13755,http://datasets.d2.mpi-inf.mpg.de/xian/standard_split.zip,SS, and ,". Unzip and put the xlsa17 and standard_split folders in the data folder. Run data_transfer.m to generate 8 .mat files ended with ""resnet.mat""."
13757,python/dpu_utils/utils/dataloading.py,"
{load,save}_json[l]_gz
","
", convenience API for loading and writing 
13759,https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/,this link,Download the CoNSeP dataset as used in our paper from ,. 
13763,https://github.com/facebookresearch/ActivityNet-Entities/blob/master/data/anet_entities_skeleton.txt#L13,file,We divide each clip evenly into 10 segments and sample the middle frame of each segment. We have clarified this in the skeleton ,.
13763,https://github.com/facebookresearch/grounded-video-description#data-preparation,provided,"First, you may want to check if the object region feature and RGB/motion frame-wise feature we "," meet your requirement. If not, you can first download the ActivityNet videos using this "
13771,https://vision.in.tum.de/data/datasets/rgbd-dataset/download,TUM RGBD,: Download the dataset from , to '$YOUR_TUM_RGBD_DIR'. Create a symbolic link to the data directory as
13772,data,data,Data preparation for training and evaluation can be found in , directory.
13773,https://github.com/OpenNMT/IntegrationTesting/tree/master/data,OpenNMT/IntegrationTesting,: An English-German translation model based on the 200k sentence dataset at ,. Perplexity: 21.
13776,https://github.com/Sekunde/3D-SIS/tree/master/datagen,datagen,Data generation code is detailed in ,.
13776,https://github.com/Sekunde/3D-SIS/blob/master/datagen/SceneSampler/main.cpp#L348-L415,saveChunkToFile,"We provide the test and validation data (.scene and images) as examples. The detailed format of data, see ",. Download the
13776,http://kaldir.vc.in.tum.de/3dsis/scannet_benchmark_test_data.zip,ScanNetV2 Benchmark Test Data (100 scenes) (801MB),"
","
"
13776,http://kaldir.vc.in.tum.de/3dsis/scannet_benchmark_validation_data.zip,ScanNetV2 Validation Data (95 scenes) (746MB),"
","
"
13776,http://kaldir.vc.in.tum.de/3dsis/scannetv2_test_data.zip,ScanNetV2 Validation Data (312 scenes) (3664MB),"
","
"
13776,http://kaldir.vc.in.tum.de/3dsis/suncg_test_data.zip,SUNCG Test Data (1355MB),"
","
"
13776,#download-validation-and-test-data,ScanNetV2 Validation Data (312 scenes), and , with following structure.
13776,#download-validation-and-test-data,ScanNetV2 Validation Data (95 scenes), and , with following structure.
13776,#download-validation-and-test-data,ScanNet Benchmark Test Data, using ,", but you need to remap the "
13776,#download-test-data,SUNCG Test Data, and , with following structure.
13776,#download-traininig-data,Training Data,"Generate training data, see ", or 
13776,#data-generation,Data Generation, or ,"
"
13777,#data-scheme,Data scheme,"
","
"
13777,#dataset,dataset,"
","
"
13777,dataloader.py#L105,dataloader.py#L105, in ,).
13777,data/ISLES.lineage,ISLES.lineage,Instruction to download the data are contained in the lineage files , and 
13777,data/wmh.lineage,wmh.lineage, and ,. They are text files containing the md5sum of the original zip.
13795,exp_data,exp_data,-The data from the psychophysical experiment: ,"
"
13795,code/analyze_noise_data.py,analyze_noise_data.py,-A script to analyze the experimental data and generate figures: ,"
"
13795,data,data,-The data from the model simulations: ,"
"
13818,https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh,COCO,The commands below reproduce YOLOv3 , results. 
13818,https://github.com/ultralytics/yolov5/tree/master/data,datasets, and , download automatically from the latest YOLOv3 
13818,https://docs.ultralytics.com/yolov5/train_custom_data,Train Custom Data,"
", 🚀 RECOMMENDED
13818,http://cocodataset.org,COCO val2017, denotes mAP@0.5:0.95 metric measured on the 5000-image , dataset over various inference sizes from 256 to 1536.
13818,http://cocodataset.org,COCO val2017, measures average inference time per image on , dataset using a 
13818,https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml,hyp.scratch-low.yaml,All checkpoints are trained to 300 epochs with default settings. Nano and Small models use ," hyps, all others use "
13818,https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml,hyp.scratch-high.yaml," hyps, all others use ",.
13818,http://cocodataset.org,COCO val2017, values are for single-model single-scale on , dataset.
13828,#using-datagenerator,DataGenerator,"
","
"
13828,#using-datagenerator,DataGenerator,"
","
"
13828,scripts/data/data_generator.py,source code,"
", for more details.
13833,https://github.com/pgcorpus/gutenberg,code,Run the ," yourself to get the latest version of the corpus, which will include all books in PG as of today."
13833,notebooks_tutorial/tutorial_01_how-to-read-a-book-metadata.ipynb,Tutorial 01: Loading a book and metadata queries,"
"," has some basic examples on how to easily load a single book; or how to query the metadata to get a selection of books, e.g. from the same author."
13836,#how-to-fine-tune-one-of-the-trained-models-on-your-own-dataset,How to fine-tune one of the trained models on your own dataset,"
","
"
13836,#how-to-fine-tune-one-of-the-trained-models-on-your-own-dataset,Read below,"
","
"
13836,https://github.com/pierluigiferrari/data_generator_object_detection_2d,here,The data generator used here has its own repository with a detailed tutorial ,"
"
13837,https://github.com/michalfaber/rmpe_dataset_server,server,Augmented samples are fetched from the ,". The network never sees the same image twice which was a problem in previous approach (tool rmpe_dataset_transformer) This allows you to run augmentation locally or on separate node. You can start 2 instances, one serving training set and a second one serving validation set (on different port if locally)"
13837,https://github.com/michalfaber/rmpe_dataset_server,rmpe_dataset_server,Download and compile the dataset server ,. This server generates augmented samples on the fly. Source samples are retrieved from previously generated hdf5 dataset file.
13841,https://github.com/shaoanlu/faceswap-GAN/blob/master/notes/README.md#13-model-evaluation-for-trumpcage-dataset,here, Evaluations of the output quality on Trump/Cage dataset can be found ,.
13847,https://github.com/PAL-UH/transferAL/tree/master/data/mars,Mars,"
","
"
13847,https://github.com/PAL-UH/transferAL/tree/master/data/supernova,Supernova,"
","
"
13863,http://corpus-texmex.irisa.fr/,SIFT1M,Quicker ADC achieves excellent performance outperforming polysemous codes in numerous configurations. We evaluated its performance for exhaustive search in the , dataset with both 64-bit and 128-bit codes.
13863,http://corpus-texmex.irisa.fr/,SIFT1000M,"We also evaluated its performance for index-based (i.e., non-exhaustive) search on the ", dataset and 
13870,#data,Data,"
","
"
13875,https://discourse.ros.org/t/introducing-the-robot-vulnerability-database/11105/7?u=vmayoral,this ROS Discourse thread,") by tackling aspects such scope and impact of the flaws (through a proper severity scoring mechanism for robots), information for facilitating mitigation, detailed technical information, etc. For a more detailed discussion, see ",.
13880,https://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/,iRobot’s Roomba J7 series robot vacuum,| 👹 Codename/theme | 🤖 Robotics technology affected | 👨‍🔬 Researchers | 📖 Description | 📅 Date | |-----|-------|-------------|-------------|------| | | ," | N/A | Personal pictures in a home environment were found in the Internet taken by an iRobot’s Roomba J7 series robot vacuum. The photos vary in type and in sensitivity. The most intimate image we saw was the series of video stills featuring the young woman on the toilet, her face blocked in the lead image but unobscured in the grainy scroll of shots below. In another image, a boy who appears to be eight or nine years old, and whose face is clearly visible, is sprawled on his stomach across a hallway floor. A triangular flop of hair spills across his forehead as he stares, with apparent amusement, at the object recording him from just below eye level. Various other home pictures that tag objects in the environment were found. | 19-19-2022 | |  | Unitree's "
13899,https://bam-dataset.org/#download,link, the data after downloading. Downloading the data is possible if you fulfill a given task (segmentation labeling). Please go to the , in order to download it. 
13902,https://www.cityscapes-dataset.com/login/,here, in CityScape dataset ,. (Require registration)
13902,http://synthia-dataset.net/downloads/,here, in SYNTHIA dataset ,.
13902,https://crcv.ucf.edu/data/adaptationseg/ICCV_dataset.zip,here,"3, Download our auxiliary pre-inferred target domain properties (Including both superpixel landmark and label distribution described in the paper) & parsed annotation ",.
13903,dataset,dataset directory,Various file format for learning with IUST-DeepFuzz and then fuzz testing is available at ,.
13903,https://m-zakeri.github.io/innovations-on-automatic-test-data-generation.html#innovations-on-automatic-test-data-generation,Innovations on Automatic Test Data Generation,"
","
"
13908,https://github.com/CAHLR/goal-based-recommendation/tree/master/synthetic_data_samples,synthetic_data_samples,"Due to FERPA privacy protection, we cannot publish the original student enrollment dataset. However, here we provide a sythetic dataset that consists of simulated student enrollment data and student major data to serve as an example of the formatting used to work with the code. The dataset is located in folder ",.
13926,https://github.com/vendi12/ODExploration_data,Open Data Exploration dataset,"
", for the conversational browsing task contains 26 transcripts annotated with dialog acts and entity spans. 
13926,https://github.com/vendi12/ODExploration_data#annotations,Codebook, for the conversational browsing task contains 26 transcripts annotated with dialog acts and entity spans. , License: MIT.
13928,https://www.data.gv.at,data.gv.at,"
", (13 conversations)
13928,https://www.opendataportal.at,opendataportal.at,"
", (13 conversations)
13929,https://github.com/JTrippas/Spoken-Conversational-Search/blob/master/SCSdata_v1.csv,SCSdata_v1.csv,: the ," is the full dataset used in Trippas, Spina, Thomas, Sanderson, Joho, and Cavedon (2020). The "
13929,https://github.com/JTrippas/Spoken-Conversational-Search/blob/master/SCSdata_v1.csv,SCSdata_v1.csv,: ,"
"
13929,https://github.com/JTrippas/Spoken-Conversational-Search/blob/master/SCSdataset.csv,(SCSdataset.csv), and ,"
"
13929,https://github.com/JTrippas/Spoken-Conversational-Search/blob/master/SCSdata_v1.csv,SCSdata_v1.csv,"
", file structure (
13929,https://data.csiro.au/dap/landingpage?pid=csiro:14550&v=2&d=true,"Bailey, Moffat, Scholer, and Thomas (2015)",Further information about the selected backstories can be found in ,.
13937,https://github.com/huggingface/datasets,"
datasets
", integrates with , to manage task data
13938,http://cocodataset.org/,MS COCO,Download , dataset:
13948,corpusreaders,the corpus-readers, you should look into , module.
13948,corpusreaders/README.md,corpusreaders," | Provides a set of NLP-friendly data structures and a number of  NLP-related utilities that support writing NLP applications, running experiments, etc. | | ", | Provides classes to read documents from corpora into 
13948,dataless-classifier/README.md,dataless-classifier, | A temporal extractor and normalizer.  | | , | Classifies text into a user-specified label hierarchy from just the textual label descriptions | | 
13955,https://maciejkula.github.io/spotlight/datasets/goodbooks.html,Spotlight,The dataset is accessible from ,", recommender software based on PyTorch."
13969,http://people.ee.ethz.ch/~ihnatova/#dataset,DPED dataset,Download , (patches for CNN training) and extract it into 
13972,https://tinyurl.com/nyu-data-zip,NYU Depth V2 (50K),"
", (4.1 GB): You don't need to extract the dataset since the code loads the entire zip file into memory when training.
13972,http://www.cvlibs.net/datasets/kitti/,KITTI,"
",": copy the raw data to a folder with the path '../kitti'. Our method expects dense input depth maps, therefore, you need to run a depth "
13972,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,inpainting method,": copy the raw data to a folder with the path '../kitti'. Our method expects dense input depth maps, therefore, you need to run a depth "," on the Lidar data. For our experiments, we used our "
13985,https://github.com/cocodataset/cocoapi,cocoapi website,Install COCOAPI referring to ,", or:"
13985,http://cocodataset.org/#download,COCO website,Download images from ,", and put train2014/val2014 splits into "
13993,https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,"Flickr8K, Flickr30K, and MSCOCO captions",Download Karpathy's , and put them in 
13993,http://mscoco.org/dataset/#download,MSCOCO 2014 images,Download the , and put them all together in 
14007,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,VGG-Faces2, and , datasets.
14016,https://www2.informatik.uni-hamburg.de/wtm/software/multiple-objects-gan/data-multi-mnist.zip,download,"
"," our data, save it to "
14016,https://github.com/facebookresearch/clevr-dataset-gen,here,CLEVR: adapted from ,"
"
14016,https://www2.informatik.uni-hamburg.de/wtm/software/multiple-objects-gan/data-clevr-main.zip,download,Main: ," our data, save it to "
14016,https://www2.informatik.uni-hamburg.de/wtm/software/multiple-objects-gan/data-clevr-cogent.zip,download,CoGenT: ," our data, save it to "
14016,https://www2.informatik.uni-hamburg.de/wtm/software/multiple-objects-gan/data-ms-coco.zip,download,"
"," our preprocessed data (bounding boxes and bounding box labels), save it to "
14016,http://cocodataset.org/#download,here,obtain the train and validation images from the 2014 split ,", extract and save them in "
14024,data,data, and , used present our findings at the 
14036,https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification/blob/master/data_loaders/torch_geometric/torch_loader.py,torch_loader.py, using the function defined in ,.
14036,https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification/blob/master/data_loaders/torch_geometric/torch_classification_example.py,torch_classification_example.py,See , for a complete working example.
14036,https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification/blob/master/Example.ipynb,notebook, module was used. The code used to generate the results can be found in the , of this repository.
14041,http://www.cs.toronto.edu/~rjliao/data/qm8.zip,preprocessed QM8 data,"To set up experiments, we need to download the ", and build our customized operators by running the following scripts:
14041,http://quantum-machine.org/datasets/,raw QM8, to preprocess the , data which requires the installation of 
14042,questions/train/data_existence,data_existence,"
",": ""Is there a mug in the room?"""
14042,questions/train/data_counting,data_counting,"
",": ""How many mugs are in the room?"""
14042,questions/train/data_contains,data_contains,"
",": ""Is there a mug in the fridge?"""
14064,#data,Data,"
","
"
14064,#data,Data,"
","
"
14064,https://github.com/openimages/dataset,Open Images, and ,". Specifically,"
14064,data/dictionary_and_semantic_hierarchy.txt,data/dictionary_and_semantic_hierarchy.txt,"As shown above, one image corresponds to one row. The first term is the original image ID of ImageNet. The followed terms separated by space are the annotations. For example, ""2367:1"" indicates class 2367 and its confidence 1. Note that the class index starts from 0, and you can find the class name from the file ",.
14064,data/train_urls_tiny.txt,train_urls_tiny.txt," is very large, here we provide a tiny file ", to demonstrate the downloading procedure.
14064,data/dictionary_and_semantic_hierarchy.txt,data/dictionary_and_semantic_hierarchy.txt,. The direct parent categories of each class can be found from the file ,". The whole semantic hierarchy includes 4 independent trees, of which the root nodes are "
14064,data/dictionary_and_semantic_hierarchy.txt,semantic hierarchy,According to the constructed ," of 11,166 categories, we augment the annotations of all URLs of ML-Images following the cateria that if one URL is annotated with category i, then all ancestor categories will also be annotated to this URL."
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-invariants,Datatracker,"
","
"
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport,Working Group Draft,"
","
"
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-recovery,Working Group Draft,"
","
"
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls,Datatracker,"
","
"
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-http,Datatracker,"
","
"
14069,https://datatracker.ietf.org/doc/html/draft-ietf-quic-qpack,Datatracker,"
","
"
14098,http://ferandrade.com/blog/2015/07/interactive-segmentation-dataset.html,Novel Dataset for Interactive Segmentation Evaluation,"
","
"
14101,http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html,notMNIST,You need to download the , dataset from 
14103,https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu_data.zip?dl=0,Ubuntu dataset,Download the ," released by (Xu et al, 2017)"
14106,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays_version_2/index.en.jsp,this paper, in CONLL format; the original goes back to ,.
14107,utils/create_data.py,"
utils/create_data.py
",Supported datasets can be downloaded to a specified directory by running ,.
14107,utils/chart_data.py,"
utils/chart_data.py
",The , script handles processing of data and charting for a specified dataset and source directory.
14113,https://github.com/bgshih/crnn/blob/master/tool/create_dataset.py,tool, format by using the , (run in 
14113,http://www.robots.ox.ac.uk/~vgg/data/text/,NIPS 2014,You can also download the training (,", "
14113,http://www.robots.ox.ac.uk/~vgg/data/scenetext/,CVPR 2016,", ",) and testing datasets prepared by us.
14118,http://cocodataset.org/#detection-leaderboard,COCO Detection Challenge," team, who won "," in 2018, and we keep pushing it forward."
14118,docs/en/1_exist_data_model.md,with existing dataset,"
","
"
14118,docs/en/2_new_data_model.md,with new dataset,"
","
"
14118,docs/en/3_exist_data_new_model.md,with existing dataset_new_model,"
","
"
14118,docs/en/tutorials/customize_dataset.md,customize_datasets,"
","
"
14118,docs/en/tutorials/data_pipeline.md,customize data pipelines,"
","
"
14122,#core-data,Core Data,"
","
"
14134,http://cocodataset.org/#detection-eval,COCO,"P.S. evaluation metric: AP, AP50, AP75, AP(small), AP(medium), AP(large), please refer to ", for detailed explanation. The inference time is measured on Nvidia 1080Ti.
14140,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction#FAQ,FAQ,7/11/2018: An , section has been added to the data extraction page.
14140,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction,Official training data,7/1/2018: , is up.
14140,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction/trial,Trial data,6/18/2018: , is up.
14140,https://github.com/DSTC-MSR/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction,data extraction,Please check the , for the input data pipeline. Note: We are providing scripts to extract the data from a Reddit 
14141,mailto:bryn@databaseconsultinggroup.com,bryn@databaseconsultinggroup.com,Bryn Rhodes ,"
"
14141,mailto:parker@frostbytedata.com,parker@frostbytedata.com,Parker Lawson ,"
"
14149,http://cocodataset.org/#detection-leaderboard,COCO Detection Challenge 2018,FishNet was used as a key component for winning the 1st place in ,.
14156,#response-metadata,Response metadata,"
","
"
14163,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry Benchmark, open-sourced stereo algorithm on , (12.Jan.2019).
14163,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC MAV Dataset,Download ," to YOUR_DATASET_FOLDER. Take MH_01 for example, you can run VINS-Fusion with three sensor types (monocular camera + IMU, stereo cameras + IMU and stereo cameras). Open four terminals, run vins odometry, visual loop closure(optional), rviz and play the bag file respectively. Green path is VIO odometry; red path is odometry under visual loop closure."
14163,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry dataset,Download ," to YOUR_DATASET_FOLDER. Take sequences 00 for example, Open two terminals, run vins and rviz respectively. (We evaluated odometry on KITTI benchmark without loop closure funtion)"
14163,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI raw dataset,Download , to YOUR_DATASET_FOLDER. Take 
14163,https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_10_03_drive_0027/2011_10_03_drive_0027_sync.zip,2011_10_03_drive_0027_synced, to YOUR_DATASET_FOLDER. Take ," for example. Open three terminals, run vins, global fusion and rviz respectively. Green path is VIO odometry; blue path is odometry under GPS global fusion."
14168,http://www.hashdata.cn/,HashData,"
","
"
14168,http://www.nttdata.com/,NTT DATA,"
","
"
14168,http://onedata.org,Onedata,"
","
"
14168,https://www.switchdatabase.com/,Switch Database,"
","
"
14168,http://www.taskdata.com/,TaskData,"
","
"
14168,http://www.nttdata.com/global/en/,NTT DATA Corporation, (,)
14173,https://seer.cancer.gov/data/access.html,page,"Data access to the SEER dataset is granted after a form submission to NCI. For details to accessing the SEER database, follow the directions on the access ",. Our project uses the May 2018 SEER dataset.
14184,https://storage.googleapis.com/bert_treccar_data/pretrained_models/BERT_Large_pretrained_on_TREC_CAR_training_set_1M_iterations.tar.gz,BERT_Large_pretrained_on_TREC_CAR...,"
", | BERT-large pretrained on TREC-CAR's training set for 1M iterations | 3.4 GB | 
14194,https://github.com/Ha0Tang/HandGestureRecognition/tree/master/datasets/sample,folder,Prepare your own dataset like in this ,.
14210,src/Backend/data,"
src/Backend/data
",Download the following files and place them in , (needed for the InferSent model):
14210,https://nlp.stanford.edu/data/glove.840B.300d.zip,Glove Embeddings,"
","
"
14225,http://cemantix.org/data/ontonotes.html,CoNLL-2012,You have to follow the instructions below to get CoNLL-2012 data ,", this would result in a directory called "
14226,https://github.com/TalLinzen/rnn_agreement/raw/master/data/wiki.vocab,wiki.vocab,"                  | from LGD, used for verb inflections (",) | | 
14226,https://github.com/facebookresearch/colorlessgreenRNNs/raw/master/data/agreement/English/generated.tab,generated.tab,               | data from Gulordava et al (,) |
14231,/tfjs-data,TensorFlow.js Data,"
",", a simple API to load and prepare data analogous to "
14231,https://www.tensorflow.org/guide/datasets,tf.data,", a simple API to load and prepare data analogous to ",.
14252,https://github.com/gluon-api/gluon-api/blob/master/docs/data.rst,Gluon Data API,"
","
"
14262,https://gustav1.ux.uis.no/downloads/ecir2019-qac/ecir2019-qac-data.zip,here,Download data from , and extract into 
14270,http://cocodataset.org/#download,here, Download the COCO train2014 and val2014 data ,. Put the COCO train2014 images in the folder 
14283,#data,Data,"
","
"
14290,./get_sdc_data.sh,"
get_sdc_data.sh
","To download The Spoken Dialog Challenge data, please use the script ",.
14300,https://seaborn.pydata.org/,Seaborn,"
","
"
14300,https://pandas.pydata.org/,Pandas,"
","
"
14321,https://checkoutapi.svea.com/docs/html/reference/web-api/data-types/index.htm,documentation," and if you wish to see more detailed data structures, see this ",.
14332,https://framenet.icsi.berkeley.edu/fndrupal/framenet_request_data,FrameNet XML data,1. Download ,"
"
14332,data/schema.zip,XSD schema files,"For a detailed account of the Berkeley FrameNet XML format, check out the ",.
14334,data/fn_en_150.7z,FrameNet 1.5,"Alternatively, we provide two MongoDB dumps for ", and 
14334,data/fn_en_170.7z,FrameNet 1.7, and ," data. If you are running MongoDB on localhost and port 27017, you can easily import the dumps once unzipped via:"
14334,https://framenet.icsi.berkeley.edu/fndrupal/framenet_request_data,FrameNet Data Request,"If you are using our dumps, please do not forget to file in a ",.
14340,https://archive.ics.uci.edu/ml/datasets/HIGGS,UCI Machine Learning Repository,"In our experiments, we used a random subset of the HIGGS dataset from the ",. This subset can be downloaded in HDF5 format from 
14353,https://bitbucket.org/ghentdatascience/cne/,CNE,", ",.
14353,https://bitbucket.org/ghentdatascience/cne/,CNE,", and/or ",.
14353,https://snap.stanford.edu/data/egonets-Facebook.html,Facebook,"
", (combined network)
14353,https://snap.stanford.edu/data/ca-GrQc.html,ArXiv GR-QC,"
","
"
14353,http://socialnetworks.mpi-sws.org/data-wosn2009.html,Facebook-wallpost,"
","
"
14353,http://snap.stanford.edu/data/ca-AstroPh.html,ArXiv Astro-Ph,"
","
"
14353,https://snap.stanford.edu/data/cit-HepPh.html,ArXiv Hep-Ph,"
","
"
14353,http://socialcomputing.asu.edu/datasets/BlogCatalog3,BlogCatalog,"
","
"
14356,https://github.com/bakwc/JamSpell/blob/master/test_data/sherlockholmes.txt,"
sherlockholmes.txt
",Prepare a utf-8 text file with sentences to train at (eg. ,) and another file with language alphabet (eg. 
14356,https://github.com/bakwc/JamSpell/blob/master/test_data/alphabet_en.txt,"
alphabet_en.txt
",) and another file with language alphabet (eg. ,)
14380,http://cocodataset.org/#download,coco website,"Download the images (2017 Train, 2017 Val, 2017 Test) from ",.
14380,http://cocodataset.org/#download,coco website,Download annotation files (2017 train/val and test image info) from ,.
14389,https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_50/2cb466df-239c-4fbb-9883-7ff784a2b5e9_Training_Set_Split_Download.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20210916%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20210916T141007Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=f7dc0bbda1625647acdef122c029cdc9a1d638408aecfa6137dc6b952e3f6263,Training_Set_Split_Download.txt,"
","
"
14389,https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_50/7dcfad42-65c6-4481-abe8-5a44339fa305_Dataset%20Description.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20210916%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20210916T141008Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=79e00ec4fc5a8d083aa77a3da93694d86525b8c9904e9850f9cb1b36b8a9c212,Dataset Description,"
","
"
14389,https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_50/7826027a-c745-4faa-94ec-bae6783fdc41_Terms%20and%20Conditions.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20210916%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20210916T141008Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=2de282e848c52d86c4827b6465d33956d148dd6cafbe8079e51de8f2961048b7,Terms and Conditions,"
","
"
14389,https://crowdai-prd.s3.eu-central-1.amazonaws.com/dataset_files/challenge_50/16772e7f-7871-4d42-a44f-5f399f40fd94_training_set_track_features_mini.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAILFF3ZEGG7Y4HXEQ%2F20210916%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20210916T141008Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=cb96dfffbe00d19bcbbf06332944c4ec6dc56aaf0778e895ba17fd7c3f42450d,Training_Set_And_Track_Features_Mini,"
"," (Updated, Sep 2021)"
14396,https://github.com/tkipf/gcn/tree/master/gcn/data,here,"Cora, Citeseer, and Pubmed datasets was taken directly from ",. CoraML dataset was taken from 
14399,http://groups.csail.mit.edu/vision/datasets/ADE20K,ADE20K,", ",", "
14399,https://www.cityscapes-dataset.com,Cityscapes,", ",", and "
14399,http://cocodataset.org,COCO,", and ", datasets. All pretrained weights are loaded automatically during use. See examples of such automatic loading of weights in the corresponding sections of the documentation dedicated to a particular package:
14404,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI,Downlaoad the , and arrange files as following:
14404,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI benchmark,Testing results on ,.
14428,#data,Data,"
", 2.1. 
14428,#data,Training data, 2.1. , 2.2. 
14428,#builddata,Building datasets, 2.2. , 2.3. 
14428,#exampledata,Example Data, 2.4. ,"
"
14437,http://vision.cs.duke.edu/DukeMTMC/data/demo_code/SCT.zip,SCT software,This program is modification of original , that can only process one camera(camera no.2) among eight cameras in Duke campus within a specific range of time.
14450,https://nbviewer.jupyter.org/github/KrishnaswamyLab/AAnet/blob/master/AAnet_torch/simulated_data_example/AAnet_vs_other_methods_on_tetrahedron.ipynb,Guided tutorial in Python,"
","
"
14462,https://www.floydhub.com/redeipirati/datasets/pytorch-mnist,uploaded it as FloydHub dataset," dataset for you, moreover I have already ", so that you can try and familiarize with 
14462,http://docs.floydhub.com/guides/basics/create_new/#create-a-new-dataset,docs,: If you want to mount/create a dataset look at the ,.
14476,https://github.com/harvardnlp/boxscore-data,Data-to-Text Datasets,This dataset is derived from one of the , (RotoWire) proposed in the paper 
14476,https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2?raw=true,here,", which is for NBA game report generation. The original data can be downloaded from ",.
14476,https://github.com/harvardnlp/data2text/blob/master/data_utils.py,the script,The original dataset is then preprocessed with a modified version of ," provided in the Data-to-Text dataset. In this step, we make sure each name of an entity (team/city/player) become a single token (e.g., "
14478,#set-up-the-classification-datasets,Set up the classification datasets,"
","
"
14478,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,set of scripts," manually, due to copyright issues.  Facebook has created a ", to help download and extract the dataset.
14484,#normalizedenormalize-data,Normalize/Denormalize data,"
","
"
14492,http://cmp.felk.cvut.cz/~qqpultar/AMOS_Patches/Metadata.zip,metadata," - 27 folders, 50 images each, with ",.  The name of each view corresponds to ID of the camera in the AMOS dataset. Metadata for all cameras are available 
14492,http://cmp.felk.cvut.cz/~qqpultar/AMOS_Patches/MetadataAll.zip,here,.  The name of each view corresponds to ID of the camera in the AMOS dataset. Metadata for all cameras are available ,.
14504,https://people.cs.umass.edu/~mccallum/data.html,Andrew McCallum's Cora Project,Large Cora is constructed from ,", and processed by our own. If you use Large Cora dataset, please cite our paper ""Label Efficient Semi-Supervised Learning via Graph Filtering"" and McCallum's paper ""Automating the Construction of Internet Portals with Machine Learning"""
14512,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp,C++ code for Oxford Building,"For mAP calculation, you also can refer to the ",. We use the triangle mAP calculation (consistent with the Market1501 original code).
14520,./docs/dataset_download.md,Download Task Data,"
","
"
14521,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,"
","
"
14521,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU v2,"
","
"
14537,https://cvml.ist.ac.at/AwA2/dataset/AwA2-features-ICML2019.zip,here," [2] - A dataset of images of various animals, together with 85 binary attributes. The used feature representations can be found ",.
14537,https://cvml.ist.ac.at/AwA2/dataset/AwA2-features-ICML2019.zip,link to the used features,"For the Animals with Attributes data, we provide a ",. We used a ResNet50 pretrained network from the 
14539,http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html,the official site,Download videos by following ,.
14539,https://20bn.com/datasets/jester,the official site,Download videos by following ,.
14546,https://webscope.sandbox.yahoo.com/catalog.php?datatype=a,Webscope: Datasets, folder. The dataset is available at ,.
14549,http://www.bigdatalab.ac.cn/~gjf/,Homepage,"
","
"
14549,http://www.bigdatalab.ac.cn/~lanyanyan/,Homepage,"
","
"
14549,http://www.bigdatalab.ac.cn/~cxq/,Homepage,"
","
"
14554,https://github.com/MedChaabane/deepRAM/blob/master/datasets/example-input-data.gz,Example input data,We have provided two preprocessing scripts to change the format of the used datasets to a format compatible with deepRAM input data format (deepRAM input data format: sequence label. See ,):
14559,https://databricks.com/,databricks,MalwareFeatureExtraction-spark-databricks.ipynb : notebook that we used on , to experiment with a pyspark implementation.
14567,https://data.lip6.fr/usrlts/,https://data.lip6.fr/usrlts/,Pretrained models are downloadable at ,.
14572,https://community.neo4j.com/c/integrations/linked-data-rdf-ontology,Neo4j community portal,"Full documentation will be available soon. In the meantime, please share your feedback in the ",.
14577,https://github.com/gabrielStanovsky/supervised-oie/blob/master/data/test.oie.conll,here, is the relabeled test dataset of OIE2016 in a json format. The orginal OIE2016 test dataset is ,. Note that the sentence order in Re-OIE2016 is different in that of OIE2016 but the sentences are almost the same.
14590,http://biplab.unisa.it/MICHE/database/MICHE_BIPLAB_DATABASE/,MICHE-I,"
",  Email: biplab@unisa.it.
14591,https://medium.com/@jason.20/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610,[here],A blog post that explains EDA is ,.
14600,https://github.com/zhijing-jin/IMT/tree/master/data/yelp/test,Test Set (200 sentences with 5 human references),Enriched test set for Yelp: ,"
"
14602,Techniques-two-data-sets/,[1],"
"," Y. Song, P. J. Schreier, D. Ramirez, and T. Hasija, ""Canonical correlation analysis of high-dimensional data with very small sample support,"" Signal Processing, vol. 128, pp. 449-458, 2016."
14602,Techniques-two-data-sets/Cross-Validation/,[2],"
"," C. Lameiro, and P. J. Schreier, ""Cross-validation techniques for determining the number of correlated components between two data sets when the number of samples is very small,"" Proc. Asilomar Conf. Signals Syst. Computers, Pacific Grove, CA, USA, November 2016."
14602,Techniques-multiple-data-sets/Bootstrap/,[3],"
"," T. Hasija, Y. Song, P. J. Schreier and D. Ramirez, ""Bootstrap-based Detection of the Number of Signals Correlated across Multiple Data Sets,"" Proc. Asilomar Conf. Signals Syst. Computers, Pacific Grove, CA, USA, November 2016."
14602,Techniques-two-data-sets/Sparse-CCA/,[4],"
"," C. Lameiro, and P. J. Schreier, ""A sparse CCA algorithm with application to model-order selection for small sample support,"" Proc. IEEE Int. Conf. Acoustics, Speech and Signal Process., New Orleans, LA, USA, March 2017."
14602,Techniques-one-data-set/Improper-Signal-Subpsace-Detection/,[5],"
"," T. Hasija,  C. Lameiro and P. J. Schreier, ""Determining the Dimension of the Improper Signal Subspace in Complex-Valued Data,"" IEEE Signal Processing Letters, vol. 24, no. 11, pp. 1606-1610, Nov. 2017."
14602,Techniques-multiple-data-sets/Complete-Model-Selection/,[6],"
"," T. Marrinan, T. Hasija, C. Lameiro and P. J. Schreier,""Complete model selection in multiset canonical correlation analysis,"" Proc. 26th European Signal Processing Conference (EUSIPCO), Rome, Italy, 2018."
14602,Techniques-multiple-data-sets/Complete-Model-Selection-Eigenvalue-Eigenvector-Test-Technique/,[7],"
"," T. Hasija, C. Lameiro, T. Marrinan,  and P. J. Schreier,""Determining the Dimension and Structure of the Subspace Correlated Across Multiple Data Sets,"" Signal Processing, Volume 176, 2020."
14602,Techniques-multiple-data-sets/Joint-Reduced-Rank-mCCA-Technique/,[8],"
"," T. Hasija and T. Marrinan,""A GLRT for estimating the number of correlated components in sample-poor mCCA,"" Submitted."
14605,./data/reader/wikipedia_tools.py,documentation,"). If you are not interested in running CLESA or KCCA, you can simply omit this requirement by setting max_wiki=0 before running the script. If otherwise, you would have to go through the ", which contains some tools and explanations on how to prepare the Wikipedia dump (you might require external tools).
14605,./data/reader/wikipedia_tools.py,wikipedia_tools.py, to extract a comparable set of documents for all of the 11 languages involved in our experiments. Technical details and ad-hoc tools might be found in , (in this repo). The toolkit allows:
14605,./dataset_builder.py,dataset_builder.py,The dataset splits are built once for all using the ," script and then pickled for fast subsequent runs. JRC-Acquis is automatically donwloaded the first time. RCV1/RCV2, despite being public, cannot be downloaded without a formal permission. Please, refer to "
14605,http://trec.nist.gov/data/reuters/reuters.html,RCV2's site, and , before proceeding.
14608,https://medium.com/health-data-science/classification-of-histopathology-images-with-deep-learning-a-practical-guide-2e3ffd6d59c5,Classification of Histopathology Images with Deep Learning: A Practical Guide,". For a practical guide and implementation tips, see the Medium post ",.
14608,https://pandas.pydata.org/,pandas,"
","
"
14639,/bea_code/create_data_wiki.ipynb,create_data_wiki.ipynb,' folder. First run ',"' to read in raw data (tar files, generated by the previous step) and output csv files. Then run '"
14639,/bea_code/create_data_conll.ipynb,create_data_conll.ipynb,' folder. First run ',"' to read in raw data (tar files, generated by the previous step) and output csv files. Then run '"
14643,https://github.com/aravind0706/flowpp/blob/master/flows_imagenet/create_imagenet_benchmark_datasets.py,here,Script to create dataset ,"
"
14650,http://mlg.ucd.ie/datasets/bbc.html,BBC Datasets Descrition,"
","
"
14650,http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip,Dataset,"
","
"
14654,#datasets,Datasets,"
","
"
14654,https://snap.stanford.edu/data/index.html#communities,Stanford SNAP network format," comments and selflinks, without backward direction specification. The same as ", Also known as 
14654,https://snap.stanford.edu/data/index.html#communities,Standford SNAP,Networks from , (unweinghted 
14655,http://dask.pydata.org,Dask, might be a good choice. For the comprehensive parallel computing , is a good choice. For the parallel execution of only the shell scripts the 
14656,http://people.sc.fsu.edu/~jburkardt/data/metis_graph/metis_graph.html,Metis graph,"The undirected unweighted input network to be clustered can be specified in the NSL (nsa/nse), ", or 
14659,https://github.com/dongfang-steven-yang/vci-dataset-citr,here,"A sister dataset of pedestrian trajectories, CITR dataset, which comes from controlled experiments of fundamental vehicle-crowd interaction, can be accessed at ",.
14659,https://github.com/dongfang-steven-yang/vci-dataset-dut/raw/master/demo-dut.mp4,here,Or you can download it ,"
"
14660,https://github.com/dongfang-steven-yang/vci-dataset-dut,here,"A sister dataset of pedestrian trajectories, DUT dataset, which consists of everyday scenarios in university campus, can be accessed at ",.
14660,https://github.com/dongfang-steven-yang/vci-dataset-citr/raw/master/demo-citr.mp4,here,Or you can download it ,"
"
14675,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/lexical_resources/wikipedia_wikidata_relations/,here,Download the data set from the paper ,. See the data set ReadMe for more information on the format and see the 
14675,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/lexical_resources/wikipedia_wikidata_relations/,data,Download the ,", if you want to replicate the experiments from the paper. Extract the archive inside "
14690,https://paperdatasets.s3.amazonaws.com/tcga.db,tcga.db,You can download the raw data using these links: , and 
14690,https://paperdatasets.s3.amazonaws.com/news.db,news.db, and ,.
14698,http://www.gti.ssr.upm.es/data/Vehicle_database.html,GTI vehicle image database," images, around 8000 images in each category.  These datasets are comprised of images taken from the ", and 
14698,http://www.cvlibs.net/datasets/kitti/,KITTI vision benchmark suite, and ,. Here is an example of one of each of the 
14702,http://cemantix.org/data/ontonotes.html,CoNLL-2012,You have to follow the instructions below to get CoNLL-2012 data ,", this would result in a directory called "
14705,https://github.com/fjxmlzn/RNN-SM#steganalysis-speech-dataset,dataset,[,]
14712,https://github.com/christos-c/bible-corpus-tools,a collection of tools for reading/processing the corpus,Follow this link for ,.
14716,https://www.msoos.org/2019/06/crystalball-sat-solving-data-gathering-and-machine-learning/,associated blog post,Build and use instructions below. Please see the , for more information.
14730,https://github.com/bfetahu/wiki_tables/tree/master/data/,table data,We have uploaded all the datasets for the TableNet evaluation as well as the extracted tables at ,. The TableNet code for alignment of tables can be found 
14765,https://www.comet.ml/site/data-scientists/,Comet," - An example of how to log metrics, hyperparameters and more from Catalyst runs to ","
"
14774,http://pandas.pydata.org/,Pandas,"
","
"
14785,vulnerability-data,vulnerability knowledge-base, consists of vulnerability data ," as well as set of tools to support the mining, curation and management of such data."
14785,https://github.com/SAP/project-kb/tree/vulnerability-data,vulnerability-data branch,"The vulnerability data of Project KB are stored in textual form as a set of YAML files, in the ",.
14795,data,data/,"
", contains the raw data from our experiments.
14795,/data,data," contains the plots and tables, based on the contents of ",", that are shown in our paper."
14796,http://jaina.cs.ucdavis.edu/datasets/adv/relu_verification/models_crown.tar,here,We provide our pre-trained MNIST and CIFAR models that are used in the paper ,.
14801,https://github.com/William-N-Havard/VGS-dataset-metadata/blob/master/synth-coco-metadata.json,synth-coco-metadata.json,"
","
"
14809,https://github.com/uclmr/jack/tree/master/data/sentihood,jack,Data was obtained from ,.
14826,data/model_50.vector,model_50.vector,Edit , and 
14826,data/sentence_sorted_by_date.csv,sentence_sorted_by_date.csv, and , according to your own EHR data.
14826,data/sentence_sorted_by_date.csv,sentence_sorted_by_date.csv,Embedding file: The medical events appeared in , and their embeddings.
14833,https://www.kaggle.com/crawford/cat-dataset,Cat-Dataset,: Download and preprocesses the ,"
"
14833,https://neerajkumar.org/databases/lfpw/,LFPW Dataset,: Preprocesses an already downloaded ZIP file of the , (Download is recommended from 
14833,https://github.com/justusschock/shapedata,shapedata, (must be installed separately). The data-handling is outsourced to ,.
14842,https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data,here,Download the fer2013.tar.gz file from ,"
"
14842,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,here,Download the imdb_crop.tar file from , (It's the 7GB button with the tittle Download faces only).
14846,https://elki-project.github.io/datatypes,DataTypes,", ",", "
14846,https://elki-project.github.io/datasets/,DataSets,", ",", "
14850,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,TUM-Mono: ,"
"
14850,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,Kitti odometry,Kitti: ,"
"
14850,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,Euroc MAV dataset,EuRoC: ,"
"
14851,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI dataset, cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the ," as stereo or monocular, in the "
14851,http://vision.in.tum.de/data/datasets/rgbd-dataset,TUM dataset," as stereo or monocular, in the "," as RGB-D or monocular, and in the "
14851,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC dataset," as RGB-D or monocular, and in the "," as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. "
14851,http://vision.in.tum.de/data/datasets/rgbd-dataset/tools,associate.py,Associate RGB images and depth images using the python script ,. We already provide associations for some of the sequences in 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/bmp_metadata.html,Native,  | MS Windows and IBM OS/2 Device Independent Bitmap       | ✔  |  ✔  | ,", "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,", ", | |        | CUR      | MS Windows Cursor Format                                | ✔  |  -  | - | |        | ICO      | MS Windows Icon Format                                  | ✔  |  ✔  | - | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | HDR      | Radiance High Dynamic Range RGBE Format                 | ✔  |  -  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | IFF      | Commodore Amiga/Electronic Arts Interchange File Format | ✔  |  ✔  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/jpeg_metadata.html#image,Native, | Joint Photographers Expert Group                        | ✔  |  ✔  | ,", "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,", ", | |   | JPEG Lossless |                                                         | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/jpeg_metadata.html#image,Native, | |   | JPEG Lossless |                                                         | ✔  |  -  | ,", "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,", ", | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | PCX      | ZSoft Paintbrush Format                                 | ✔  |  -  | , | |        | DCX      | Multi-page PCX fax document                             | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | DCX      | Multi-page PCX fax document                             | ✔  |  -  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,   | PICT     | Apple QuickTime Picture Format                          | ✔  |  ✔  | , | |        | PNTG     | Apple MacPaint Picture Format                           | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | PNTG     | Apple MacPaint Picture Format                           | ✔  |  -  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | PAM      | NetPBM Portable Any Map                                 | ✔  |  ✔  | , | |        | PBM      | NetPBM Portable Bit Map                                 | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | PBM      | NetPBM Portable Bit Map                                 | ✔  |  -  | , | |        | PGM      | NetPBM Portable Grey Map                                | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | PGM      | NetPBM Portable Grey Map                                | ✔  |  -  | , | |        | PPM      | NetPBM Portable Pix Map                                 | ✔  |  ✔  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | PPM      | NetPBM Portable Pix Map                                 | ✔  |  ✔  | , | |        | PFM      | Portable Float Map                                      | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | |        | PFM      | Portable Float Map                                      | ✔  |  -  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,"  | Adobe Photoshop Document                                | ✔  | (✔) | Native, "," | |        |  PSB     | Adobe Photoshop Large Document                          | ✔  |  -  | Native, "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard," | |        |  PSB     | Adobe Photoshop Large Document                          | ✔  |  -  | Native, ", | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | SGI      | Silicon Graphics Image Format                           | ✔  |  -  | , | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,    | TGA      | Truevision TGA Image Format                             | ✔  |  ✔  | , | |ThumbsDB| Thumbs.db| MS Windows Thumbs DB                                    | ✔  |  -  | - | OLE2 Compound Document based format only | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/tiff_metadata.html#ImageMetadata,Native, | Aldus/Adobe Tagged Image File Format                    | ✔  |  ✔  | ,", "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,", ", | |        | BigTIFF  |                                                         | ✔  |  ✔  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/tiff_metadata.html#ImageMetadata,Native, | |        | BigTIFF  |                                                         | ✔  |  ✔  | ,", "
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard,", ", | | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | Google WebP Format                                      | ✔  |  -  | , | | XWD    | XWD      | X11 Window Dump Format                                  | ✔  |  -  | 
14866,https://docs.oracle.com/en/java/javase/11/docs/api/java.desktop/javax/imageio/metadata/doc-files/standard_metadata.html,Standard, | | XWD    | XWD      | X11 Window Dump Format                                  | ✔  |  -  | , |
14866,https://search.maven.org/remotecontent?filepath=com/twelvemonkeys/imageio/imageio-metadata/3.9.4/imageio-metadata-3.9.4.jar,imageio-metadata-3.9.4.jar,"
","
"
14866,https://search.maven.org/remotecontent?filepath=com/twelvemonkeys/imageio/imageio-metadata/3.0.2/imageio-metadata-3.0.2.jar,imageio-metadata-3.0.2.jar,"
","
"
14871,https://cloud.google.com/dataflow/,Google Cloud Dataflow," is required; it's the way that efficient distributed computation is supported. By default, Apache Beam runs in local mode but can also run in distributed mode using ", and other Apache Beam 
14879,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/sc09.tar.gz,download,To reproduce our paper results (9.18 +- 0.04) for the SC09 (,") training dataset, run"
14899,http://www.cs.sfu.ca/~colour/data/shi_gehler/,Shi's Re-processing of Gehler's Raw Dataset,"
","
"
14899,http://www.cs.sfu.ca/~colour/data/shi_gehler/,"
Shi's Re-processing of Gehler's Raw Dataset:","
","
"
14907,https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder,"
ImageFolder
","To load the dataset, you can use the ", class in 
14911,http://olafhartig.de/brTPF-ODBASE2016/#data,BrTPF experimental study,". The dataset contains 10^7 triples, and the queries used are conjunctive SPARQL queries with STAR, PATH and SNOWFLAKE shapes. They are taken from the ",". These queries vary in complexity, with very high and very low selectivity. All queries are available in the "
14912,https://www.wikidata.org/wiki/Q12136,Q12136, (German). The data sets are filtered by instances of Wikidata classes , (disease) and 
14912,https://www.wikidata.org/wiki/Q515,Q515, (disease) and ," (city). Datasets are randomized and split into 70% training, 10% validation and 20% test documents."
14922,https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets,DENSE dataset webpage,Download the Gated2Depth dataset and the models from the ,". Please download the zip files of the synthetic and real dataset, and the models into separate folders."
14926,https://data.mendeley.com/datasets/xwzzkxtf9s/1,here,The UW-IOM dataset can be found ,.
14926,https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data,here,The TUM Kitchen dataset can be found ,". We relabeled the dataset, and labels can be dounloaded in this repository under the folder ""Labels_TUM""."
14933,https://smoosavi.org/datasets/lstw,Large-Scale Traffic and Weather Events Dataset,": The first step is to extract traffic and weather events/entities from the raw traffic and weather data, and create a dataset such as ",. In 
14933,https://smoosavi.org/datasets/lstw,here,A large-scale dataset of traffic and weather event data is used as input. Check , for the latest version of such a dataset. The data comes in the form of a single CSV file. We use the same data as input to both short and long-term pattern discovery processes.
14941,https://www.mn.uio.no/math/english/people/aca/vegarant/data/storage2.zip,https://www.mn.uio.no/math/english/people/aca/vegarant/data/storage2.zip,In order to make this code run you will have to download and install the neural networks we have considered. Most of the necessary data can be downloaded from , (4.9 GB). Please note that you will have to modify all paths in the source files so that they point to the data. You will also need to add the directory 
14947,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,KITTI depth completion benchmark, entry on the , at the time of submission of the paper.
14947,www.cvlibs.net/datasets/kitti/,Kitti dataset,The ," has been used. First download the dataset of the depth completion. Secondly, you'll need to unzip and download the camera images from kitti. I used the file "
14951,multigrain/datasets/retrieval.py,datasets/retrieval.py," is in progress, but one may already use the dataloaders implemented in ", for this purpose.
14951,multigrain/datasets/finetune_p.py,finetune_p.py,In appendix E. we report fine-tuning results on several pretrained networks. This experience can be reproduced using the ," script. For example, in the case of "
14958,https://visualgenome.org/static/data/dataset/image_data.json.zip,image meta data,We use a part of Visual Genome dataset for data augmentation. The , and the 
14958,https://visualgenome.org/static/data/dataset/question_answers.json.zip,question answers, and the , of Version 1.2 are needed to be placed in 
14958,http://images.cocodataset.org/annotations/annotations_trainval2017.zip,here,We use MS COCO captions to extract semantically connected words for the extended word embeddings along with the questions of VQA 2.0 and Visual Genome. You can download in ,". Since the contribution of these captions is minor, you can skip the processing of MS COCO captions by removing "
14958,https://github.com/jnhwkim/ban-vqa/blob/master/dataset.py#L393,line, option in this ,.
14976,http://www.micc.unifi.it/vim/3dfaces-dataset/index.html#!prettyPhoto,here,"First, apply for license and download the dataset ","
"
15001,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,their website,7-Scenes: Download from ,.
15001,https://robotcar-dataset.robots.ox.ac.uk/,here,RobotCar: Register & Download from ,. Remark: we adopt the 
15001,https://github.com/ori-mrg/robotcar-dataset-sdk,preprocessing toolkit from the RobotCar authors,. Remark: we adopt the , and generate data first in 
15007,https://data.kitware.com/api/v1/item/5fdaf1dd2fa25629b99843f8/download,User's Quick-Start Guide,The ,", "
15007,https://data.kitware.com/api/v1/item/63e3f5ff046e924d0df1989a/download,"VIAME v0.20.0 Windows, GPU Enabled, Mirror2 (.zip)","
","
"
15007,https://data.kitware.com/api/v1/item/63e5b6777b0dfcc98f6670d7/download,"VIAME v0.20.0 Windows, CPU Only, Mirror2 (.zip)","
","
"
15007,https://data.kitware.com/api/v1/item/63e47330046e924d0df1989d/download,"VIAME v0.20.0 Linux, GPU Enabled, Mirror2 (.tar.gz)","
","
"
15007,https://data.kitware.com/api/v1/item/63e5b5ef7b0dfcc98f6670d3/download,"VIAME v0.20.0 Linux, CPU Only, Mirror2 (.tar.gz)","
","
"
15007,https://data.kitware.com/api/v1/item/602296172fa25629b95482f6/download,"SEAL Windows 7/8/10, GPU Enabled (.zip)","
","
"
15007,https://data.kitware.com/api/v1/item/602295642fa25629b9548196/download,"SEAL Windows 7/8/10, CPU Only (.zip)","
","
"
15007,https://data.kitware.com/api/v1/item/6023362a2fa25629b957c365/download,"SEAL CentOS 7, GPU Enabled (.tar.gz)","
","
"
15007,https://data.kitware.com/api/v1/item/6023359c2fa25629b957c2f3/download,"SEAL Generic Linux, GPU Enabled (.tar.gz)","
","
"
15007,https://data.kitware.com/api/v1/item/5e30b8ffaf2e2eed3545bff6/download,"Arctic Seals Models, Windows","
","
"
15007,https://data.kitware.com/api/v1/item/5e30b283af2e2eed3545a888/download,"Arctic Seals Models, Linux","
","
"
15024,https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f,An Intuitive Explanation of Beam Search,"
","
"
15026,#preprocess-md-data,Preprocess MD data,"
","
"
15026,#data,Data,"
","
"
15026,http://conda.pydata.org/,conda,. After installing ,", run the following command to create a new "
15026,#preprocess-md-data,preprocess MD data,"Before training the model, you should split your MD trajectory into three parts for training, validation, and testing. Then, following the steps in ",", you can construct graphs for these three trajectories and obtain "
15026,data/,instructions,"To reproduce our results or use the data from our paper, see these ",.
15030,https://www.tensorflow.org/datasets/catalog/ag_news_subset,tensorflow-datasets,"
", is used to support AG's News dataset.
15049,https://www.jstor.org/stable/25051187?seq=1#metadata_info_tab_contents,"Li and Zhang, 1998", and ,.
15049,https://github.com/tkipf/gcn/tree/master/gcn/data,the public data splits,", which corresponds to ",". Due to space limit, please download reddit dataset from "
15054,examples/tutorial#convert-to-dataset,examples/tutorial, See ,.
15063,http://ais.informatik.uni-freiburg.de/projects/datasets/fr360/,Freiburg Campus 360 dataset, from the ,", each point has (x,y,z) "
15074,https://github.com/yuhaozhang/tacred-relation/tree/master/dataset/tacred,here, in a similar format like ,.
15075,https://github.com/yuhaozhang/tacred-relation/tree/master/dataset/tacred,here, in a similar format like ,.
15081,https://github.com/cbaziotis/datastories-semeval2017-task4/blob/master/models/neural/keras_models.py,models/neural/keras_models.py,If what you are just interested in the source code for the model then just see ,.
15084,https://etsin.avointiede.fi/dataset/urn-nbn-fi-csc-kata20170601153214969115,FI-2010 dataset,. The dataset was based on the ,.
15086,https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f,Training a text classifier with Flair,"
","
"
15086,https://towardsdatascience.com/zero-and-few-shot-learning-c08e145dc4ed,Zero and few-shot learning,"
","
"
15086,https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3,Benchmarking NER algorithms,"
","
"
15086,https://towardsdatascience.com/clinical-natural-language-processing-5c7b3d17e137,Clinical NLP,"
","
"
15086,https://towardsdatascience.com/docker-image-for-nlp-5402c9a9069e,A docker image for Flair,"
","
"
15090,https://zdzhaoyong.github.io/GSLAM/dataset.html,Dataset Plugin for GSLAM,"
","
"
15090,http://www.cvlibs.net/datasets/kitti/,KITTI,| Name    |    Channels        |   Description    | | ------- |:------------------:|:-------------:| | ,"   | Stereo,Pose        |               | | "
15090,https://vision.in.tum.de/data/datasets/mono-dataset,TUMMono,"   | Stereo,Pose        |               | | ", | Monocular          | | | 
15090,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUMRGBD, | Monocular          | | | ," | RGBD,Pose          || | "
15090,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoc," | RGBD,Pose          || | ","   | IMU,Stereo         || | "
15090,http://zhaoyong.adv-ci.com/downloads/npu-dronemap-dataset/,NPUDroneMap,"   | IMU,Stereo         || | ","| GPS,Monocular   || | CVMono | Monocular           | Online camera or video dataset using opencv.|"
15090,https://zdzhaoyong.github.io/GSLAM/dataset.html,implement dataset plugins by own,Users can also ,.
15091,https://github.com/xthan/polyvore-dataset,here,The original Polyvore dataset we used in our paper is first proposed ,". After downloaded the datasets, you can put them in the folder "
15108,#data-structure,Data Structure,"
","
"
15108,#working-with-the-data-structure,Working with the data structure,"
","
"
15110,https://github.com/mloskot/json_benchmark/blob/master/data/canada.json,source,"
", | 2199KB | Contour of Canada border in 
15110,https://github.com/RichardHightower/json-parsers-benchmark/blob/master/data/citm_catalog.json,source,"
", | 1737KB | A big benchmark file with indentation used in several Java JSON parser benchmarks. 
15115,#dataset,Dataset,"
","
"
15151,https://github.com/mcoavoux/multilingual_disco_data,https://github.com/mcoavoux/multilingual_disco_data, to reparse the development set with each model. The scripts to preprocess and generate the data in the parser input format are in the following repository: ,.
15170,http://pandas.pydata.org/,Pandas,"
","
"
15171,http://www.cvlibs.net/datasets/kitti/,KITTI,Given that you have already downloaded the ," odometry and raw datasets, the provided python script "
15174,https://github.com/tudelft3d/3D_Metadata_ADE/blob/master/UML/CityGML_3DMD_ADE_0_2.eap,Enterprise Architect file,This , contains the UML model.
15174,https://github.com/tudelft3d/3D_Metadata_ADE/blob/master/XSD/3DMD_ADE.xsd,XML Schema file,This is the derived , for 3DMD ADE.
15174,https://github.com/tudelft3d/3D_Metadata_ADE/tree/master/Documentation/BrowsableSchema,here,Browsable schema for the ADE can be accessed ,.
15174,https://github.com/tudelft3d/3D_Metadata_ADE/tree/master/XSD/codelists,Codelists,Codelists can be accessed here ,.
15184,data,data,", ",", and "
15184,doc/data.md,Data simulation,"
","
"
15187,./data_utils.py,data_utils.py," are required, please refer to ", for more detail.
15191,./data_template.data,data_template.data,", ",", and "
15191,http://sci2s.ugr.es/keel/datasets.php,KEEL website,. For further information visit ,.
15193,http://nlp.stanford.edu/data/glove.6B.zip,here,Download the GloVe pretrained word vectors from ,", and keep "
15198,https://github.com/UCDenver-ccp/CRAFT/wiki/Primary-references-for-the-CRAFT-corpus,CRAFT Reference,"To cite the CRAFT corpus, please see the ", wiki page.
15203,#data,Data,"
","
"
15204,http://ache.readthedocs.io/en/latest/data-formats.html,data formats documentation,"For more details on how to configure data formats, see the ", page.
15207,https://github.com/BayesWatch/sequential-imagenet-dataloader,BayesWatch's ImageNet Loader,"
", is required for ImageNet training.
15210,http://ir.hit.edu.cn/~dytang/paper/acl2015/dataset.7z,this data,"The notebook contains an example of trained model on IMDB movie review dataset. I could not get the original IMDB dataset that the paper referred to, so I have used ","
"
15223,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI raw dataset,You need to download ," first, then the raw data is processed with the following three steps:"
15223,https://www.cityscapes-dataset.com/downloads/,Cityscapes website,  from ," (registration is needed to download the data), then the data is processed with the following three steps:"
15225,https://datatracker.ietf.org/doc/draft-hdevalence-cfrg-ristretto/,RFC has been proposed,An , to standardise Ristretto over Ed25519.  This RFC is compatible with 
15235,https://perso.liris.cnrs.fr/marc.plantevit/doku/doku.php?id=data_sets,DBLP,"
","
"
15238,https://sunlightfoundation.com/blog/2013/08/20/a-modern-approach-to-open-data/,Eric's blog post, and the Sunlight Foundation in 2013 (see ,") and is currently maintained by GovTrack.us and other contributors. For more information about data in Congress, see the "
15238,https://congressionaldata.org/,Congressional Data Coalition,") and is currently maintained by GovTrack.us and other contributors. For more information about data in Congress, see the ",.
15241,https://k8s-testgrid.appspot.com/sig-big-data#tf-minigo-presubmit,Test Dashboard,"
","
"
15245,https://github.com/allenai/allennlp/blob/master/tutorials/notebooks/data_pipeline.ipynb,documentation,". The first takes a sequence of words and returns their IDs, and the second gets the IDs and returns the vectors. Look at the implementations in this repository and in the AllenNLP repository, and read the ", there.
15248,fetch_data.sh,fetch_data.sh,The , script creates a 
15250,https://gitlab.com/parseme/sharedtask-data/tree/master/1.1,Parseme's gitlab page,") including 5, 2 and 4 sentences in train, dev and test respectively for a trial run. To obtain the data used in the experiments, download train, test, and dev files for each language from ",.
15266,https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center,Blood Transfusion Service Center,"
","
"
15266,https://archive.ics.uci.edu/ml/datasets/teaching+assistant+evaluation,Teaching  Assistant  Evaluation,"
","
"
15266,https://www.kaggle.com/uciml/pima-indians-diabetes-database,Pima Indian Diabetes,"
","
"
15266,https://archive.ics.uci.edu/ml/datasets/energy+efficiency,Energy Efficency,"
","
"
15266,http://archive.ics.uci.edu/ml/datasets/yacht+hydrodynamics,Yacht Hydrodynamics,"
","
"
15268,https://github.com/xinyadu/nqg/tree/master/data/processed,Split-1,"
"," was originally released by Du et al., which we can't directly use as there is no information about answer positions. As a result, we use their provided "
15268,https://github.com/xinyadu/nqg/tree/master/data,doclist-xxx.txt," was originally released by Du et al., which we can't directly use as there is no information about answer positions. As a result, we use their provided ", files to generate our own data (provided along this repository). We mistakenly report their train/dev/test split in our paper.
15268,https://www.cs.rochester.edu/~lsong10/downloads/nqg_data.tgz,here,We release our data ,"
"
15268,./src/NP2P_data_stream.py#L51,data loading code,"Please note that the rich annotation isn't necessary for our system, so you can simply modify the "," to not requiring the ""annotation"" fields."
15270,https://github.com/golsun/SpaceFusion/blob/master/data/reddit.py,Reddit,We provided scripts to generate , and process 
15270,https://github.com/golsun/SpaceFusion/blob/master/data/switchboard.py,Switchboard, and process , datasets as well as a 
15270,https://github.com/golsun/SpaceFusion/blob/master/data/toy,toy dataset, datasets as well as a , in this repo for debugging.
15270,https://github.com/golsun/SpaceFusion/blob/master/data/README.md,here,Please check , for more details.
15287,./data_example.data,data_example.data, and ,. For further information visit 
15287,http://sci2s.ugr.es/keel/datasets.php,KEEL website,. For further information visit ,.
15289,https://huggingface.co/docs/transformers/model_doc/data2vec,Data2Vec,"
","
"
15307,jforests/src/main/resources/sample-ranking-data.zip,here,"For this tutorial, we will use the sample data set which is available ",.
15323,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,"11,485 input-reference pairs (size 320x320) extracted from ",.
15328,http://odds.cs.stonybrook.edu/letter-recognition-dataset/,Letter,"
",", "
15328,http://odds.cs.stonybrook.edu/cardiotocogrpahy-dataset/,cardio,", ",", "
15328,http://odds.cs.stonybrook.edu/optdigits-dataset/,opticaldigts,", ", and 
15328,http://odds.cs.stonybrook.edu/pendigits-dataset/,pen, and , datasets is available in 
15328,https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/OPQMVF/UNGQHH&version=1.0,Satellite,. , dataset can be download in 
15328,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF,Unsupervised Anomaly Detection Benchmark, dataset can be download in ,. These datasets are also included in file folder 
15328,https://github.com/WangXuhongCN/adVAE/tree/master/datasets,"""datasets""",. These datasets are also included in file folder ,.
15334,#dataset-description,Dataset Description,"
","
"
15347,http://archive.ics.uci.edu/ml/datasets/Student+Performance,Student Performance Dataset,Code for preprocessing the , can be found in 
15350,datasets,datasets,The datasets can be found in the folder ,". Like in the original word2vec-toolkit, the files to be evaluated are named "
15350,datasets,datasets,"If you want to extend or modify the test data, edit the respective source files in the folder ",: 
15350,datasets,datasets,Adding new datasets and models: add the raw dataset into ,", generate the "
15355,https://mila.quebec/en/publications/public-datasets/m-vad/,M-VAD,We collect and release a new set of annotations for the Montreal Video Annotation Dataset (,).
15355,https://github.com/aimagelab/mvad-names-dataset/releases/tag/1.0,here,The dataset can be downloaded ,. 
15355,https://mila.quebec/en/publications/public-datasets/m-vad/,M-VAD website,Please note that the original M-VAD video clips are not included in this dataset and can be downloaded from the official ,.
15355,https://github.com/aimagelab/mvad-names-dataset/releases/tag/1.0,Release section,"Official splits are available, along with the M-VAD Names dataset, in the ",.
15355,https://github.com/aimagelab/mvad-names-dataset/releases/tag/1.0,Release section,Refined M-VAD captions can be downloaded in the ,.
15364,datasets,datasets,The datasets can be found in the folder ,". Like in the original word2vec-toolkit, the files to be evaluated are named "
15364,datasets,datasets,"If you want to extend or modify the test data, edit the respective source files in the folder ",: 
15364,datasets/ngram,ngram, in the folder ,.
15364,datasets,datasets,Adding new datasets and models: add the raw dataset into ,", generate the "
15368,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking,Download the dataset from ,.
15368,http://www.cvlibs.net/download.php?file=data_tracking_velodyne.zip,velodyne,You will need to download the data for ,", "
15368,http://www.cvlibs.net/download.php?file=data_tracking_calib.zip,calib,", ", and 
15368,http://www.cvlibs.net/download.php?file=data_tracking_label_2.zip,label_02, and ,.
15369,./examples/random_data.ipynb,GitHub,Random overdetermined ill-conditioned and consistent linear systems (,", "
15369,https://nbviewer.jupyter.org/github/amkatrutsa/preckacz/blob/master/examples/random_data.ipynb,Nbviewer,", ",).
15370,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,here,Download the KITTI 3D object detection dataset from , and organize them as follows.
15370,kitti/prepare_data_refine.py#L888,here,"Run following command to prepare pickle files for car training. We use the first stage predicted results. If you don't use the default directory in the first stage, you should change the corresponding directory in ", and 
15370,kitti/prepare_data_refine.py#L904,here, and , before running following commands. The pickle files will be saved in 
15383,http://datafromsky.com/,here,"Experiments 1 and 2 require a DataFromSky dataset, their website is ",". Experiment 3 generates synthetic data for the experiment, and can be run without any external dataset."
15391,http://robotics.ethz.ch/~asl-datasets/iros_2017_voxblox/data.bag,here,Cow and lady data set can be downloaded ,. A 
15394,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,glove.840B.300d.zip,Download the pretrained Glove word ebeddings ,.
15396,https://github.com/nmhkahn/CARN-pytorch#dataset,repo,"We use the same protocols of CARN, our prior work. Please see the details on this ",.
15398,charades_dataset.py,charades_dataset.py,This relied on having the optical flow and RGB frames extracted and saved as images on dist. , contains our code to load video segments for training.
15398,charades_dataset_full.py,charades_dataset_full.py, contains the code to load a pre-trained I3D model and extract the features and save the features as numpy arrays. The , script loads an entire video to extract per-segment features.
15403,https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz,MPII Human Pose Dataset,"If you would like to pretrain an EpipolarPose model on MPII data, please download image files from ", (12.9 GB). Extract it under 
15404,https://data.pyg.org/whl,here,"We alternatively provide pip wheels for all major OS/PyTorch/CUDA combinations, see ",.
15404,https://data.pyg.org/whl,here, in order to prevent a manual installation from source. You can look up the latest supported version number ,.
15412,http://nlp.cs.washington.edu/entity_type/data/ultrafine_acl18.tar.gz,http://nlp.cs.washington.edu/entity_type/data/ultrafine_acl18.tar.gz,download data from ,", unzip if and put it under "
15418,https://www.kaggle.com/google-nlu/text-normalization/data,here,available ,"
"
15418,https://www.kaggle.com/google-nlu/text-normalization/data,this,The scripts make use of ," UTF-8 header-only library. You don't need to download anything from its repository since the files are copied here, but you can go and check his work."
15420,datasets,datasets,The datasets can be found in the folder ,". Like in the original word2vec-toolkit, the files to be evaluated are named "
15420,datasets,datasets,"If you want to extend or modify the test data, edit the respective source files in the folder ",: 
15420,datasets,datasets,Adding new datasets and models: add the raw dataset into ,", generate the "
15435,https://github.com/Braamling/learning-to-rank-webpages-based-on-visual-features/blob/master/dataset.md,here,The dataset can be found ,.
15435,https://github.com/Braamling/contextual-search-features-for-large-datasets-in-spark,contextual-search-features-for-large-datasets-in-spark,"
", contains all the neccary Scala Spark code to generate the contextual features in VITOR from the raw ClueWeb12 dataset. The resulting data will be in the same format as the LETOR dataset.
15445,neurite/py/dataproc.py,dataproc,"
",: a set of tools for processing medical imaging data for preparation for training/testing
15446,https://visualdialog.org/datahttp://cvlab.postech.ac.kr/research/attmem/,VisDial v0.9,This repository contains experiments on two datasets: , and 
15446,https://s3.amazonaws.com/visual-dialog/data/v0.9/visdial_params.json,here,). The vocabulary for the VisDial v0.9 dataset is ,.
15446,https://github.com/ronghanghu/n2nmn#download-and-preprocess-the-data-1,here,"To extract image features, please follow instructions ",.
15471,http://jmcauley.ucsd.edu/data/amazon/,here," Besides, the original Amazon datasets (including user-item interaction history and item associations) are provided by Professor Mcauley. You can download them ",.
15478,https://bdd-data.berkeley.edu/,here,2.1 Download the Berkeley Deep Drive (BDD) Object Detection Dataset ,. 
15478,http://www.cvlibs.net/datasets/kitti/eval_object.php,here,2.2 Download the KITTI Object Detection Dataset ,.
15487,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow Datasets,Download ,", "
15487,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo,KITTI 2012,", ",", "
15487,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI 2015,", ","
"
15493,data/,data,We use the same datasets as in [1][2][3]. They are stored in the folder ,. Please 
15516,https://github.com/binhvq/news-corpus,news data," (dim=300, 6GB, trained on 20GB of ", and Wiki-data of ETNLP. | | Elmo                         | 
15519,https://www.cityscapes-dataset.com/,official website,Cityscapes dataset can be downloaded from the , (registration required).
15519,https://www.crcv.ucf.edu/research/data-sets/human-actions/ucf101/,official website,UCF-101 dataset can be downloader from the ,"
"
15535,http://mlg.ucd.ie/datasets/bbc.html,BBC articles," [27], "," [28], "
15538,https://github.com/bodacea/countryname/blob/master/countryname/databases/ISO3166ErrorDictionary.csv,ISO3166ErrorDictionary,"
", for common country mispellings 
15543,#generating-metadata,Generating metadata,"
","
"
15543,#accessing-oracle-database,here,The RMLMapper is built using Maven. As it is also tested against Oracle (check ," for details), it needs a specific set-up to run all tests. That's why we recommend to build without testing: "
15548,http://FoelliX.de/downloads/benchmarks/data_droidbench30.zip,DroidBench 3.0,Here you find an updated version of the , benchmark (
15548,http://FoelliX.de/downloads/benchmarks/data_taintbench10.zip,TaintBench,) and the new , benchmark (
15548,http://FoelliX.de/downloads/benchmarks/data_droidbench30.zip,DroidBench 3.0,"
","
"
15548,http://FoelliX.de/downloads/benchmarks/data_taintbench10.zip,TaintBench,"
","
"
15557,data/AerialImageDataset,Inria Aerial Image Dataset,"
","
"
15557,data/bradbury_buildings_roads_height_dataset,"Aerial imagery object identification dataset for building and road detection, and building height estimation","
","
"
15557,data/mapping_challenge_dataset,Mapping Challenge from CrowdAI Dataset,"
","
"
15557,projects/mapalign/dataset_utils,dataset_utils,All scripts for dataset handling relative to the alignment project are located in the , folder. See the README in that folder for instructions on dataset pre-processing.
15557,data/AerialImageDataset/read.py,read.py, script based on this one: ,. Then write a 
15557,projects/mapalign/dataset_utils/preprocess_aerial_image_multires.py,preprocess_aerial_image_multires.py, script based on this one: ,. The 
15559,code/code_annotation/dataset/train_qt_new_cleaned/,here, file ,.
15559,code/CodeRetrieval-Main/data/,here,All data can be found ,". The data can be accessed using ""pickle"". This folder basically contains:"
15560,http://asrl.utias.utoronto.ca/datasets/mrclam/,Website,"
","
"
15560,http://asrl.utias.utoronto.ca/datasets/mrclam/,http://asrl.utias.utoronto.ca/datasets/mrclam/,You can dowload data at , and then extract data in the folder 
15579,http://www.caida.org/data/passive/passive_2016_dataset.xml,http://www.caida.org/data/passive/passive_2016_dataset.xml,CAIDA-2016 - ,"
"
15579,http://www.caida.org/data/passive/ddos-20070804_dataset.xml,http://www.caida.org/data/passive/ddos-20070804_dataset.xml,CAIDA-DDoS - ,"
"
15598,/data/construct_dataset_Market.m,/data/construct_dataset_Market.m,Then run ," in MATLAB. If you prefer to use another dataset, just modify the MATLAB code accordingly. The processed Market-1501 and DukeMTMC-reID are available in "
15598,/data/construct_dataset_MSMT17.m,/data/construct_dataset_MSMT17.m,Then run ," in MATLAB. If you prefer to use another dataset, just modify the MATLAB code accordingly. Again, the processed MSMT17 is available in "
15600,https://github.com/tensorflow/datasets,TensorFlow Datasets,: Observations is in the process of being replaced by ,". Unlike Observations, TensorFlow Datasets is more performant, provides pipelining for >2GB data sets and all of "
15600,https://github.com/tensorflow/tensor2tensor/tree/06862886125f5b5d262e2b0e6ffeba059dc0d57d/tensor2tensor/data_generators,Tensor2Tensor's,". Unlike Observations, TensorFlow Datasets is more performant, provides pipelining for >2GB data sets and all of ",", and better interfaces with "
15604,./data/acl_2017,./data/acl_2017,"We structured the dataset into sections each corresponding to a venue or an arxiv category, e.g., ", and 
15604,./data/arxiv.cs.cl_2007-2017,./data/arxiv.cs.cl_2007-2017, and ,". Each section is further split into the train/dev/test splits (same splits used in the paper). Due to licensing constraints, we provide instructions for downloading the data for some sections instead of including it in this repository, e.g., "
15604,./data/nips_2013-2017/README.md,./data/nips_2013-2017/README.md,". Each section is further split into the train/dev/test splits (same splits used in the paper). Due to licensing constraints, we provide instructions for downloading the data for some sections instead of including it in this repository, e.g., ",.
15609,./data/generate_splits.py,generate_splits.py,Run , to convert the .mat dataset file to a dataframe and generate the 4 splits.
15609,./data/,data,). Place 'videos_160' in the , directory. If you wish to use a different input configuration you must download the YouTube videos (URLs provided in dataset) and preprocess the videos yourself. I have provided 
15609,./data/preprocess_videos.py,preprocess_videos.py, directory. If you wish to use a different input configuration you must download the YouTube videos (URLs provided in dataset) and preprocess the videos yourself. I have provided , to help with that.
15614,https://pandas.pydata.org/,pandas,"
","
"
15622,https://github.com/fidler-lab/curve-gcn/tree/dataloader,dataloader, here. We also provide the ,. We will be using GitHub to keep track of issues with the code and to update on availability of newer versions (also available on website and through e-mail to signed up users).
15622,https://www.cityscapes-dataset.com/downloads/,website,Download the Cityscapes dataset (leftImg8bit_trainvaltest.zip) from the official , [11 GB]
15631,https://www.cs.ucr.edu/~eamonn/time_series_data/UCR_TS_Archive_2015.zip,UCR archive,The data used in this project comes from the ,", which contains the 85 univariate time series datasets."
15631,https://pandas.pydata.org/,pandas,"
","
"
15633,http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html,CompCars,"For setting up the datasets, please download ", and 
15641,http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip,here,. The dataset can be downloaded ,", which is from "
15652,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz,images,Oxford-102 flowers: , and 
15652,http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,images,Caltech-200 birds: , and 
15683,http://mridata.org,http://mridata.org,Fully sampled datasets can be downloaded from , using the python script and text file.
15683,http://mridata.org,http://mridata.org,: Contains files with ISMRMRD files directly from ,"
"
15683,http://mridata.org/,An Open Archive for Sharing MRI Raw Data.,"Ong F, Amin S, Vasanawala SS, Lustig M. "," In: ISMRM & ESMRMB Joint Annual Meeting. Paris, France; 2018. p. 3425."
15698,https://www.mongodb.com/docs/manual/core/data-modeling-introduction/,Read about Schemas,"
","
"
15714,https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=0,RESIDE dataset,"
", (
15716,https://github.com/NVlabs/Deep_Object_Pose/tree/master/scripts/nvisii_data_gen#handling-objects-with-symmetries,symmetrical objects,2022/03/30 - Update on the NViSII script to handle ,.  Also the NViSII script is compatible with the original training script. Thanks to Martin Günther.
15716,https://github.com/NVlabs/Deep_Object_Pose/tree/master/scripts/nvisii_data_gen,readme,2021/12/13 - Added a NViSII script to generate synthetic data for training DOPE. See this , for more details. We also added the update training and inference (without ROS) scripts for the NViSII paper 
15716,https://github.com/swtyree/hope-dataset/,here,.  The HOPE dataset can be found , and is also part of the 
15716,https://bop.felk.cvut.cz/datasets/#HOPE,BOP challenge, and is also part of the ,"
"
15716,https://github.com/swtyree/hope-dataset/,here,The HOPE dataset can be found , and is also part of the 
15716,https://bop.felk.cvut.cz/datasets/#HOPE,BOP challenge, and is also part of the ,.
15726,https://github.com/Cimagroup/Experiments-Representative-datasets/blob/master/notebooks/auxiliary_fun.py,auxiliary_fun.py,"In all of them three sets were considered, the original dataset, the dominating dataset and a random dataset. Besides, the Algorithm based in proximity graphs and dominating sets was implemented and can be found in the ",.
15737,#data-sets,Data Sets, data set in [1] stored using Python pickle module. See , for more details.
15740,#data,Noisy data,"
","
"
15740,https://datashare.is.ed.ac.uk/handle/10283/2791,https://datashare.is.ed.ac.uk/handle/10283/2791,The data used to train and test our system is available publicly on the Edinburgh DataShare website at ,. Information on how the dataset is constructed can be found in 
15745,src/tests/data/angled_plates/camera_matrix.json,example,) format. See this , for the formatting.
15759,http://snap.stanford.edu/data/index.html,SNAP dataset," folder of the project, taken from the ",.
15762,data_split/readme.md,data_split/README,"If you want to create your own probing tasks for other languages or with different settings, see ","
"
15768,https://github.com/uclmr/jack/tree/master/data/sentihood,dataset mirror," has failed, we use the ", listed in 
15768,http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-restaurant-reviews-train-data/479d18c0625011e38685842b2b6a04d72cb57ba6c07743b9879d1a04e72185b8/,SemEval-2014 ABSA Restaurant Reviews - Train Data,Train Data is available in , and Gold Test Data is available in 
15768,http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-test-data-gold-annotations/b98d11cec18211e38229842b2b6a04d77591d40acd7542b7af823a54fb03a155/,SemEval-2014 ABSA Test Data - Gold Annotations, and Gold Test Data is available in ,. See directory: 
15775,https://github.com/IPNUISTlegal/underwater-test-dataset-U45-,U45,"  | FGAN-based model, loss function formulation |  | ", | | 
15776,https://data.vision.ee.ethz.ch/cvl/DIV2K/,[Official Link],Download training set DIV2K , or DF2K 
15793,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5 Dataset,Download the , as the source domain and unzip it to  
15793,https://www.cityscapes-dataset.com,Cityscapes Dataset,Download the , as the target domain and unzip it to  
15793,https://www.cityscapes-dataset.com/benchmarks/#instance-level-results,the testing server,"Note that, to test performance on the testing set, we provide scripts to generate 1024x2048 outputs which are compatible with ",.
15804,https://github.com/artetxem/vecmap/blob/master/get_data.sh,data,download ,"
"
15824,https://www.categoricaldata.net/fql.html,CQL,"
", can be downloaded if the version in the main directory is out of date.
15825,https://github.com/felipelouza/bwsd/blob/master/dataset/input.100.txt,dataset/input.100.txt,To run a test with d=10 strings from , using Alg. 1 
15831,https://huggingface.co/docs/transformers/model_doc/data2vec,Data2Vec,"
","
"
15834,http://www.nature.com/articles/sdata20152,Mainland 2015, | , | | 
15837,http://synthia-dataset.net/,SYNTHIA,"A dataset needs to be prepared to be used for training. In our experiments, we use the "," dataset. However, any other dataset containing correponding RGB, depth and class label images is suitable for training this model. We have provided a simple, albeit inefficient, python script ("
15838,https://www.dropbox.com/s/7mcngr3xhlaj5uc/LAG_database_part_1.rar?dl=0,Dropbox,We have uploaded the first part of our LAG database at , under request. Please contact us for the passport.
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-bm25-b8.tar,Quantized BM25,| Corpora                                                                                                                                   |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,                                           | 1.2 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil-noexp.tar,uniCOIL (noexp), | | ,                                    | 2.7 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil.tar,uniCOIL (d2q-T5), | | ,                                         | 3.4 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil-tilde-expansion.tar,uniCOIL (TILDE), | | ,                          | 3.9 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-deepimpact.tar,DeepImpact, | | ,                                            | 3.6 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-distill-splade-max.tar,SPLADEv2, | | ,                                      | 9.9 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-splade_distil_cocodenser_medium.tar,SPLADE-distill CoCodenser-medium, | | , | 4.9 GB | 
15841,https://rgw.cs.uwaterloo.ca/pyserini/data/msmarco-passage-splade-pp-ed.tar,SPLADE++ CoCondenser-EnsembleDistil, | | ,                         | 4.2 GB | 
15841,https://rgw.cs.uwaterloo.ca/pyserini/data/msmarco-passage-splade-pp-sd.tar,SPLADE++ CoCondenser-SelfDistil, | | ,                             | 4.8 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil-noexp.tar,MS MARCO V1 doc: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,                   |  11 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil.tar,MS MARCO V1 doc: uniCOIL (d2q-T5), | | ,                        |  19 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_noexp_0shot.tar,MS MARCO V2 passage: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,            |  24 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_0shot.tar,MS MARCO V2 passage: uniCOIL (d2q-T5), | | ,                 |  41 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_noexp_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,       |  55 GB | 
15841,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (d2q-T5), | | ,            |  72 GB | 
15841,docs/regressions-beir-v1.0.0-nfcorpus-flat.md,+, | | NFCorpus   | ,       | 
15841,docs/regressions-beir-v1.0.0-nfcorpus-flat-wp.md,+,       | ,     | 
15841,docs/regressions-beir-v1.0.0-nfcorpus-multifield.md,+,     | ,       | 
15841,docs/regressions-beir-v1.0.0-nfcorpus-unicoil-noexp.md,+,       | , | 
15841,docs/regressions-beir-v1.0.0-nfcorpus-splade-distil-cocodenser-medium.md,+, | , | | NQ         | 
15848,https://huggingface.co/datasets/strombergnlp/broad_twitter_corpus,strombergnlp/broad_twitter_corpus,There's a reader for the corpus on Hugging Face Hub called ,"
"
15852,#big-data,Big Data,"
","
"
15852,#databases,Databases,"
","
"
15852,https://github.com/krzjoa/awesome-python-data-science#readme,Data Science,"
", - Data analysis and machine learning.
15852,https://github.com/academic/awesome-datascience#readme,Data Science,"
","
"
15852,https://github.com/siboehm/awesome-learn-datascience#readme,Tutorials,"
","
"
15852,https://github.com/0xnr/awesome-bigdata#readme,Big Data,"
","
"
15852,https://github.com/awesomedata/awesome-public-datasets#readme,Public Datasets,"
","
"
15852,https://github.com/igorbarinov/awesome-data-engineering#readme,Data Engineering,"
","
"
15852,https://github.com/leomaurodesenv/game-datasets#readme,Game Datasets,"
", - Materials and datasets for Artificial Intelligence in games.
15852,https://github.com/taosdata/awesome-tdengine#readme,TDengine,"
"," - An open-source time-series database with high-performance, scalability, and SQL support."
15852,https://github.com/jdorfman/awesome-json-datasets#readme,Datasets,"
","
"
15852,https://github.com/javierluraschi/awesome-dataviz#readme,Data Visualization,"
","
"
15852,https://github.com/cytodata/awesome-cytodata#readme,Cytodata,"
", - Image-based profiling of biological phenotypes for computational biologists.
15855,https://github.com/ekstroem/dataMaid,dataMaid,"
", (CRAN package) - automated checks of data validity.
15855,https://www.linkedin.com/pulse/automated-exploratory-data-analysis-r-xander-horn/,LinkedIn, (GitHub package) - automated EDA with uni- and bivariate plots. An article with an introduction can be found on ,.
15855,https://www.r-bloggers.com/xray-the-r-package-to-have-x-ray-vision-on-your-datasets/,blog post, (CRAN package) - first look at the data - distributions and anomalies. More in the ,.
15855,https://github.com/elastacloud/automatic-data-explorer/,automatic-data-explorer,"
", (GitHub package) - basic EDA and creating Markdown reports from multiple R scripts.
15855,https://github.com/sfu-db/dataprep,DataPrep,"
", (pip library) - data preparation library with an EDA package. 
15855,https://www.slideshare.net/VictorZabalza/automated-data-exploration-building-efficient-analysis-pipelines-with-dask-81328214,Presentation about the library, (pip library) - fast calculation of summary statistics and correlations. ,.
15855,https://medium.com/datadriveninvestor/automated-exploratory-data-analysis-test-driving-ms-excels-ideas-feature-514f34d944e8#9cad-50ae56ae88ac,Testing MS Excel's autoEDA tool,"
","
"
15862,neurocomputing/unsupervised_real_time_anomaly_detection_for_streaming_data,Sources,"
","
"
15863,http://kaggle.com/bryanpark/german-single-speaker-speech-dataset,CSS German,|16:42:45|Hokuspokus |,| |el|Greek|
15863,http://kaggle.com/bryanpark/greek-single-speaker-speech-dataset,CSS Greek,|04:08:14| Rapunzelina|,| |es|Spanish|1. 
15863,http://kaggle.com/bryanpark/spanish-single-speaker-speech-dataset,CSS Spanish,|23:49:49|Tux  |,| |fi|Finnish|1. 
15863,http://kaggle.com/bryanpark/finnish-single-speaker-speech-dataset,CSS Finnish,|10:32:03|Harri Tapani Ylilammi  |,| |fr|French|1. 
15863,http://kaggle.com/bryanpark/french-single-speaker-speech-dataset,CSS French,|19:09:03|Gilles G. Le Blanc |,| |hu|Hungarian|
15863,http://kaggle.com/bryanpark/hungarian-single-speaker-speech-dataset,CSS Hungarian,|10:00:25|   Diana Majlinger|,| |ja|Japanese|
15863,http://kaggle.com/bryanpark/japanese-single-speaker-speech-dataset,CSS Japanese,|14:55:36|ekzemplaro|,| |nl|Dutch|
15863,http://kaggle.com/bryanpark/dutch-single-speaker-speech-dataset,CSS Dutch,|14:06:40|Bart de Leeuw  |,| |ru|Russian|1. 
15863,http://kaggle.com/bryanpark/russian-single-speaker-speech-dataset,CSS Russian,|21:22:10 |Mark Chulsky|,| |zh|Chinese|1. 
15863,http://kaggle.com/bryanpark/chinese-single-speaker-speech-dataset,CSS Chinese,|06:27:04|Jing Li |,|
15864,https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator,Dataset,", ",", "
15864,https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data,Sentiment Analysis on Movie Reviews,", ","
"
15871,http://downloads.dbpedia.org/2015-10/core-i18n/en/persondata_en.tql.bz2,Dbpedia Person Data,"
","
"
15884,https://kaggle.com/bryanpark/korean-single-speaker-speech-dataset,KSS Dataset, 4. ,"
"
15889,https://cl.asahi.com/api_data/jnc-jamul.html,JAMUL/JNC corpus,". In this repository, we provide preprocess scripts  of ", and evaluation scripts for Japanese summarization in ROUGE metric.
15889,https://cl.asahi.com/api_data/jnc-jamul.html,more details,"orpus (JNC) is a collection of 1,829,231 pairs of the three lead sentences of articles and their print headlines published from 2007 to 2016. We use this dataset to train our seq2seq model. You can can get JNC corpus for a fee (", ).
15898,#downloading-the-data,Downloading the data,"
","
"
15898,#preparing-the-data,Preparing the data,"
","
"
15898,https://www.wikidata.org/,Wikidata,We trained a PBG model on the full ," graph, using a "
15898,https://dl.fbaipublicfiles.com/torchbiggraph/wikidata_translation_v1.tsv.gz,here, to represent relations. It can be downloaded ," (36GiB, gzip-compressed). We used the truthy version of data from "
15898,https://dumps.wikimedia.org/wikidatawiki/entities/,here," (36GiB, gzip-compressed). We used the truthy version of data from "," to train our model. The model file is in TSV format as described in the above section. Note that the first line of the file contains the number of entities, the number of relations and the dimension of the embeddings, separated by tabs. The model contains 78 million entities, 4,131 relations and the dimension of the embeddings is 200."
15899,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,StanfordCar,"
",.
15905,https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/,Stanford Dialogue Corpus,"To create this dataset, we used a dataset that Stanford makes publicly available for download at the following link: ",". Our dataset contains MTurk, crowd-sourced rewrites of dialogues in the Stanford dataset to facilitate research in dialogue state tracking using natural language as the interface. For details of the contextual query rewrite dataset creation please refer to this "
15909,https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/depcc.html,depcc,"The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occured at the beginning of s2. They were extracted from the ", web corpus.
15917,./CVPR2019_codes/OriNet_CVUSA/input_data_rgb_ori_m1_1_augument.py,input_data_rgb_ori_m1_1_augument.py,", first download it, and then modify the img_root variable in ", (line 12)
15917,./CVPR2019_codes/OriNet_CVACT/input_data_ACT.py,input_data_ACT.py,"
", is used in 
15917,./CVPR2019_codes/OriNet_CVACT/input_data_ACT_test.py,input_data_ACT_test.py,"To test Geo-localization performances on ACT_test dataset, you need to use ", in the evaluation script 
15921,https://github.com/ansymo/msr2013-bug_dataset,here,"Lamkanfi et al. [MSR'13] contributed a dataset with over 200.000 reported bugs extracted from the Eclipse and Mozilla projects. Besides providing a single snapshot of a bug report, they also include all the incremental modifications as performed during the lifetime of the bug report. The dataset is currently available ",.
15922,#dataset,Dataset,"
","
"
15923,#dataset,Dataset,"
","
"
15931,data/zhang15/dbpedia_csv/classLabelsDBpedia.csv,classLabelsDBpedia.csv,"
",: A summary of classes in DBpedia and linked nodes in ConceptNet.
15931,data/20-newsgroups/clean/classLabels20news.csv,classLabels20news.csv,"
",: A summary of classes in 20news and linked nodes in ConceptNet.
15931,data/zhang15/dbpedia_csv/dbpedia_random_group_0.25.txt,0.25,Random selection of seen/unseen classes in DBpedia with unseen rate , and 
15931,data/zhang15/dbpedia_csv/dbpedia_random_group_0.5.txt,0.5, and ,.
15931,data/20-newsgroups/clean/20news_random_group_0.25.txt,0.25,Random selection of seen/unseen classes in 20news with unseen rate , and 
15931,data/20-newsgroups/clean/20news_random_group_0.5.txt,0.5, and ,.
15932,http://horatio.cs.nyu.edu/mit/tiny/data/tiny_images.bin,images (227GB),Download [,] [
15932,http://horatio.cs.nyu.edu/mit/tiny/data/tiny_metadata.bin,metadata (57GB),] [,] [
15932,http://horatio.cs.nyu.edu/mit/tiny/data/tiny_index.mat,words,] [,] [
15932,https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py,here, is adapted from [,]
15937,https://competitions.codalab.org/competitions/19790#learn_the_details-data-set-format,CodaLab,You can find the leaderboard from ,.
15954,https://pandas.pydata.org/,pandas,", ",", "
15975,https://sir2data.github.io/,SIR^2 dataset,"Three sub-datasets, namely ‘Objects’, ‘Postcard’, ‘Wild’ from ","
"
15977,https://github.com/LPorcaro/musicner/tree/master/data,README,". Once received, go to the data ", page for more info.
15978,https://box.vicos.si/skokec/villard/detectron-model-yaml-DFG-dataset.zip,detectron-model-yaml-DFG-dataset.zip,"
","
"
15978,https://box.vicos.si/skokec/villard/detectron-model-yaml-DFG-dataset-augmented.zip,detectron-model-yaml-DFG-dataset-augmented.zip,"
","
"
15985,http://www.cs.utexas.edu/~gdurrett/data/gender.data.tgz,number and gender data,We also require ," produced by Shane Bergsma and Dekang Lin in in ""Bootstrapping Path-Based Pronoun Resolution"". Download this, untar/gzip it, and put it at "
15993,https://github.com/anchen1011/toflow/blob/master/download_dataset.sh,here,"Download the Vimeo90K triplet dataset for video frame interpolation task, also see ", by 
16012,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/patty/,here, from ,.
16016,http://data.allenai.org/arc/,ARC Challenge,If you are also interested in the ,", our Question-to-Choice BiLSTM max-out model obtains an accuracy of 33.9% on the Test set (without extensive hyper-parameter tuning)."
16018,https://github.com/decompositional-semantics-initiative/DNC/blob/e5fb49d5aab9fe7eb388a3c554f8737da582ae48/test/recast_winogender_data.json#L15-L27,two,", from the ML fairness literature). The gender parity score measures the percentage of instances where model predictions are unaffected by swapping pronoun gender (as in these ","
"
16018,https://github.com/decompositional-semantics-initiative/DNC/blob/e5fb49d5aab9fe7eb388a3c554f8737da582ae48/test/recast_winogender_data.json#L41-L53,examples,"
","). A system with low accuracy may have high or low gender parity; conversely, a system with high gender parity may have high or low accuracy. Thus, both metrics are computed to capture this (potential) trade-off."
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,Caffe models, for PyTorch are converted from , authors of [1] provide.
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,authors' site,"To download VGGFace2 dataset, see ",.
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,Caffe models,The followings are PyTorch models converted from , authors of [1] provide.
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,Meta.tar.gz, in ,"
"
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,Meta.tar.gz, in ,"
"
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,Meta.tar.gz, in ,"
"
16024,https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,site,"ZQ. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman, VGGFace2: A dataset for recognising faces across pose and age, 2018. ",", "
16026,http://weegee.vision.ucmerced.edu/datasets/landuse.html,"
UC Merced Data Set
",In our paper we have experimented with two remote sensing benchmark archives - , (UCMD) and 
16026,http://weegee.vision.ucmerced.edu/datasets/landuse.html,UCMD,"First, download ", dataset or the 
16027,https://github.com/tttthomasssss/iwcs2019/tree/master/datasets,datasets,All datasets are in the , folder of this repository.
16027,https://github.com/tttthomasssss/iwcs2019/blob/master/datasets/aux_verb_agreement.txt,dataset,The , is tab separated and contains correct (e.g. 
16027,https://github.com/tttthomasssss/iwcs2019/blob/master/datasets/translation_operation.txt,dataset,The ," is tab separated and contains the auxiliary, the corresponding inflected verb and the infinitive form of a verb. The goal is to learn a translation operation from infinitive forms to inflected forms (or contextualised forms if the tense uses auxiliaries). Evaluation has been done using Mean Reciprocal Rank (MRR) - see our "
16027,https://github.com/tttthomasssss/iwcs2019/blob/master/datasets/TEA.txt,TEA,"
","
"
16027,https://github.com/tttthomasssss/iwcs2019/blob/master/datasets/TEA.txt,TEA,"
","
"
16027,https://github.com/tttthomasssss/iwcs2019/blob/master/datasets/TEA.txt,TEA,"
","
"
16033,data/README.md,README.md,See instructions in data/,.
16041,https://console.cloud.google.com/storage/browser/mathematics-dataset,Pre-generated files,"
","
"
16042,https://github.com/yangkevin2/coronavirus_data,data GitHub repo, and the associated , for information about our recent efforts to use Chemprop to identify drug candidates for treating COVID-19.
16042,#data,Data,"
","
"
16042,#weighted-training-by-target-and-data,Weighted training by target and data,"
","
"
16046,https://github.com/cornellNLP/ASQ/tree/master/data,data,"The dataset has been split into train, test, heldout sets, with 8865, 2500, 10000 test instances each. Each set is saved as an individual json file inside the ", directory. We have further reserved 500 instances for human annotations. Check inside the 
16053,http://nlp.stanford.edu/data/glove.6B.zip,GloVe Vectors,"
","
"
16059,https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/,JIGSAWS: The JHU-ISI Gesture and Skill Assessment Working Set,You will need the , to re-run the experiments of the paper.
16059,https://pandas.pydata.org/,Pandas,"
","
"
16060,https://vision.in.tum.de/data/datasets/rgbd-dataset/download,here,Download TUM RGBD dataset from ,.
16065,http://nlp.stanford.edu/data/glove.6B.zip,GloVe Embeddings,"
","
"
16069,https://github.com/fredhohman/summit-data,"
summit-data
",". For the Summit data, visit: ",.
16069,https://github.com/fredhohman/summit-data,"
summit-data
",Download the data from ,:
16070,data/imagenet,"
data/imagenet.json
","
",: metadata for each class including aggregated activations
16070,data/feature-vis/channel/,"
data/feature-vis/channel/
","
",: channel feature visualizations
16070,data/feature-vis/diversity-0/,"
data/feature-vis/diversity-0/
","
",: diversity feature visualizations (1/4)
16070,data/feature-vis/diversity-1/,"
data/feature-vis/diversity-1/
","
",: diversity feature visualizations (2/4)
16070,data/feature-vis/diversity-2/,"
data/feature-vis/diversity-2/
","
",: diversity feature visualizations (3/4)
16070,data/feature-vis/diversity-3/,"
data/feature-vis/diversity-3/
","
",: diversity feature visualizations (4/4)
16070,data/feature-vis/dataset-p/,"
data/feature-vis/dataset-p/
","
",: positive dataset examples for each channel
16070,data/attribution-graphs/,"
data/ag/
","
",: attribution graphs for each class
16070,https://github.com/fredhohman/summit-data/issues,open an issue,For questions or support , or contact 
16074,poldeepner/data/kpwr-toronto.txt,KPWr: Toronto Dominion Center,Sample file: ,"
"
16084,https://ibm-data-and-ai.ideas.ibm.com/ideas/DB2CON-I-92,this request for an Apple Silicon version of the driver, Please support , to show IBM that you are interested in a native solution.
16084,https://public.dhe.ibm.com/ibmdl/export/pub/software/data/db2/drivers/odbc_cli/,Db2 LUW ODBC and CLI Driver,The ODBC and CLI driver is available for download at ,. Refer to (
16084,https://public.dhe.ibm.com/ibmdl/export/pub/software/data/db2/drivers/odbc_cli/,CLIDRIVER,"If you intend to install the clidriver manually, Following are the details of the client driver versions that you can download from ", to be able to connect to databases on non-LUW servers. You would need the client side license file as per Version for corresponding installation.:
16085,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,"
","
"
16085,https://github.com/sshan-zhao/GASDA/tree/master/datasets,datasets,Prepare the two datasets according to the datalists (*.txt in ,)
16086,http://synthia-dataset.net/downloads/,here,: Please first follow the instructions ," to download the images. In this work, we used the "
16086,https://www.cityscapes-dataset.com/,Cityscape,: Please follow the instructions in , to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:
16086,https://www.mapillary.com/dataset,Mapillary,: Please follow the instructions in , to download the images and validation ground-truths. The Mapillary dataset directory should have this basic structure:
16087,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FNMT4HP&version=DRAFT,here,The data sets used in the paper can be downloaded ,.
16088,https://github.com/clovaai/deep-text-recognition-benchmark#download-lmdb-dataset-for-traininig-and-evaluation-from-here,training and evaluation data, | , | 
16088,http://www.robots.ox.ac.uk/~vgg/data/text/,MJSynth (MJ), training datasets : ,[1] and 
16088,http://www.robots.ox.ac.uk/~vgg/data/scenetext/,SynthText (ST),[1] and ,[2]  validation datasets : the union of the training sets 
16088,http://cs-chan.com/downloads_CUTE80_dataset.html,CUTE,"[8], and ",[9].
16088,https://github.com/clovaai/deep-text-recognition-benchmark/blob/c27abe6b4c681e2ee0784ad966602c056a0dd3b5/dataset.py#L148,data filtering part,"Test CRNN[10] model. If you want to evaluate IC15-2077, check ",.
16088,https://github.com/clovaai/deep-text-recognition-benchmark/blob/f2c54ae2a4cc787a0f5859e9fdd0e399812c76a3/dataset.py#L126-L146,data filtering,: skip , when creating LmdbDataset.
16094,./example_data_folder,example_data_folder,"For running the tool the input data is expected to be a nifti volume with size of [256,224,72], if the scans have a different size they will be crop or padd to the correct size. Additionally the scans have to be arrange as follows(or see ",", "
16102,https://github.com/allenai/bilm-tf#how-to-do-fine-tune-a-model-on-additional-unlabeled-data,this,You can further fine-tune BioELMo on other corpora using the Tensorflow checkpoint. See , for details.
16103,http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/NERdata.zip,"
Named Entity Recognition
","
","
"
16103,http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/REdata.zip,"
Relation Extraction
","
","
"
16106,http://nlp.stanford.edu/data/muj/broader-metaphor-features.tar.gz,here, scripts. You can also download them directly ,.
16108,https://github.com/facebookresearch/MUSE#get-evaluation-datasets,here,You can get evaluation datasets from ,.
16112,./toxic_fool/data,./Data,4. , - 
16113,https://20bn.com/datasets/jester#download,here,Download videos ,.
16113,http://crcv.ucf.edu/data/UCF101.php,here,Download videos and train/test splits ,.
16116,https://sites.google.com/site/iwsltevaluation2016/data-provided,IWSLT 2016,Data used in the paper can be downloaded from the , (for en-fr and cs-en) and from 
16118,http://www.cs.cmu.edu/~fmri/science2008/data.html,Words Data,The , by Mitchell et al. (2008)
16118,https://sites.lsa.umich.edu/cnllab/2016/06/11/data-sharing-fmri-timecourses-story-listening/,Alice Data,The , by Brennan et al. (2016)
16119,https://datalab.snu.ac.kr/~jkjang,Jung-Gi Jang,MoonJeong Park (Pohang University of Science and Technology) , (Seoul National University) 
16122,http://www.cs.cmu.edu/~glai1/data/race/,here,RACE: Please submit a data request ,". The data will be automatically sent to you. Create a ""data"" directory alongside ""src"" directory and download the data."
16122,http://nlp.stanford.edu/data/glove.6B.zip,http://nlp.stanford.edu/data/glove.6B.zip,glove.6B.zip: ,"
"
16126,http://mattmahoney.net/dc/textdata.html,text8," and generate statistics from a text file, or a directory that contains a collection of text files. For example, in order generate word statistics for "," (http://mattmahoney.net/dc/text8.zip), you can do:"
16130,http://cocodataset.org/#format-data,here,Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found ,". Note that we don't use some fields, so the following may be omitted:"
16131,http://cocodataset.org/#detection-leaderboard,COCO Detection Challenge," team, who won "," in 2018, and we keep pushing it forward."
16143,http://nlp.stanford.edu/data/glove.twitter.27B.zip,http://nlp.stanford.edu/data/glove.twitter.27B.zip, directory are not used and instead all the training happens from scratch. This requires that the file of pre-trained GLoVe embeddings is downloaded from ,", unzipped and placed in the "
16152,https://github.com/liuwei16/ALFNet/blob/master/generate_data.py,./generate_data.py,"Dataset preparation. We have provided the cache files of training and validation subsets. Optionally, you can also follow the "," to create the cache files for training and validation. By default, we assume the cache files is stored in '$ALFNet/data/cache/cityperson/'."
16158,https://github.com/zhunzhong07/person-re-ranking/tree/master/evaluation/data/CUHK03,person-re-ranking,Download the train/test split protocol from ,. Put the two mat files 
16167,https://cs.jhu.edu/~qliu24/ZSSBIR/dataset.zip,here,Download the resized TUBerlin Ext and Sketchy Ext dataset and our zeroshot train/test split files from ,. Put the unzipped folder to the same directory of this project.
16170,https://developer.ibm.com/exchanges/data/all/contracts-proposition-bank/,Contract,: Two domain-specific Propbank released (,", "
16170,https://developer.ibm.com/exchanges/data/all/finance-proposition-bank/,Finance,", ",)!
16178,data,data,Put a csv file in the , directory. Each row represents a single document and the first column should always contain the text. Note that a header line is mandatory.
16178,data,data,Export trained paragraph vectors to a csv file (vectors are saved in the , directory).
16180,https://ieee-dataport.org/open-access/remasc-realistic-replay-attack-corpus-voice-controlled-systems,IEEE DataPort,"
","
"
16180,https://ieee-dataport.org/faq/how-do-i-access-dataset-ieee-dataport,IEEE account,", which offers a high-speed download (you will need an "," to download, which is also free)."
16180,https://ieee-dataport.org/open-access/remasc-realistic-replay-attack-corpus-voice-controlled-systems,IEEE DataPort,[,]
16180,https://ieee-dataport.org/faq/how-do-i-access-dataset-ieee-dataport,IEEE account,",  it is free. You will need an "," to download, which is also free."
16181,http://www.nltk.org/data.html,instructions,Then install the NLTK data (,)
16181,http://visionandlanguage.net/VIST/dataset.html,VIST,This model was trained using the Stories of Images-in-Sequence (SIS) from the ," dataset. Mostly, each story from SIS is a 5 sentence text where each sentence corresponds to one of the 5 images in the sequence. You can download this dataset from the official site."
16181,https://github.com/tensorflow/models/tree/master/research/im2txt#prepare-the-training-data,im2txt, file provided in this project. This script works exactly like the one from ,", the difference relies on the structure of the "
16182,https://s3.ca-central-1.amazonaws.com/ehsk-research/data/InferConvAI/InferConvAI_v1.3_tsv.tgz,InferConvAI_v1.3_tsv.tgz,"
", (84MB download / 236MB uncompressed)
16182,https://s3.ca-central-1.amazonaws.com/ehsk-research/data/InferConvAI/InferConvAI_v1.3_jsonl.tgz,InferConvAI_v1.3_jsonl.tgz,"
", (74MB download / 274MB uncompressed)
16189,https://github.com/ColumbiaDVMM/Heated_Up_Softmax_Embedding/tree/master/dataset,code,"Alternatively, you can use the ", to directly  download and pre-process the datasets.
16192,#get-the-filtered-raw-kp20k-training-dataset,Get the filtered raw KP20k training dataset,"
","
"
16192,#data-preprocess,Data preprocess,"
","
"
16192,https://www.dropbox.com/s/lgeza7owhn9dwtu/Processed_data_for_onmt.zip?dl=1,here,"), and replace the digit with ""<digit>"" token for all the text. You can download the processed data ",.
16206,https://github.com/surrealyz/pdfclassifier/tree/master/data/extracted_structural_paths,here,The hidost structural paths are ,.
16206,https://github.com/surrealyz/pdfclassifier/tree/master/data/traintest_all_500test,here,The extracted training and testing libsvm files are ,. The 500 seed malware samples with network activities from EvadeML are in the test set.
16206,https://github.com/surrealyz/pdfclassifier/blob/master/data/seeds_hash_list.txt,500 seed malware hash list.,"
",. Put these PDFs under 
16211,https://www.crcv.ucf.edu/data/UCF101.php,official website,You can download the original UCF101 dataset from the ,. And then extarct RGB images from videos and finally extract optical flow data using TVL1 method. 
16215,http://conda.pydata.org/docs/using/envs.html,conda environment," because it comes with many Python packages already installed and it is easy to work with. After installing Anaconda, you should create a ", so you do not destroy your main installation in case you make a mistake somewhere:
16234,doc/data_simu,Data Simulation,"
","
"
16237,https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html,Google Speech Commands Dataset,For evaluating the proposed and the baseline models we use ,.
16237,https://github.com/hyperconnect/TC-ResNet/tree/master/speech_commands_dataset,speech_commands_dataset/,Follow instructions in ,"
"
16258,http://cvlabwww.epfl.ch/data/multiview/denseMVS.html,fountain-P11, in src/example to generate the image retrieval results The usage is simply “./image_search <sift_list> <output_dir> [depth] [branch_num] [sift_type] [num_matches] [thread_num]”. We add a small image dataset , to illustrate this process. 
16265,https://github.com/google-research-datasets/RxR,RxR,). We also provide the , training script there.
16270,https://bingyaohuang.github.io/pub/CompenNeSt++/photometric_cmp_data,benchmark dataset,Download CompenNet , and extract to 
16270,data,"
CompenNet/data
", and extract to ,"
"
16281,http://nlp.stanford.edu/data/glove.840B.300d.zip,300-d embeddings from 840B Common Crawl,GloVe - ,"
"
16286,http://nlp.stanford.edu/data/glove.840B.300d.zip,GLoVe embeddings,You need to download pretrained 840B 300d ,", and pretrained original size ELMo embedding "
16309,https://github.com/nicola-decao/BNAF/tree/master/data,data,"
",: Data classes to handle the real datasets.
16326,https://www.plant-phenotyping.org/datasets-download,offical website,These steps are described in detail for CVPPP dataset next. You can download the dataset from the ,". For instance segmentation, we use only A1 subset of the dataset, 128 annotated images in total."
16331,data/acdc.lineage,acdc.lineage,Instruction to download the data are contained in the lineage files ,. They are text files containing the md5sum of the original zip.
16351,http://dataprotocols.org/data-packages/,datapackage,This ," only list cities above 15,000 inhabitants. Each city is associated with its country and subcountry to reduce the number of ambiguities. Subcountry can be the name of a state (eg in United Kingdom or the United States of America) or the major administrative section (eg ''region'' in France''). See "
16355,https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/28075/Z1ZFYG&version=25.0,ICEWS18,There are four datasets: ,", "
16355,https://www.wikidata.org/wiki/Wikidata:Main_Page,WIKI,", ",", and "
16355,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/,YAGO,", and ",. These datasets are for the 
16355,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,"Bordes et al., 2013",| Baselines   | Code                                                                      | Embedding size | Batch size | |-------------|---------------------------------------------------------------------------|----------------|------------| | TransE (,)      | 
16358,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/,YAGO,Documents are originally taken from , and 
16358,https://www.wikidata.org/wiki/Wikidata:Main_Page,Wikidata, and ,.
16364,https://20bn.com/datasets/something-something/v1,Something-Something-v1,Download , dataset and extract it in ./datasets/
16372,https://visualdialog.org/data,VisDialv1.0,Evaluation is done on ,.
16374,https://www-nlpir.nist.gov/projects/duc/data.html,NIST,Get the DUC 2005/2006 datasets from ,.
16374,https://www-nlpir.nist.gov/projects/duc/data.html,here,You can get the NIST data from ,". Only the DUC 2005 and 2006 SCUs generated as part of this research, and the original system scores are available here. Once you have the full data, put them in the relevant DUC_data folder."
16383,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI, translational error on the , odometry sequences with 
16383,https://www.dropbox.com/s/ey41xsvfqca30vv/data.zip,url,Download reformated pickle format of the 00-11 KITTI IMU raw data at this ,", extract and copy then in the "
16403,https://github.com/shaohua0116/Multiview2Novelview#datasets,datasets,". We provide codes, ",", and "
16403,http://www.cvlibs.net/datasets/kitti/,KITTI,) as well as real and synthesized scenes (, and 
16403,http://synthia-dataset.net/,Synthia, and ,). We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.
16406,https://www.jsvine.com/consulting/pdf-data-extraction/,Jeremy,"👋 This repository’s maintainers are available to hire for PDF data-extraction consulting projects. To get a cost estimate, contact ", (for projects of any size or complexity) and/or 
16415,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC dataset,DSM is a novel approach to monocular SLAM. It is a fully direct system that estimates the camera trajectory and a consistent global map. Is is able to detect and handle map point reobservations when revisiting already mapped areas using the same photometric model and map points. We provide examples to run the SLAM system in the , and with custom videos. We also provide an optional GUI for 3D visualization of the system results.
16415,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,Download a sequence (ASL format) from ,.
16417,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow_detail.php?benchmark=stereo&error=3&eval=all&result=8da072a8f49d792632b8940582d5578c7d86b747,GC-Net,| Models | Non-Occluded	| All Area | |---|---|---| | ,| 1.77	| 2.30 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow_detail.php?benchmark=stereo&error=3&eval=all&result=8da072a8f49d792632b8940582d5578c7d86b747,PSMNet,| 1.77	| 2.30 | | , | 1.49	| 1.89 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow_detail.php?benchmark=stereo&error=3&eval=all&result=b2d616a45b7b7bda1cb9d1fd834b5d7c70e9f4cc,GANet-15, | 1.49	| 1.89 | | , | 1.36 | 1.80 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow_detail.php?benchmark=stereo&error=3&eval=all&result=95af4a21253204c14e9dc7ab8beb9d9b114cfb9d,GANet-deep, | 1.36 | 1.80 | | , | 1.19 | 1.60 |
16417,http://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&result=70b339586af7c573b33a4dad14ea4a7689dc9305,GC-Net,| Models | Non-Occluded	| All Area | |---|---|---| | , | 2.61 | 2.87 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&result=efb9db97938e12a20b9c95ce593f633dd63a2744,PSMNet, | 2.61 | 2.87 | | , | 2.14 | 2.32 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&result=59cfbc4149e979b63b961f9daa3aa2bae021eff3,GANet-15, | 2.14 | 2.32 | | , | 1.73 | 1.93 | | 
16417,http://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&result=ccb2b24d3e08ec968368f85a4eeab8b668e70b8c,GANet-deep, | 1.73 | 1.93 | | , | 1.63 | 1.81 |
16421,http://www.vision.caltech.edu.s3-us-west-2.amazonaws.com/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,CUB,The following script will prepare the , dataset for training by downloading to the ./resource/datasets/ folder; which will then build the data list (train.txt test.txt):
16422,data/raw,data/raw,Raw data is in ,"
"
16422,data/gold,data/gold,Gold segmented data is in ,"
"
16422,data/rst-dt_tiny_file_list.txt,data/rst-dt_tiny_file_list.txt,"
", contains list of files used from 
16422,data/gold_paper,data/gold_paper,version of gold data used in paper is in ,"
"
16422,data/corenlp_raw.sh,corenlp.sh,use our version of , to parse the input: 
16423,http://jmcauley.ucsd.edu/data/amazon/index.html,here.,"
","
"
16448,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,ETHZ EuroC Dataset,"
","
"
16448,https://vision.in.tum.de/data/datasets/visual-inertial-dataset,TUM Visual Inertial Dataset,"
","
"
16465,data/README.md,Prepare THUMOS14 data,"
",.
16465,load_data.py,"
load_data.py
", in ,.
16477,http://www.rdfhdt.org/datasets,http://www.rdfhdt.org/datasets,. (This dataset has been downloaded from , and extracted using the HDT [2] software at 
16490,https://www.iit.it/web/pattern-analysis-and-computer-vision/audio-visually-indicated-actions-dataset,project's website,First we need to download the dataset from the , following the instructions described therein. The dataset is delivered as a single compressed zip file.
16510,documentation/dataset_format.md,Dataset conversion,"
","
"
16510,documentation/manual_data_splits.md,Manual data splits,"
","
"
16511,./data/fonts/UbuntuMono-R.ttf,Ubuntu mono font,"
", is distributed under 
16511,./data/fonts/LICENCE.txt,UBUNTU FONT LICENCE, is distributed under ,"
"
16516,https://github.com/tom-pelsmaeker/deep-generative-lm/tree/master/dataset,data folder,"Download and pre-process the Penn Treebank data, see the ",.
16516,https://github.com/tom-pelsmaeker/deep-generative-lm/tree/master/dataset,dataset,"
",: expected location of data. Contains code for preprocessing and batching PTB.
16521,http://rpg.ifi.uzh.ch/datasets/gehrig_et_al_iccv19/N-Cars.zip,here,The N-Cars dataset can be downloaded ,.
16523,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html,pandas.DataFrame,The result of this short script would be as follows. The estimation is given in a , format.
16525,https://github.com/vmzakharov/dataframe-ec,Dataframe EC,", ",", "
16525,https://github.com/FasterXML/jackson-datatypes-collections,Jackson Datatypes Collections,", ","
"
16529,http://snap.stanford.edu/data/index.html,Stanford SNAP,The small graphs used in our experiments can be obtained from the ," repository. We recommend using the soc-LiveJournal graph, and have provided a python script to download this graph, symmetrize it, and store it in the the format(s) used by Aspen. This can be done using the SNAPToAdj software in the "
16530,https://git.uwaterloo.ca/jimmylin/hedwig-data,"
hedwig-data
","Option 2. Our school-hosted repository, ",:
16562,https://pandas.pydata.org/,pandas,"
","
"
16566,./doc/new_dataset_guide.md,New Dataset,"
",: Instructions to train KPConv networks on your own data.
16567,https://github.com/rbgirshick/py-faster-rcnn/tree/master/data,Faster R-CNN, (originally from ,)
16567,http://cocodataset.org/#download,here,"Download the images (2014 Train, 2014 Val, 2017 Test) from ","
"
16569,https://github.com/AkariUeda/DLAforElsagate/tree/master/data_preprocessing,"
Data Preprocessing
","
",: Extract/Generate low-level data (static and/or motion)
16574,https://oval.cs.stanford.edu/releases/#section-datasets,our website,"To reproduce the machine learning results in Stanford papers that use Genie (including the PLDI 2019 paper and the ACL 2020 paper), please use the associated artifacts, available for download from ",". The artifact includes all the necessary datasets (including ablation and case studies), pretrained models and evaluation scripts. Please follow the instructions in the README file to reproduce individual experiments."
16578,https://pandas.pydata.org/,pandas,"
","
"
16580,http://www.bigdatalab.ac.cn/~gjf/,Homepage,"
","
"
16580,http://www.bigdatalab.ac.cn/~lanyanyan/,Homepage,"
","
"
16580,http://www.bigdatalab.ac.cn/~cxq/,Homepage,"
","
"
16582,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI's dataset,"This version of the app assumes the LiDAR data to be stored in a binary float matrix (.bin extension). Each column is a point, where the rows are in the following order: x, y, z, and intensity (little endian). See the 3D Velodyne point clouds in ", for example.
16586,https://www.wikidata.org/,Wikidata, for ,". It is kept synchronous with Wikidata in real time, encouraging users to improve the results of their entity linking tasks by contributing back to Wikidata."
16593,https://www.luge.ai/#/luge/dataDetail?id=5,DuIE2.0数据集,"
","
"
16593,https://ernie-github.cdn.bcebos.com/data-msra_ner.tar.gz,MSRA_NER数据集,"
","
"
16593,./applications/tasks/data_distillation,数据蒸馏,数据蒸馏（,）
16597,data/neural/vocab/README.md,this,". Also, check ", to download our processed word embeddings and word2id file.
16607,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/,LIBSVM,"mushroom.mat"" dataset, which is present in ``data"" folder, and sets some values to different parameters which can be tuned depending upon the problem and the solver. This compares the selected solvers for their convergence against time and accuracy. All required datasets can be downloaded from ", website and need to be in matlab format (can be converted using LIBSVM library).
16609,https://github.com/openai/gpt-2-output-dataset,released a dataset,We have also , for researchers to study their behaviors.
16612,www.mathieuramona.com/wp/data/jamendo/,Jamendo dataset by Mathieu Ramona,The experiments use the public ,". To download and prepare it, open the cloned or extracted repository in a bash terminal and execute the following scripts (in this order):"
16613,https://openpai.readthedocs.io/en/latest/manual/cluster-user/how-to-manage-data.html,(refer to here),. Users could use cluster provisioned storages and custom storages in their jobs. The cluster provisioned storages are well integrated and easy to configure in a job ,.
16623,https://github.com/deepinsight/insightface/blob/master/recognition/data/rec2image.py,this code, and decode it using ,.
16650,http://vhosts.eecs.umich.edu/vision//activity-dataset.html,Collective,", ","
"
16650,http://vml.cs.sfu.ca/wp-content/uploads/volleyballdataset/volleyball.zip,volleyball,Download , or 
16655,https://github.com/nmiculinic/minion-data,minion-data, is the entrypoint and should be started through there. You can also see how the config file is parsed and used. Data is formated via gzipped protobuf format defined in , repository.
16658,https://www.dropbox.com/s/0e89uzd2yepq9r4/migration-slr-data.zip?dl=0,here,: Data necessary for reproducing the results is available for download ,. Processed results are available 
16658,https://www.dropbox.com/s/0e89uzd2yepq9r4/migration-slr-data.zip?dl=0,here,Data available for download ,. Download and unzip the file over the current directory:
16660,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI website,The KITTI Stereo 2015 dataset from the ,"
"
16660,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI stereo 2015 leaderboard, error of 2.67% on the ,", exactly the same as our original CRL with the in-house interpolation layer."
16660,http://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&result=f791987e39ecb04c1eee821ae3a0cd53d5fd28c4,link,"For your information, this is a group of results taken from the evaluation page of KITTI. To browse for more results, please click this ",.
16668,https://paperswithcode.com/sota/stress-strain-relation-on-non-linear?p=data-driven-computing-in-elasticity-via,"
PWC
",Data-Driven Computing in Elasticity via Chebyshev Approximation ,"
"
16668,https://github.com/ykanno22/data_driven_kernel_regression,Matlab Code,'s ,.
16675,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5 Dataset,Download the , as source dataset
16675,https://www.cityscapes-dataset.com/,Cityscapes Dataset,Download the , as target dataset
16680,https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus/downloads/smsCorpus_en_2015.03.09_all.json,"2,740K"," | 55,835 English | JSON | "," | 55,835 Chinese | SQL | "
16680,https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus/downloads/smsCorpus_zh_2015.03.09.json,"1,700K"," | 31,465 Chinese | JSON | "," | 31,465"
16680,https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus,Kaggle,Our dataset has been added to ,! Please consider participating a competition!
16681,/datasets/data.md,datasets/data.md,Please see , for detail.
16685,#use-the-code-with-your-own-data, Use the code with your own data ,"
","
"
16685,#use-the-code-with-your-own-data, use the code with your own data ," section for reproducing the results presented in our paper, or to the ", section if you want to use our code for your own task and/or with your own data.
16694,https://github.com/facebookresearch/MUSE/blob/master/data/get_evaluation.sh#L99-L100,here,"Note: Requires bash 4. The download of Europarl is disabled by default (slow), you can enable it ",.
16700,http://cs.uky.edu/~jacobs/datasets/cvusa/,CVUSA,"
","
"
16725,https://nlp.cs.unc.edu/data/jielei/tvqa/tvqa_public_html,new link," Our original web server is down due to a hardware failure, please access data, website, and submission/leaderboard from this ",.
16725,./data,./data,"). It consists of 152.5K QA pairs from 21.8K video clips, spanning over 460 hours of video. The questions are designed to be compositional, requiring systems to jointly localize relevant moments within a clip, comprehend subtitles-based dialogue, and recognize relevant visual concepts. Download TVQA data from ",.
16725,https://nlp.cs.unc.edu/data/jielei/tvqa/tvqa_public_html/explore.html,click here,See examples in video: ,"
"
16725,https://nlp.cs.unc.edu/data/jielei/tvqa/files/det_visual_concepts_hq.pickle.tar.gz,download link,Visual Concepts Feature: object labels and attributes from object detector ,.
16726,https://nlp.cs.unc.edu/data/jielei/tvqa/tvqa_public_html/download_tvqa_plus.html,new link,", please use this ","
"
16729,http://nlp.stanford.edu/data/glove.840B.300d.zip,Common Crawl GloVe embeddings,"Also, if you want to run experiments you need to scikit and download the ", and change your EMBEDDING_LOCATION in the utils.py file to point to where you have them downloaded.
16732,http://rose1.ntu.edu.sg/datasets/actionrecognition.asp,NTU-RGB+D,"For NTU-RGB+D dataset, you can download it from ",. And put the dataset in the file path:
16733,/dataset,dataset,"
","
"
16741,https://github.com/StanfordVL/taskonomy/tree/master/data,dataset,"
","
"
16741,https://github.com/StanfordVL/taskonomy/tree/master/data,DATASET, | , | |:-----|:-----| | The 
16741,https://docs.omnidata.vision/starter_dataset_download.html#Examples,instructions for how to download the full dataset," folder contains information and statistics about the dataset, some sample data, and ",. | | 
16741,https://github.com/StanfordVL/taskonomy/tree/master/data,"
cauthron
", | , |
16747,http://www.oldoccitancorpus.org,http://www.oldoccitancorpus.org,"Olga Scrivner, Michaël Paul McGuire, Sandra Kübler, Barbara Vanve et E.D. Blodgett, éd., Old Occitan Corpus – Digital Collection, Bloomington, 2016, URL : ", (visité le 28/05/2018).
16771,https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27,SHAP feature attribution framework,"Women belonging to the socially disadvantaged caste-groups in India have historically been engaged in labour-intensive, blue-collar work. We study whether there has been any change in the ability to predict a woman’s work-status and work-type based on her caste by interpreting machine learning models using the ",". We find that caste is now a less important determinant of work for the younger generation of women compared to the older generation. Moreover, younger women from disadvantaged castes are now more likely to be working in white-collar jobs."
16775,https://pmb.let.rug.nl/data.php,here," with the path to the C&C parser. Then, please download PMB version 2.1.0 ", and put it to 
16791,http://www.fki.inf.unibe.ch/databases/iam-handwriting-database,IAMDB dataset,IMDBLabelGeneration: to generate the target labels required for evaluation by extending the ,"
"
16798,http://www.msmarco.org/dataset.aspx,MS MARCO re-ranking dataset,Get the , & clone this repository
16804,https://homepages.wmich.edu/~hillenbr/voweldata.html,recordings,", where we demonstrate that ", of spoken vowels can be classified as their waveforms propagate through a trained inhomogeneous material distribution.
16804,https://homepages.wmich.edu/~hillenbr/voweldata.html,website,"The machine learning examples in this package are designed around the task of vowel recognition, using the dataset of raw audio recordings available from Prof James Hillenbrand's ",". However, the core modules provided by this package, which are described below, may be applied to other learning or inverse design tasks involving time-series data."
16806,https://github.com/ashafahi/free_adv_train/tree/master/datasets,Datasets section,"To prepare the data, please see ",.
16817,https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r,Machine Learning with R,"
"," by Brett Lantz is a book that provides an introduction to machine learning using R. As far as I can tell, Packt Publishing does not make its datasets available online unless you buy the book and create a "
16826,https://github.com/abhipec/HAnDS/tree/master/datasets,"Datasets (Wiki-FbF, Wiki-FbT and 1k-WFB-g) along with the preprocessed Wikipedia and Freebase.","
","
"
16879,multivih5datareaderop/README.md,readme, directory. See the corresponding , for more details.
16887,#data,Data,"
","
"
16890,http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz,Amazon,"For the full dataset, the raw dataset link are ",", "
16890,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649,Taobao,", ", and 
16890,https://tianchi.aliyun.com/dataset/dataDetail?dataId=22482,XLong, and ,.
16891,https://deepmind.com/research/open-source/open-source-datasets/kinetics/,Kinetics dataset,"
","
"
16891,http://jhmdb.is.tue.mpg.de/challenge/JHMDB/datasets,JHMDB,"
","
"
16895,http://conda.pydata.org/miniconda.html,Miniconda," Python distribution. If you have limited disk space, the ", installer is recommended. After installing Miniconda and adding the path of folder 
16895,https://data.mendeley.com/datasets/mr39zgc7y5,here,"After cloning the repository, you can download some pre-generated datasets ", . The datasets described below are organized as 
16895,https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_hdf.html,"
to_hdf
", data model using the , function of 
16895,http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html,Pandas DataFrame,You can load a dataset into a ,", with the corresponding "
16897,http://named-data.net/,Named-Data Networking project,the ,"
"
16905,https://github.com/gidariss/wDAE_GNN_FewShot/blob/master/low_shot_learning/datasets/imagenet_dataset.py#L19,imagenet_dataset.py, Download the ImageNet dataset and set in , the path to where the dataset resides in your machine.
16914,http://im2recipe.csail.mit.edu/dataset,here,We use pytorch v0.5.0 and python 3.5.2 in our experiments. You need to download the Recipe1M dataset from , first.
16924,https://github.com/stratosphere/metadata-ms,Metacrate, and ,". Furthermore, it supports "
16924,https://github.com/stratosphere/metadata-ms,Metacrate,"
","
"
16941,https://snap.stanford.edu/data/higgs-twitter.html,Twitter Higgs,"Here we provide Vickers dataset as an example, you can download all the other datasets from ",，
16941,http://deim.urv.cat/~manlio.dedomenico/data.php,Multiplex (old),，,", or "
16941,http://deim.urv.cat/~alephsys/data.html,Multiplex (new),", or ",". You can also use your own multiplex network dateset, as long as it fits the following template."
16943,http://jmcauley.ucsd.edu/data/amazon,Source,"Amazon contains 10,166 nodes and 148,865 edges. ","
"
16943,https://snap.stanford.edu/data/higgs-twitter.html,Source,"Twitter contains 10,000 nodes and 331,899 edges. ","
"
16943,http://socialcomputing.asu.edu/datasets/YouTube,Source,"YouTube contains 2,000 nodes and 1,310,617 edges. ","
"
16955,https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel,"
torch.nn.parallel.DistributedDataParallel
", is deprecated. Use ,"
"
16956,https://github.com/peiyunh/tiny/blob/master/data/demo/selfie.jpg,Original image,"
","
"
16959,https://raw.githubusercontent.com/ocatak/malware_api_class/master/sample_analysis_data.csv,Sample dataset,"
","
"
16961,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUM RGB-D Benchmark," The provided code is not optimized, nor in an easy-to-read shape. It is provided ""as is"", as a prototype implementation of our paper. Use it at your own risk. Moreover, compared to the paper, this implementation lacks the features that make it able to deal with invalid measurements. Therefore, it will not produce good models for the "," scenes. To test it, please use our "
16961,http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/,Bonn RGB-D Dynamic Dataset," scenes. To test it, please use our ","
"
16961,https://vision.in.tum.de/data/datasets/rgbd-dataset,TUM RGB-D Benchmark, is the path to the directory of a dataset in the format of the , (e.g. 
16961,http://www.ipb.uni-bonn.de/data/rgbd-dynamic-dataset/,here,). Some example datasets can be found ,.
16978,https://www.dropbox.com/s/txz0ms0axqf7jrl/ipinyou.contest.dataset.7z?dl=0,Dropbox,The raw data of iPinYou can be downloaded from ,.
16978,https://github.com/rk2900/make-ipinyou-data,here,The feature engineering code is ,", which is forked and slightly different to the original repository."
16985,https://github.com/facebookresearch/XLM#1-preparing-the-data,I.1,: Follow the same procedure as in ,", and download multiple monolingual corpora, such as the Wikipedias."
16985,https://github.com/facebookresearch/XLM/blob/master/get-data-nmt.sh,get-data-nmt.sh,"The English-French, English-German and English-Romanian models are the ones we used in the paper for MT pretraining. They are trained with monolingual data only, with the MLM objective. If you use these models, you should use the same data preprocessing / BPE codes to preprocess your data. See the preprocessing commands in ",.
16986,#tokenize-corpus,tokenization,", and use the above ", to generate wordpiece-level data. Rename the shuffix 
16989,https://deepmind.com/research/open-source/open-source-datasets/,data sets,", ",", and "
16992,#datasets,Datasets,"
","
"
16992,https://www.center-tbi.eu/data,Center TBI dataset,"
","
"
16992,https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html,Boston Housing Dataset,"
","
"
16992,https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html,Boston Housing dataset,A script which trains and evaluates all models on the , can be found 
16995,corpus_dataset_filename.txt,Long list of audio files,"
", used to train the recogniser
16998,http://cocodataset.org/#download,COCO dataset,"If you'd like to train LightTrack, download the ", and the 
17005,http://www.robots.ox.ac.uk/~vgg/data/text/,Synth 90k,In this repo you will find a model trained on the , dataset. When the tfrecords file of synth90k dataset has been successfully generated you may evaluated the model by the following script
17005,http://www.robots.ox.ac.uk/~vgg/data/text/,here,Download the whole synth90k dataset , And extract all th files into a root dir which should contain several txt file and several folders filled up with pictures. Then you need to convert the whole dataset into tensorflow records as follows
17011,https://github.com/openai/glow/tree/master/data_loaders/generate_tfr,here,"If you are interested, see the scripts ", to learn how these tfrecord files were generated.
17031,https://github.com/naver/kapture/blob/master/doc/datasets.adoc,here,"If you want to convert your own dataset into kapture, please find some examples ",.
17034,https://zenodo.org/record/1196312/files/Libraries.io-open-data-1.2.0.tar.gz,libraries.io,"To get the raw dataset, download the source file from ",. Extract the file.
17036,https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c,Ecoffet,"In addition to that, training uses either dice or crossentropy loss, where for the latter precomputed weightmaps emphasising lesion/liver(boundary)pixels are used. Both loss functions can also be extended to add a focal loss term taken and adapted from ",.
17039,http://bensapp.github.io/flic-dataset.html,FLIC, and ,).
17039,https://github.com/bearpaw/pytorch-pose/blob/master/data/mscoco/README.md,COCO Readme,"For training/testing on COCO, please refer to ",.
17039,http://www.cims.nyu.edu/~tompson/data/mpii_valid_pred.zip,Tompson et al. CVPR 2015,Run the following command in terminal to evaluate the model on MPII validation split (The train/val split is from ,).
17039,http://www.cims.nyu.edu/~tompson/data/mpii_valid_pred.zip,Tompson et al. CVPR 2015, to evaluate your predictions. The evaluation code is ported from  ,.
17039,http://cocodataset.org/#keypoints-challenge2017,MSCOCO (single person),[x] ,"
"
17043,http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html,LRW, is avaliable at https://youtu.be/eH7h_bDRX2Q. This code can be applied directly in , and 
17043,http://spandh.dcs.shef.ac.uk/gridcorpus/,GRID, and ,". The outputs from the model are visualized here: the first one is the synthesized landmark from ATnet, the rest of them are attention, motion map and final results from VGnet."
17043,http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html,LRW,Download and unzip the training data from ,"
"
17051,https://mxnet.incubator.apache.org/architecture/note_data_loading.html,Design Note: Design Efficient Deep Learning Data Loading Module,"
","
"
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/FERplus_dir/jianfei_occlusion_list.txt,ferplusocclusion,You can find occlusion dataset(,", "
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/AffectNet_dir/occlusion_affectnet_list.txt,affectnetocclusion,", ","), pose(>30)("
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/FERplus_dir/pose_30_ferplus_list.txt,ferpluspose30,"), pose(>30)(",", "
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/AffectNet_dir/pose_30_affectnet_list.txt,affectnetpose30,", ",) and pose(>45)(
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/FERplus_dir/pose_30_ferplus_list.txt,ferpluspose45,) and pose(>45)(,", "
17058,https://github.com/kaiwang960112/Challenge-condition-FER-dataset/blob/master/AffectNet_dir/pose_45_affectnet_list.txt,affectnetpose45,", ",) list.
17065,https://webhose.io/datasets,WebHose English News Articles, and ," articles has been included to better balance the classes. Corpus is mainly intended for use in training deep learning algorithms for purpose of fake news recognition. The dataset is still work in progress and for now, the public version includes only 9,408,908 articles (745 out of 1001 domains)."
17065,https://webhose.io/datasets,WebHose,"
","
"
17066,https://deepmind.com/research/open-source/open-source-datasets/kinetics/,Kinetics-400," uses temporal information from videos to correct errors in single-image 3D pose estimation.  In this repository, we provide results from applying this algorithm on the "," dataset.  Note that this is not an exhaustive labeling: at most one person is labeled per frame, and frames which the algorithm has identified as outliers are not labeled."
17067,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-2, on the , dataset and evaluates their perplexity. The model is either a 
17083,https://vincentarelbundock.github.io/Rdatasets/articles/data.html,HTML index,"
","
"
17083,https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/datasets.csv,CSV index,"
","
"
17096,https://www2.cs.sfu.ca/~colour/data/shi_gehler/,ColorChecker,We have trained on ColorChecker and NUS-8 datasets. Please refer to , and 
17096,./create_data_lmdb.sh,create_data_lmdb.sh,Generate LMDB files (refer to , and 
17100,https://youtube-vos.org/dataset/vis/,here,Download YouTubeVIS from ,.
17122,https://archive.ics.uci.edu/ml/datasets/Multiple+Features,Handwritten,"
","
"
17122,https://archive.ics.uci.edu/ml/datasets.html,Reuters,"
","
"
17150,https://vissarion.github.io/tutorials/volesti_tutorial_pydata.html,Tutorial given to PyData meetup,"
","
"
17156,data/README.md,README, to the 'data' folder. See more details in this ,.
17159,https://github.com/fastnlp/nlp-dataset/raw/master/text%20classification/mtl16.zip,dataset:mtk16,] [,]
17159,https://github.com/fastnlp/nlp-dataset/raw/master/text%20style%20transfer/imdb.zip,dataset:imdb,][,]
17160,https://graphics.soe.ucsc.edu/data/BodyModels/index.html,SPRING, and ,. No commercial usage of the data is allowed.
17172,http://iesl.cs.umass.edu/downloads/multi-step-reasoning-iclr19/data.tar.gz,here,We are making the pre-processed data and paragraph vectors available so that is is easier to get started. They can downloaded from ,". (41GB compressed, 56GB decompressed; user/pass: guest/guest). If you need the pretrained paragraph encoder used to generate the vectors, feel free to get in touch with me. After un-taring, you will find a directory corresponding to each dataset. Each directory further contains:"
17174,https://github.com/stanford-futuredata/dawn-bench-entries,this FAQ,"Note that DAWNBench timings do not include validation time, as in ",", but do include initial preprocessing, as indicated "
17189,https://sites.google.com/site/assistmentsdata/home/2012-13-school-data-with-affect,ASSISTments 2012-2013,"
", (assistments12)
17189,http://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp,KDD Cup 2010 EDM Challenge,The two last datasets come from the ,. Datasets need to be downloaded and put inside each corresponding data folder in data. The main dataset (
17190,#datasets,Datasets,"
","
"
17190,https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#,BIWI,"
","
"
17190,https://icv.tuit.ut.ee/databases/,SASE,"
","
"
17195,#training-data,Training data (for both CADec and DocRepair),"
","
"
17195,https://github.com/lena-voita/good-translation-wrong-in-context/tree/master/consistency_testsets/scoring_data,this directory,In , you will find 
17199,http://mscoco.org/dataset/#download,link,Download the mscoco images from  ,. You need 2014 training images and 2014 val. images. You should put the train2014/ and val2014/ in the ./data/images/ directory.
17199,http://mscoco.org/dataset/#download,official website,You may download the mscoco captions from the , or use the download bash script provided by us.
17199,http://cocodataset.org/,COCO," team for providing Pytorch, "," team for providing dataset, "
17206,http://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools,SICK,"
","
"
17206,https://www.kaggle.com/c/fake-news-pair-classification-challenge/data,ByteDance,"
","
"
17217,http://www.slideshare.net/larsga/linking-data-without-common-identifiers,This presentation," page lists real examples of using Duke, complete with data and configurations. ", has more of the big picture and background.
17220,https://tianchi.aliyun.com/dataset/dataDetail?dataId=56,Ad Display/Click Data on Taobao.com,Download Dataset ,"
"
17224,https://dl.dropboxusercontent.com/s/7qjye3efr0czux8/dataset_pose.zip,training dataset, that I used and the generated , (1427 images already split into training and validation).
17228,./Part_0_Prepare_dataset_Magnatagatune.ipynb,Jupyter Notebook - Part_0_Prepare_dataset_Magnatagatune,(,)
17228,https://www.audiocontentanalysis.org/data-sets/,List of Datasets,"
", by Alexander Lerch
17240,#running-and-evaluating-the-amore-upf-model-on-the-semeval-test-data,Section above,. See the , for the description of the files stored therein.
17240,#running-and-evaluating-the-amore-upf-model-on-the-semeval-test-data,Section above,See the , for details.
17263,http://www.cs.pomona.edu/~dkauchak/simplification/data.v1/data.v1.split.tar.gz,William Coster and David Kauchak (2011)," (test, lowercased). Only Wikipedia data is uploaded (same splits as ","). The remaining datasets should be obtained from their respective sources in LDC.  For example, issue the following commands to train and evaluate LSTM-Large on Wikipedia using GPU0:"
17310,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,"
", and 
17310,https://www.cityscapes-dataset.com,Cityscapes, and ,"
"
17310,http://www.cvlibs.net/download.php?file=data_depth_annotated.zip,KITTI Depth Annotated,"
","
"
17310,http://www.cvlibs.net/download.php?file=data_depth_annotated.zip,link,You can download depth annotated data from this ,. sudo pip3 install rospkg catkin_pkg Please go to 
17331,http://cocodataset.org/#download,COCO 2014 dataset,Download ,"
"
17333,http://jmcauley.ucsd.edu/data/amazon,Amazon-book,You can find the full version of recommendation datasets via ,", "
17333,http://www.cp.jku.at/datasets/LFM-1b/,Last-FM,", ",", and "
17333,https://www.yelp.com/dataset/challenge,Yelp2018,", and ",.
17359,download_data.ipynb,download_data.ipynb,Download the helically-forced turbulence simulation dataset using the , notebook.
17359,download_data.ipynb,here,Data links are provided ,"
"
17378,https://numba.pydata.org/,Numba,"
","
"
17381,#3-data-format,Data Format,"
",      3.1 
17381,#4-data-utilities,Data Utilities,"
",      4.1 
17381,#43-computing-dataset-statistics,Computing Dataset Statistics,      4.3 ,"
"
17394,http://www.cs.tut.fi/sgn/arg/dcase2016/task-sound-event-detection-in-synthetic-audio#audio-dataset,isolated sound events dataset from DCASE 2016 task 2," provides four-channel directional microphone recordings from a tetrahedral array configuration. Both formats are extracted from the same microphone array, and additional information on the spatial characteristics of each format can be found below. The participants can choose one of the two, or both the datasets based on the audio format they prefer. Both the datasets, consists of a development and evaluation set. The development set consists of 400, one minute long recordings sampled at 48000 Hz, divided into four cross-validation splits of 100 recordings each. The evaluation set consists of 100, one-minute recordings. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings the collected IRs were convolved with ",". Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB."
17398,https://www.cs.jhu.edu/~s.zhang/data/AMR/ckpt-amr-2.0.tar.gz,ckpt-amr-2.0.tar.gz,Here are pre-trained models: , and 
17398,https://www.cs.jhu.edu/~s.zhang/data/AMR/ckpt-amr-1.0.tar.gz,ckpt-amr-1.0.tar.gz, and ,". To use them for prediction, simply download & unzip them, and then run "
17412,https://github.com/stanford-futuredata/dawn-bench-entries/pull/1,example PR, (,"), with JSON (and TSV where applicable) result files in the format outlined below."
17427,http://jmcauley.ucsd.edu/data/amazon/,Amazon Book Data,"
","
"
17427,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&userId=1,Taobao Data,"
","
"
17427,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&userId=1,Taobao Data,First download ," to get ""UserBehavior.csv.zip"", then execute the following command."
17429,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI Dataset,Download the raw data of ,. This dataset is for training and eigen split evaluation.
17429,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php,KITTI 2015 scene flow dataset,Download , and save it in KITTI_PATH. This dataset is for optical flow and kitti split evaluation.
17436,#data-preprocessing,Data preprocessing,"
","
"
17436,#data,Data,"
","
"
17437,https://github.com/deepmind/dsprites-dataset,https://github.com/deepmind/dsprites-dataset,Download from ,"
"
17445,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry dataset,Download , to YOUR_DATASET_FOLDER and set the 
17454,https://linqs.soe.ucsc.edu/data,LINQS,": scientific publications citation network, from ","
"
17454,https://linqs.soe.ucsc.edu/data,LINQS,": scientific publications citation network, from ","
"
17469,http://snap.stanford.edu/data/ca-HepTh.html,Arxiv High Energy Physics Theory Collaboration Network, to reproduce this experiment on the ,.
17479,https://www.yosyshq.com/tabby-cad-datasheet,Tabby CAD Suite,Yosys is part of the , and the 
17479,https://www.yosyshq.com/tabby-cad-datasheet,Tabby CAD Suite, for a , Evaluation License and download link
17483,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
17487,./scripts/datasets,GluonNLP Datasets,"To facilitate both the engineers and researchers, we provide command-line-toolkits for downloading and processing the NLP datasets. For more details, you may refer to ", and 
17489,https://github.com/pclucas14/GansFallingShort/tree/master/synthetic_data_experiments#synthetic-task,here, that was used to launch the hyperparameter search. More info can be found ,"
"
17489,https://github.com/pclucas14/GansFallingShort/blob/master/real_data_experiments/scripts/news_rs.py,hyperparameter script, folder. We also provie the , used for this experiment. We give more detail on reproducing the EMNLPNEWS dataset results 
17489,https://github.com/pclucas14/GansFallingShort/tree/master/real_data_experiments#real-data-experiments,here, used for this experiment. We give more detail on reproducing the EMNLPNEWS dataset results ,"
"
17490,http://papers.nips.cc/paper/9574-unsupervised-discovery-of-temporal-structure-in-noisy-data-with-dynamical-components-analysis,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis,Implementation of the methods and analyses in ,.
17490,https://www.kaggle.com/selfishgene/historical-hourly-weather-data?select=temperature.csv,Temperature,"
", - We used the 30 US cities from temperature.csv.
17490,https://github.com/mmalekzadeh/motion-sense/tree/master/data,Accelerometer,"
", - We used std_6/sub_19.csv from A_DeviceMotion_data.zip
17493,https://github.com/tflearn/tflearn/blob/master/tflearn/datasets/oxflower17.py,tflearn oxflower17,", ", and 
17493,https://pandas.pydata.org/pandas-docs/stable/install.html,pandas,"
","
"
17493,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,GTSRB,"
",: we provide scripts to download it.
17493,http://www.robots.ox.ac.uk/~vgg/data/flowers/17/index.html,Flower,"
",: we provide scripts to download it.
17506,http://nlp.stanford.edu/data/glove.6B.zip,"
Pre-trained glove embedding
","
",: 
17508,https://www.yelp.com/dataset/challenge,yelp,"
",: negative sentiment (0) <--> positive sentiment (1)
17508,https://github.com/raosudha89/GYAFC-corpus,GYAFC,"
",: informal text (0) <--> formal text (1)
17508,https://github.com/raosudha89/GYAFC-corpus,https://github.com/raosudha89/GYAFC-corpus,"). If you want to download the train and validation dataset, please follow the guidance at ",". And then, name the corpora of two styles as the yelp dataset."
17508,#extend-to-other-tasks-and-datasets,Extend to other tasks and datasets,"If you want to use your own datasets, please follow the guidance of next section ",.
17511,https://github.com/aachenhang/crowdcount-mcnn/tree/master/data_preparation,code,Generate the density maps by using the ,.
17513,#user-content-data-preparation,data preparation,"(Before running scripts, make sure for the ", )
17543,http://groups.csail.mit.edu/vision/datasets/ADE20K/,ADE20K,"
","
"
17543,http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip,MIT Scene Parsing Benchmark,"
","
"
17543,https://www.cityscapes-dataset.com/,Cityscapes,"
","
"
17543,http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUDv2,"
","
"
17543,https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html#sec_datasets,here,Download data for desired dataset(s) from list of URLs ,.
17544,https://www.nist.gov/srd/nist-special-database-19,NIST Special Database 19, dataset was generated from the original data found in the , with the goal to match the MNIST preprocessing as closely as possible.
17544,https://pytorch.org/vision/stable/generated/torchvision.datasets.QMNIST.html#torchvision.datasets.QMNIST,torchvision,Update - The Pytorch QMNIST loader described in section 2.4 below is now included in ,.
17544,https://www.nist.gov/srd/nist-special-database-19,NIST Special Database 19,"The official NIST training data (series hsf0 to hsf3, writers 0 to 2099) was written by NIST employees. The official testing data (series hsf4, writers 2100 to 2599) was written by high-school students and is considered to be substantially more challenging. Since machine learning works better when training and testing data follow the same distribution, the creators of the MNIST dataset decided to distribute writers from both series into their training and testing sets. The QMNIST extended labels trace each training or testing digit to its source in the ",". Since the QMNIST training set and the first 10000 examples of the QMNIST testing set exactly match the MNIST training and testing digits, this information can also be used for the standard MNIST dataset. The extended labels are found in the following files."
17544,https://www.nist.gov/srd/nist-special-database-19,NIST Special Database 19, data files provide preprocessed images and extended labels for all digits appearing in the ," in partition and writer order. Column 5 of the extended labels give the index of each digit in this file. We found three duplicate digits in the NIST dataset. Column 6 of the extended labels then contain the index of the digit for which this digit is a duplicate. Since duplicate digits have been eliminated from the QMNIST/MNIST training set and testing set, this never happens in the "
17544,https://pytorch.org/vision/stable/generated/torchvision.datasets.QMNIST.html#torchvision.datasets.QMNIST,torchvision,Update - The Pytorch QMNIST loader described here is now included in ,.
17556,doc/dataStructures,Data structures,"
","
"
17560,https://github.com/thunlp/Fast-TransX/tree/master/data,[Download],You can download FB15K and WN18 from ,", and the more datasets can also be found in (""https://github.com/thunlp/KB2E"")."
17575,https://github.com/ydanila/n-metadata-extractor/tree/xmp-core,XMP core port,"For example, for the ", with this prebuilt 
17575,https://github.com/ydanila/n-metadata-extractor/tree/xmp-core,XMP core port,"Configuration also could be specified in an options file. In this case, for the ", with this prebuilt 
17579,https://www.cs.rochester.edu/~lsong10/downloads/sembleu_data.zip,Here,If you're developing a new metric and would like to have a comparison. , is the 100 AMR graphs and the corresponding system outputs.
17583,https://surprise.readthedocs.io/en/stable/getting_started.html#load-a-custom-dataset,Dataset handling,Alleviate the pain of ,. Users can use both 
17583,https://grouplens.org/datasets/movielens/,Movielens, datasets (,", "
17583,https://eigentaste.berkeley.edu/dataset/,Jester,", ","), and their own "
17583,https://grouplens.org/datasets/movielens/,Movielens,"Here are the average RMSE, MAE and total execution time of various algorithms (with their default parameters) on a 5-fold cross-validation procedure. The datasets are the ", 100k and 1M datasets. The folds are the same for all the algorithms. All experiments are run on a laptop with an intel i5 11th Gen 2.60GHz. The code for generating these tables can be found in the 
17583,http://grouplens.org/datasets/movielens/100k,Movielens 100k,| ,                                                                         |   RMSE |   MAE | Time    | |:---------------------------------------------------------------------------------------------------------------------------------------|-------:|------:|:--------| | 
17583,https://grouplens.org/datasets/movielens/1m,Movielens 1M,| ,                                                                             |   RMSE |   MAE | Time    | |:----------------------------------------------------------------------------------------------------------------------------------------|-------:|------:|:--------| | 
17589,https://biendata.com/competition/zhihu/,"
Zhihu
",", ", and 
17595,#dataset-preparation,Dataset Preparation,"
","
"
17595,#data-structure,Data structure,"
","
"
17595,#input-data,Input data,"
","
"
17595,dataset_preparation/,"
dataset_preparation/
","You need to extract frame-level features for each video to run the codes. To extract features, please check ",.
17595,dataset_preparation/,"
dataset_preparation/
","To generate the file list, please check ",.
17596,https://github.com/trappmartin/BayesianSumProductNetworks/releases/download/v1.0/data_predictions_scripts.tar,download,All dataset and predictions can be found under: ,"
"
17598,https://www.kaggle.com/c/imagenet-object-localization-challenge/data,Kaggle,You can download ImageNet from ,.
17605,https://www.nvidia.com/en-us/data-center/a100/,NVIDIA A100, TensorFloat-32 (TF32) is the new math mode in , GPUs for handling the matrix math also called tensor operations. TF32 running on Tensor Cores in A100 GPUs can provide up to 10x speedups compared to single-precision floating-point math (FP32) on Volta GPUs. TF32 is supported in the NVIDIA Ampere GPU architecture and is enabled by default.
17616,https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT,here,28 x 28 greyscale Omniglot from ,"
"
17617,http://www.hylap.org/meta_data/svm/,here,"The two meta-data sets used in our experiments are available in the folder ""data"". More information is available on our project website. Visualizations of the SVM meta-data set can be found ", and the creation of the Weka meta-data set can be found 
17617,http://www.hylap.org/meta_data/weka/,here, and the creation of the Weka meta-data set can be found ,.
17620,/data/adhoc8_ap.csv,"
data/adhoc8_ap.csv
", (see for instance file ,"). Then, edit file "
17628,#datasets-instructions,Datasets instructions,"
","
"
17641,https://towardsdatascience.com/ai-feynman-2-0-learning-regression-equations-from-data-3232151bd929,this Medium article,Please check , for a more detailed eplanation of how to get the code running.
17657,https://github.com/HaohanWang/HFC/blob/master/utility/dataLoader.py,dataLoader.py,"
","
"
17661,https://huggingface.co/datasets/imagenet_sketch,dataset,Hugging Face ,"
"
17669,https://www.robots.ox.ac.uk/~vgg/data/flowers/,Oxford Flowers dataset,", and the ",", move into "
17669,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ,We present results mainly for images from the CelebA validation set and from ,", along with additional results for a random selection of out-of-distribution qualitative images ""in the wild."" All test sets used in our experiments are available in the directory "
17676,https://archive.ics.uci.edu/ml/datasets/HIGGS,UCI, from ,"
"
17677,https://github.com/LiYingwei/Regional-Homogeneity/tree/master/data,Subset,"
", of ImageNet validation set (5k images).
17688,https://datajoint.io,DataJoint,"
", - an open-source relational framework for scientific data pipelines.
17688,https://github.com/mara/data-integration,Mara,"
"," -  A lightweight, opinionated ETL framework, halfway between plain scripts and Apache Airflow."
17688,https://datalad.org,DataLad,"
", - git and git-annex based data version control system with lightweight provenance capture/re-execution support.
17688,https://github.com/dataform-co/dataform,Dataform,"
", - Dataform is a framework for managing SQL based operations in your data warehouse.
17688,https://community.hitachivantara.com/s/article/data-integration-kettle,Pentaho Kettle,"
"," - A plataform that delivers poweful ETL capabilities, using a groundbreaking, metadata-driven approach."
17694,https://www.data.gouv.fr/fr/datasets/registre-parcellaire-graphique-rpg-contours-des-parcelles-et-ilots-culturaux-et-leur-groupe-de-cultures-majoritaire/,Registre parcellaire graphique (RPG),"
", of the French National Geographic Institute (IGN)
17694,http://isprs.stream-up.tv/media-221-breizhcrops-a-time-series-dataset-for-crop-type-mapping,here,ISPRS virtual congress video can be found ,"
"
17695,https://www.data.gouv.fr/fr/datasets/registre-parcellaire-graphique-rpg-contours-des-parcelles-et-ilots-culturaux-et-leur-groupe-de-cultures-majoritaire/,Registre parcellaire graphique (RPG),"
", of the French National Geographic Institute (IGN)
17695,http://isprs.stream-up.tv/media-221-breizhcrops-a-time-series-dataset-for-crop-type-mapping,here,ISPRS virtual congress video can be found ,"
"
17697,https://blog.ethereum.org/2014/03/28/schellingcoin-a-minimal-trust-universal-data-feed/,blog,Vitalik's , describing schellingcoin-derivated data feeds.
17708,https://www.tensorflow.org/datasets/,TensorFlow Datasets API,We utilize the ,. Running any of our code that requires data will automatically download the requisite data.
17710,http://alt.qcri.org/semeval2016/task8/index.php?id=data-and-tools,SemEval-2016,"The code was mostly developed during 2012 and 2013, and has undergone many fixes and updates. Note that the versions distributed for "," were numbered 2.0–2.0.2, but these predate this repository and the "
17724,https://www.packtpub.com/big-data-and-business-intelligence/practical-time-series-analysis?utm_source=github&utm_medium=repository&utm_campaign=9781788290227,Practical Time-Series Analysis,This is the code repository for ,", published by "
17724,https://www.packtpub.com/big-data-and-business-intelligence/practical-real-time-data-processing-and-analytics?utm_source=github&utm_medium=repository&utm_campaign=9781787281202,Practical Real-time Data Processing and Analytics,"
","
"
17724,https://www.packtpub.com/big-data-and-business-intelligence/building-python-real-time-applications-storm?utm_source=github&utm_medium=repository&utm_campaign=9781784392857,Building Python Real-Time Applications with Storm,"
","
"
17725,http://cloud.google.com/dataflow/,Google Cloud Dataflow,", ",", and "
17725,http://cloud.google.com/dataflow/,Google Cloud Dataflow, submits the pipeline to the ,.
17725,https://github.com/dataArtisans/flink-dataflow,dataArtisans/flink-dataflow, runs the pipeline on an Apache Flink cluster. The code has been donated from , and is now part of Beam.
17725,https://github.com/cloudera/spark-dataflow,cloudera/spark-dataflow, runs the pipeline on an Apache Spark cluster. The code has been donated from , and is now part of Beam.
17727,http://convai.io/#personachat-convai2-dataset,http://convai.io/#personachat-convai2-dataset,. More details about this dataset can be found at the ConvAI2 homepage at ,.
17727,https://github.com/vsharecodes/percvae/tree/master/data,"
./data
"," for training / validating and testing. For data preparation, please refer to instructions and example files in ",.
17756,data/README.md,data/,"Due to ethical constraints, we do not host a static dataset. Instead, we provide scripts which allow for data to be scraped from a public scamlist and dating site. For more information, see ","
"
17762,https://sigsep.github.io/datasets/musdb.html,MUSDB18 dataset,"For audio source separation experiments, you will need to download the ", from 
17770,https://github.com/openeventdata/es-geonames,es-geonames,See the , for the code used to produce this index.
17770,https://github.com/openeventdata/mordecai/releases,Releases,An earlier verion of this software was donated to the Open Event Data Alliance by Caerus Associates.  See , or the 
17770,https://github.com/openeventdata/mordecai/tree/legacy-docker,legacy-docker, or the , branch for the 2015-2016 and the 2016-2017 production versions of Mordecai.
17770,https://github.com/openeventdata/mordecai/issues,issues page,Contributions via pull requests are welcome. Please make sure that changes pass the unit tests. Any bugs and problems can be reported on the repo's ,.
17773,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,Get some datasets from , .
17773,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,Run on a dataset from , using
17773,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,The format assumed is that of ,". However, it should be easy to adapt it to your needs, if required. The binary is run with:"
17773,https://github.com/tum-vision/mono_dataset_code,https://github.com/tum-vision/mono_dataset_code,Use a photometric calibration (e.g. using , ).
17788,https://www.kaggle.com/c/outbrain-click-prediction/data,Outbrain Click Prediction competition,Our CTR data is extracted from the ,.
17788,experiments/CTR/prepare_data,"
experiments/CTR/prepare_data/
",Pre-processing steps and scripts to obtain the same Cross-Validation splits used in our paper are available in ,.
17791,https://github.com/cocodataset/cocoapi,COCO API, and ,.  The training and evaluation dataset are referenced from 
17791,https://github.com/cocodataset/cocoapi,COCOAPI,Configure ,"
"
17791,https://github.com/cocodataset/cocoapi,COCOAPI,We modify the , to meet our need. Here you have to soft link the pycocotool to the root directory for invoking.
17791,https://www.mvtec.com/company/research/datasets/mvtec-d2s/,D2SA,",","] to our layer based annotation. If you find those two dataset are useful, please cite their "
17804,./data,"
data
","
", folder contains the code that generates samples for the experiments.
17804,./data/mix_gaussian.py,"
mix_gaussian.py
","
", contains the code for the mixed Gaussian distribution.
17804,./data/gaussian.py,"
gaussian.py
","
", contains the code for gaussian distribution.
17818,http://cocodataset.org/#home,COCO dataset,", for the purpose of providing the image pairs, i.e., the person image and its texture image. The texture image stores the RGB texture of the full person 3D surface. In particular, we use 929 raster-scanned texture maps provided by the SURREAL dataset to generate the image pairs. On SURREAL, all faces in the texture image are replaced by an average face of either man or woman. We generate 9,290 different meshes of diverse poses/shapes/viewpoints. For each texture map, we assign 10 different meshes and render these 3D meshes with the texture image. Then we obtain in total 9,290 different synthesized (person image, texture image) pairs. To simulate real-world scenes, the background images for rendering are randomly sampled from ",. Each synthetic person image is centered on a person with resolution 256x128. The resolution of the texture images is 256x256. The PIT dataset can be downloaded from 
17818,torchreid/datasets/__init__.py,torchreid/datasets/__init__.py, dataset as an example for description. See , for details. The data managers of image reID are implemented in 
17818,torchreid/data_manager.py,torchreid/data_manager.py, for details. The data managers of image reID are implemented in ,.
17818,https://github.com/zhunzhong07/person-re-ranking/tree/master/evaluation/data/CUHK03,person-re-ranking,Download the train/test split protocal from ,. What you need are 
17831,http://pandas.pydata.org/,pandas,"
","
"
17837,https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz,Speech Commands dataset,"In the experiments of this work, we use the "," (2.3GB), which is publically accessible. You don't need to download it if you want to train the target model from scratch because the training code will automatically download it."
17848,https://github.com/warnikchow/3i4k/tree/master/data/train_val_test,here,"(19.06.06) We provide train & validation, and test set separately ",", for an easier Keras-based implementation."
17848,https://github.com/warnikchow/3i4k/blob/master/data/fci.txt,"
A Renewed version of the final corpus
",(19.02.28) ,"
"
17848,https://github.com/warnikchow/3i4k/blob/master/data/fci.txt,dataset, or the ,", cite the following:"
17849,https://towardsdatascience.com/atomic-scale-deep-learning-34238feda632,summary, and Bethany Connolly published a , in Towards Data Science.
17849,https://github.com/Jeffrey-Ede/datasets/wiki,here,A training dataset with 161069 non-overlapping 512x512 crops from STEM images is available ,.
17857,https://relational.fit.cvut.cz/dataset/CORA,cora,cora: the , dataset (cora)
17857,https://linqs.soe.ucsc.edu/data,citeseer,citeseer: the , dataset (citeseer)
17857,https://linqs.soe.ucsc.edu/data,pubmed,pubmed: the , citation dataset (pubmed)
17858,https://towardsdatascience.com/how-to-improve-your-image-classifier-with-googles-autoaugment-77643f0be0c9,Blogpost, Wrote a , about AutoAugment and Double Transfer Learning.
17859,http://horatio.cs.nyu.edu/mit/tiny/data/tiny_images.bin,tiny_images.bin,"
","
"
17861,https://www.nature.com/articles/sdata201422,QM9,130K molecules from ,"
"
17861,https://en.wikipedia.org/wiki/ZINC_database,ZINC,250K molecules from ,"
"
17861,http://moleculenet.ai/datasets-1,MoleculeNet,160K+ molecules from various , datasets
17861,https://www.emolecules.com/info/products-data-downloads.html,eMolecules Database,36M+ molecules from the ,". Due to its large size, this dataset is not included on the repository. To run tests on it, please download the dataset into the "
17866,http://nlp.stanford.edu/data/glove.6B.zip,LINK,"6B tokens, 300d  Glove word vectors are used ","
"
17866,https://tac.nist.gov/data/index.html,TAC 08/09/10/11, and , dataset with your request and approval.
17885,https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/dataset/dataset_speech.json,Political speech dataset,"
","
"
17885,https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/dataset/dataset_blog.json,Blog dataset for age and gender,"
","
"
17885,https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/translate_models/age_translator.pth.tar,Age translator,"
", for adult (23-40 years) to teenager(13-19 years) and vice-versa.
17885,https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/translate_models/gender_translator.pth.tar,Gender translator,"
","
"
17885,https://datasets.d2.mpi-inf.mpg.de/rakshith/a4nt_usenix/translate_models/speechObamaAndTrump_translator.pth.tar,Speech translator,"
", for Obama to Trump and vice-versa.
17886,https://seekstorm.com/blog/very-data-cleaning-of-product-names-company-names-street-names/,"Very fast Data cleaning of product names, company names & street names","
","
"
17886,http://storage.googleapis.com/books/ngrams/books/datasetsv2.html,Google Books Ngram data,"
","
"
17886,https://github.com/dataiku/dss-plugin-nlp-preparation/tree/master/resource/dictionaries,Frequency dictionaries,"
","
"
17886,https://github.com/LuminosoInsight/wordfreq/tree/master/wordfreq/data,Frequency dictionaries,"
","
"
17918,http://jmcauley.ucsd.edu/data/amazon,Source,Amazon Review ,"
"
17918,https://grouplens.org/datasets/movielens/20m/,Source,Movielens-20M ,"
"
17918,https://tianchi.aliyun.com/dataset/dataDetail?dataId=9716,Source,Taobao Cloud Theme Click Dataset ,"
"
17933,https://joshua.incubator.apache.org/data/fisher-callhome-corpus/,JHU,"This dataset contains fluent English reference translations re-written with disfluencies removed for the LDC Fisher Spanish dataset. They were collected via Mechanical Turk using the original disfluent translations. It complements the original LDC Fisher audio and transcripts, and translations collected by ",", creating a parallel disfluent/fluent corpus to promote further research in producing fluent output from disfluent data."
17933,https://github.com/joshua-decoder/fisher-callhome-corpus,here, maps Fisher speech features to match these translation ids. Please see , for more information.
17948,http://corpus-tools.org/annis,ANNIS,", and the easiest way to search in the corpus is using ",. Here is 
17948,http://corpus-tools.org/annis,ANNIS,"annis/ - The entire merged corpus, with all annotations, as a relANNIS 3.3 corpus dump, importable into ","
"
17963,test_dataset,"
test_dataset
",". CBLasso takes a long time to run, therefore, the result of running CBLasso on ", is precomputed and provided in 
17963,test_dataset/cblasso_results,"
test_dataset/cblasso_results
", is precomputed and provided in ,. Performance of CBLasso is obtained with 
17963,`generate_dataset.py`,"
generate_dataset.py
","
", provides the code to generate data. An example usage is shown below:
17963,test_dataset,"
test_dataset
",The particular instance of test data used in the original paper is available in the ,.
17990,https://github.com/marcobn/musicntwrk/blob/master/DOCS/data.md,data,"
","
"
17991,https://github.com/stanford-futuredata/Willump-Simple,here,Update:  We've released a new and simpler rewrite of Willump available ,.  We'd recommend anyone interested in experimenting with the project to check out that version first!
17991,https://github.com/stanford-futuredata/weld-willump/tree/llvm-st,here,"Then install the llvm-st branch of our Weld fork, weld-willump. Its repository and installation instructions are available ",.
17991,https://github.com/stanford-futuredata/Willump/blob/master/BENCHMARKS.md,benchmarks guide,"For information on reproducing the experiments run in the Willump paper, please see our ",.
17995,https://github.com/Leonard-Xu/CWE/tree/master/data,"(Chen et al., IJCAI, 2015)", folder are provided by ,.
18007,https://biendata.com/competition/idiom/,competition,We are organizing a ," adapted from the ChID dataset. For the adapted data and baseline codes of the competition, please refer to "
18007,https://biendata.com/competition/idiom/,here,"We are organizing a competition based on our ChID dataset, and "," is the website. The adapted corpus establishes up connections between blanks, and adopts a new type of problem. A list of passages (not an isolated one) is provided and the answers need to be selected from a given set of candidate idioms with fixed length (for more details, please refer to the competition website)."
18012,http://vision.cs.duke.edu/DukeMTMC/data/misc/DukeMTMC-VideoReID.zip,[Direct Link],DukeMTMC-VideoReID: ,"
"
18018,xsum-human-evaluation-data.tar.gz,Human Evaluation Data,"
","
"
18030,https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition,Epileptic  Seizure  Recognition  Dataset (2001),"
",". Real Multivariate Time Series dataset contains 11,500 instances and 179 attributes."
18030,http://www.epilepsiae.eu/project_outputs/european_database_on_epilepsy,EPILEPSIAE project database,"
",. Real Multivariate Time Series dataset contains 30 instances.
18034,http://www.cim.mcgill.ca/dscnn-data/ModelNet40_rendered_rgb.tar,tarball file,"The experiment scripts are included in folder exp_scripts. See comments in the scripts for detail. Before you proceed, please download Modelnet40 dataset from http://modelnet.cs.princeton.edu/ and render it using tools provided in utils (credit to MVCNN authors). Here we provide rendered rgb images of ModelNet40 dataset in a ",", rendered depth maps in a "
18034,http://www.cim.mcgill.ca/dscnn-data/Modelnet40_Depth.tar,tarball file,", rendered depth maps in a ",", rendered surface normal maps in a "
18034,http://www.cim.mcgill.ca/dscnn-data/Modelnet40_Surf.tar,tarball file,", rendered surface normal maps in a ", for your convenience.
18040,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,KITTI depth completion,Download the , dataset (
18040,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI raw, and download the , dataset using 
18040,https://www.cityscapes-dataset.com/,Cityscapes,Download the , dataset and place it in 
18057,https://github.com/visipedia/fg_geo/blob/master/data/yfcc100m_geolocated_inat2017species.csv.zip,download,YFCC100M with Geolocation (,)
18058,https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/summarization/multi_news.py,"

Tensorflow datasets

","
","
"
18059,https://seaborn.pydata.org/,seaborn,"
", 0.9.0
18066,https://github.com/ArrowLuo/DOER/tree/master/data/amazon,amazon,Amazon Embedding can be found in ,", and Yelp Embedding can be found in "
18066,https://github.com/ArrowLuo/DOER/tree/master/data/yelp,yelp,", and Yelp Embedding can be found in ",.
18079,data/sarcasm_data.json,"
data/sarcasm_data.json
",The annotations and transcripts of the audiovisual clips are available at ,". Each instance in the JSON file is allotted one identifier (e.g. ""1_60"") which is a dictionary of the following items:"
18079,http://nlp.stanford.edu/data/glove.840B.300d.zip,"Common Crawl pretrained GloVe word vectors of size 300d, 840B tokens",Download , somewhere.
18081,https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/yelp,Yelp dataset,Download the pre-processed datasets from , and 
18081,https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon,Amazon dataset, and ,", which include"
18089,https://worldview.earthdata.nasa.gov/,NASA Worldview,We are using satellite images provided by ,. Go to the 
18098,https://www.kaggle.com/mtaboada/sfu-opinion-and-comments-corpus-socc,Download SFU Opinion and Comments Corpus,"
","
"
18106,https://pymia.readthedocs.io/en/latest/examples.dataset.html,pymia documentation,). Please refer to the , on how to create your own dataset.
18115,#data-understanding,Data Understanding,"
","
"
18115,#data-validation,Data Validation,"
","
"
18115,#data-preparation,Data Preparation,"
","
"
18115,https://bigdata.cs.ut.ee/ismartml/,'Tool',"
",  |
18115,https://databricks.com/product/automl-on-databricks#resource-link,"
Website
", |            x                                                   | |   Databricks  | 2019 |  Python  | SparkMlib                  | Hyperopt                                              |       x       |  √ |                         × , |            x                                                   |
18115,https://github.com/stanford-futuredata/macrobase,"
Github
", | ,"
"
18115,https://toolbox.google.com/datasetsearch,"
URL
",2018 | Google Search Engine for Datasets | ,"
"
18115,https://datahub.csail.mit.edu/www/,"
URL
", | ,"
"
18123,https://github.com/facebookresearch/fastMRI/blob/8abe6eaeeb3d4504f26dc77adffb02a4be41d6f4/fastmri/data/subsample.py#L344-L475,"Offset Sampling Improves Deep Learning based Accelerated MRI Reconstructions by Exploiting Symmetry (A. Defazio, 2019)","
","
"
18123,https://github.com/facebookresearch/fastMRI/tree/master/fastmri/data/README.md,data README,The , has a bare-bones example for how to load data and incorporate data transforms. This 
18134,https://github.com/nju-websoft/BootEA/tree/master/dataset,here,The raw datasets of DWY100K can also be found ,.
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_3DBuildingModel.md,3-D Building Model,| Data/Layer Name | Description | Preview | | ---|---|---| ," | 3-D Building Models representing every NYC building present in the 2014 aerial survey. Models are based on a hybrid of the CItyGML Level of Detail (LOD) 1 (simple/prismatic buildings with flat roof detail) and LOD 2 (includes roof structure details) with approximately 100 iconic buildings modeled to LOD 2.  Highlights of the model include the differentiation of building components including roof, facades, and ground plane.  | "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_AddressPoint.md,Address Point,"
", | Address points are part of the Citywide Street Centerline (CSCL) database. The data represent visually identifiable and Borough President assigned addresses within NYC.  Placement of the address points are approximately five feet within the respective building footprint along  street front the address is assigned.  | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_AerialImagery.md,Aerial Imagery,"
", | Raster file of vertical aerial imagery covering New York City. Imagery is captured every 2 years during spring/summer months and corrected through computer processes to remove distortions caused by elevation changes and camera angles. | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_BuildingFootprints.md,Building Footprints,"
", | Building footprints represent the perimeter outline of each building. Divisions between adjoining buildings are determined by tax lot divisions.  | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_BIDs.md,Business Improvement Districts,"
"," | Polygons representing the extent of Business Improvement Districts (BIDs). A BID is a public/private partnership in which property and business owners elect to make a collective contribution to the maintenance, development, and promotion of their commercial district.  | "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_Contours.md,Contours,"
", | Continuous lines representing points of equal elevation  at 2-ft intervals. | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_DigitalTaxMap.md,Digital Tax Map,"
"," | The official tax maps for the City of New York are maintained by the Department of Finance, Tax Map Office. Tax maps show the lot lines, the block and lot numbers, the street names, lot dimensions, and easements. The Digital Tax Map includes features and tables for "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_AirRightsCondos.md,Air Rights Condos," | The official tax maps for the City of New York are maintained by the Department of Finance, Tax Map Office. Tax maps show the lot lines, the block and lot numbers, the street names, lot dimensions, and easements. The Digital Tax Map includes features and tables for ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_AirRightsHolders.md,Air Rights Holders,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_AirRightsLots.md,Air Rights Lots,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_Boundary.md,Boundary,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_Condo.md,Condo,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_CondoUnits.md,Condo Units,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_LotFacePossessionHooks.md,Lot Face Possession Hooks,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_REUCLots.md,REUC Lots,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_SubterraneanLots.md,Subterranean Lots,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_TaxBlockPolygon.md,Tax Block Polygon,", ",", "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_TaxLotFace.md,Tax Lot Face,", ",", and "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_TaxLotPolygon.md,Tax Lot Polygon,", and ",. | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_LiDAR_Summary.md,LiDAR,"
"," | Raster files and point clouds from 2010 and 2017 LiDAR capture covering New York City. The 2010 LiDAR includes one hydro-flattened digital elevation model (DEM). The 2017 LiDAR includes classed and unclassed point clouds in laz format, bare earth DEM, hydroflattened DEM, hydro-enforced DEM (filled and unfilled), highest hit, intensity imagery, 8-class land cover, and tree canopy change (2010-2017). | "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_PointsOfInterest.md,Points Of Interest,"
", | Point of Interest (aka Common Place) are point representations of locations that can be referred to by name and may or may not have an address. The data is a compilation of a variety of agency data and is a component of the CSCL database. | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_TidalShoreline.md,Shoreline (Tidally Coordinated),"
"," | A dataset representing the shorelines of tidally influenced areas of New York City. This dataset derived from the 2017 Light Detection and Ranging (LiDAR) data capture. In most areas LiDAR was captured during low tide (defined here as a range between Mean Lower Low Water (MLLW) and MLLW + 30% of the mean tide range). All bathymetric LiDAR and most near-infrared (NIR) LiDAR was captured during low tide. Flights flown over NIR LiDAR shoreline areas that are largely made up of sea walls and riprap were not flown during low tide. See dataset attributes for which areas are tidally-coordinated. Tidal gauges used to determine low tide: Kings Point, Bergen West, Sandy Hook, and The Battery. |"
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_SubwayEntrances.md,Subway Entrances,"
", | Point representing approximate location of NYC Transit Subway Entrances.  | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_SubwayStations.md,Subway Stations,"
", | Point representing approximate location of NYC Transit Subway Stations.  | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_SubwayLines.md,Subway Lines,"
", | Line representing NYC Transit Subway Lines. Lines are optimized for cartographic representation in web applications.  | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_StreetCenterline.md,Street Centerline,"
"," | Street Centerline is a single line representation of New York City streets representing each separate carriageway (e.g., two lines represent the divided roadway of Park Avenue South). The centerlines contain address ranges, traffic directions, road and segment types. Street Centerline is part of the Citywide Street Centerline (CSCL)  database that supports multiple agencies, including the emergency 911 dispatching systems.  | "
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_WiFiHotspots.md,WiFi Hotspots,"
", | Public Wi-Fi Hotspot locations throughout the 5 boroughs including LinkNYC-Citybridge. | 
18135,https://github.com/CityOfNewYork/nyc-geo-metadata/blob/master/Metadata/Metadata_ZipCodeBoundaries.md,Zip Code Boundaries,"
", | Geographic approximation of USPS zip code boundaries.  | 
18143,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove, is the path of your decompressed , embedding.
18145,dataset,dataset,The , folder contains the 
18156,https://github.com/tensorflow/datasets/actions/workflows/pytest.yml,"
Unittests
","
","
"
18156,https://badge.fury.io/py/tensorflow-datasets,"
PyPI version
","
","
"
18156,https://www.tensorflow.org/datasets/overview,"
Tutorial
","
","
"
18156,https://www.tensorflow.org/datasets/api_docs/python/tfds,"
API
","
","
"
18156,https://www.tensorflow.org/datasets/catalog/overview#all_datasets,"
Catalog
","
","
"
18156,https://www.tensorflow.org/datasets/overview,"
getting started guide
","To install and use TFDS, we strongly encourage to start with our ",. Try it interactively in a 
18156,https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb,Colab notebook,. Try it interactively in a ,.
18156,https://www.tensorflow.org/datasets/overview,Tutorials and guides,"
","
"
18156,https://www.tensorflow.org/datasets/catalog/overview#all_datasets,available datasets,List of all ,"
"
18156,https://www.tensorflow.org/datasets/api_docs/python/tfds,API reference,The ,"
"
18156,https://www.tensorflow.org/guide/data_performance,best practices,: TFDS follows , and can achieve state-of-the-art speed
18156,https://github.com/tensorflow/datasets/issues,feedback,"If those use cases are not satisfied, please send us ",.
18156,https://www.tensorflow.org/datasets/add_dataset,our guide,Adding a dataset is really straightforward by following ,.
18156,https://github.com/tensorflow/datasets/issues/new?assignees=&labels=dataset+request&template=dataset-request.md&title=%5Bdata+request%5D+%3Cdataset+name%3E,Dataset request GitHub issue,Request a dataset by opening a ,.
18156,https://github.com/tensorflow/datasets/labels/dataset%20request,set of requests,And vote on the current , by adding a thumbs-up reaction to the issue.
18160,data/README.md,README,"), but the ", file gives details about software and packages to install and further information about the data.
18175,https://github.com/cocodataset/cocoapi,COCOAPI,Install ,:
18185,https://duc.nist.gov/data.html,the DUC website,Download the DUC01/02/04 data from , and extract the data to folder 'data/'.
18188,https://kaggle.com/c/freesound-audio-tagging-2019/data,from Kaggle,Download the dataset ,: 
18189,#datasets,Datasets,"
","
"
18189,https://www.ims.uni-stuttgart.de/data/durel,Dataset,| Dataset | Language | Corpus 1 | Corpus 2 | Download | Comment | | --- | --- | --- | --- | --- | --- | | DURel | German | DTA18 | DTA19  | ,", "
18189,https://www.ims.uni-stuttgart.de/data/surel,Dataset, | | SURel | German | SDEWAC | COOK | ,", "
18189,https://www.ims.uni-stuttgart.de/data/lsc-simul,Dataset, | | SemCor LSC | English | SEMCOR1 | SEMCOR2 | ,", "
18189,https://www.ims.uni-stuttgart.de/data/lsc-simul,Corpora,", ", | | | SemEval Eng | English | CCOHA 1810-1860 | CCOHA 1960-2010 | 
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Dataset, | | | SemEval Eng | English | CCOHA 1810-1860 | CCOHA 1960-2010 | ,", "
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Corpora,", ", | | | SemEval Ger | German | DTA 1800-1899 | BZND 1946-1990 | 
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Dataset, | | | SemEval Ger | German | DTA 1800-1899 | BZND 1946-1990 | ,", "
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Corpora,", ", | | | SemEval Lat | Latin | LatinISE -200-0 | LatinISE 0-2000 | 
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Dataset, | | | SemEval Lat | Latin | LatinISE -200-0 | LatinISE 0-2000 | ,", "
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Corpora,", ", | | | SemEval Swe | Swedish | Kubhist2 1790-1830 | Kubhist2 1895-1903 | 
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Dataset, | | | SemEval Swe | Swedish | Kubhist2 1790-1830 | Kubhist2 1895-1903 | ,", "
18189,https://www.ims.uni-stuttgart.de/data/sem-eval-ulscd,Corpora,", ", | | | RuSemShift1 | Russian | RNC 1682-1916 | RNC 1918-1990 | 
18189,https://github.com/diacr-ita/data/tree/master/test,Dataset, | | | DIACR-Ita | Italian | Unità 1945-1970 | Unità 1990-2014 | ,", "
18196,https://github.com/letitbeat/data-path-discovery/blob/master/LICENSE,LICENSE.md,This project is licensed under the MIT License - see the , file for details
18198,https://github.com/ratishsp/mlb-data-scripts,mlb-data-scripts,. Scripts to create the MLB dataset are available at ,.
18198,https://github.com/harvardnlp/boxscore-data,boxscore-data repo,The boxscore-data json files can be downloaded from the ,.
18199,https://github.com/harvardnlp/boxscore-data,boxscore-data repo,The boxscore-data associated with the above paper can be downloaded from the ,", and this README will go over running experiments on the RotoWire portion of the data; running on the SBNation data (or other data) is quite similar."
18199,https://github.com/harvardnlp/boxscore-data,boxscore-data repo, models and results reflecting the newly cleaned up data in the , are now given below.
18205,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC," for different datasets, including the ", datasets. These can be used as a reference for customization according to specific needs.
18208,https://storage.googleapis.com/disentanglement_dataset/Final_Dataset/mpi3d_toy.npz,link,mpi3d_toy (simplistic rendered):  ,"
"
18208,https://storage.googleapis.com/disentanglement_dataset/Final_Dataset/mpi3d_realistic.npz,link,mpi3d_realistic (realistic rendered): ,"
"
18208,https://storage.googleapis.com/disentanglement_dataset/Final_Dataset/mpi3d_real.npz,link,mpi3d_real (real-world images): ,"
"
18214,corpus_tree.txt,this, directory should look like ,.
18214,data_tree.txt,this, should now look like ,)
18225,data%20preprocessing.ipynb,data preprocessing notebook,"First, use the ", to prepare your data for the model. You should apply the code for different splits separately.
18227,http://dataspartan.co.uk/,DataSpartan, group at Imperial College London. Kenneth Co is partially supported by ,.
18229,./data/make_datafiles.py,data/make_datafiles.py,You can use our preprocessing codes (, and 
18229,./data/rouge_not_a_wrapper.py,data/rouge_not_a_wrapper.py, and ,) and follow their instrunctions of 
18229,https://github.com/abisee/cnn-dailymail#option-2-process-the-data-yourself,Option 2,) and follow their instrunctions of , to obtain the preprocessed data for our model.
18244,https://github.com/yyaghoobzadeh/WIKI-PSE/tree/master/dataset,dataset,"(For only the (word, semantic-classes) datasets, please visit this directory: ",.)
18244,http://cistern.cis.lmu.de/WIKI-PSE/data.zip,data.zip,"
","
"
18248,http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz,direct link,) which can be downloaded from  http://ai.stanford.edu/~amaas/data/sentiment/ or this ,.
18261,https://www.kaggle.com/c/avito-context-ad-clicks/data,Full Avito data,"
","
"
18265,https://github.com/Samurais/wikidata-corpus,wikidata-corpus,data is built based on ,.
18265,https://github.com/Samurais/wikidata-corpus,wikidata-corpus,"
","
"
18268,data,data/,"
", contains the log data from our experiments.
18268,data,data/," contains the plots and tables, based on the contents of ",", that are shown in our paper."
18268,data/certify/cifar10/finetune_cifar_from_imagenetPGD2steps/PGD_10steps_30epochs_multinoise/2-multitrain/eps_64/cifar10/resnet110/noise_0.12/test/sigma_0.12,these results,. You should get similar to ,.
18268,data/certify/cifar10/finetune_cifar_from_imagenetPGD2steps/PGD_10steps_30epochs_multinoise/2-multitrain/eps_64/cifar10/resnet110/,our certification results,This generate the below plot using ,. Modify the paths inside 
18268,data,data/,This code reads from ," i.e. the logs that were generated when we certifiied our trained models, and automatically generates the tables and figures that we present in the paper."
18273,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-2,"The model comes with instructions to train: word level language models over the Penn Treebank (PTB), "," (WT2), and "
18273,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-103," (WT2), and ", (WT103) datasets. (The code and pre-trained model for WikiText-103 will be merged into the branch soon.)
18280,http://acube.di.unipi.it/tmn-dataset/,TagMyNews, from ,". One data sample per line, the text and the label are separated by "
18297,https://www.kaggle.com/google/google-landmarks-dataset,here,"As a reference, the previous version of the Google Landmarks dataset (referred to as Google Landmarks dataset v1, GLDv1) is available ",". Note that we do NOT plan to maintain GLDv1, so we STRONGLY encourage you to use mainly GLDv2."
18297,https://s3.amazonaws.com/google-landmark/metadata/train.csv,"
https://s3.amazonaws.com/google-landmark/metadata/train.csv
", is an integer. Available at: ,.
18297,https://s3.amazonaws.com/google-landmark/metadata/train_clean.csv,"
https://s3.amazonaws.com/google-landmark/metadata/train_clean.csv
", is a space-separated list of string train image IDs. Available at: ,. Courtesy of team 
18297,https://s3.amazonaws.com/google-landmark/metadata/train_attribution.csv,"
https://s3.amazonaws.com/google-landmark/metadata/train_attribution.csv
"," is a 16-character string, and the other fields are strings of variable length. Available at: ",.
18297,https://s3.amazonaws.com/google-landmark/metadata/train_label_to_category.csv,"
https://s3.amazonaws.com/google-landmark/metadata/train_label_to_category.csv
", is a Wikimedia URL referring to the class definition. Available at: ,.
18297,https://s3.amazonaws.com/google-landmark/metadata/index.csv,"
https://s3.amazonaws.com/google-landmark/metadata/index.csv
", is a 16-character string. Available at: ,.
18297,https://s3.amazonaws.com/google-landmark/metadata/index_image_to_landmark.csv,"
https://s3.amazonaws.com/google-landmark/metadata/index_image_to_landmark.csv
", is an integer. Available at: ,.
18297,https://s3.amazonaws.com/google-landmark/metadata/index_label_to_category.csv,"
https://s3.amazonaws.com/google-landmark/metadata/index_label_to_category.csv
", is a Wikimedia URL referring to the class definition. Available at: ,.
18297,https://s3.amazonaws.com/google-landmark/metadata/test.csv,"
https://s3.amazonaws.com/google-landmark/metadata/test.csv
", is a 16-character string. Available at: ,.
18297,https://github.com/tensorflow/models/tree/master/research/delf/delf/python/datasets/google_landmarks_dataset,here,"We make available the ResNet101-ArcFace baseline model from the paper, see instructions ",.
18297,https://github.com/tensorflow/models/tree/master/research/delf/delf/python/google_landmarks_dataset,DELF github repository,"The metric computation scripts have been made available, via the ",", see the python scripts "
18314,https://sites.google.com/view/reside-dehaze-datasets,RESIDE,"[2]. A Benchmark for Single Image Dehazing, ","
"
18315,https://seaborn.pydata.org/index.html,seaborn,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/gh-pages/Picture1.png,Table 1,"The chemical compound graph datasets are in “.sdf” or “.smi” format, and other graph dataset are represented as “.nel” format. All these graph datasets can be handle by frequent subgraph miner packages such as Moss [1] or other softwares. These graphs can be easily converted to other formats handled by Matlab or other softwares. A summarization of our graph datasets is given in ",.
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/NCI_full.zip?raw=true,NCI_full.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/NCI_balanced.zip?raw=true,NCI_balanced.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/PTC_pn.zip?raw=true,PTC_pn.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/PTC_mtl.zip?raw=true,PTC_mtl.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/DBLP_v1.zip?raw=true,DBLP_v1.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/Twitter-Graph.zip?raw=true,Twitter-Graph.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/Brain.zip?raw=true,Brain.zip,"
","
"
18336,https://github.com/shiruipan/graph_datasets/blob/master/Graph_Repository/example.sdf,example.sdf,"
","
"
18340,http://jmcauley.ucsd.edu/data/amazon/links.html,Amazon Electronic,"
","
"
18340,https://www.yelp.com/dataset/challenge,Yelp Restaurant,"
","
"
18340,http://cseweb.ucsd.edu/~jmcauley/datasets.html#multi_aspect,RateBeer,"
","
"
18342,https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+April+9,https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+April+9,2020.04.09 ,"
"
18342,https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-+ONNX+Community+Workshop+-+October+14,https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-+ONNX+Community+Workshop+-+October+14,2020.10.14 ,"
"
18342,https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=35160391,https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=35160391,2021.03.24 ,"
"
18342,https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=46989689,https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=46989689,2021.10.21 ,"
"
18342,https://wiki.lfaidata.foundation/display/DL/ONNX+Community+Day+-+June+24,https://wiki.lfaidata.foundation/display/DL/ONNX+Community+Day+-+June+24,2022.06.24 ,"
"
18360,./data/creat_BERT_embedding.py,creat_BERT_embedding.py,BERT: Refer to , and 
18360,./data/creat_BERT_embedding_2_targets.py,creat_BERT_embedding_2_targets.py, and , to create BERT Embedding if need.
18367,http://cocodataset.org/#download,here,Download the coco2014 dataset(train and val) from ,. You should put the folder 
18367,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,link,Download the preprocessed COCO captions from , from Karpathy's homepage and unzip it to directory 
18375,https://datashare.is.ed.ac.uk/handle/10283/3368,this,The WikiCatSum dataset is available in , repository and also on HuggingFace datasets (follow this 
18375,https://huggingface.co/datasets/GEM/wiki_cat_sum,link, repository and also on HuggingFace datasets (follow this ,).
18379,https://github.com/facebookresearch/UnsupervisedMT/blob/master/NMT/get_data_enfr.sh,UnsupervisedMT/NMT/get_data_enfr.sh,", first prepare large corpora of cloze questions (potentially using the functionality in this repository) and a large corpus of natural questions. Preprocess these corpora by adapting ",", and train using the example script in "
18384,https://www.travis-ci.org/maxfriedrich/deid-training-data,"
Build Status
","
","
"
18386,./data/,data,The ," is provided as JSON files exported from the StatsBomb Data API, in the following structure:"
18386,./data/competitions.json,"
competitions.json
",Competition and seasons stored in ,.
18386,./data/matches/,"
matches
","Matches for each competition and season, stored in ",". Each folder within is named for a competition ID, each file is named for a season ID within that competition."
18386,./data/events/,"
events
","Events and lineups for each match, stored in ", and 
18386,./data/lineups/,"
lineups
", and , respectively. Each file is named for a match ID.
18386,./data/three-sixty/,"
three-sixty
","StatsBomb 360 data for selected matches, stored in ",. Each file is named for a match ID.
18402,./data/mounted_police.jpg,mounted_police.jpg,"
",", by "
18402,./data/people_drinking.jpg,people_drinking.jpg,"
",", by "
18402,./data/rugby_players.jpg,rugby_players.jpg,"
",", by "
18405,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D website,"FlyingThings3D: Download and unzip the ""Disparity"", ""Disparity Occlusions"", ""Disparity change"", ""Optical flow"", ""Flow Occlusions"" for DispNet/FlowNet2.0 dataset subsets from the ", (we used the paths from 
18405,https://lmb.informatik.uni-freiburg.de/data/FlyingThings3D_subset/FlyingThings3D_subset_all_download_paths.txt,this file, (we used the paths from ,", now they added torrent downloads) . They will be upzipped into the same directory, "
18405,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI Scene Flow Evaluation 2015,KITTI Scene Flow 2015 Download and unzip , to directory 
18406,http://resources.arcgis.com/en/communities/geodata/,ArcGIS Geodata Resource Center,"
","
"
18412,data/README.md,Datasets,"
","
"
18412,dataset_creation/README.md,Tools for Creating our datasets,"
","
"
18419,http://labda.inf.uc3m.es/ddicorpus,DDI,"
"," extraction 2013 corpus is a collection of 792 texts selected from the DrugBank database and other 233 Medline abstracts In our benchmark, we use 624 train files and 191 test files to evaluate the performance and report the macro-average F1-score of the four DDI types."
18419,https://academic.oup.com/database/article/doi/10.1093/database/bay073/5055578,Extracting chemical-protein relations with ensembles of SVM and deep learning models,: Peng et al. 2018. ,". Database: the journal of biological databases and curation, 2018."
18430,https://github.com/lberrada/InferSent/tree/ali-g#download-datasets,preparation instructions,To reproduce the SNLI experiments: follow the , and run  
18442,https://github.com/oscartackstrom/sentence-sentiment-data,Täckström Dataset,", ",", "
18444,https://github.com/tpimentelms/meaning2form/tree/master/datasets/northeuralex,NorthEuraLex,Information about used datasets and where to get them from is available in: , and 
18444,https://github.com/tpimentelms/meaning2form/tree/master/datasets/celex,CELEX, and ,.
18448,https://github.com/allenai/allennlp/blob/5f9fb419273f99c949ccdabab22fdc8e9b895c1c/allennlp/data/dataset_readers/dataset_utils/ontonotes.py#L274,here,", remember to add ""pos_tag != '-'"" in line 274 ",.
18456,https://github.com/Tessil/hat-trie#wikipedia-dataset,Wikipedia dataset,The benchmark protocol is the same as for the ,.
18457,https://www.kaggle.com/jessicali9530/celeba-dataset,https://www.kaggle.com/jessicali9530/celeba-dataset, from ," (or other source), and put it in "
18457,https://github.com/deepmind/dsprites-dataset,https://github.com/deepmind/dsprites-dataset, from , and put it in the 
18461,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,website,"Download kitti images (from left and right cameras), ground-truth files (labels), and calibration files from their ", and save them inside the 
18461,http://vhosts.eecs.umich.edu/vision//activity-dataset.html,collective activity dataset,To evaluate on of the ," (without any training) we selected 6 scenes that contain people talking to each other. This allows for a balanced dataset, but any other configuration will work."
18461,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,website,We provide evaluation on KITTI for models trained on nuScenes or KITTI. Download the ground-truths of KITTI dataset and the calibration files from their ,. Save the training labels (one .txt file for each image) into the folder 
18465,https://www.med.upenn.edu/sbia/brats2018/data.html,BraTS 2018 dataset,We trained with the ,", which is available from the organizers of the "
18466,http://rxinformatics.umn.edu/data/MayoSRS.csv,here,We evaluated BioWordVec for medical word pair similarity. We used the MayoSRS (101 medical term pairs; download ,) and UMNSRS_similarity (566 UMLS concept pairs; download 
18473,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow,"
","
"
18473,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI,"
","
"
18473,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow,You have to download the , and 
18473,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI, and , datasets. The structures of the datasets are shown in below.
18473,#1-train-sDNet-from-scratch-on-sceneflow-dataset,1 Train SDNet from Scratch on SceneFlow Dataset,"
","
"
18473,#2-train-sdnet-on-kitti-dataset,2 Train SDNet on KITTI Dataset,"
","
"
18477,https://sites.google.com/view/totally-looks-like-dataset,Totally Looks Like,"Let's build a text-to-image search using CLIP-as-service. Namely, a user can input a sentence and the program returns matching images. We'll use the ", dataset and 
18477,https://sites.google.com/view/totally-looks-like-dataset,Totally Looks Like,"Alternatively, you can go to "," official website, unzip and load images:"
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/labels.npy,labels,"We provide the probabilities, embedding and ", of each image in the ImageNet validation so that the results can be reproduced easily.
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_softmax.npy,FixResNet50_Softmax.npy,| Model | Softmax | Embedding | |:---:|:------------------------------------------------------------:|:------------------------------------------------------------:| | FixResNet-50|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_embedding.npy,FixResNet50Embedding.npy, |, | | FixResNet-50 (
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_softmax_v2.npy,FixResNet50_Softmax_v2.npy,)|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50_embedding_v2.npy,FixResNet50Embedding_v2.npy, |, | | FixResNet-50 CutMix|
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_softmax.npy,FixResNet50_CutMix_Softmax.npy, | | FixResNet-50 CutMix|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_embedding.npy,FixResNet50_CutMix_Embedding.npy, |, | | FixResNet-50 CutMix (
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_softmax_v2.npy,FixResNet50_CutMix_Softmax_v2.npy,)|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/ResNet50CutMix_embedding_v2.npy,FixResNet50_CutMix_Embedding_v2.npy, |, | | FixPNASNet-5|
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/PNASNet_softmax.npy,FixPNASNet_Softmax.npy, | | FixPNASNet-5|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/PNASNet_embedding.npy,FixPNASNet_Embedding.npy, |, | | FixResNeXt-101 32x48d|
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_softmax.npy,FixResNeXt101_32x48d_Softmax.npy, | | FixResNeXt-101 32x48d|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_embedding.npy,FixResNeXt101_32x48d_Embedding.npy, |, | | FixResNeXt-101 32x48d  (*)|
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_softmax_v2.npy,FixResNeXt101_32x48d_Softmax_v2.npy, | | FixResNeXt-101 32x48d  (*)|, |
18486,https://dl.fbaipublicfiles.com/FixRes_data/FixRes_Extracted_Features/IGAM_Resnext101_32x48d_embedding_v2.npy,FixResNeXt101_32x48d_Embedding_v2.npy, |, |
18498,https://motchallenge.net/data/MOT17Det.zip,MOT17Det,Download MOT data Dataset can be downloaded here: ,", "
18498,https://motchallenge.net/data/MOT16Labels.zip,MOT16Labels,", ",", "
18498,https://motchallenge.net/data/MOT16-det-dpm-raw.zip,MOT16-det-dpm-raw,", ", and 
18498,https://motchallenge.net/data/MOT17Labels.zip,MOT17Labels, and , . 2. Unzip all the data by executing:
18498,https://motchallenge.net/data/MOT17Det.zip,MOT17Det,Download MOT Dataset can be downloaded here: ,", "
18498,https://motchallenge.net/data/MOT16Labels.zip,MOT16Labels,", ",", "
18498,https://motchallenge.net/data/MOT16-det-dpm-raw.zip,MOT16-det-dpm-raw,", ", and 
18498,https://motchallenge.net/data/MOT17Labels.zip,MOT17Labels, and ,.
18498,https://motchallenge.net/data,"
motchallenge
",: ,"
"
18499,https://www.tensorflow.org/tutorials/generative/data_compression,lossy data compression tutorial,"You can use this library to build your own ML models with end-to-end optimized data compression built in. It's useful to find storage-efficient representations of your data (images, features, examples, etc.) while only sacrificing a small fraction of model performance. Take a look at the ", or the 
18499,https://www.tensorflow.org/datasets/catalog/clic,CLIC dataset,", the ", will be downloaded using TensorFlow Datasets.
18504,./data/nus21,/data/nus21, and put it into a specific directory. Then you should modify the prefixs of all paths in ,"
"
18504,./data/imagenet,/data/imagenet,Please download the dataset (ILSVRC >= 2012) and put it into a specific directory. Then you should modify the prefixs of all paths in ,"
"
18508,http://robotips.fr/data/bin-poppler-win32.tar.gz,http://robotips.fr/data/bin-poppler-win32.tar.gz,includes : , (uncompress to uconfig/bin/)
18508,http://robotips.fr/data/include-poppler.tar.gz,http://robotips.fr/data/include-poppler.tar.gz,DLLs : , (uncompress to uconfig/)
18517,#where-can-i-get-a-dataset-for-deep-xi,Where can I get a dataset for Deep Xi?,"
","
"
18517,https://ieee-dataport.org/open-access/deep-xi-dataset,Deep Xi dataset,Each available model is trained using the ,. Please see 
18517,https://ieee-dataport.org/open-access/deep-xi-dataset,Deep Xi dataset,Average objective scores obtained over the conditions in the test set of the ,. 
18526,./data/README.md,data/README,"To run on the enwik8 dataset, first download and prepare the data (see ", for details):
18532,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove.840B.300d,"GloVe vectors are required, please download ", first. run 
18550,https://github.com/tuetschek/e2e-dataset,E2E challenge,"
","
"
18551,https://www.wikidata.org/,Wikidata,"Additionally, you will need embeddings for entities/relations in the "," knowledge graph, as well as access to the knowledge graph itself (in order to look up entity aliases/related entities). For convenience, we provide pre-trained embeddings and pickled dictionaries containing the relevant portions of Wikidata "
18557,data/NELL.08m.165.zip,a snapshot here, or ,"
"
18567,http://crcv.ucf.edu/data/ucf-qnrf/,here,. Download UCF-QNRF dataset from ,.
18571,./data/README.md,this README,More information about obtaining and using the LFT and ONB datasets can be found in ,.
18581,https://archive.org/details/image-editing-dataset,mirror,", ",).
18581,https://github.com/google-research-datasets/birds-to-words,Github, and ,.
18587,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb I and II,The x-vector extractor and PLDA parameters were trained on ," using data augmentation (additive noise and reverberation), while the whitening transformation was learned from the DIHARD II development set."
18609,./dataset_tool.py,dataset_tool.py,"To obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided ",:
18626,#data,Data,"
","
"
18626,#lmdb-data,LMDB Data,"
","
"
18626,#raw-data,Raw Data,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/trrosetta.tar.gz,here,. We provide a pytorch implementation and dataset to allow you to play around with the model. Data is available ,". This is the same as the data in the original paper, however we've added train / val split files to allow you to train your own model reproducibly. To use this model"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/pfam.tar.gz,Pretraining Corpus (Pfam),"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/secondary_structure.tar.gz,Secondary Structure,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/proteinnet.tar.gz,Contact (ProteinNet),"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/remote_homology.tar.gz,Remote Homology,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/fluorescence.tar.gz,Fluorescence,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/stability.tar.gz,Stability,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/pfam.tar.gz,Pretraining Corpus (Pfam),"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/secondary_structure.tar.gz,Secondary Structure,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/proteinnet.tar.gz,Contact (ProteinNet),"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/remote_homology.tar.gz,Remote Homology,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/fluorescence.tar.gz,Fluorescence,"
","
"
18626,http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/stability.tar.gz,Stability,"
","
"
18627,http://www.qizhexie.com/data/RACE_leaderboard.html,RACE accuracy,Model | , | SQuAD1.1 EM | SQuAD2.0 EM --- | --- | --- | --- BERT-Large | 72.0 | 84.1 | 78.98 XLNet-Base | | | 80.18 XLNet-Large | 
18627,https://www.cs.cmu.edu/~glai1/data/race/,RACE,The code for the reading comprehension task , is included in 
18627,https://www.cs.cmu.edu/~glai1/data/race/,official website,(1) Download the RACE dataset from the , and unpack the raw data to 
18633,/data/original_data,data/original_data/,Put the micro-lens image arrays into , and ground-truths into 
18633,/data/original_GT,data/original_GT/, and ground-truths into ,.
18633,/data/val/annotations/,val/annotations/,Copy the labels to the ,: 
18633,/data/val/JPGImages/,val/JPGImages/,Copy the original images to the ,: 
18633,/data/train/,data/train/,Put training and testing data in , and 
18633,/data/val/,data/val/, and ,. 5-fold cross-validation is used in the project. The generated image index are in the 
18638,https://github.com/gabegrand/adversarial-vqa/blob/master/data_prep/vqa_v2.0_cp/download_vqa_2.0_cp.sh,script,. You can use this , to download VQA-CP v2. The 
18638,https://github.com/gabegrand/adversarial-vqa/blob/master/data_prep/vqa_v2.0_cp/make_trainval_split.py,"
make_trainval_split.py
", to download VQA-CP v2. The , script will generate the train / valid / test split used in our paper.
18638,https://github.com/gabegrand/adversarial-vqa/blob/master/data_prep/data_preprocess.md,https://github.com/gabegrand/adversarial-vqa/blob/master/data_prep/data_preprocess.md,Instructions for preprocessing can be found at ,.
18640,https://github.com/thunlp/OpenNRE#provided-data,nyt.tar file,download the ,.
18646,https://papers.nips.cc/paper/8674-data-cleansing-for-models-trained-with-sgd,Data Cleansing for Models Trained with SGD,"S. Hara, A. Nitanda, T. Maehara, ",". Advances in Neural Information Processing Systems 32 (NeurIPS'19), 2019."
18653,http://usewod.org/data-sets.html,USEWOD," (2) traces of DBpedia LDF server's real log, from "," dataset, for the period of 14th October 2014-27th February 2015."
18657,https://github.com/deepmind/dsprites-dataset,dSprites dataset github repository,To run experiments with the dSprites dataset you will need to clone the , into 
18668,https://github.com/bitextor/bicleaner-data/releases/latest,downloaded, or , as a part of a language pack. You just need to 
18668,https://github.com/bitextor/bicleaner-data/releases/latest,bicleaner-data,In case you need to train a new classifier (i.e. because it is not available in the language packs provided at ,"), you can use "
18668,https://fasttext.cc/docs/en/supervised-tutorial.html#getting-and-preparing-the-data,convention, according to FastText ,. It should be lowercased and tokenized.
18670,https://github.com/jorgegus/autotext_data,Datasets,Public data: ,"
"
18676,https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets,DENSE dataset webpage,Download the benchmark data from the ,.
18681,https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms,here,All the readily available transforms are described ,.
18681,[here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).,On the fly data processing,: ,"
"
18682,#datasets,Datasets,"
","
"
18685,https://github.com/Jeffrey-Ede/datasets/wiki,here,Our training dataset containing 161069 crops from STEM images is available ,.
18691,http://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/,MAPS,obtain the , dataset
18692,https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data),german-credit,"
","
"
18692,https://archive.ics.uci.edu/ml/datasets/student+performance,student-performance,"
","
"
18702,english/data_to_text_generation.md,Data-to-Text Generation,"
","
"
18703,data/exp/few-shot-setup/NLP-TDMS/paperVersion,data/exp/few-shot-setup/NLP-TDMS/paperVersion,We release the training/testing datasets for all experiments described in the paper. You can find them under the data/exp directory. The results reported in the paper are based on the datasets under the ," directory. We later further clean the datasets (e.g., remove five pdf files from the testing datasets which appear in the training datasets with a different name) and the clean version is under the "
18703,data/exp/few-shot-setup/NLP-TDMS,data/exp/few-shot-setup/NLP-TDMS," directory. We later further clean the datasets (e.g., remove five pdf files from the testing datasets which appear in the training datasets with a different name) and the clean version is under the ", folder. Below we illustrate how to run experiments on the NLP-TDSM dataset in the few-shot setup to extract TDM pairs.
18703,./data/exp/few-shot-setup/NLP-TDMS/,directory with our train and test data, to point to the ,", we can run the textual entailment system with  "
18703,data/NLP-TDMS/downloader/README.md,README,Follow the instructions in the , in 
18703,data/NLP-TDMS/downloader/,data/NLP-TDMS/downloader/, in , to download the entire collection of raw PDFs of the NLP-TDMS dataset.  The downloaded PDFs can be moved to 
18703,./data/NLP-TDMS/pdfFile,data/NLP-TDMS/pdfFile, to download the entire collection of raw PDFs of the NLP-TDMS dataset.  The downloaded PDFs can be moved to ," (i.e., "
18703,nlpLeaderboard/src/main/java/com/ibm/sre/data/corpus/NlpTDMSReader.java,NlpTDMSReader,We release the parsed NLP-TDMS and ARC-PDN corpora. , and 
18703,nlpLeaderboard/src/main/java/com/ibm/sre/data/corpus/ArcPDNReader.java,ArcPDNReader, and , in the corpus package illustrate how to read section and table contents from PDF files in these two corpora.
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | - | - |  0.8583 | 0.8456 |-|-|-|-|-|-| | BERTBiLSTMCRF-IO | , | 0.9629 | - | 0.9221 | - | B-BERTBiLSTMCRF-IO | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | 0.9629 | - | 0.9221 | - | B-BERTBiLSTMCRF-IO | , | 0.9635 | - | 0.9229 | - | B-BERTBiLSTMAttnCRF-IO | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | 0.9635 | - | 0.9229 | - | B-BERTBiLSTMAttnCRF-IO | , | 0.9614 | - | 0.9237 | - | B-BERTBiLSTMAttnNCRF-IO | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | 0.9614 | - | 0.9237 | - | B-BERTBiLSTMAttnNCRF-IO | , | 0.9631 | - | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | CSE | , | - | - | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | - | BERT-LARGE | , | 0.966 | - | 0.928 | - | BERT-BASE | 
18709,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,CoNLL-2003, | 0.966 | - | 0.928 | - | BERT-BASE | , | 0.964 | - | 0.924 | -
18714,#dataset-format,Dataset Format,"
","
"
18714,https://pandas.pydata.org/,Pandas,"
","
"
18714,https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f,K-Fold cross validation," To evaluate TwinSVM performance, You can either use ", or split your data into training and test sets. 
18714,https://github.com/mir-am/LightTwinSVM/tree/master/dataset,here," To help you prepare your dataset and test the program, three datasets are included ",.
18720,http://ai.stanford.edu/~amaas/data/sentiment/,here, and IMDB data from ,. Then run the corresponding data processing code to generate data file.
18730,https://github.com/fMoW/dataset,Functional Map of the World (fMoW) Dataset,This code is centered around the ,". The pre-processing and partitioning scripts assume that this dataset is being used, but most of the code is general, and can be adapted to other datasets."
18730,https://github.com/fMoW/dataset,corresponding repo,Instructions for downloading the fMoW dataset are included in the ,". You will need to download the train and val partitions of the ""fMoW-full"" dataset. This data contains the 4 and 8-band multispectral tiff files needed in this code."
18745,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/ukp_sentential_argument_mining_corpus/index.en.jsp,UKP Sentential Argument Mining Corpus,"You can download pre-trained models from here, which were trained on all eight topics of the ",:
18745,https://public.ukp.informatik.tu-darmstadt.de/reimers/2019_acl-BERT-argument-classification-and-clustering/models/argument_classification_ukp_all_data.zip,argument_classification_ukp_all_data.zip,"
","
"
18745,https://public.ukp.informatik.tu-darmstadt.de/reimers/2019_acl-BERT-argument-classification-and-clustering/models/argument_classification_ukp_all_data_large_model.zip,argument_classification_ukp_all_data_large_model.zip,"
","
"
18745,https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/ukp_argument_aspect_similarity_corpus/ukp_argument_aspect_similarity_corpus.en.jsp,UKP Argument Aspect Similarity Corpus, - trained on the complete ,"
"
18761,https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic),Breast Cancer,"
","
"
18761,https://archive.ics.uci.edu/ml/datasets/hepatitis,Hepatitis,"
","
"
18761,https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset),Indian Liver Patient,"
","
"
18761,https://archive.ics.uci.edu/ml/datasets/Cardiotocography,Cardiotocography,"
","
"
18761,https://archive.ics.uci.edu/ml/datasets/thyroid+disease,Thyroid,"
","
"
18761,https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits,Handwritten Digits,"
","
"
18770,https://github.com/vuptran/sesemi/tree/master/datasets,directories, commands in the same directory where the code resides. Ensure the datasets have been downloaded into their respective ,.
18773,https://github.com/EVulHunter/EVulHunter#description-of-test-data,here,The description of test data is ,.
18777,https://datashare.is.ed.ac.uk/handle/10283/2211,VCC2016,Download and unzip , dataset to designated directories.
18780,https://pypi.org/project/DALI-dataset/,pypi,", for reading and working with dali_data. It is stored in this repository and presented as a python package. Dali_code has its own versions controlled by this github. The release and stable versions can be found at ",.
18787,https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5#geting-the-data,here,"Pruning examples use ImageNet 1k dataset which needs to be downloaded beforehand. Use standard instructions to setup ImageNet 1k for Pytorch, e.g. from ",.
18794,#esc-50-dataset-for-environmental-sound-classification,Overview,"
", | 
18794,https://github.com/karoldvl/paper-2015-esc-dataset,ESC: Dataset for Environmental Sound Classification - paper replication data,"
","
"
18796,http://rpg.ifi.uzh.ch/davis_data.html,Event-Camera Dataset,To get a bag file from the ,:
18797,http://rpg.ifi.uzh.ch/davis_data.html,Event Camera Dataset,We provide a minimal example to process events from a plain text file. You can use the event text files from the the ,", e.g. "
18797,http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.zip,here,", e.g. ",.
18797,http://rpg.ifi.uzh.ch/davis_data.html,Event Camera Dataset,"Alternatively, you can also play a rosbag file. You can use rosbags from the from the the ",", e.g. "
18797,http://rpg.ifi.uzh.ch/datasets/davis/shapes_rotation.bag,here,", e.g. ",.
18804,https://www.cityscapes-dataset.com/,Cityscapes,"
", + 
18806,http://lasics.dcc.ufrj.br/~danilo/files/easyesa/data_wikipedia_en_2013.tar.gz,English Wikipedia 2013,Database and Indexes: , (
18806,http://lasics.dcc.ufrj.br/~danilo/files/easyesa/data_wikipedia_en_2013.tar.gz,English Wikipedia 2013,You can download the EasyESA database and indexes for , (
18807,http://millionsongdataset.com/,Million Song Dataset,This repo contains code and information for singing-voice-based ,. It can be used for singing voice or singer relevant tasks.
18808,http://millionsongdataset.com/,Million Song Dataset, This is created from the , by running singing voice detection. MSD-singer repo is 
18808,http://sigsep.github.io/datasets/musdb.html,musdb18, and ,. Create your own mashup. Code is under 
18809,http://cvit.iiit.ac.in/projects/mip/drishti-gs/mip-dataset2/enter.php,DGS, with parameter '--data-dir'. Dataset download link: ,"
"
18811,http://socialcomputing.asu.edu/datasets/BlogCatalog3,Source,BlogCatalog ,"
"
18811,http://leitang.net/code/social-dimension/data/blogcatalog.mat,Preprocessed,"
","
"
18811,http://leitang.net/code/social-dimension/data/flickr.mat,Flickr,"
","
"
18811,http://leitang.net/code/social-dimension/data/youtube.mat,YouTube,"
","
"
18823,data,data,The , directory contains select data for running the experiments. Additional data should be downloaded into the 
18823,data,data, directory contains select data for running the experiments. Additional data should be downloaded into the , directory: 
18824,#data,Data,"
","
"
18824,#preprocessed-dataset,Preprocessed Dataset Section,Preprocessed dataset is available in the ,.
18841,https://timelydataflow.github.io/timely-dataflow/,long-form text,". It is a work in progress, but mostly improving. There is more ", in 
18841,https://github.com/timelydataflow/timely-dataflow/blob/master/timely/examples/simple.rs,timely/examples/simple.rs,", which should allow you to start writing timely dataflow programs like this one (also available in ",):
18841,https://github.com/timelydataflow/timely-dataflow/blob/master/examples/hello.rs,examples/hello.rs,"For a more involved example, consider the very similar (but more explicit) ",", which creates and drives the dataflow separately:"
18841,https://docs.rs/timely/latest/timely/dataflow/operators/index.html,"
Timely dataflow
","
",": Timely dataflow includes several primitive operators, including standard operators like "
18841,https://github.com/timelydataflow/differential-dataflow,"
Differential dataflow
","
",": A higher-level language built on timely dataflow, differential dataflow includes operators like "
18841,https://github.com/frankmcsherry/dataflow_join,a streaming worst-case optimal join implementation,"There are also a few applications built on timely dataflow, including ", and a 
18841,https://github.com/timelydataflow/timely-dataflow/issues,issue tracker,"If you like the idea of getting your hands dirty in timely dataflow, the ", has a variety of issues that touch on different levels of the stack. For example:
18841,https://github.com/timelydataflow/timely-dataflow/issues/111,does more copies of data than it must,Timely currently ,", in the interest of appeasing Rust's ownership discipline most directly. Several of these copies could be elided with some more care in the resource management (for example, using shared regions of one "
18841,https://github.com/timelydataflow/timely-dataflow/issues/114,a list of nice to have features,"We recently landed a bunch of logging changes, but there is still "," that haven't made it yet. If you are interested in teasing out how timely works in part by poking around at the infrastructure that records what it does, this could be a good fit! It has the added benefit that the logs are timely streams themselves, so you can even do some log processing on timely. Whoa..."
18841,https://github.com/timelydataflow/timely-dataflow/issues/77,integrating Rust ownership idioms into timely dataflow,There is an open issue on ,". Right now, timely streams are of cloneable objects, and when a stream is re-used, items will be cloned. We could make that more explicit, and require calling a "
18845,#data,Data,"
","
"
18845,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html,Install the NVIDIA container toolkit:,"
","
"
18845,https://www.kaggle.com/c/carvana-image-masking-challenge/data,Kaggle website,The Carvana data is available on the ,.
18846,https://www.cityscapes-dataset.com/method-details/?submissionID=7836,85.4%,". Notably, the reseachers from Nvidia set a new state-of-the-art performance on Cityscapes leaderboard: ", via combining our HRNet + OCR with a new 
18846,https://www.cityscapes-dataset.com/benchmarks/,Cityscapes leaderboard,HRNet + OCR + SegFix: Rank #1 (84.5) in ,. OCR: object contextual represenations 
18846,https://www.cityscapes-dataset.com/benchmarks/,Cityscapes leaderboard,Rank #1 (83.7) in ,. HRNet combined with an extension of 
18846,https://www.cityscapes-dataset.com/,Cityscapes,You need to download the ,", "
18847,https://www.ritchievink.com/blog/2018/06/05/clustering-data-with-dirichlet-mixtures-in-edward-and-pymc3/,dirichlet gaussian mixture models,"
","
"
18854,https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip,Glove,Download the pretrained ,", and put it as "
18864,data-pipeline-examples/README.md,data-pipeline-examples,"
"," This project contains a set of examples that demonstrate how raw data in various formats can be loaded, split and preprocessed to build serializable (and hence reproducible) ETL pipelines."
18872,http://data.dws.informatik.uni-mannheim.de/Cat2Ax/,here,"If you don't want to run the extraction yourself, you can find the results ",.
18873,https://grouplens.org/datasets/movielens/,here,"MovieLens-100K, MovieLens-1M and MovieLens-10M datasets are publicly available ",. The Youtube dataset is introduced in this 
18875,https://data.dgl.ai/dataset/ppi.zip,data,"To run experiments on this task, you need to download the ", from.
18893,https://mindboggle.info/data,MindBoggle101, dataset used in ,"
"
18894,https://archive.ics.uci.edu/ml/datasets/leaf,UCI's machine learning repository,This repository contains some codes that I tried in classifying different leaves and possibly will be more organised once a good model is achieved. The data set for test can be obtained from ,", "
18894,http://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/,Swedish leaf dataset,", ", and 
18894,https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set,UCI's 100 leaf, and ,. We intended to only use shape information currently. The classifier should also work for classifying 1 dimensional time series. Some tested time series can be downloaded 
18894,http://timeseriesclassification.com/dataset.php,here,. We intended to only use shape information currently. The classifier should also work for classifying 1 dimensional time series. Some tested time series can be downloaded ,. The details of can be found in this short [paper].(https://arxiv.org/pdf/1907.00069.pdf)
18900,#download-pretrain-models-and-published-dataset,Download Pretrain Model and Dataset,step2: ,"
"
18901,http://www.fki.inf.unibe.ch/databases/iam-graph-database,IAM Graph Database, These datasets are taken from the ,". You can use them for scientific work, but are requested to include the following reference to your paper:"
18904,https://dataset.org/dream/,DREAM,"Here, we show the usage of this baseline using a demo designed for ",", a dialogue-based three-choice machine reading comprehension task."
18904,https://github.com/nlpdata/dream,DREAM repo, from the , to 
18925,https://archive.ics.uci.edu/ml/datasets/adult,Adult Census Dataset,In this example we load the ,* which is a built-in demo dataset. We use CTGAN to learn from the real data and then generate some synthetic data.
18925,https://datacebo.com,DataCebo," in 2016. After 4 years of research and traction with enterprise, we created "," in 2020 with the goal of growing the project. Today, DataCebo is the proud developer of SDV, the largest ecosystem for synthetic data generation & evaluation. It is home to multiple libraries that support synthetic data, including:"
18926,https://docs.sdv.dev/sdgym/customization/datasets,Customized Datasets,"For more information, see the docs for ",.
18926,https://datacebo.com,DataCebo," in 2016. After 4 years of research and traction with enterprise, we created "," in 2020 with the goal of growing the project. Today, DataCebo is the proud developer of SDV, the largest ecosystem for synthetic data generation & evaluation. It is home to multiple libraries that support synthetic data, including:"
18937,https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/,smallNORB, Download , dataset.
18942,https://github.com/mvaldenegro/marine-debris-fls-datasets/releases/tag/watertank-v1.0,here,The full dataset release containing marine debris in the OSL water tank can be found ,.
18942,https://github.com/mvaldenegro/marine-debris-fls-datasets/releases/tag/watertank-match-v1.0,here,This tasks consists of learning to match two sonar image patches. The subdataset release as HDF5 files is available ,.
18942,https://github.com/mvaldenegro/marine-debris-fls-datasets/tree/master/md_fls_dataset/data/watertank-segmentation,here,"Per-pixel segmentation labels have been made by Deepak Singh from Netaji Subhas Institute Of Technology, and consists of 11 classes plus background. This task is available ",. A preprint about this dataset is coming soon.
18942,https://github.com/mvaldenegro/marine-debris-fls-datasets/tree/master/md_fls_dataset/data/turntable-cropped,here,"The turntable dataset is captured with the sonar in a fixed position and pose, while objects are placed in a rotating turntable, allowing to capture a full yaw rotation of each object. These images are available ",. For now only a classification task is available.
18948,#datasets,event-time datasets,", and a collection of ",". In addition, some useful preprocessing tools are available in the "
18948,https://nbviewer.jupyter.org/github/havakv/pycox/blob/master/examples/04_mnist_dataloaders_cnn.ipynb,04_mnist_dataloaders_cnn.ipynb,"
",: Using dataloaders and convolutional networks for the MNIST data set. We repeat the 
18948,https://www.jstor.org/stable/2529811?seq=1#metadata_info_tab_contents,paper,", 31(4):863–872, 1975. [",]
18949,https://docs.seldon.io/projects/seldon-core/en/latest/examples/graph-metadata.html,"predictors, transformers, routers, combiners, and more",Powerful and rich inference graphs made out of ,.
18949,https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/metadata.html,"training system, data and metrics",Metadata provenance to ensure each model can be traced back to its respective ,.
18952,https://www.getambassador.io/docs/emissary/latest/topics/running/statistics/#datadog,Datadog,", and ",", and comprehensive "
18953,./database.db,./database.db,The database is stored in the ," file. It is stored as a SQLite3 database, so that it can be easily distributed and shared. For convenience, we suggest to explore the database's content using "
18953,./database.db,./database.db, in ,.
18957,https://openml.github.io/automlbenchmark/benchmark_datasets.html,benchmarking datasets,Curated suites of , from 
18957,https://www.openml.org/s/218/data,OpenML, from ,.
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/e2e-mlt.h5,e2e-mlt,"
",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/e2e-mltrctw.h5,e2e-mlt-rctw,", ","
"
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Arabic.zip,Arabic,Synthetic MLT Data (,", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Bangla.zip,Bangla,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Chinese.zip,Chinese,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Japanese.zip,Japanese,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Korean.zip,Korean,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Latin.zip,Latin,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Hindi.zip,Hindi,", ",   )
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Arabic_gt.zip,Arabic,and converted GT to icdar MLT format (see: http://rrc.cvc.uab.es/?ch=8&com=tasks) (,", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Bangla_gt.zip,Bangla,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Chinese_gt.zip,Chinese,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Japanese_gt.zip,Japanese,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Korean_gt.zip,Korean,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Latin_gt.zip,Latin,", ",", "
18962,http://ptak.felk.cvut.cz/public_datasets/SyntText/Hindi_gt.zip,Hindi,", ",  )
18966,https://www.influxdata.com,InfluxDB,", ",", "
18974,https://github.com/hfbassani/pbml/tree/master/Datasets/Realdata,these,"For example, to run experiments for ", real datasets the arguments will be as follows:
18975,https://github.com/hfbassani/pbml/tree/master/Datasets/Realdata,these,"For example, to run experiments for ", real datasets:
18988,https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/,"
FLAML
","
", provides automated tuning for LightGBM (
18989,https://youtube-vos.org/dataset/download,YouTube-VOS,: Download from ,", note we only need the training part("
18999,https://towardsdatascience.com/analyzing-tweets-with-nlp-in-minutes-with-spark-optimus-and-twint-a0c96084995f,"Analyzing Tweets with NLP in minutes with Spark, Optimus and Twint","
","
"
19018,http://cocodataset.org/#download,coco,Download , dataset and extract the images to 
19027,https://github.com/jeffreyhuang1/two-stream-action-recognition/blob/master/dataloader/spatial_dataloader.py#L21,funcition, and this , to fit the UCF101 dataset on your device.
19027,https://github.com/jeffreyhuang1/two-stream-action-recognition/blob/master/dataloader/motion_dataloader.py#L32,funcition, and this , to fit the UCF101 dataset on your device.
19046,https://grammatech.github.io/gtirb/md__aux_data.html,Standard AuxData Schemata," describes the structure for common types of auxiliary data such as function boundary information, type information, or results of common analyses in ",.
19046,#using-serialized-gtirb-data,easy and efficient use from any programming language,"), enabling ",.
19060,https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb,Deepnote,Or open it in ,: 
19064,examples/datagen,[examples/datagen],More info ,", "
19064,examples/datagen,[examples/datagen], for training data training read ,.
19064,examples/datagen,[examples/datagen],Please see , and 
19074,#dataset-download,Dataset Download,Follow the instructions from the simulator under ,", "
19074,#dataset-preprocessing,Dataset Preprocessing,", ", and 
19074,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2," using a simple blending approach. As the depth images contain many missing values (corresponding to shiny, bright, transparent, and distant surfaces, which are common in the dataset) we apply a simple crossbilateral filter based on the ", code to fill all but the largest holes. A couple of things to keep in mind:
19084,https://github.com/aptnotes/data,APTnotes, :small_blue_diamond: ,"
"
19084,https://www.welivesecurity.com/2020/11/12/hungry-data-modpipe-backdoor-hits-pos-software-hospitality-sector/,"[ESET] Hungry for data, ModPipe backdoor hits POS software used in hospitality sector",Nov 12 - , | 
19084,https://blog.trendmicro.com/trendlabs-security-intelligence/glupteba-campaign-hits-network-routers-and-updates-cc-servers-with-data-from-bitcoin-transactions/,[Trend Micro] Glupteba Campaign Hits Network Routers and Updates C&C Servers with Data from Bitcoin Transactions,Sep 04 - , | 
19084,https://securelist.com/luckymouse-hits-national-data-center/86083/,[Kaspersky] LuckyMouse hits national data center to organize country-level waterholing campaign,Jun 13 - , | 
19084,https://securingtomorrow.mcafee.com/mcafee-labs/analyzing-operation-ghostsecret-attack-seeks-to-steal-data-worldwide/,[McAfee] Analyzing Operation GhostSecret: Attack Seeks to Steal Data Worldwide,Apr 24 - , | 
19084,https://www.nccgroup.trust/uk/about-us/newsroom-and-events/blogs/2018/april/decoding-network-data-from-a-gh0st-rat-variant,[NCCGroup] Decoding network data from a Gh0st RAT variant,Apr 17 - , | 
19084,http://www.pwc.co.uk/issues/cyber-security-data-privacy/research/the-keyboys-are-back-in-town.html,[PwC] The KeyBoys are back in town,Nov 02 - , | 
19084,https://blog.gdatasoftware.com/blog/article/dissecting-the-kraken.html,[G DATA] Dissecting the Kraken,May 07 - , | 
19084,https://blog.gdatasoftware.com/blog/article/babar-espionage-software-finally-found-and-put-under-the-microscope.html,[G DATA] Babar: espionage software finally found and put under the microscope,Feb 18 - , | 
19084,https://blog.gdatasoftware.com/blog/article/analysis-of-project-cobra.html,[G DATA] Analysis of Project Cobra,Jan 20 - , | 
19084,https://blog.gdatasoftware.com/blog/article/evolution-of-sophisticated-spyware-from-agentbtz-to-comrat.html,[G DATA] Evolution of Agent.BTZ to ComRAT,Jan 15 - , | 
19084,http://blog.gdatasoftware.com/blog/article/the-uroburos-case-new-sophisticated-rat-identified.html,"[GDATA] The Uroburos case- Agent.BTZ’s successor, ComRAT",Nov 11 - , | 
19084,https://blog.gdatasoftware.com/blog/article/operation-toohash-how-targeted-attacks-work.html,[GData] Operation TooHash,Oct 31 - , | 
19084,http://www.contextis.com/services/research/white-papers/crouching-tiger-hidden-dragon-stolen-data/,"[contextis] Crouching Tiger, Hidden Dragon, Stolen Data",Mar 12 - , | 
19086,http://dregon.inria.fr/datasets/signal-processing-cup-2019/,SPCUP19 page,) from the , on the Dregon Dataset webpage;
19100,https://archive.ics.uci.edu/ml/datasets/iris,Iris dataset,We're using the ,", so let's load it and perform an 80/20 train/test split."
19104,dataset,dataset folder,See the , to create a dataset for analysis from the 
19104,analysis/dataset/export_full.csv.zip,dataset analysed in the paper, to replicate analytical results from the paper. The ," is provided, so that the two replication steps can be done independently."
19122,http://crcv.ucf.edu/data/ucf-qnrf/,Dropbox ,Download the original UCF-QNRF Dataset [Link: ,]
19122,https://github.com/aachenhang/crowdcount-mcnn/tree/master/data_preparation,code,Generate the density maps by using the ,.
19127,http://pandas.pydata.org/pandas-docs/stable/install.html,pandas,"
", >= 0.19.2
19127,http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus,TEDLIUM release 2, and ," corpus. Total number of sentences in the training set composed of the above three corpus is 240,612. Valid and test set is built using only LibriSpeech and TEDLIUM corpuse, because VCTK corpus does not have valid and test set. After downloading the each corpus, extract them in the 'asset/data/VCTK-Corpus', 'asset/data/LibriSpeech' and 'asset/data/TEDLIUM_release2' directories."
19137,https://github.com/google-research-datasets/simulated-dialogue,sim-M and sim-R,", ","
"
19138,https://github.com/miguelgondu/sc2reaper/blob/master/using_sc2reaper_database.md,This documentation, was storing the data. , shows how the default database is constructed and how you can use pymongo to extract relevant information.
19140,http://www.casmacat.eu/corpus/news-commentary.html,CASMACAT, and/or , for details.
19145,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K 800 training images,Data Set: ,.
19146,http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz,link,initial_setup.py - Downloads and prepares Speech Commands V2 dataset (,).
19148,http://www.esri.com/news/arcuser/0199/crimedata.html,"Williamson et al., 1999","In essence, DREDGE provides density-based line points which optimize the distance to a dataset of coordinates along those lines, with larger bandwidths leading to a decrease in summed line length and an increase in the average distance to the nearest line. Since DREDGE was initially developed to be applied to crime incident data, the default bandwidth calculation follows a best-practice approach that is well-accepted within quantitative criminology, using the mean distance to a given number of nearest neighbors (","). Since practitioners in that area of study are often interested in the highest-density regions of a dataset, the tool also features the possibility to specify a top-percentage level for a "
19148,https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2,Chicago Data Portal,"As an example, for UCR-defined Part I crimes from the "," from 2018, the above call to the "
19153,http://cucis.ece.northwestern.edu/projects/DataSets/IRNet/sample/sample_model.data-00000-of-00001,sample/sample_model.data-00000-of-00001,"
", (64 MB)- part of model trained using the sample command above.
19153,http://cucis.ece.northwestern.edu/projects/DataSets/IRNet/training-data/oqmd-c.csv,training-data/oqmd-c.csv,"
", (49 MB)- dataset used as sample in 
19153,http://cucis.ece.northwestern.edu/projects/DataSets/IRNet/training-data/sample_train_set.csv,training-data/sample_train_set.csv,"
", (9.3 MB)- training dataset for model training using the sample command above.
19153,http://cucis.ece.northwestern.edu/projects/DataSets/IRNet/training-data/sample_test_set.csv,training-data/sample_test_set.csv,"
", (1.1 MB)- validation/test set for model training using the sample command above.
19162,http://www.nersc.gov/about/nersc-staff/data-analytics-services/wahid-bhimji/,Wahid Bhimji,"
","
"
19163,http://www.nersc.gov/about/nersc-staff/data-analytics-services/wahid-bhimji/,Wahid Bhimji,"
","
"
19167,#eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks,Easy data augmentation (EDA),"
","
"
19171,report/report_data.tar.gz,Download vulnerability report,"
","
"
19171,PQ/pq_data.tar.gz,Download tarball of PQ dataset,"
","
"
19171,RQ1/rq1_data.tar.gz,Download tarball of RQ1 dataset,"
","
"
19171,RQ2/rq2_data.tar.gz,Download tarball of RQ2 dataset,"
","
"
19186,#waymo-open-dataset-baselines,Waymo Open Dataset,Add performance of several models trained with full training set of ,.
19186,#waymo-open-dataset-baselines,Waymo Open Dataset,Add PointPillar related baseline configs/results on ,.
19186,#waymo-open-dataset-baselines,Waymo Open Dataset,Improve the performance of all models on ,. Note that you need to re-prepare the training/validation data and ground-truth database of Waymo Open Dataset (see 
19186,tools/cfgs/dataset_configs/waymo_dataset.yaml,"
USE_SHARED_MEMORY
",Support config , to use shared memory to potentially speed up the training process in case you suffer from an IO problem.
19186,#waymo-open-dataset-baselines,Waymo Open Dataset,[2020-11-10] The , has been supported with state-of-the-art results. Currently we provide the configs and results of 
19186,tools/cfgs/dataset_configs/waymo_dataset.yaml,"
DATA_CONFIG.SAMPLED_INTERVAL
",We provide the setting of ," on the Waymo Open Dataset (WOD) to subsample partial samples for training and evaluation, so you could also play with WOD by setting a smaller "
19193,#datasets,Datasets,"
","
"
19193,datasets/smart_contract_audit_findings,Smart Contract Audit Findings,| Dataset | Date | | --- |---| | , | Aug 2019 |
19193,https://www.datadoghq.com/,Datadog, | May 2022 | 8 | | | | , | May 2022 | 6 | | | | 
19193,https://www.datadoghq.com/,Datadog, | Apr 2022 | 4 | | | | , | May 2022 | 6 | | | | 
19193,https://www.datadoghq.com/,Datadog Agent, | Sept 2021 | 4 | | | , | Aug 2021 | 8 | | | 
19193,https://www.westerndigital.com/company/newsroom/press-releases/2020/2020-09-03-western-digital-sets-a-new-standard-in-data-protection,Western Digital, | Jan 2020 | 4 | , | 
19197,https://pandas.pydata.org/,Pandas,"
","
"
19199,data_analysis,experimental data and analysis scripts,It contains including source code for software used in an experiment as well as ,.
19202,/nagisa/data/sample_datasets,sample datasets,. Note that you put EOS between sentences. Refer to , and 
19216,data/calib_config.yml,calibration config file, with a , as
19216,data/board.png,default pattern image here, to toggle between normal and mirrored display. You can find the ,". After convergence, the resulting calibration properties will be written as "
19224,data-formats,data-formats,Define how data is presented to end users or analysts. See the , directory.
19231,https://github.com/nikitakit/parser-data-gen,our PTB data generation," with one tree per line (for example, as produced by "," code), you can parse the tokens in these sentences and compute parse evaluation scores using the following:"
19231,https://github.com/nikitakit/parser-data-gen,our PTB data generation,"Obtain treebank files for your corpus, and put them into files with one parse tree per line (for example, as produced by "," code). Optional: remove functional annotations and extraneous non-terminals, like TOP, which are all discarded in standard evaluation, using "
19268,#download-data,Download the data,"
","
"
19268,#download-data,download the data,"If you wish to test your synthesis environment on human-composed music, you first need to ",". Then, if you have both your "
19270,https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.GraphSAINTSampler,reference implementation,We thank Matthias Fey for providing a , in the PyTorch Geometric library.
19276,https://s-rl-toolbox.readthedocs.io/en/latest/guide/envs.html#generating-data,Generating Data,Although the data can be generated easily using the RL repo in simulation (cf ,"), we provide datasets with a real baxter:"
19279,#2-dsprite-textures-dataset,dSprite-textures dataset,"
","
"
19279,https://github.com/deepmind/dsprites-dataset,dSprite,dSprite-textures is a synthetic dataset adapted from the ," dataset by adding texture patterns to all shapes. Additionally, we add labels for shape class, bounding box, and mask. Textures are cropped from images in the "
19279,https://www.robots.ox.ac.uk/~vgg/data/dtd/index.html,Describable Textures Dataset," dataset by adding texture patterns to all shapes. Additionally, we add labels for shape class, bounding box, and mask. Textures are cropped from images in the ",.
19279,https://github.com/deepmind/dsprites-dataset,dSprite dataset,This repository builds upon code from several projects and individuals: ," by Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner "
19279,https://www.robots.ox.ac.uk/~vgg/data/dtd/index.html,Describable Textures Dataset," by Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner "," by Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi "
19291,https://github.com/huggingface/datasets,datasets,": loads data from plaintext, tsv, and huggingface's ","
"
19291,https://github.com/masakhane-io/masakhane-mt/blob/master/starter_notebook-custom-data.ipynb,starter notebook,"
", Masakhane - Machine Translation for African Languages in 
19302,#parsing-data-with-plato,Parsing data with Plato,"
","
"
19317,./datasets/README.md,More details >>>,"
","
"
19322,https://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/,[download],"Save the file in the root folder of the project. To reproduce the paper results, we recommend ~3 hours of polyphonic piano music ",.
19322,http://www.stefanlattner.at/data/mnist_rot.pyc.bz,this link, using , and place it in the data folder: 
19324,https://pytorch.org/docs/stable/torchvision/datasets.html,torchvision.datasets,"The dataset for the ""polyphonic music modeling"" experiment is already included in audio/data/. For other experiments that are based on much larger datasets, the data needs to be downloaded (from ", or 
19332,data,data,The data provided with replication package is listed in directory ," The data is stored in different formats. (e.g. pickle, db, csv, etc..)"
19338,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU Depth v2,"
","
"
19348,http://camma.u-strasbg.fr/datasets,Cholec80,We use the dataset ,.
19349,http://corpus-texmex.irisa.fr/,original website,| Data set  | Download                 | dimension | nb base vectors | nb query vectors | original website                                               | |-----------|--------------------------|-----------|-----------------|------------------|----------------------------------------------------------------| | SIFT1M    |,"| 128       | 1,000,000       | 10,000           | "
19349,http://corpus-texmex.irisa.fr/,original website,"| 128       | 1,000,000       | 10,000           | ",             | | GIST1M    |
19349,http://corpus-texmex.irisa.fr/,original website,             | | GIST1M    |,"| 128       | 1,000,000       | 1,000            | "
19349,http://corpus-texmex.irisa.fr/,original website,"| 128       | 1,000,000       | 1,000            | ",             | | Crawl     | 
19349,http://downloads.zjulearning.org.cn/data/crawl.tar.gz,crawl.tar.gz,             | | Crawl     | ," (1.7GB)     | 300       | 1,989,995       | 10,000           | "
19349,http://downloads.zjulearning.org.cn/data/glove-100.tar.gz,glove-100.tar.gz,                    | | GloVe-100 | ," (424MB) | 100       | 1,183,514       | 10,000           | "
19360,https://www.dropbox.com/s/abjkcnbxy7qy39h/data_set.bag?dl=0,data_set.bag,Download the files ,", "
19360,https://www.dropbox.com/s/97kk4n6inde94c3/wind_estimator_training_dataset.mat?dl=0,here,"This package presents four alternative versions for estimation of wind velocity, namely: two Extended Kalman Filter approaches (or model-based approaches); a Neural Network (or data-driven approach); and a hybrid estimator which performs a fusion between the model-based and data-driven estimators. The training dataset used for the Neural Network can be found ",.
19371,http://vllab.ucmerced.edu/ytsai/CVPR17/real_data.zip,here,"Download our complete set of real composite images, including our harmonization results ",.
19375,https://github.com/boudinfl/ake-datasets,Florian Boudin,"This repository contains 20 annotated datasets of Automatic Keyphrase Extraction made available by the research community. Following are the datasets and the original papers that proposed them. If you know more datasets, and want to contribute, please, notify me. You might also want to have a look at ", keyphrase extraction repository.
19375,datasets/110-PT-BN-KP.zip,110-PT-BN-KP,: ,"
"
19375,datasets/500N-KPCrowd-v1.1.zip,500N-KPCrowd-v1.1,: ,"
"
19375,datasets/cacic.zip,cacic,: ,"
"
19375,datasets/citeulike180.zip,citeulike180,: ,"
"
19375,datasets/fao30.zip,fao30,: ,"
"
19375,datasets/fao780.zip,fao780,: ,"
"
19375,datasets/Inspec.zip,Inspec,: ,"
"
19375,datasets/kdd.zip,kdd,: ,"
"
19375,datasets/Krapivin2009.zip,Krapivin2009,: ,"
"
19375,datasets/Nguyen2007.zip,Nguyen2007,: ,"
"
19375,datasets/pak2018.zip,pak2018,: ,"
"
19375,datasets/PubMed.zip,PubMed,: ,"
"
19375,datasets/Schutz2008.zip,Schutz2008,: ,"
"
19375,datasets/SemEval2010.zip,SemEval2010,: ,"
"
19375,datasets/SemEval2017.zip,SemEval2017,: ,"
"
19375,datasets/theses100.zip,theses100,: ,"
"
19375,https://github.com/zelandiya/keyword-extraction-datasets/blob/ba4966ccceafb1c159cdc42f8e8dc630eff126d4/theses100.zip,Originally downloaded from zelandiya github account,: ,"
"
19375,datasets/wicc.zip,wicc,: ,"
"
19375,datasets/wiki20.zip,wiki20,: ,"
"
19375,datasets/WikiNews.zip,WikiNews,: ,"
"
19375,datasets/www.zip,www,: ,"
"
19379,https://kelvins.esa.int/proba-v-super-resolution/data/,Kelvin Competition,Download the PROBA-V dataset from the , and save it under 
19400,#SIP-dataset,SIP dataset,"
","
"
19402,https://github.com/Microsoft/CameraTraps/blob/main/data_management/README.md#coco-cameratraps-format,COCO Camera Traps,Converting frequently-used metadata formats to , format
19402,http://lila.science/datasets/nacti,NACTI,"Image credit USDA, from the ", data set.
19418,https://github.com/Andrewsher/ATLAS-dataset-generate-h5file,这里,数据集：ATLAS数据集[1]，包含229个case，采用5折交叉验证。数据采用,所示的方法进行预处理得到h5文件。
19419,https://github.com/Andrewsher/ATLAS-dataset-generate-h5file,这里,数据集：ATLAS数据集[1]，包含229个case，划分了训练集、测试集和验证集。数据采用,所示的方法，对训练集、测试集合验证集分别进行预处理，得到3个h5文件。
19422,https://darkpatterns.cs.princeton.edu/data/,here,The data from the checkout crawls can be downloaded ,.
19423,https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_Veracity_Classification/6392078,PHEME 6392078 dataset,Augmented Raw data is based on ,. We have two versions of augmented rumor corpus.
19423,https://figshare.shef.ac.uk/articles/Credibility_corpus_fine-tuned_ELMo_contextual_language_model_for_early_rumor_detection_on_social_media/11591775/1,shef.data.11591775.v1,Credbank Fine-tuned ELMo(ELMo_CREDBANK) used in LLD paper can be downloaded via figshare ," with version ""12262018.hdf5"""
19423,https://figshare.shef.ac.uk/articles/Credibility_corpus_fine-tuned_ELMo_contextual_language_model_for_early_rumor_detection_on_social_media/11591775/1,shef.data.11591775.v1,Credbank Fine-tuned ELMo used in ASONAM 2019 paper can be downloaded via figshare ," with the version ""10052019.hdf5"" ."
19432,http://community.opscode.com/cookbooks/database,database,See the , for resources and providers that can be used for managing PostgreSQL users and databases.
19450,https://towardsdatascience.com/exploring-electronic-health-records-with-medcat-and-neo4j-f376c03d8eef,Exploring Electronic Health Records with MedCAT and Neo4j,: ,"
"
19450,https://towardsdatascience.com/integrating-transformers-with-medcat-for-biomedical-ner-l-8869c76762a,Integrating 🤗 Transformers with MedCAT for biomedical NER+L,: ,"
"
19450,https://towardsdatascience.com/medcat-introduction-analyzing-electronic-health-records-e1c420afa13a,Towards Data Science,. Read more about MedCAT on ,.
19452,https://www.data.gouv.fr/fr/datasets/cass,here,. Information about this dataset can be found ,).
19455,qa+adapter/re-organize_dataset/mix_dataset.py,mix_dataset.py, and the script for re-organize this dataset is ,.
19462,https://github.com/nmrksic/neural-belief-tracker/tree/master/data/woz,download,WOZ2.0: ,"
"
19462,http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/,download,MultiWOZ: ,"
"
19467,https://www.kaggle.com/c/painter-by-numbers/data,Kaggle's painter-by-numbers dataset,Get style images: Download train.zip from ,"
"
19497,https://archive.ics.uci.edu/ml/datasets/wine+quality,Wine quality, and ," datasets which contain, respectively, 10000 labelled images of handwritten digits (test-set) and physicochemical data for 6498 variants of red and white wine. To retrieve our baseline we performed the full-rank PCA on the MNIST and (red) Wine datasets and retrieved the first and second principal components. Then, on the same datasets, we applied Federated PCA (F-PCA) with rank estimate "
19499,https://github.com/gpapamak/maf#how-to-get-the-datasets,original MAF repository,The datasets are taken from the ,. Follow the 
19499,https://github.com/gpapamak/maf#how-to-get-the-datasets,instructions,. Follow the , to get them.
19503,https://pandas.pydata.org,pandas,"
", (0.23.0)
19514,https://github.com/slacgismo/solar-data-tools,Solar Data Tools,"We actually recommend that users generally not invoke this software directly. Instead, we recommend using the API provided by ",.
19519,#fetching-data-with-ajax-requests,Fetching Data with AJAX Requests,"
","
"
19519,#injecting-data-from-the-server-into-the-page,Injecting Data from the Server into the Page,"
","
"
19519,#progressive-web-app-metadata,Progressive Web App Metadata,"
","
"
19519,#injecting-data-from-the-server-into-the-page,described here,". Since Create React App produces a static HTML/CSS/JS bundle, it can’t possibly read them at runtime. To read them at runtime, you would need to load HTML into memory on the server and replace placeholders in runtime, just like ",. Alternatively you can rebuild the app on the server anytime you change them.
19541,https://bdd-data.berkeley.edu/,here,Download the Berkeley Deep Drive dataset ,. It is only necessary to download the Images and Labels files.
19545,data_loader.py,data_loader.py,data loader to read complex-valued raw MRI data and sensitivity maps - ,"
"
19545,data,data,"
", : contains dependant functions used in training or deploying VS-Net and is written by 
19554,#data-set-up,Data set-up,"
","
"
19554,#data-set-up,Data Set-up,"Download the audio files (i.e. the Audio 1/5, Audio 2/5, ..., Audio 5/5), do your feature extraction and follow the instructions at the ", section.
19554,#data-set-up,Data Set-up,"Download the audio files, do your feature extraction and follow the instructions at the ", section.
19554,#data-set-up,Data Set-up,"Download the audio files, do your feature extraction and follow the instructions at the ", section.
19556,https://magenta.tensorflow.org/datasets/nsynth,here,"We use the NSynth dataset, which is available ",". For the data augmentation, the following plugins were used:"
19561,http://euridika.math.hr:1846/Jacobi/FLAPW-data/,download,The testing dataset is available for ," (please, conserve the bandwidth by downloading only what is of interest to you)."
19563,https://github.com/wesm/pydata-book,GitHub of materials and Jupyter notebooks,"
","
"
19563,https://altair-viz.github.io/user_guide/data.html,Altair user guide,"
","
"
19568,http://www-rech.telecom-lille.fr/DHGdataset/,DHG-14/28 Dataset,"We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method for hand gesture recognition. The key idea is to first construct a fully-connected graph from a hand skeleton, where the node features and edges are then automatically learned via a self-attention mechanism that performs in both spatial and temporal domains. The code of training our approach for skeleton-based hand gesture recognition on the ", and the 
19568,http://www-rech.telecom-lille.fr/DHGdataset/,DHG-14/28 Dataset,Download the , or the 
19570,#datasets,datasets,"
","
"
19579,appendix/dataset/dataset.txt,Applications,"
","
"
19579,appendix/data/features,Feature adoption,"
","
"
19579,appendix/data/evolution_trends,Evolution trend,"
","
"
19590,https://github.com/openai/gpt-2-output-dataset,released a dataset,We have also , for researchers to study their behaviors.
19604,https://github.com/faisal-iut/linkPrediction/tree/master/dataset,Dataset,"
","
"
19604,https://github.com/faisal-iut/linkPrediction/blob/master/dataset/apnea-all%2C3.csv,Apnea dataset,"
", - All keywords are atleast 3-degree keywords
19604,https://github.com/faisal-iut/linkPrediction/blob/master/dataset/apnea-distinct_keyword.csv,Apnea keyword lists,"
", - Keyword list with integer id
19604,https://github.com/faisal-iut/linkPrediction/blob/master/dataset/obesity-all%2C3.csv,Obesity dataset,"
", - All keywords are atleast 3-degree keywords
19604,https://github.com/faisal-iut/linkPrediction/blob/master/dataset/obesity-distinct_keyword.csv,Obesity keyword lists,"
", - Keyword list with integer id
19605,http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2003ApJ...589..444G&data_type=BIBTEX&db_key=AST&nocookieset=1,bibtex citation,", ",)
19611,https://archive.ics.uci.edu/ml/datasets/Letter+Recognition,Letter Recognition,"
","
"
19611,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#dna,DNA,"
","
"
19611,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps,USPS,"
","
"
19611,https://archive.ics.uci.edu/ml/datasets/ISOLET,ISOLET,"
","
"
19611,https://archive.ics.uci.edu/ml/datasets/Gisette,Gisette,"
","
"
19611,http://archive.ics.uci.edu/ml/datasets/SUSY,SUSY,"
","
"
19611,https://ubicomp.eti.uni-siegen.de/home/datasets/icmi18/,WESAD,"
","
"
19612,https://raw.githubusercontent.com/ry/tensorflow-resnet/master/data/tensorflow-resnet-pretrained-20160509.tar.gz.torrent,tensorflow-resnet-pretrained-20160509.tar.gz.torrent,"
",  464M
19626,https://trec.nist.gov/data/reuters/reuters.html,Reuters RCV1," project, and the "," corpora. We converted the provided XML dumps into the dataset format above, using the "
19626,https://research.spotify.com/datasets,Spotify,"We also run experiments with the Million Playlist Dataset (MPD), provided by ",", and IREON, provided by "
19630,https://github.com/harvardnlp/boxscore-data,boxscore-data,"Since this script simply removed duplicate records from the original dataset, the data format is the same as that of the original rotowire dataset. Please refer to ", repo.
19630,https://github.com/harvardnlp/boxscore-data,boxscore-data,You can download the original dataset (Wiseman'2017) from the ," repo, and then, transform it as bellow. The script will create a directory ""rotowire-modified"", which contains train.json, valid.json, and test.json files."
19632,http://cocodataset.org,MSCOCO,"Run the following from the home directory of this repository to install python dependencies, download BAM models, download ", and 
19641,https://github.com/iremeyiokur/multipie_ear_dataset,Multi-PIE ear dataset,This dataset is the extended version of , [3].
19643,https://langui.sh/2016/12/09/data-driven-decisions,awesome blog post, for his ,.
19649,https://mybinder.org/v2/gh/ContinuumIO/anaconda-package-data/master?filepath=anaconda_package_data_example.ipynb,"
badge
",Check out an example notebook using this data on Binder: ,"
"
19649,https://mybinder.org/v2/gh/ContinuumIO/anaconda-package-data/master?urlpath=panel/anaconda_package_data_dashboard,"
app
","
","
"
19655,https://www.kaggle.com/c/allstate-purchase-prediction-challenge/data,Allstate,"
","
"
19655,https://pytorch.org/cppdocs/api/classtorch_1_1data_1_1datasets_1_1_m_n_i_s_t.html,MNIST,"
","
"
19657,#preparing-data,Preparing data,"
","
"
19670,http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip,vimeo triplet dataset,Download Vimeo90k training data from ,.
19675,https://github.com/Unbabel/translator2vec/releases/download/v1.0/keystrokes_dataset.zip,here,Dataset available ,.
19676,https://lmb.informatik.uni-freiburg.de/data/what3d/voxels.zip,voxel grids, experiments in our paper: ,", "
19676,https://lmb.informatik.uni-freiburg.de/data/what3d/points.zip,point clouds,", ",", "
19676,https://lmb.informatik.uni-freiburg.de/data/what3d/renderings.zip,renderings,", ", and 
19676,https://lmb.informatik.uni-freiburg.de/data/what3d/splits.zip,train/test splits, and ,.
19677,https://www.dropbox.com/s/9xofxxsn6nwum2p/PIRM_dataset.zip?dl=0,this link,. The PIRM dataset can be found in ,"
"
19677,https://www.pirm2018.org/dataset.html,validation/test set,Copy the , (HR images only) into the 
19683,#datasets,Datasets,"
","
"
19683,#dataset-format,Format,"
","
"
19696,#datasets,Datasets,"
","
"
19696,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Oxford flower 102,"
","
"
19696,./download-glas-dataset.sh,./download-glas-dataset.sh,GlaS: ,.
19696,./download-caltech-ucsd-birds-200-2011-dataset.sh,./download-caltech-ucsd-birds-200-2011-dataset.sh,Caltech-UCSD Birds-200-2011:  ,"
"
19696,./download-Oxford-flowers-102-dataset.sh,./download-Oxford-flowers-102-dataset.sh,Oxford flower 102: ,"
"
19704,#data,Data,"
","
"
19719,http://rpg.ifi.uzh.ch/datasets/netvlad/vd16_pitts30k_conv5_3_vlad_preL2_intra_white.zip,here,Download the checkpoint ,(1.1 GB). Extract the zip and move its contents to the 
19719,http://rpg.ifi.uzh.ch/datasets/netvlad/example_stats.mat,this mat,"To verify that you get the correct output, download ", (83MB) and put it into the 
19719,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,grayscale odometry data of KITTI, for further testing which ensures that place recognition performance is consistent between the Matlab and Python implementations. This test requires the , to be linked in the main folder of the repo.
19729,data/data_structure.md,here,. The graph nodes and edges are generally defined as described in the paper. The detailed data structure is explained ,.
19734,http://cocodataset.org/#home,MS COCO 2017,", ",", "
19734,https://github.com/cocodataset/cocoapi,COCO API,"
","
"
19734,http://cocodataset.org/#format-data,MS COCO format,All annotation files follow ,.
19734,http://cocodataset.org/#format-data,MS COCO format,"If you want to add your own dataset, you have to convert it to ",.
19735,http://cocodataset.org/#home,MS COCO 2017,", ",", "
19735,https://github.com/cocodataset/cocoapi,COCO API,"
","
"
19735,http://cocodataset.org/#format-data,MS COCO format,All annotation files follow ,.
19735,http://cocodataset.org/#format-data,MS COCO format,"If you want to add your own dataset, you have to convert it to ",.
19738,https://www.cityscapes-dataset.com/anonymous-results/?id=16896cc219a6d5af875f8aa3d528a0f7c4ce57644aece957938eae9062ed8070,Detailed result,|104|,| |Cityscapes(Fine)|from scratch|train|
19743,http://www.sociopatterns.org/datasets/hypertext-2009-dynamic-contact-network/,'Hypertext 2009'-dataset,In order to download the SocioPatterns ," and visualize it interactively, do the following."
19744,https://sobigdata.d4science.org/group/sobigdata-gateway/explore?siteId=20371853,"
SBD++
","
","
"
19744,https://sobigdata.d4science.org/group/sobigdata-gateway/explore?siteId=20371853,SoBigData++, functionalities without installing anything on your machine consider using the preconfigured Jupyter Hub instances offered by ,.
19747,./data,data,"
",: 
19747,https://github.com/xthan/polyvore-dataset,Polyvore, datasets based on ,.
19747,./data/,data,"Pretrained model weights can be found in the links. The train, validation and test split is provided in ",.
19756,https://github.com/paulcon/as-data-sets,Active Subspaces Data Sets,"To see active subspace in action on real science and engineering applications, see the "," repository, which contains several Jupyter notebooks applying the methods to study input/output relationships in complex models."
19756,https://github.com/paulcon/as-data-sets,repo," directory, the [active subspaces website] (http://activesubspaces.org/applications/), and the Active Subspaces Data Sets ",.
19764,https://github.com/sktime/sktime-tutorial-pydata-global-2021,Video Tutorial,"
","
"
19771,https://warwick.ac.uk/fac/sci/dcs/research/tia/data/crchistolabelednucleihe/,[Data],Colon cancer dataset ,"
"
19775,http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database/download-the-iam-on-line-handwriting-database,here,"
","
"
19781,https://bit.ly/nlp_adv_data,here,Our 7 datasets are ,.
19781,https://github.com/jind11/TextFooler/tree/master/data,data,--dataset_path: The path to the dataset. We put the 1000 examples for each dataset we used in the paper in the folder ,.
19784,https://github.com/houchengbin/DynWalks/tree/master/data,README.md under the data folder,"If you would like to use your own dataset, please see the ",.
19788,https://github.com/wangpinggl/TREQS/tree/master/mimicsql_data,mimicsql_data,"In this work, we are also releasing a large-scale dataset MIMICSQL for Question-to-SQL generation task in healthcare domain. The MIMICSQL dataset is provided in folder ", in this repository. More details about MIMICSQL dataset are provided below.
19792,./DATA/test_gendataBFsig.m,test_gendataBFsig.m,Enter the SHAPES / DATA folder and run the , script to generate sample files containing simulated data.
19792,./DATA/test_gendataBFsig.m,test_gendataBFsig.m,. It runs SHAPES on data generated by ,". Hence, use the latter to first generate some data."
19810,https://www.luge.ai/#/luge/dataDetail?id=5,DuIE2.0数据集,"
","
"
19810,https://ernie-github.cdn.bcebos.com/data-msra_ner.tar.gz,MSRA_NER数据集,"
","
"
19810,./applications/tasks/data_distillation,数据蒸馏,数据蒸馏（,）
19817,https://en.wikipedia.org/wiki/MNIST_database,MNIST data,: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying ,", a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the "
19817,http://conda.pydata.org/docs,docs,Per the Anaconda ,:
19817,http://conda.pydata.org/miniconda.html,"
miniconda
",Install ," on your computer, by selecting the latest Python version for your operating system. If you already have "
19817,http://conda.pydata.org/docs/using/envs.html,environment,"
",.
19817,http://conda.pydata.org/miniconda.html,miniconda,"
", on your machine. Detailed instructions:
19831,https://pandas.pydata.org/,"
pandas
","
", for result analysis.
19836,http://www.nersc.gov/about/nersc-staff/data-analytics-services/karthik-kashinath/,Karthik Kashinath,", ",", "
19836,http://www.nersc.gov/about/nersc-staff/data-analytics-services/prabhat/,Prabhat,", ",", "
19841,https://duc.nist.gov/data.html,the DUC website,Download the DUC01/02/04 data from , and extract the data to folder 'data/'.
19842,https://github.com/getalp/mass-dataset/blob/master/scripts/force-align.py,this,1.2. The audios were converted from multi to single channel and forced aligned by using , script.
19842,https://github.com/getalp/mass-dataset/blob/master/scripts/alignment/,here,"For each language, the audios were sliced in verses considering the output of 1.3. and the generated texgrids (2.). More details available ",.
19842,https://github.com/getalp/mass-dataset/blob/master/scripts/check-verses.py,this,Use ," script to tenerate a CSV files listing the verses available for each language. As not all the verses of a given language exist in another language, this CSV file can be use to get a list of verses common to all languages."
19843,data,data,The data (Flickr8K speech and image features) needed to run the model is here: https://drive.google.com/file/d/14OVoyKibsslVwgYxxgd-s3dbA4bHUZtf/view?usp=sharing Unpack it in the , directory.
19844,https://github.com/getalp/mass-dataset,the following scripts,1) Download the data (or build the corpus yourself using ,)
19844,https://github.com/getalp/mass-dataset/blob/master/scripts/check-verses.py,the following script,Build the train/val/test splits with build_splits.py. This script will need a CSV file as input which sums up which verses are available for which language. This CSV file can be computed with ,". If you downloaded the pre-computed mel-spectrograms, this file was packed with the mel-spectrograms and is available "
19849,#code--data-preparation,Code & Data Preparation,"
","
"
19849,#get-the-skeleton-data,Get the Skeleton Data,"
","
"
19850,https://github.com/big-data-lab-team/paper-big-data-engines/tree/SC19,https://github.com/big-data-lab-team/paper-big-data-engines/tree/SC19,Repository version available at: ,"
"
19850,https://github.com/big-data-lab-team/paper-big-data-engines/tree/CPE,https://github.com/big-data-lab-team/paper-big-data-engines/tree/CPE,Repository version available at: ,"
"
19855,https://console.cloud.google.com/storage/gqn-dataset,here,Raw data files referred to in this document are available to download ,. To download the datasets you can use the 
19865,https://github.com/googlecreativelab/quickdraw-dataset/blob/master/README.md#numpy-bitmaps-npy,https://github.com/googlecreativelab/quickdraw-dataset/blob/master/README.md#numpy-bitmaps-npy,"
","
"
19870,https://tianchi.aliyun.com/dataset/dataDetail?dataId=52,https://tianchi.aliyun.com/dataset/dataDetail?dataId=52,Taobao: ," the released dataset is been changed a litte by Alibaba. If you want the totally same dataset as the paper used, you can click "
19870,http://jmcauley.ucsd.edu/data/amazon/links.html,http://jmcauley.ucsd.edu/data/amazon/links.html,Amazon: ,"
"
19878,#data,Data,"
","
"
19898,https://structured3d-dataset.org/,Structured3D dataset,2019.08.19: Report results on ,. (See 
19898,https://github.com/sunset1995/HorizonNet/blob/master/README_ST3D.md#dataset-preparation,the tutorial,See , to prepare training/validation/testing for HorizonNet.
19898,https://github.com/sunset1995/HorizonNet/blob/master/README_ZInD.md#dataset-preparation,the tutorial,See , to prepare training/validation/testing for HorizonNet.
19901,https://github.com/ssarfraz/SPL/tree/master/FCC_dataset,FCC_Dataset, | Contains the proposed loss formulation for both Tensorflow and Pytorch | ├ , | Contains scripts for recreating the FCC dataset and data preparations for training/testing | ├ 
19902,http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip,SciTail,Download and unzip , dataset to 
19902,http://cs.stanford.edu/people/mengqiu/data/qg-emnlp07-data.tgz,evaluation scripts,Download and unzip ,. Use the 
19905,http://cocodataset.org/#download,COCO,"Download MS COCO, ImageNet2012, NUS_WIDE in their official website: ",", "
19913,https://cran.r-project.org/package=cluster.datasets,cluster.datasets,. The real-world datasets are from R ’s , package.
19922,https://github.com/JinLi711/UTGN/blob/master/UTGN/data_processing/convert_to_tfrecord.py,"
convert_to_tfrecord.py
", data set or create a new one from scratch using the ," script), the following directory structure must be created:"
19922,https://github.com/JinLi711/UTGN/tree/master/UTGN/data_processing,data_processing,Below is an example of how to do this using the supplied scripts (in ,") and one of the pre-trained models, assumed to be unzipped in "
19922,https://github.com/aqlaboratory/proteinnet/blob/master/docs/raw_data.md,here,) used in building PSSMs can be obtained from ,. The script below assumes that 
19927,http://bdd-data.berkeley.edu/,BDD100K,The whole dataset is available at ,.
19927,./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py,lanenet_data_processor.py,Reminder: you should check , and 
19927,./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py,lanenet_data_processor_test.py, and ," to ensure that the processing of image path is right. You are recommended to use the absolute path in your image path list. Besides, this code needs batch size used in training and testing to be consistent. To enable arbitrary batch size in the testing phase, please refer to "
19935,data/deals,download, and is split into train and test parts (,"). To run the experiment, copy RuleKit JAR file into "
19935,data/methane,download, and concerns the problem of predicting methane concentration in a coal mine. The set is split into separate testing and training parts distributed in ARFF format (,"). For demonstration needs, a smaller version of these datasets suffixed with "
19935,../../wiki/1-Batch-interface#13-dataset-definition,Dataset definition,"
","
"
19935,https://github.com/adaa-polsl/RuleKit/blob/master/data/bone-marrow-uci.arff,"
Bone marrow transplant: children
",The repository contains the datasets used in the GuideR study. We also provide the latest UCI revision of the ," dataset. We recommend using this dataset at it contains lots of improvements compared to the previous release (e.g., textual encoding of attribute values)."
19941,http://www.viratdata.org/,VIRAT Ground,"For our dataset, we used the videos from ", and 
19941,data/refexp.csv,this link,The dataset can be downloaded using ,". In our annotation format, every row contains "
19941,http://www.viratdata.org/,VIRAT Ground Dataset Release2.0,"To extract features from scratch, ", and 
19942,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,depth completion,Please download KITTI , dataset. The structure of data directory:
19943,https://www.kaggle.com/martingorner/kannada-martin-keras-with-tf-data-augmentation,Martin Gorner’s TF-data-augmentation,"
","
"
19944,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/,here, are extracted from NASA MOSFET Thermal Overstress Aging Data Set which is available ,. Please cite their paper if you are going to use their data samples. Here is its BibTeX:
19956,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,StanfordCar,"
",.
19965,https://github.com/mhardalov/bg-reason-BERT/blob/master/data/bg_rc-v1.0.json,this link,"Our goal is to build a task in a low-resource language, such as Bulgarian, as close as possible to the multiple-choice reading comprehension in a high-resource language such as English. The dataset can be downloaded from ",.
19966,http://files.deeppavlov.ai/deeppavlov_data/bg_cs_pl_ru_cased_L-12_H-768_A-12.tar.gz,"
BERT, Slavic Cased
","
","
"
19966,http://files.deeppavlov.ai/deeppavlov_data/ner_bert_slav.tar.gz,"
NER, Slavic Cased
","
","
"
19970,https://github.com/jfzhang95/pytorch-video-recognition/blob/master/dataloaders/dataset.py,dataset.py,"These models were trained in machine with NVIDIA TITAN X 12gb GPU. Note that I splited train/val/test data for each dataset using sklearn. If you want to train models using official train/val/test data, you can look in ",", and modify it to your needs."
19982,http://vision.cs.duke.edu/DukeMTMC/data/misc/DukeMTMC-VideoReID.zip,data link,Download and unzip the dataset from the official github page. (,) 
20015,https://github.com/ruotianluo/self-critical.pytorch/blob/master/data/README.md,ruotianluo/self-critical.pytorch,"For more details and other dataset, see ","
"
20015,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,link,Download preprocessed coco captions from , from Karpathy's homepage. Extract 
20016,documentation/dataset_format.md,Dataset conversion,"
","
"
20016,documentation/manual_data_splits.md,Manual data splits,"
","
"
20021,https://github.com/Helsinki-NLP/prosody/tree/master/data,data, The corpus is available in the , folder. Clone this repository or download the files separately.
20024,https://github.com/godka/comyco-video-description-dataset,Video Description Dataset,"
","
"
20038,https://github.com/zhunzhong07/person-re-ranking/tree/master/evaluation/data/CUHK03,here,"Download ""cuhk03_new_protocol_config_detected.mat"" from ", and put it with cuhk-03.mat. We need this new protocol to split the dataset.
20040,https://www.med.upenn.edu/sbia/brats2018/data.html,BRATS2018," , Tumor MRI data in ", and Stroke MRI data in 
20051,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,CARS,", ",", "
20075,https://data.ciirc.cvut.cz/public/projects/2019VisionBasedNavigation/resources/model-checkpoints.tar.gz,https://data.ciirc.cvut.cz/public/projects/2019VisionBasedNavigation/resources/model-checkpoints.tar.gz,"
","
"
20077,https://github.com/cvjena/twitter-flood-dataset,this repository," trained on the European Flood 2013 dataset and evaluated on the Twitter images. You can find both, the Twitter dataset and the models, in ",.
20077,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/flickr100k.html,here,"To create a realistic image retrieval scenario, distractor images from the Flickr100k dataset are usually added to this dataset, which can be obtained ",.
20077,metadata.json,"
metadata.json
",The metadata for the images fetched from Wikimedia Commons is provided in the file ,". It contains an array of objects, each one describing a particular image using the following attributes:"
20087,https://www.crcv.ucf.edu/data/ucf-qnrf/,Link,1、 Dowload Dataset UCF-QNRF ,"
"
20104,#51-gan-tree-for-single-channel-dataset,GAN Tree for Single Channel Dataset,"
","
"
20104,#52-gan-tree-for-single-channel-mixed-dataset,GAN Tree for Single Channel Mixed Dataset,"
","
"
20104,#53-gan-tree-for-multiple-channel-mixed-dataset,GAN Tree for Multiple Channel Mixed Dataset,"
","
"
20104,#61-generated-gan-tree-for-single-channel-mixed-dataset,Generated GAN Tree for Single Channel Mixed Dataset,"
","
"
20104,#62-generated-gan-tree-for-multiple-channel-mixed-dataset,Generated GAN Tree for Multiple Channel Mixed Dataset,"
","
"
20107,http://adni.loni.usc.edu/data-samples/access-data/,this link,"The training of the FunFuseAn network was done with 500 MRI-PET image pairs available at Alzheimer’s Disease Neuroimaging Initiative (ADNI) as mentioned in the paper. Although, the data is available for public use, it is still required to apply for getting the access to the repository by filling out a questionaire. In case you are interested to obtain the data, please apply at ",". For conducting the inference of our network, in addition to the ADNI test data, we also used 10 image pairs from "
20115,https://pan.baidu.com/s/1usnQtW-YodlPUQ1TNrrafw#list/path=%2Fdataset%2Fkg4rs,https://pan.baidu.com/s/1usnQtW-YodlPUQ1TNrrafw#list/path=%2Fdataset%2Fkg4rs,Step 1. Download the data from , or 
20118,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove.840B.300d_dict.npy,"Glove word embeddings in our work, please download the file ", in this folder.
20129,http://jmcauley.ucsd.edu/data/amazon/,Amazon Product Data,"
","
"
20129,http://jmcauley.ucsd.edu/data/amazon/qa/,Amazon QA Data,"
","
"
20129,https://github.com/amazonqa/amazonqa/blob/master/src/prepro/preprocess_data.sh,script,The , generates the raw train/val/test product splits by combining the well known amazon reviews and questions dataset for all the categories.
20129,https://github.com/amazonqa/amazonqa/blob/master/src/prepro/create_data.sh,script,The , creates question-answers pairs with query-relevant review snippets and is_answerable annotation by a trained classifier. More details regarding this step are mentioned in the section 3.1 Data Processing.
20131,http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip,rectified images,Download the , from 
20131,http://roboimagedata.compute.dtu.dk/?page_id=36,DTU benchmark, from , and unzip it to 
20134,README.md#5-data-analysis,Data analysis,"
","
"
20134,README.md#52-transformed-data-tensors,Transformed data tensors,"
","
"
20134,scikit_tt/data_driven/,"
scikit_tt/data_driven
", combines data-driven methods with tensor network decompositions in order to significantly reduce the computational costs and/or storage consumption for high-dimensional data sets. Different methods can be found in the directory ,.
20134,scikit_tt/data_driven/tdmd.py,"
tdmd.py
",tDMD is an extension of the classical dynamic mode decomposition which exploits the TT format to compute DMD modes and eigenvalues. The algorithms below can be found in ,. See [
20134,scikit_tt/data_driven/transform.py,"
transform.py
"," provides methods to construct the counterparts of tranformed basis matrices, so-called transformed data tensors. The algorithms in ", include general basis decompositions [
20134,scikit_tt/data_driven/regression.py,"
regression.py
",Our toolbox provides different tensor-based methods to solve regression problems on transformed data tensors in the least-squares sense. The algorithms can be found in ,. The following functions have been implemented so far.
20134,scikit_tt/data_driven/tedmd.py,"
tedmd.py
",". Given a data set and list of basis functions, tEDMD can be used to approximate eigenvalues and eigenfunctions of evolution operators, i.e., Perron-Frobenius and Koopman operators. The basic procedures of tEDMD - combinations of the TT format and so-called AMUSE - are implemented in ",.
20134,scikit_tt/data_driven/tgedmd.py,"
tgedmd.py
",]. The algorithms can be found in ,.
20134,scikit_tt/data_driven/ulam.py,"
ulam.py
", can be used to approximate the corresponding Perron-Frobenius operator in TT format. The algorithms can be found in ,. See [
20136,http://images.cocodataset.org/zips/train2014.zip,Training (13GB),I'm using the MSCOCO '14 Dataset. You'd need to download the , and 
20136,http://images.cocodataset.org/zips/val2014.zip,Validation (6GB), and , images.
20136,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,"Andrej Karpathy's training, validation, and test splits",We will use ,". This zip file contain the captions. You will also find splits and captions for the Flicker8k and Flicker30k datasets, so feel free to use these instead of MSCOCO if the latter is too large for your computer."
20136,https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/blob/master/datasets.py,"
datasets.py
", in ,.
20136,https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset,"
Dataset
",This is a subclass of PyTorch ,. It needs a 
20136,https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader,"
DataLoader
", will be used by a PyTorch , in 
20136,https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#dataset,downloaded data, folders from your ,.
20137,#data-collection,Data Collection,"
","
"
20137,#data-cleaning,Data Cleaning,"
","
"
20137,https://github.com/vumaasha/Atlas/tree/master/dataset#11-taxonomy-generation,here,"We gathered taxonomy for the clothing top level category from popular Indian e-commerce fashion sites. We analyzed popular products, niche, and premium clothing products across these stores and developed our taxonomy with 52 category paths. The list of 52 category paths and additional details can be found ","
"
20137,https://github.com/vumaasha/Atlas/tree/master/data_collection,section,"For all categories in taxonomy tree, we collected product data and its images from popular Indian E-commerce stores. Web scraping tools like Scrapy and Selenium were used to extract the product title, breadcrumb, image and price of each product. Check out this ", to know more about our data collection strategy for 
20137,https://github.com/vumaasha/Atlas/tree/master/models/zoomed_vs_normal#data-cleaning,here,More details about the architecture of CNN Model and how we used it to clean our dataset can be found ,.
20138,#data-collection,Data Collection,"
","
"
20138,#data-cleaning,Data Cleaning,"
","
"
20138,https://github.com/vumaasha/Atlas/tree/master/dataset#11-taxonomy-generation,here,"We gathered taxonomy for the clothing top level category from popular Indian e-commerce fashion sites. We analyzed popular products, niche, and premium clothing products across these stores and developed our taxonomy with 52 category paths. The list of 52 category paths and additional details can be found ","
"
20138,https://github.com/vumaasha/Atlas/tree/master/data_collection,section,"For all categories in taxonomy tree, we collected product data and its images from popular Indian E-commerce stores. Web scraping tools like Scrapy and Selenium were used to extract the product title, breadcrumb, image and price of each product. Check out this ", to know more about our data collection strategy for 
20138,https://github.com/vumaasha/Atlas/tree/master/models/zoomed_vs_normal#data-cleaning,here,More details about the architecture of CNN Model and how we used it to clean our dataset can be found ,.
20143,https://github.com/feralvam/easse/tree/master/easse/resources/data/test_sets,"
evaluation sets
",Commonly used ,.
20143,https://github.com/feralvam/easse/tree/master/easse/resources/data/system_outputs,"
system outputs
",Literature , to compare to.
20147,http://pandas.pydata.org/,Pandas,"
", Python library v. 20.3+
20150,https://github.com/datajms,Jean-Matthieu Schertzer, and ,.
20159,https://github.com/emorynlp/bert-2019/tree/master/data,"
data
",See ,. All preprocessing scripts are placed in 
20169,#suggested-datasets,Suggested Datasets,"
","
"
20169,#using-your-data,Using Your Data,"
","
"
20169,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,link,CoNLL-2003 English ,"
"
20185,http://www.geniaproject.org/genia-corpus,GENIA corpus,Here we use the ,  and shared it in the 
20186,cqr_real_data_example.ipynb,cqr_real_data_example.ipynb,Please refer to , for basic usage. Comparisons to competitive methods and additional usage examples of this package can be found in 
20186,cqr_synthetic_data_example_1.ipynb,cqr_synthetic_data_example_1.ipynb, for basic usage. Comparisons to competitive methods and additional usage examples of this package can be found in , and 
20186,cqr_synthetic_data_example_2.ipynb,cqr_synthetic_data_example_2.ipynb, and ,.
20186,https://archive.ics.uci.edu/ml/datasets/BlogFeedback,Blog,"
",: BlogFeedback data set.
20186,https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure,Bio,"
",: Physicochemical  properties  of  protein  tertiary  structure  data  set.
20186,https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset,Bike,"
",: Bike  sharing  dataset  data  set.
20186,http://archive.ics.uci.edu/ml/datasets/communities+and+crime,Community,"
",: Communities   and   crime   data   set.
20186,http://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength,Concrete,"
",: Concrete compressive strength data set.
20186,https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset,Facebook Variant 1 and Variant 2,"
",: Facebook  comment  volume  data  set.
20186,https://github.com/yromano/cqr/blob/master/get_meps_data/README.md,this explanation,The Medical Expenditure Panel Survey (MPES) data can be downloaded using the code in the folder /get_meps_data/ under this repository. It is based on , (code provided by 
20186,https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181,MEPS_19,"
",": Medical expenditure panel survey,  panel 19."
20186,https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181,MEPS_20,"
",": Medical expenditure panel survey,  panel 20."
20186,https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192,MEPS_21,"
",": Medical expenditure panel survey,  panel 21."
20188,https://git.uwaterloo.ca/jimmylin/Castor-data,"
Castor-data
","
",": embeddings, datasets, etc."
20195,https://github.com/hpatches/hpatches-dataset,HPatches, and ," datasets. First, clone the dependencies with "
20201,https://lmb.informatik.uni-freiburg.de/resources/datasets/voc_dataset.tar.gz,Link,Download the dataset(,) and extract in 
20201,https://lmb.informatik.uni-freiburg.de/resources/datasets/pascal_context_labels.tar.gz,Link,Download the annotations(,) and extract in 
20201,https://www.cityscapes-dataset.com/,Link,Download the dataset from the Cityscapes dataset server(,"). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in "
20203,https://www.tensorflow.org/datasets/overview,here,". You can create your own dataset class, it simply needs to return a 4D tensor, and count the total number of elements. See ", on how to use tensorflow datasets.
20208,https://pandas.pydata.org/,pandas,"
", (0.22.0; for DailyDialog dataset)
20218,http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/fg/fgdata.tar.gz,here," folder, or refer to ","
"
20230,https://people.csail.mit.edu/yujia/files/distributional-signatures/data.zip,here,We ran experiments on a total of 6 datasets. You may download our processed data ,.
20230,https://trec.nist.gov/data/reuters/reuters.html,link," version, available at the link provided. | RCV1 (",") | Due to the licensing agreement, we cannot release the raw data. Instead, we provide a list of document IDs and labels. You may request the dataset from the link provided. | | Reuters-21578 ("
20230,https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html,link,") | Due to the licensing agreement, we cannot release the raw data. Instead, we provide a list of document IDs and labels. You may request the dataset from the link provided. | | Reuters-21578 (",) | Processed data available. | | Amazon reviews (
20230,http://jmcauley.ucsd.edu/data/amazon/,link,) | Processed data available. | | Amazon reviews (,) | We used a subset of the product review data. Processed data available. | | HuffPost headlines (
20230,https://www.kaggle.com/rmisra/news-category-dataset,link,) | We used a subset of the product review data. Processed data available. | | HuffPost headlines (,) | Processed data available. | | FewRel (
20241,https://ieee-dataport.org/open-access/file-fragment-type-fft-75-dataset,FFT-75, command in Unix-like systems but with much more cool techniques up its sleeve. It beats several previous benchmarks on the biggest play field there is right now.  FiFTy comes with pre-trained models for six scenarios and for block sizes of 512 and 4096 bytes.  It is retrainable for a subset of our studied filetypes and can be scaled up for newer filetypes and other block sizes too. Please find our corresponding paper at https://arxiv.org/abs/1908.06148 and the ready-to-use open access datasets at ,.
20241,https://ieee-dataport.org/open-access/file-fragment-type-fft-75-dataset],FFT-75 dataset,You need to download the ,.
20250,#custom-model-and-dataset,templates,". To implement custom models and datasets, check out our ",". To help users better understand and adapt our codebase, we provide an "
20250,docs/datasets.md,Datasets,"
","
"
20250,data/template_dataset.py,template,"If you plan to implement custom models and dataset for your new applications, we provide a dataset ", and a model 
20251,https://bingyaohuang.github.io/pub/CompenNeSt++/full_cmp_data,benchmark dataset (~11G), and CompenNet++ ,.
20251,https://bingyaohuang.github.io/pub/CompenNeSt++/full_cmp_data,benchmark dataset (~11G),Download CompenNet++ , and extract to 
20251,data,"
data/
", and extract to ,"
"
20258,../master/datasets,datasets,We provide the dirty and the clean version of a number of ,.
20290,https://database.lichess.org/,lichess.org database, knowledge in the game of crazhyouse for supervised neural networks is based on human played games of the ,.
20299,https://paperswithcode.com/sota/text-style-transfer-on-yelp-review-dataset?p=style-transfer-for-texts-to-err-is-human-but,"
PWC
","
","
"
20309,https://github.com/IdeasLabUT/CHIP-Network-Model/tree/master/storage/datasets,storage/datasets,All datasets used in this repo are either available in the , directory or will be automatically downloaded by the preprocessing script.
20314,/data,data,Download and place data in the ," directory, then uncompress them. First run "
20314,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TORICY,Dataverse, and ,. See more details in this 
20314,/data/README.md,data description,. See more details in this ,.
20316,https://pytorch.org/docs/stable/nn.html#dataparallel,Dataparallel,) for significantly faster training (around 4x speedup from pytorch ,)
20316,#caveats-in-distributeddataparallel,caveat in using distributed training, instead. This trains using the DistributedDataParallel mode. (Also see , below)
20319,https://pandas.pydata.org/,Pandas," evaluate the accuracy of NILM algorithms. If you are a new Python user, it is recommended to educate yourself on ",", "
20329,https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/,JIGSAWS dataset,You will need the , to re-run the experiments of the paper.
20330,http://cocodataset.org/#home,Microsoft COCO,"
", - 80 common object categories
20331,http://pascal.inrialpes.fr/data/evve/,EVVE,"
", - Event-based Video Retrieval
20335,#alternative-dataset-and-features-download-links,other sources,"If download speed is slower than expected, the pre-trained model could also be downloaded from ",. Please help put the downloaded file at 
20335,#alternative-dataset-and-features-download-links,Alternative Download,Download faster-rcnn features for MS COCO train2014 (17 GB) and val2014 (8 GB) images (VQA 2.0 is collected on MS COCO dataset). The image features are also available on Google Drive and Baidu Drive (see , for details).
20335,#alternative-dataset-and-features-download-links,Alternative Download,). The image features are also available on Google Drive and Baidu Drive (see , for details).
20335,https://nlp.stanford.edu/data/gqa/eval.zip,this official evaluator, while our local evaluator only calculates the exact matching. Please use , (784 MB) if you want to have the exact number without submitting.
20335,#alternative-dataset-and-features-download-links,Alternative Download,Download the NLVR2 image features for train (21 GB) & valid (1.6 GB) splits. The image features are also available on Google Drive and Baidu Drive (see ," for details). To access to the original images, please follow the instructions on "
20335,http://cocodataset.org,MS COCO,Run on the whole , and 
20335,http://cocodataset.org/#captions-2015,COCO caption,", ",", "
20335,http://cocodataset.org/#download,MS COCO official website,"Download the MS COCO train2014, val2014, and test2015 images from ",.
20335,http://cocodataset.org/#home,MS COCO,"We thank all the authors and annotators of vision-and-language datasets (i.e., ",", "
20340,http://data.csail.mit.edu/soundnet/soundnet_models_public.zip,here,"We provide pre-trained models that are trained over 2,000,000 unlabeled videos. You can download the 8 layer and 5 layer models ",. We recommend the 8 layer network.
20342,http://nlp.stanford.edu/data/glove.6B.zip,"
Pre-trained glove embedding
","
",: 
20350,https://www.broadinstitute.org/data-sciences-platform,Broad Institute,"
","
"
20350,https://datalogue.io,Datalogue,"
","
"
20352,http://www.teradata.com,Teradata,"
","
"
20357,https://www.yelp.com/dataset/challenge,Yelp,"
","
"
20357,https://snap.stanford.edu/data/web-FineFoods.html,Amazon,"
","
"
20371,https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/,Pen-digit data,: Codes for , learning
20380,https://github.com/tsunghan-wu/Depth-Completion/blob/master/doc/data.md,dataset section,. Detailed instructions for downloading the dataset are described in the ,"
"
20381,https://github.com/nlpyang/PreSumm/tree/dev/raw_data,raw_data directory,There are example input files in the ,"
"
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset,data/pcfgset,The folder ," contains six folders with the data to compute the overall task accuracy for PCFG set for a model, and the data and scripts to conduct the five main tests in the evaluation paradigm -- "
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset/systematicity,systematicity," contains six folders with the data to compute the overall task accuracy for PCFG set for a model, and the data and scripts to conduct the five main tests in the evaluation paradigm -- ",", "
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset/productivity,productivity,", ",", "
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset/substitutivity,substitutivity,", ",", "
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset/localism,localism,", ", and 
20383,https://github.com/i-machine-think/am-i-compositional/tree/master/data/pcfgset/overgeneralisation,overgeneralisation, and ,. In the folders for these tests you can find also the data to conduct the auxiliary tests that we conducted in the paper to further clarify the results of the different tests in the evaluation paradigm.
20385,https://data.bris.ac.uk/data/dataset/2g1n6qdydwa9u22shpxqzp0t8m,here,", where you can find instructions on how to use the script. Your copy of the dataset (either EPIC-KITCHENS-100 or EPIC-KITCHENS-55) should have the same folder structure provided in the script (which can be found ","). Also you should untar each video's frames in its corresponding folder, e.g for "
20391,http://www.qizhexie.com/data/RACE_leaderboard.html,RACE Reading Comprehension Dataset Leaderboard,"
","
"
20391,https://www.nvidia.com/en-us/data-center/a100/,A100, to perform scaling studies and use up to 3072 ," GPUs for the largest model. Each cluster node has 8 NVIDIA 80GB A100 GPUs. The graph below shows that we scale nearly linear up to 1 trillion parameter models running on 3072 GPUs. Note that these results are from benchmark runs and these models were not trained to convergence; however, the FLOPs are measured for end-to-end training, i.e., includes all operations including data loading, optimization, and even logging."
20391,#data-preprocessing,Data Preprocessing,"
","
"
20391,#datasets,Datasets,"
","
"
20391,#collecting-wikipedia-training-data,Collecting Wikipedia Training Data,"
","
"
20391,#collecting-gpt-webtext-data,Collecting GPT Webtext Data,"
","
"
20391,./tools/preprocess_data.py,"
preprocess_data.py
", flag in , The other metadata are optional and are not used in training.
20391,./tools/preprocess_data.py,"
preprocess_data.py
",Further command line arguments are described in the source file ,.
20391,http://www.cs.cmu.edu/~glai1/data/race/,RACE dataset,The following script finetunes the BERT model for evaluation on the ,. The 
20391,https://www.kaggle.com/quora/question-pairs-dataset,Quora Question Pairs,". Because the matching tasks are quite similar, the script can be quickly tweaked to work with the ", (QQP) dataset as well.
20391,#data-preprocessing,above, as described ," to include sentence breaks in the produced index. If you'd like to use Wikipedia data for GPT training you should still clean it with nltk/spacy/ftfy, but do not use the "
20406,https://github.com/ricsinaruto/Seq2seqChatbots/tree/master#generate-data,generate_data,{,", "
20412,https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford 5k, (June 2022): We updated download files for , and 
20412,https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris 6k, and ," images to use images with blurred faces as suggested by the original dataset owners. Bear in mind, ""experiments have shown that one can use the face-blurred version for benchmarking image retrieval with negligible loss of accuracy""."
20412,https://www.kaggle.com/google/google-landmarks-dataset,"
google-landmarks-2018 (gl18)
", and ," train datasets. For this architecture, there is no need to compute whitening as post-processing step (typically the performance boost is insignificant), although one can do that, as well. For example, multi-scale evaluation of ResNet101 with GeM with projection layer trained on "
20412,https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford 5k, (June 2022): We updated download files for , and 
20412,https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris 6k, and ," images to use images with blurred faces as suggested by the original dataset owners. Bear in mind, ""experiments have shown that one can use the face-blurred version for benchmarking image retrieval with negligible loss of accuracy""."
20413,https://github.com/ThiagoCF05/DeepNLG/tree/master/evaluation/data,here,"For reproducibility reasons, the data used in the experiments can be found ",", whereas the results are "
20418,http://cocodataset.org/#download,here,"Next, set up the COCO dataset. You can download it from ",", and update the paths in "
20418,https://github.com/cocodataset/cocoapi,here," to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from ",.
20422,http://www.vision.caltech.edu/~gvanhorn/datasets/inaturalist/fgvc5_competition/categories.json.tar.gz,1, datasets (,", "
20422,https://storage.googleapis.com/inat_data_2018_us/train2018.json.tar.gz,2,", ",", "
20422,https://storage.googleapis.com/inat_data_2018_us/train_val2018.tar.gz,3,", ",) into the folder 
20429,https://database.lichess.org/,lichess.org database, knowledge in the game of crazhyouse for supervised neural networks is based on human played games of the ,.
20443,https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb,"Training a Simple Neural Network, with TensorFlow Dataset Data Loading","
","
"
20475,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5,Download ," dataset. Since GTA-5 contains images with different resolutions, we need to resize all images to 1052x1914."
20475,https://www.cityscapes-dataset.com/,Cityscapes,Download ,.
20484,https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/amie/,AMIE+,Folder AMIE contains data arranged in the structure which is designed to be compatible with ,;
20487,https://exchangelabsgmu-my.sharepoint.com/personal/xguo7_masonlive_gmu_edu/Documents/dataset_DGT/processed_RF_data.zip,Processed RF data,"
","
"
20487,http://www.image-net.org/challenges/LSVRC/2010/#data,ILSVRC2010,"
",". It contains about 1.2M images for training (with 650 ∼ 3000 images per class), 50K images for validation and 150K images for testing."
20491,https://multimediacommons.wordpress.com/yfcc100m-core-dataset/,YFCC100M,You can find the full version of dataset via , and raw data 
20492,http://www.sysu-hcp.net/kinect2-human-pose-dataset-k2hpd/,link,K2HPD Body Pose Dataset [,]
20509,https://datatracker.ietf.org/doc/html/rfc9171,RFC 9171,Bundle Protocol Version 7 (,)
20509,https://datatracker.ietf.org/doc/html/rfc9174,RFC 9174,) (,"), including:"
20517,https://trec.nist.gov/data/reuters/reuters.html,text data,", ", (
20517,https://www.yelp.com/dataset/challenge,Yelp,"
", (
20517,https://dtai.cs.kuleuven.be/clus/hmcdatasets/,FunGO,"
","
"
20524,http://cocodataset.org/,Microsoft COCO,"HCOCO, containing 42k synthesized composite images, is generated based on "," dataset. The foreground region is corresponding object segmentation mask provided from COCO. Within the foreground region, the appearance of COCO image is edited using various color transfer methods. "
20524,http://data.csail.mit.edu/graphics/fivek/,MIT-Adobe FiveK,HAdobe5k is generated based on ," dataset. Provided with 6 editions of the same image, we manually segment the foreground region and exchange foregrounds between 2 versions. "
20530,https://github.com/xthan/polyvore-dataset/blob/master/category_id.txt,category_id.txt,"
", contains the maping between category ID and category name. Thanks 
20530,https://github.com/kristenvaccaro/fashion-data,[dataset],"
","
"
20534,https://ai.tencent.com/ailab/nlp/en/data/Tencent_AILab_ChineseEmbedding.tar.gz,here,. Download it ,.
20534,https://ai.tencent.com/ailab/nlp/en/data/Tencent_AILab_ChineseEmbedding.tar.gz,Tencent Embeddings,Download pretrained embeddings Download ,", extract it and put it in "
20545,https://pypdf.readthedocs.io/en/stable/user/metadata.html,metadata, and , from PDFs as well.
20551,data.csv,CSV format,Raw results in ,.
20557,https://ibm.ent.box.com/v/paccmann-pytoda-data,here,The following data from ,:
20557,https://github.com/PaccMann/paccmann_datasets/blob/master/pytoda/smiles/smiles_language.py,SMILESLanguage,A pickled , object (
20557,https://ibm.ent.box.com/v/paccmann-pytoda-data/file/548614344106,README.md, please refer to the , and to the manuscript for details on the datasets used and the preprocessing applied.
20557,https://ibm.ent.box.com/v/paccmann-pytoda-data/folder/91897885403,pretrained models," the workload required to run the full pipeline is intesive and might not be straightforward to run all the steps on a desktop laptop. For this reason, we also provide ", that can be downloaded and used to run the different steps.
20560,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking dataset,The experiments were done on ,.
20561,http://www.cvlibs.net/datasets/kitti/raw_data.php,raw KITTI dataset,You can download the entire , by running:
20561,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,benchmark split," of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new ", or the 
20561,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,odometry split, or the , by setting the 
20561,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,new KITTI depth benchmark,  | Evaluate with the improved ground truth from the , | | 
20561,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,new KITTI depth benchmark,        | The , test files. |
20561,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI odometry dataset,"For this evaluation, the ","
"
20562,http://en.wikipedia.org/wiki/Algebraic_data_type,Algebraic Data Types,This library implements , for Java. ADT4J provides annotation processor for 
20563,#updating-deeply-nested-immutable-data-structure,Updating deeply nested immutable data structure,"
","
"
20563,#extensible-algebraic-data-types,Extensible algebraic data types,"
","
"
20563,#generalized-algebraic-data-types,Generalized Algebraic Data Types,"
","
"
20563,#updating-deeply-nested-immutable-data-structure,updating deeply nested immutable data structures,"By returning a function, modifiers and setters allow for a lightweight syntax when ",.
20579,https://www.eecs.yorku.ca/~kamel/sidd/dataset.php,SIDD Medium Dataset,The real-world VDN denoiser was trained on the ,", and directly tested on the SIDD and DND benchmarks."
20584,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,"
acdc_challenge
",Database at: ,. An atlas of the heart in each projection can be found at this 
20585,https://biendata.com/competition/ccks_2019_ipre/data/,the competition website,Please download the data from ,", then unzip files and put them in "
20594,data,data,Data is available in the sub-directory ,", with a specific "
20594,data/LICENSE,LICENSE,", with a specific ", file.
20595,http://www.robots.ox.ac.uk/~lz/DEM_cvpr2017/data.zip,here,Download the datasets from , and put them into the 
20620,https://github.com/soskek/bookcorpus,BookCorpus, and ," datasets, preprocess them using the scripts under "
20622,https://github.com/google-research-datasets/paws/tree/master/pawsx,here,"We released PAWS-X, a multilingual version of PAWS for six languages. See ", for more details.
20622,https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pairs,"This dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the ", (QQP) dataset.
20622,https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pairs,"The first pair has different semantic meaning while the second pair is a paraphrase. State-of-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing datasets such as the ",.
20622,https://github.com/google-research-datasets/paws/tree/master/wiki_raw_and_mapping#raw-sentences-and-mappings-for-paws-wiki,here,We also release source sentences that are used to generate this dataset and their mappings. Please see , for more details.
20622,https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pairs,This corpus contains pairs generated from the , corpus. We cannot directly distribute the raw 
20622,https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pairs dataset," corpus, first download the original ", and save the tsv file to some location 
20622,https://github.com/google-research-datasets/paws/tree/master/pawsx,here,"This corpus contains translations of the PAWS examples in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. Please see ", for more details.
20637,https://github.com/dykang/PASTEL/blob/master/data/data_v2.zip,data/data_v2.zip, of our dataset in ,: 
20651,https://github.com/Amazing-J/structural-transformer/tree/master/corpus_sample/baseline_corpus,./corpus_sample/baseline_corpus, from Stanford corenlp to preprocess our data. We also provide sample input for baseline (,).
20651,https://github.com/Amazing-J/structural-transformer/blob/master/corpus_sample/five_path_corpus/train_concept_no_EOS_bpe,corpus_sample/.../train_concept_no_EOS_bpe, For example: ,"
"
20651,https://github.com/Amazing-J/structural-transformer/blob/master/corpus_sample/all_path_corpus/train_edge_all_bpe,corpus_sample/all_path_corpus, represents the structural relationship in the AMR graph. We give the corresponding corpus sample ,. Each line in the corpus represents the structural relationship between all nodes in an AMR graph. Assuming $$n$$ concept nodes are input( 
20651,https://github.com/Amazing-J/structural-transformer/tree/master/corpus_sample/five_path_corpus,corpus_sample/five_path_corpus,"To overcome the data sparsity in the above feature-based method, we view the structural path as a label sequence. We give the corresponding corpus sample ", . We split the 
20655,https://www.fc.up.pt/addi/ph2%20database.html,Skin Lesion Segmentation PH2 Dataset,", ", and 
20655,https://www.kaggle.com/kmader/finding-lungs-in-ct-data/data,Lung segmentation, and , dataset added.)
20655,https://www.kaggle.com/kmader/finding-lungs-in-ct-data/data,Kaggle,1- Download the Lung Segmentation dataset from , link and extract it. 
20655,https://www.kaggle.com/kmader/finding-lungs-in-ct-data/data,Lung kaggle, Lung Segmentation | , | 
20663,#datasets,Datasets,"
","
"
20663,https://github.com/shehzaadzd/MINERVA/tree/master/datasets/data_preprocessed/FB15K-237,Here,. take a look: ,"
"
20671,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove,. Download , file and change the path in 'AGDT/thumt/thumt/bin/trainer.py' correspondingly. The dataset we used is from 
20677,https://data.aliyun.com/product/learn,PAI,"This is an implementation on experiment of offline JD dataset rather than the online official version. There may be differences between results reported in the paper and the released one, because the former one is achieved in distribution tensorflow on our internal deep learning platform ",.
20681,https://visualdialog.org/data,VisDial,"(optional) For the ""End-to-end + Visual"" baseline, first download images from ", to the 
20696,https://www.pecanstreet.org/dataport/,Dataport,We use the ," dataset for evaluation purpose. It is thelargest public residential home energy dataset, which containsthe appliance-level and household aggregate energy consumptionsampled every minute from 2012 to 2018."
20706,http://www.visualdataweb.de/webvowl/#iri=https://raw.githubusercontent.com/i40-Tools/RAMIOntology/master/rami.ttl,WebVOwl API,. You can find a visualization of the latest version can be created using the ,.
20706,https://linked-data-fu.github.io/,Linked Data-Fu,) can be implemented using N3 rules as specified by ,.
20706,http://www.visualdataweb.de/webvowl/,WebVOwl," is the tool of choice. As previously mentioned, ", is a great service to create visualizations.
20706,https://linked-data-fu.github.io/,Linked Data-Fu,The N3 reasoning rules can be applied through ,:
20710,https://github.com/softbankrobotics-research/qibullet/tree/master/qibullet/robot_data/LICENSE,license,"The installation of the additional resources will automatically be triggered if you try to spawn a Pepper, NAO or Romeo for the first time. If qiBullet finds the additional resources in your local folder, the installation won't be triggered. The robot meshes are under a specific ",", you will need to agree to that license in order to install them. More details on the installation process can be found on the "
20720,https://gitlab.com/sutd_nlp/overlapping_mentions/tree/master/data/GENIA,here,The full GENIA data can be found ,.
20734,https://github.com/mstrise/dep2label-eye-tracking-data/blob/master/dep2label/preprocessing_human_data/encoded_labels.py,script,A dependency tree can be encoded into labels using the ,.
20734,https://github.com/mstrise/dep2label-eye-tracking-data/blob/master/dep2label/preprocessing_human_data/human_data.py,code,"In addition, it is necessary to prepare and preprocess the human data. You can split and extract the gaze features as we did by using this ",.
20734,https://github.com/mstrise/dep2label-eye-tracking-data/blob/master/config/parallel.config,config file,"
", for 
20734,https://github.com/mstrise/dep2label-eye-tracking-data/blob/master/config/disjoint.config,config file,"
", for 
20741,http://snap.stanford.edu/data/CollegeMsg.html,here, is available from the SNAP dataset collection ,", while the "
20741,https://www.cs.cornell.edu/~arb/data/temporal-reddit-reply/index.html,here, dataset can be found ,". Finally, the household power consumption dataset has been downloaded from the UCI ML repository, and is available "
20741,https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption,here,". Finally, the household power consumption dataset has been downloaded from the UCI ML repository, and is available ",.
20745,https://vcl.iti.gr/dataset/human4d/,"
Project Page
","
","
"
20747,https://github.com/huggingface/datasets,datasets, : RONEC v2 is available on HuggingFace's ," library. To use, simply "
20747,https://huggingface.co/datasets/ronec,here, - please see the simple format below. You can also explore the dataset ,.
20755,https://circleci.com/gh/PolyAI-LDN/conversational-datasets,"
CircleCI
","
","
"
20755,`https://www.tensorflow.org/tutorials/load_data/tf_records`,Tensorflow record files,or as , containing serialized 
20755,https://www.tensorflow.org/tutorials/load_data/tf_records#data_types_for_tfexample,tensorflow example, containing serialized , protocol buffers.
20755,https://cloud.google.com/dataflow/,Google Dataflow," scripts, run on ",". This parallelises the data processing pipeline across many worker machines. Apache Beam requires python 2.7, so you will need to set up a python 2.7 virtual environment:"
20755,https://cloud.google.com/dataflow/quotas,quota,"Dataflow will run workers on multiple Compute Engine instances, so make sure you have a sufficient ", of 
20776,https://github.com/golsun/SpaceFusion/tree/master/data#multi-ref-reddit,script,"), can be generated using this ",.
20776,https://github.com/golsun/StyleFusion/tree/master/data/Holmes,here,"), avaialble ","
"
20776,https://github.com/golsun/StyleFusion/tree/master/data/arXiv,here,"), can be obtained following instructions ","
"
20776,https://github.com/golsun/StyleFusion/tree/master/data/toy,toy dataset,A , is provied as an example following the format described above.
20776,https://github.com/golsun/StyleFusion/tree/master/data/test,here,"
","
"
20776,https://github.com/golsun/StyleFusion/blob/master/data/README.md,here,See , for more details and instructions.
20788,http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml,IBM Claim Stance Dataset, model and was fine-tuned on the ,. The model files are hosted on 
20788,http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml,IBM Claim Stance Dataset,| Domain | Application | Industry  | Framework | Training Data | Input Data | | --------- | --------  | -------- | --------- | --------- | --------------- | | Natural Language Processing (NLP) | Sentiment Analysis | General | TensorFlow | , | Text |
20788,http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Project,IBM Claims Stance Dataset,"
", and 
20788,http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Project,LICENSE 1, | ,"
"
20790,http://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/,MAPS,obtain the , dataset
20809,https://github.com/khanhptnk/hanna-private/tree/master/data,Download data,"
",.
20813,https://github.com/kuanghuei/SCAN#data-pre-processing-optional,data pro-processing,", where we set #regions=36 in our paper. We provide the precomputed image features for the testing datasets adopted from Flickr 8k/30k, MSCOCO and Pascal. If you want to evalute on your own captioning dataset, please follow the instruction of ", in 
20813,#download-data-and-pretrained-scan-model,above format,Prepare your new captioning dataset in the ,. Run the above command 
20819,https://github.com/wenhuchen/Table-Fact-Checking/blob/master/tokenized_data/train_examples.json,Training Set,"
",|
20819,https://github.com/wenhuchen/Table-Fact-Checking/blob/master/tokenized_data/val_examples.json,Validation Set,|,|
20819,https://github.com/wenhuchen/Table-Fact-Checking/blob/master/tokenized_data/test_examples.json,Test Set,|,: The data has beeen tokenized and lower cased. You can directly use them to train/evaluation your model.
20828,#datasets,Datasets,"
","
"
20829,#datasets,Datasets,"
","
"
20833,http://www.geniaproject.org/genia-corpus/pos-annotation,GENIA,Put the corpus ," into the ""../GENIA/"" directory"
20834,http://www.geniaproject.org/genia-corpus/pos-annotation,GENIA,Put the corpus ," into the ""../GENIA/"" directory"
20837,https://github.com/ratishsp/data2text-plan-py,data2text-plan-py," (Gong, H., Feng, X., Qin, B., & Liu, T.; EMNLP 2019); this code is based on ",.
20846,https://dl.fbaipublicfiles.com/torchbiggraph/wikidata_translation_v1.tsv.gz,Download,) [,]
20850,https://github.com/nyu-dl/dl4mt-nonauto#downloading-datasets--pre-trained-models,here,WMT'16 English to Romania (EN-RO) can be obtained from ,.
20853,https://huggingface.co/docs/transformers/model_doc/data2vec,Data2Vec,"
","
"
20854,https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs,duplicate questions identification,", ", et al. More details about the underneath model can be found in our 
20856,https://github.com/herobd/NAF_dataset/releases/tag/v1.0,release, from the Github , and put it in the dataset directory root.
20866,https://travis-ci.org/neurodata/graspy,"
Build Status
","
","
"
20866,https://codecov.io/gh/neurodata/graspy,"
codecov
","
","
"
20866,https://graspy.neurodata.io/tutorial.html,tutorial section,Please visit the , in the official website for more in depth usage.
20866,https://graspy.neurodata.io/contributing.html,contribution guidelines,We welcome contributions from anyone. Please see our , before making a pull request. Our 
20866,https://github.com/neurodata/graspy/issues,issues, before making a pull request. Our ," page is full of places we could use help! If you have an idea for an improvement not listed there, please "
20866,https://github.com/neurodata/graspy/issues/new,make an issue," page is full of places we could use help! If you have an idea for an improvement not listed there, please ", first so you can discuss with the developers.
20866,https://github.com/neurodata/graspy/blob/master/LICENSE,Apache 2.0 License,This project is covered under the ,.
20866,https://github.com/neurodata/graspy/issues,issues,We appreciate detailed bug reports and feature requests (though we appreciate pull requests even more!). Please visit our , page if you have questions or ideas.
20867,https://github.com/cocodataset/cocoapi,pycocotools,"
","
"
20867,https://github.com/kuanghuei/SCAN/blob/master/README.md#data-pre-processing-optional,here, to obtain image features for fair comparison. More details about data pre-processing (optional) can be found ,". All the data needed for reproducing the experiments in the paper, including image features and vocabularies, can be downloaded from "
20876,https://taalmaterialen.ivdnt.org/download/lassy-klein-corpus6/,Lassy Small,"This repository is required to access and play with the æthel dataset, which contains typelogical analyses for the majority of the ", corpus. Begin by cloning the project locally and unzipping the dump file in 
20887,http://aix360.mybluemix.net/data,AI Explainability 360 interactive experience,The , provides a gentle introduction to the concepts and capabilities by walking through an example use case for different consumer personas. The 
20887,aix360/data/README.md,aix360/data/README.md,"If you'd like to run the examples and tutorial notebooks, download the datasets now and place them in their respective folders as described in ",.
20895,http://cocodataset.org/#download,here,Download the COCO 2014 dataset from ,". In particualr, you'll need the 2014 Training and Validation images. "
20895,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,here, Then download Karpathy's Train/Val/Test Split. You may download it from ,.
20895,https://github.com/cocodataset/cocoapi,here,"If you want to do evaluation on COCO, download the COCO API from ", if your on Linux or from 
20895,http://cocodataset.org/#download,here,Download the COCO 2014 dataset from ,". In particualr, you'll need the 2014 Training and Validation images. "
20895,http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip,here, Then download Karpathy's Train/Val/Test Split. You may download it from ,.
20895,https://github.com/cocodataset/cocoapi,here,"If you want to do evaluation on COCO, download the COCO API from ", if your on Linux or from 
20896,https://github.com/radioML/dataset,source code of RadioML's generation,See ,.
20913,#code-and-data-preparation,Code and Data Preparation,"
","
"
20913,#download-datasets,Download Datasets,"
","
"
20922,#datasets,Datasets,"
","
"
20922,./download-glas-dataset.sh,./download-glas-dataset.sh,GlaS: ,.
20922,https://github.com/jeromerony/survey_wsl_histology/tree/init-branch/datasets-split,datasets-split,See ,.
20922,https://github.com/jeromerony/survey_wsl_histology/blob/init-branch/datasets-split/README.md,datasets-split/README.md,Detailed documentation: ,.
20926,https://storage.cloud.google.com/gcc-data/OID-rated-image-caption/v1/oid.rated-image-captions.v1.zip?organizationId=433637338589,Download,"
","
"
20926,https://storage.cloud.google.com/gcc-data/OID-rated-image-caption/v2/oid.rated-image-captions.v2.zip?organizationId=433637338589,Download,"
","
"
20926,https://storage.cloud.google.com/gcc-data/cvpr2019/cvpr2019.cc-workshop.zip?organizationId=433637338589&_ga=2.185119273.-1459768570.1557891230,Download,"
","
"
20926,http://www.conceptualcaptions.com/winners-and-data,Leaderboard,. This dataset has ratings for the top 5 models in the challenge (,"). The images in this set are disjoint from the images in all other versions above, and our recommendation is to use this as a test set for all versions of the Image Caption Quality Dataset."
20931,http://data.dws.informatik.uni-mannheim.de/rdf2vec/,here,. You can find the graph embeddings ,.
20935,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,link,Download pretrained GloVe embeddings with this , and extract 
20937,https://archive.ics.uci.edu/ml/datasets/kdd+cup+1999+data,KDDCUP,", ",", "
20937,https://archive.ics.uci.edu/ml/datasets/Record+Linkage+Comparison+Patterns,Record Linkage,", ",", "
20942,#making-predictions-on-existing-datasets,Making predictions on existing datasets,"
","
"
20942,#working-with-new-datasets,Working with new datasets,"
","
"
20942,doc/data.md,data, folder for documentation with more details on the ,", "
20942,https://github.com/dwadden/dygiepp/blob/master/doc/data.md#converting-data-labeled-with-brat,here, annotations to DyGIE. See , for more details. Thanks to @serenalotreck for this feature.
20942,https://ai2-s2-mechanic.s3-us-west-2.amazonaws.com/data/data.zip,Download the dataset,"
","
"
20942,https://biocreative.bioinformatics.udel.edu/news/corpora/chemprot-corpus-biocreative-vi/,ChemProt,The , corpus contains entity and relation annotations for drug / protein interaction. The ChemProt preprocessing requires a separate environment:
20942,doc/data.md,doc/data.md,"For more information on ACE relation and event preprocessing, see ", and 
20942,doc/data.md,data.md,". For detailed descriptions, see ",. The results will go in 
20942,doc/data.md,data.md," field, see ",.
20942,docs/data.md,docs/data.md,"The predictions include the predict labels, as well as logits and softmax scores. For more information see, ",.
20942,doc/data.md#formatting-a-new-dataset,Formatting a new dataset,Follow the instructions as described in ,.
20942,doc/model.md#multi-dataset-training,here, field for your new dataset matches the label namespaces for the pretrained model. See ," for more on label namespaces. To view the available label namespaces for a pretrained model, use "
20942,#making-predictions-on-existing-datasets,existing datasets,Make predictions the same way as with the ,:
20943,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7Scenes,We support the , and 
20943,http://robotcar-dataset.robots.ox.ac.uk/,Oxford RobotCar, and , datasets right now. You can also write your own PyTorch dataloader for other datasets and put it in the 
20943,https://github.com/samarth-robo/robotcar-dataset-sdk/tree/master,this fork,Download ," of the dataset SDK, and run "
20945,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,Download , to 'dataset' file
20966,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking Benchmark, directory. You need to download and unzip the data from the , and put them in the 
20966,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking Benchmark,. We use RRC detection for the ,.
20974,http://www.cis.lmu.de/ocrworkshop/data/pocoto/,here,The lastest compiled binary can be downloaded ,.
20979,http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls,database url,Set an environment variable containing your ,". If you don't, nashi will create a sqlite database called ""test.db"" in your working directory."
20981,http://www.tmbdev.net/ocrdata-split/,UW3 dataset,) was trained on the , (link seems to be down)
20982,https://huggingface.co/datasets/allenai/csabstruct,Huggingface Hub,CSAbstrcut is also available on the ,.
20989,https://data.fit.fraunhofer.de/index.php/s/4yXxzSoRgnI18XY,here,The preprocessed data can be downloaded from , with the password of '123' (without quote).
20994,hdk/docs/data_retention.md,cl_dram_dma_data_retention," | The Virtual Ethernet framework facilitates streaming Ethernet frames from a network interface (or any source) into the FPGA for processing and back out to some destination. Possible use cases for this include deep packet inspection, software defined networking, stream encryption or compression, and more. | | Pipelined Workload Applications | ",| 
20994,SDAccel/examples/aws/data_retention,SDAccel,"
", | Demonstrates how to preserve data in DRAMs while swapping out accelerators. Applications that use a temporal accelerator pipeline can take advantage of this feature to reduce latency between FPGA image swaps  | | Digital Up-Converter using High Level Synthesis | 
21000,http://sourceforge.net/projects/mulan/files/datasets/nuswide-cVLADplus.rar,here, from , and run 
21004,data/dataset_splits,dataset splits,A model has to be trained and evaluated using the four different ,". Afterwards, the resulting evaluation json files should be merged into a single json containing the results for all 24 held out concept pairs. The average recall@5 and some other statistics can be visualized using "
21007,https://github.com/mmalekzadeh/privacy-preserving-bandits/tree/master/experiments/Criteo/criteo_dataset,"
create_datasets.ipynb
",", for the first time, the script ", should be used. You should first set this parameter (number of rows) in the  
21007,https://github.com/mmalekzadeh/privacy-preserving-bandits/tree/master/experiments/Criteo/criteo_dataset,"
create_datasets.ipynb
",", build the dataset, and then run the Criteo experiment. Please see ", for more dtail.
21020,https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet,here, from ,.
21022,#data,Data,"
","
"
21024,process_data/,here,Follow the instructions ,.
21037,https://www.cs.ucr.edu/~eamonn/time_series_data/,here,. We used the 85 datasets listed ,.
21037,https://www.cs.ucr.edu/~eamonn/time_series_data/,website,", which can be downloaded from this ",.
21038,https://github.com/gepetto/example-robot-data,example-robot-data,"
"," (optional for examples, install Python loaders)"
21040,docs/dataset_format_description.md,this format,Repository for data collection in CARLA version 0.8.4. The data collected follows ,.
21040,dataset_configurations/coil_training_dataset_singlecamera.py,dataset configuration file,The user can configure a ,". This file contains a set of start/end positions, weathers and number of dynamic objects to appear on every data collection episode. Further, the user also configure a "
21040,docs/dataset_format_description.md,with several measurements and sensors stored on a specific format,This configuration file and the expert demonstrator are used inside a collector module that can be replicated on several docker instances. This instances produce a dataset on ,.
21040,docs/dataset_format_description.md,data format,By default it opens three CARLAs per GPU. Note you will see some connection closed errors for a while until the CARLA dockers are ready to receive connections. The collected data will have the following ,"
"
21043,https://github.com/sisinflab/LODrecsys-datasets,freely available mapping," resources, we exploited a ",. For the remaining one (
21050,data/human_kinect.pkl,here,We designed a simple card-making task with a goal to make a Baymax birthday card. The dataset is available at ,. Description of the data is in 
21050,data/README.md,here,. Description of the data is in ,"
"
21053,https://20bn.com/datasets/jester,jester dataset,Download the , or 
21053,https://20bn.com/datasets/something-something/v2,something-something-v2 dataset, or ,. Decompress them into the same folder and use 
21053,process_dataset.py,process_dataset.py,. Decompress them into the same folder and use ," to generate the index files for train, val, and test split. Poperly set up the train, validatin, and category meta files in "
21053,datasets_video.py,datasets_video.py," to generate the index files for train, val, and test split. Poperly set up the train, validatin, and category meta files in ",". To convert the something-something-v2 dataset, you can use the "
21056,https://districtdatalabs.com/,"
District Data Labs
","
","
"
21060,https://cloud.google.com/dataflow/,Google Cloud Dataflow," is required; it's the way that efficient distributed computation is supported. By default, Apache Beam runs in local mode but can also run in distributed mode using ", and other Apache Beam 
21066,data_utils.py,data_utils.py, function in , which maps from the original format to the simplified format. Only the original format is provided by our 
21068,https://github.com/eric-xw/AREL/tree/master/data/save,IRL-ini-iter100-*,We uploaded our checkpoints and meta files to the ,. Please load the model from these folders by running
21075,https://github.com/microsoft/tensorwatch/blob/master/notebooks/data_exploration.ipynb,See notebook,"
",.
21075,https://github.com/microsoft/tensorwatch/blob/master/notebooks/data_exploration.ipynb,Exploring Data Using T-SNE,"
","
"
21092,https://github.com/cocodataset/cocoapi,pycocotools,"
","
"
21096,#datasets,Datasets,"
","
"
21098,https://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration,"
Challenging data sets for point cloud registration algorithms
",Download the ,. Extract the zip file of each sequence corresponding to the point clouds in base frame in the 
21105,http://www.qizhexie.com/data/RACE_leaderboard,leaderboard,You can compare them with other results on the ,.
21120,http://lixirong.net/data/sigir2015/flickr4m.word2vec.tar.gz,pre-trained word2vec model,"
",: learned from social tags of over 4 million Flickr images (flickr4m) using 
21120,http://lixirong.net/data/sigir2015/flickr4m-tag.tar.gz,here,. The original flickr4m tags can be downloaded ,.
21120,data/synset_words_ilsvrc12_test1k.txt,ImageNet ilsvrc12_test1k, projects an (unlabeled) image to this layer. The training label set is ,. Our code assumes that probabilistic relevance score of each training label with respect to the image has been pre-computed and stored. see the provided sample set 
21127,https://web.archive.org/web/20190913054023/http://www.frdb.org/language/eng/page/data/scheda/bank-of-italy-survey-of-households-income-and-wealth/doc_pk/9019,Simona Baldi and Michele Pellizzari,"For many applications, it is useful to have a unique identifier for individuals across waves of the survey (from 1989 and onwards). Unique identifiers are not provided in the original data, however they can be inferred somewhat reliably using Stata code developed by ",.
21133,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,Get ImageNet,"
"," if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily."
21154,https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs,QQP,Download , dataset:
21171,https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader,"
DataLoader
", and PyTorch's ,.
21180,https://github.com/jongpillee/music_dataset_split/tree/master/MSD_split,Million Song Dataset,) and  the , (
21214,http://cognimuse.cs.ntua.gr/database,extended Cognimuse dataset,We use the ,"
"
21215,http://bnci-horizon-2020.eu/database/data-sets,BNCI Horizon 2020,"
","
"
21225,https://github.com/microsoft/human-pose-estimation.pytorch#data-preparation,https://github.com/microsoft/human-pose-estimation.pytorch#data-preparation," dataset, please see the ",.
21228,http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/,Databases and Information Systems group,", the ", at the 
21228,https://www.wikidata.org,Wikidata,", ",", and "
21228,https://www.wikidata.org,Wikidata,"
",: the latest 
21228,https://dumps.wikimedia.org/wikidatawiki/entities/20170612/wikidata-20170612-all-BETA.ttl.bz2,version,: the latest , of 
21238,https://www.cityscapes-dataset.com/anonymous-results/?id=5ee0f5098e160aa56db6e9ed01c5fbc73d4ac736b6b61751b50ad31067b0d5bd,here,GALD-Net: , GFF-Net:
21238,https://www.cityscapes-dataset.com/method-details/?submissionID=3719,here, GFF-Net:, Both are (
21242,http://oscar.skoltech.ru/data/2018-02-13_1418/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-02-13_1418/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-02-13_1418/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-02-13_1523/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-02-13_1523/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-02-13_1523/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-02_1239/right.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-02_1239/right_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-02_1239/right.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1322/right.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1322/right_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1322/right.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1325/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1325/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1325/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1336/right.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1336/right_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1336/right.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1354/right.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1354/right_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1354/right.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1357/right.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1357/right_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-07_1357/right.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1316/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1316/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1316/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1324/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1324/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1324/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1347/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1347/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1347/left.webm,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1418/left.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1418/left_jpgs.tar,link, | , | 
21242,http://oscar.skoltech.ru/data/2018-03-16_1418/left.webm,link, | , | 
21245,http://cocodataset.org/#format-data,MS COCO Object Detection,                                                       | ✔️     | ✔️     | | ,                                  | ✔️     | ✔️     | | 
21245,http://cocodataset.org/#format-data,MS COCO Keypoints Detection,                                  | ✔️     | ✔️     | | ,                               | ✔️     | ✔️     | | 
21245,https://www.tensorflow.org/tutorials/load_data/tfrecord,TFrecord,                               | ✔️     | ✔️     | | ,                              | ✔️     | ✔️     | | 
21245,https://www.aitribune.com/dataset/2018051063,Market-1501,                                                  | ✔️     | ✔️     | | ,                                      | ✔️     | ✔️     | | 
21245,https://www.cityscapes-dataset.com/login/,Cityscapes,                       | ✔️     | ✔️     | | ,                                          | ✔️     | ✔️     | | 
21245,http://www.cvlibs.net/datasets/kitti/,KITTI,                                          | ✔️     | ✔️     | | ,                                                   | ✔️     | ✔️     | | 
21245,https://www.cvlibs.net/datasets/kitti/raw_data.php,Kitti Raw Format,                                                   | ✔️     | ✔️     | | ,                           | ✔️     | ✔️     | | 
21245,https://docs.supervise.ly/data-organization/00_ann_format_navi,Supervisely Point Cloud Format,                                                          | ✔️     | ✔️     | | , | ✔️     | ✔️     |
21245,https://software.intel.com/en-us/articles/computer-vision-annotation-tool-a-universal-approach-to-data-annotation,Intel Software: Computer Vision Annotation Tool: A Universal Approach to Data Annotation,"
","
"
21245,https://venturebeat.com/2019/03/05/intel-open-sources-cvat-a-toolkit-for-data-labeling/,"VentureBeat: Intel open-sources CVAT, a toolkit for data labeling","
","
"
21248,https://seaborn.pydata.org/,seaborn,"
", - helper plotting library for some charts
21248,https://pandas.pydata.org/,Pandas,"
", - helper data manipulation library
21258,mailto:cameron@ivdatacenter.com,cameron@ivdatacenter.com,Installation instructions for Debian Linux (by Cameron Camp ,):
21260,http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=ca96b8137feb7a636f3d774c408b1243d8a6e0df,first place,KittiSeg performs segmentation of roads by utilizing an FCN based model. The model achieved , on the Kitti Road Detection Benchmark at submission time. Check out our 
21260,http://www.cvlibs.net/download.php?file=data_road.zip,http://www.cvlibs.net/download.php?file=data_road.zip,Retrieve kitti data url here: ,"
"
21260,README.md#manage-data-storage,Manage data storage, instead of downloading the data yourself. The script will also extract and prepare the data. See Section , if you like to control where the data is stored.
21260,data//demo/demo.png,demo.png, to obtain a prediction using , as input.
21283,https://archive.ics.uci.edu/ml/datasets/Repeat+Consumption+Matrices,here,The data used for this project can be found in the UCI Machine Learning repository ,.
21296,https://github.com/Xiaoming-Yu/DMIT/blob/master/data/template_dataset.py,Dataset,You can implement your , and 
21297,http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz,movie review dataset,Download the , and put 
21321,https://github.com/wwzjer/Semi-supervised-IRR/tree/master/data/rainy_image_dataset/real_input,Weiwei, from the link provided by , .
21328,https://github.com/cocodataset/cocoapi,pycocotools,"
","  — for COCO dataset, also available from pip."
21328,http://cocodataset.org/#download,coco website,: Download the coco images and annotations from ,.
21329,http://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/,this article,More elaboration about COCO dataset labels can be found in ,"
"
21346,http://corpus-tools.org/pepper/,Pepper,Luke Gessler has written a module for the ," tool so that STREUSLE data can be converted to other Pepper-supported formats, including PAULA XML and ANNIS. See "
21356,http://nlp.stanford.edu/data/glove.twitter.27B.zip,glove pre-trained embedding, and ,.
21365,http://cocodataset.org/#download,Coco,": We execute correspondence and bidirectional retrieval experiments between the scientific figures and their captions. The corpora used are Scigraph or Semantic Scholar. Also, for the bidirectional retrieval task we support ",(2014)  and 
21368,https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/,UCR archive, contains pdf documents with results comparing timage to other systems evaluated on the , as printable versions and also relative comparisions on performance.
21371,#downloading-data,Downloading Data,"
","
"
21371,#dataset,Dataset,"
","
"
21371,http://data.allenai.org/downloads/missingfact/kgd.tar.gz,here,The Knowledge Gap Dataset (KGD) can be downloaded from ,". To help with running the model training experiments, we provide a script that downloads the training datasets specifically used for the GapQA model. These training datasets have predictions from the key term identification model as well as the retrieved sentences from our corpus."
21371,http://data.allenai.org/arc/arc-corpus/,here,"Since the ARC corpora is extremely large and requires an ElasticSearch instance, we provide JSONL files containing prefetched sentences in the training datasets above. If you would like to change the retrieval, the ARC corpus can be downloaded ","
"
21374,examples/run_more/go_data.sh,"
examples/run_more/go_data.sh
","For easy preparing, simply run ",".  This is an one-step script to get and prepare all the data (might need much disk space, majorly for embeddings files)."
21374,examples/run_more/prepare_data.py,"
examples/run_more/prepare_data.py
",Please refer to , for the data preparation step.
21375,#data-preparation,Data Preparation,"
","
"
21375,https://github.com/uclanlp/CrossLingualDepParser#data-preparation,[Direct Link], repository. ,"
"
21378,http://buildingparser.stanford.edu/dataset.html,S3DIS dataset, Download the ,.
21381,https://smoosavi.org/datasets/lstw,LSTW, to generate raw feature vectors. Each vector represents a geographical region of size 5km x 5km (that we call it a geohash) during a 15 minutes time interval. This code uses ," dataset for traffic events data, raw weather observation records for weather-related attributes (check "
21381,https://smoosavi.org/datasets/lstw,LSTW, to generate description to vector representation for geographical regions. The main inputs for this process are , and 
21385,https://github.com/ermongroup/MetaIRL/tree/master/data_fusion_discrete/maze_wall_meta_irl_imitcoeff-0.01_infocoeff-0.1_mbs-50_bs-16_itr-20_preepoch-1000_entropy-1.0_RandomPol_Rew-2-32/2019_05_14_02_33_17_0,here,We provided a pretrained IRL model ,", which will be loaded by the following codes by default."
21393,#data,Data,"
","
"
21393,#data-details,Data Details,"
","
"
21393,#data-acquisition,Data Acquisition,"
","
"
21393,#downloading-data-from-s3,Downloading Data from S3,"
","
"
21393,#data-details,this section,"For more information about how to obtain the data, see ",.
21393,https://towardsdatascience.com/how-docker-can-help-you-become-a-more-effective-data-scientist-7fc048ef91d5,blog post," to satisfy GPU-compute related dependencies.  For those who are new to Docker, this ", provides a gentle introduction focused on data science.
21393,#data-details,Data Details,"For more about the data, see "," below, as well as "
21393,#downloading-data-from-s3,data download,The licenses for source code used as data for this project are provided with the , for each language in 
21394,http://irvlab.cs.umn.edu/resources/usr-248-dataset,USR-248,Proposed dataset: ,"
"
21395,https://raw.githubusercontent.com/aio-libs/aiohttp-demos/master/demos/imagetagger/tests/data/mobilenet.h5,mobilenet,: , [
21395,https://netron.app?url=https://raw.githubusercontent.com/aio-libs/aiohttp-demos/master/demos/imagetagger/tests/data/mobilenet.h5,open, [,]
21395,https://raw.githubusercontent.com/ApolloAuto/apollo/master/modules/prediction/data/traced_online_pred_layer.pt,traced_online_pred_layer,: , [
21395,https://netron.app?url=https://raw.githubusercontent.com/ApolloAuto/apollo/master/modules/prediction/data/traced_online_pred_layer.pt,open, [,]
21416,#downloading-the-data,obtain the CommonsenseQA data,"Then, ",", and download the "
21427,https://data.vision.ee.ethz.ch/sagea/lld/#paper,LLD-logo dataset,For this paper we removed all text-based images from the , and extended the remaining logos with image based logos and illustrations scraped off of Google images.
21439,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,link,Download pretrained GloVe embeddings with this , and extract 
21440,https://www.visuallocalization.net/datasets/,HERE,The dataset we use in this paper is from CMU-Seasons ,. And it is then divided into 
21445,data_loader.py,data_loader.py,I provide the base class ,", showing which class attributes and functions must be implemented if you want to train our model on other datasets. Plus I added "
21445,example_data_loader.py,example_data_loader.py,", showing which class attributes and functions must be implemented if you want to train our model on other datasets. Plus I added ",", showing a minimal example of using the base class."
21448,https://databricks.com/session/deep-learning-on-apache-spark-at-cerns-large-hadron-collider-with-intel-technologies,Presentation at Spark Summit SF 2019,"
","
"
21448,https://databricks.com/session_eu19/deep-learning-pipelines-for-high-energy-physics-using-apache-spark-with-distributed-keras-on-analytics-zoo,Presentation at Spark Summit EU 2019,"
","
"
21463,https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntudata.zip,Ubuntu,"For reproducing the performance of TripleNet, please download the datasets of ", and 
21469,https://nlp.stanford.edu/data/gpt_analysis/data.zip,data.zip,Download and unzip , (1.1GB) to make a directory called 
21469,https://nlp.stanford.edu/data/gpt_analysis/stories_spacy_annotated.zip,stories_spacy_annotated.zip,"The Spacy annotation can take a long time. If you want to download the precomputed ones for our generated stories, you should download ", (14 GB). Unzip it into the 
21472,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K dataset,Download DIV2K training data (800 training + 100 validtion images) from , and put in the file DIV2K.
21478,http://millionsongdataset.com/tasteprofile/,here,Download the Echonest dataset (TRIPLETS FOR 1M USERS) from ,.
21484,https://github.com/google-research-datasets/conceptual-captions,Conceptual Captions,. We have released the pre-trained model on , dataset and fine-tuned models on COCO Captions and Flickr30k for image captioning and VQA 2.0 for VQA.
21484,#data_prep,data prep,", see ",") and it will be attached as a volume to our container. Finally, install "
21484,https://github.com/facebookresearch/pythia#data,Pythia,"(Optional, only for VQA) Download the VQA 2.0 annotation (based on ",):
21517,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,CARS196,"
","
"
21519,#cleaning-up-the-database,Cleaning up the database,"
","
"
21519,#datasets,Datasets,"
","
"
21519,#adding-new-datasets,Adding new datasets,"
","
"
21519,gnnbench/data/,gnnbench/data/,Following attributed graph datasets are currently included (located in the , directory)
21519,gnnbench/data/io.py,gnnbench/data/io.py, archives. See , and 
21519,#adding-new-datasets,Adding new datasets, and , for information about reading and saving data in this format.
21519,gnnbench/data/io.py,SparseGraph,"To add a new dataset, convert your data to the ", format and save it to an 
21530,https://www.nature.com/articles/sdata2018251#data-citations,VQA-RAD dataset, for close-end on ,". For the detail, please refer to "
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/question_data,question_data,   The preprocessed data is in ,. 
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/question_data/dia_data,question_data/dia_data,. , contains the pair data where the raw data is downloaded from 
21534,http://coai.cs.tsinghua.edu.cn/file/QGdata.zip,here, contains the pair data where the raw data is downloaded from ,. The large scale unpaired question data is under 
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/question_data/zhihu_data,question_data/zhihu_data,. The large scale unpaired question data is under ,.
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/sentiment_data,sentiment_data,   The preprocessed data is in ,. The paired data is in 
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/sentiment_data/tweet_dia,sentiment_data/tweet_dia,. The paired data is in , and the unpaired data is in 
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/sentiment_data/tweet_dia,sentiment_data/tweet_data, and the unpaired data is in ,.
21534,https://github.com/TobeyYang/S2S_Temp/tree/master/question_data/zhihu_data,question_data/zhihu_data,"Firstly, you can obtain the template prior (NHSMM) with data in ", as follows:
21556,http://pandas.pydata.org/,pandas,"
", for logging to csv
21564,http://cocodataset.org/#download,download link,MSCOCO 2017 (,)
21595,https://github.com/bomb2peng/DFGC_starterkit/tree/master/DFGC-21%20dataset,[here],We are releasing DeepFake Game Competition (DFGC) dataset ,"
"
21601,#datasets,Datasets,"
","
"
21614,https://github.com/googlei18n/corpuscrawler/,Corpus Crawler,The model used by the Zawgyi detector has been trained on several megabytes of data from web sites across the internet.  The data was obtained using the , tool.
21627,https://snap.stanford.edu/data/,SNAP," folder there's a file 'cit-HepPh-new.txt' which is a timestamped citation network dataset from ArXiV high energy physics phenomenology, available from ",. We will use the script 
21630,https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip,Glove,(optional) Download the pretrained ,", and put it as "
21636,https://hazy.com/blog/2020/07/09/how-to-generate-sequential-data,Hazy builds on new technique to generate sequential and time‑series synthetic data,Hazy: (1) ,", (2) "
21636,https://medium.com/towards-artificial-intelligence/generating-synthetic-sequential-data-using-gans-a1d67a7752ac,Generating Synthetic Sequential Data using GANs,", (2) ","
"
21636,https://www.kdnuggets.com/2022/06/generate-synthetic-timeseries-data-opensource-tools.html,Generate Synthetic Time-series Data with Open-source Tools,Gretel.ai: ,"
"
21651,https://download.visinf.tu-darmstadt.de/data/from_games/,"
GTA5 datasets
",Download ,", which contains 24,966 annotated images with 1914×1052 resolution taken from the GTA5 game. We use the sample code for reading the label maps and a split into training/validation/test set from "
21651,https://download.visinf.tu-darmstadt.de/data/from_games/code/read_mapping.zip,here,", which contains 24,966 annotated images with 1914×1052 resolution taken from the GTA5 game. We use the sample code for reading the label maps and a split into training/validation/test set from ",". In the experiments, we resize GTA5 images to 1280x720."
21651,https://www.cityscapes-dataset.com/,"
Cityscapes
",Download ,", which contains 5,000 annotated images with 2048 × 1024 resolution taken from real urban street scenes. We resize Cityscapes images to 1024x512 (or 1280x640 which yields sightly better results but costs more time)."
21651,http://synthia-dataset.net/download/808/,"
SYNTHIA-RAND-CITYSCAPES
",Download ," consisting of 9,400 1280 × 760 synthetic images. We resize images to 1280x760."
21651,https://yihsinchen.github.io/segmentation_adaptation_dataset/,"
NTHU dataset
",Download ,", which consists of images with 2048 × 1024 resolution from four different cities: Rio, Rome, Tokyo, and Taipei. We resize images to 1024x512, the same as Cityscapes."
21660,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
21663,http://insightdata.ai/,Insight Data Science Artificial Intelligence,The paraphraser was developed under the , program.
21667,data,data,"DRSs are sets of clauses, with each clause on a new line and with DRSs separated by a white line. Lines starting with '%' are ignored. The whitespace separates the values of a clause, so it can be spaces or tabs. Everything after a '%' is ignored, so it is possible to put a comment there. Check out ", to see examples.
21667,http://pmb.let.rug.nl/data.php,the PMB webpage,"For full releases, please see ",.
21668,https://github.com/MISP/misp-objects/blob/main/objects/open-data-security/definition.json,objects/open-data-security,"
", - An object describing an open dataset available and described under the open data security model. ref. https://github.com/CIRCL/open-data-security.
21668,https://github.com/MISP/misp-objects/blob/main/objects/pcap-metadata/definition.json,objects/pcap-metadata,"
", - Network packet capture metadata.
21668,https://github.com/MISP/misp-objects/blob/main/objects/probabilistic-data-structure/definition.json,objects/probabilistic-data-structure,"
", - Probabilistic data structure object describe a space-efficient data structure such as Bloom filter or similar structure.
21669,https://snap.stanford.edu/nqe/bio_data.zip,here,The biological interaction network data used in the paper can be downloaded ,. Unzip the data in your working directory.
21696,data,data,"
", -- information on how to generate artificial data
21705,https://ieee-dataport.org/open-access/university-toronto-foot-mounted-inertial-navigation-dataset,IEEE dataport,"Alternatively, the dataset can be downloaded through ",", which is accessible without any membership."
21708,http://conda.pydata.org/miniconda.html,http://conda.pydata.org/miniconda.html,Install miniconda ,"
"
21711,https://sites.google.com/site/friendstvcorpus/,Friends TV Corpus,This is a quick and dirty implementation of a chatbot using the ,.
21715,http://www.cs.cornell.edu/~arb/data/index.html,this webpage,Download the datasets from ,. Our scripts assume that the binary files (described below) are in the 
21726,https://github.com/jleuschn/dival/blob/master/dival/datasets/lodopab_dataset.py,lodopab_dataset, class defined in ,.
21728,https://www.kaggle.com/kagglesre/blacklist-speakers-dataset,MCE2018,. , is used as the dataset for this model
21728,https://www.kaggle.com/kagglesre/blacklist-speakers-dataset,MCE2018,Download the data from ,.
21739,https://linqs.soe.ucsc.edu/data,LINQS, repository. Please check the , website for raw data  
21753,https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html,this,For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See , for more information on bottleneck features.
21754,https://bitbucket.org/rjust/fault-localization-data,Scripts and annotations for evaluating FL techniques,"
","
"
21765,https://github.com/dinashi/Denoising_3D_Face/blob/master/model_evaluation/output_on_test_data/expression/inputs.pkl,inputs.pkl," to denoise 3DMM parameters stored as a pandas dataframe in a .pkl file. The keys are 'sp', 'ep' and 'tp' (see ", as an example). The arguments are:
21772,http://honeybadger.uni.lu/datascience/addresses.txt,here,You can download the list of the contract addresses from ,.
21773,https://github.com/MiuLab/RCT-Gen/tree/master/data,data/, section. The transformed sample csv file can be found in ,.
21784,http://learn2learn.net/docs/learn2learn.data/,"
learn2learn.data
","
",: 
21794,https://docs.sentinel-hub.com/api/latest/#/API/data_access?id=cloud-masks-and-cloud-probabilities,technical documentation, and ,.
21794,https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l1c/#harmonize-values,documentation,". By default, the data is already harmonized according to ",. The API is supported in Python with 
21800,https://static.cdn.yle.fi/10m/voitto/data_v1.zip,here,"Instead of using Scores API, local data can be used. All game data from the 2017-2018 series of Mestis and Naisten Liiga can be fetched from ",. The match IDs of the Mestis games range from 3648 to 3947. For Naisten Liiga the IDs range from 4951 to 5070. You can find IDs for specific matches using 
21800,data/,data folder,Data should be placed unzipped in the ,.
21801,https://github.com/scoopmatic/data-processing,data-processing,"
","
"
21801,https://github.com/TurkuNLP/hockey-text-generation-corpus,Data set of manually aligned game statistics and news text segments,"
","
"
21835,https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,Euroc, on the ," dataset, we use a photo-realistic Unity-based simulator to test Kimera. The simulator provides:"
21835,http://web.mit.edu/sparklab/datasets/uHumans/,uHumans,"
", (released with [3])
21835,http://web.mit.edu/sparklab/datasets/uHumans2/,uHumans2,"
", (released with [4])
21839,https://travis-ci.com/datamllab/PyODDS,"
Build Status
","
","
"
21839,https://coveralls.io/github/datamllab/PyODDS?branch=master,"
Coverage Status
","
","
"
21839,https://www.taosdata.com/en/getting-started/#Install-from-Package,this instruction,"To install the TDengine as the back-end database service, please refer to ",.
21839,https://www.taosdata.com/en/documentation/connector/#Python-Connector,this handbook,"To enable the Python client APIs for TDengine, please follow ",.
21841,https://saliency.tuebingen.ai/datasets.html,here,", whose detail can be found ","
"
21848,#configured-datasets,Datasets,"
","
"
21848,#add-a-dataset,Add a Dataset,"
","
"
21848,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#Downloads,HMDB51,|   Dataset      |        Task(s)           | |:--------------:|:------------------------:| |,      | Activity Recognition   | |
21848,https://www.crcv.ucf.edu/data/UCF101.php,UCF101,      | Activity Recognition   | |,                                                    | Activity Recognition   | |
21848,http://cocodataset.org/#download,MSCOCO 2014,                      | Video Object Detection | |,"                                                       | Object Detection, Keypoints| |"
21848,https://github.com/MichiganCOG/ViP/tree/master/datasets/templates,here,The JSON skeleton templates can be found ,"
"
21848,https://github.com/MichiganCOG/ViP/tree/master/datasets/scripts,here,Existing scripts for datasets can be found ,"
"
21848,https://github.com/MichiganCOG/ViP/blob/master/datasets/templates/dataset_template.py,here,Example skeleton dataset can be found ,"
"
21857,https://www.cl.cam.ac.uk/research/nl/bea2019st/#data,here,All the public GEC datasets used in the paper can be obtained from ,"
"
21861,https://ekzhu.github.io/datasketch/lsh.html,MinHash LSH," paper, with additional position filter optimization. This algorithm still has the same worst-case complexity as the brute-force algorithm, however, by taking advantage of skewness in empirical distributions of set sizes and frequencies, it often runs much faster (even better than ",).
21861,https://ekzhu.github.io/datasketch/lsh.html,"
datasketch.MinHashLSH
", and similarity threshold 0.5. The running time of , is also shown below for comparison (
21861,https://snap.stanford.edu/data/soc-Pokec.html,Pokec social network (relationships), Accuracy | |---------|--------------|--------------|---------|------|--| | ,: from-nodes are set IDs; to-nodes are elements | 1432693 | 27.31 | 10m49s | 11m4s | Precision: 0.73; Recall: 0.67 | | 
21861,https://snap.stanford.edu/data/soc-LiveJournal1.html,LiveJournal,: from-nodes are set IDs; to-nodes are elements | 1432693 | 27.31 | 10m49s | 11m4s | Precision: 0.73; Recall: 0.67 | | ,: from-nodes are set IDs; to-nodes are elements | 4308452 | 16.01 | 28m51s | 31m58s | Precision: 0.79; Recall: 0.74|
21861,https://ekzhu.github.io/datasketch/lsh.html,"
datasketch.MinHashLSH
", and similarity threshold 0.5. The query sets are sampled from the dataset itself. The running time of , is also shown below for comparison (
21861,https://snap.stanford.edu/data/soc-Pokec.html,Pokec social network (relationships), Accuracy | |--|--|--|--|--|--|--| | ,: from-nodes are set IDs; to-nodes are elements | 1432693 | 10k | 27.31 | Indexing: 1m7s; Querying (90pct): 2.3ms | Indexing: 9m23s; Querying (90pct): 0.72ms | Precision: 0.90; Recall: 0.88 | | 
21861,https://snap.stanford.edu/data/soc-LiveJournal1.html,LiveJournal,: from-nodes are set IDs; to-nodes are elements | 1432693 | 10k | 27.31 | Indexing: 1m7s; Querying (90pct): 2.3ms | Indexing: 9m23s; Querying (90pct): 0.72ms | Precision: 0.90; Recall: 0.88 | | ,: from-nodes are set IDs; to-nodes are elements | 4308452 | 10k | 16.01 | Indexing: 2m32s; Querying (90pct): 1.6ms | Indexing: 30m58s; Querying (90pct): 2.1ms | Precision: 0.85; Recall: 0.78|
21861,https://ekzhu.github.io/datasketch/lshensemble.html#containment,Containment,"
",: intersection size divided by the size of the first set (or query set); set 
21865,https://ci.appveyor.com/project/thedataking/selfrando/branch/master,"
Appveyor build status
","
","
"
21869,#data,Data,"
","
"
21873,/dataset/office_real,/dataset/office_real,Sample training data can be found in the folders , and 
21873,/dataset/gazebo_sim,/dataset/gazebo_sim, and ,. The entire dataset can be downloaded by clicking the link here: 
21874,https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982?source=friends_link&sk=c8553bfed861b1a7932f739d26f487c8,Scaled_YOLOv4,"
","
"
21874,https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7,YOLOv4,"
","
"
21874,#datasets,Datasets,"
","
"
21874,#datasets,Datasets,"
","
"
21874,https://github.com/AlexeyAB/darknet/tree/47c7af1cea5bbdedf1184963355e6418cb8b1b4f#how-to-train-pascal-voc-data,click by the link,", ... ",)
21876,https://github.com/yburda/iwae/blob/master/datasets/OMNIGLOT/chardata.mat,OMNIGLOT,"
","
"
21882,http://vowl.visualdataweb.org/webvowl.html,WebVOWL,Graphical representation with the online , tool: 
21882,http://www.visualdataweb.de/webvowl/#iri=https://raw.githubusercontent.com/LaraHack/linkflows_model/master/Linkflows.ttl,Linkflows visualization, tool: ,"
"
21884,#knowledge-grounded-hate-countering-dataset,Knowledge-grounded hate countering dataset,                                      | Multilingual expert-based HS/CN pairs dataset on Islamophobia. | 2019         | https://aclanthology.org/P19-1271.pdf             | | , | HS/CN pairs with the background knowledge corresponding to the CN.     | 2021         | https://aclanthology.org/2021.findings-acl.79.pdf |
21885,https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools,Pycocotools,"
","
"
21910,https://towardsdatascience.com/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3,Train and Deploy the Mighty BERT based NLP models using FastBert and Amazon SageMaker,Please refer to my blog , that provides detailed explanation on using SageMaker with FastBert.
21910,https://towardsdatascience.com/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3,Train and Deploy the Mighty BERT based NLP models using FastBert and Amazon SageMaker,"
","
"
21937,https://github.com/INK-USC/ConNet/tree/master/crowdsourcing/data/AMT,AMT,"
","
"
21937,https://github.com/INK-USC/ConNet/tree/master/crowdsourcing/data/AMTC,AMTC,"
","
"
21937,https://github.com/INK-USC/ConNet/tree/master/crossdomain/data/ud-treebanks-v2.3/UD_English-GUM,"Universal Dependencies - GUM (Zeldes, 2017)","
", - included
21949,https://github.com/datamllab/rlcard/actions/workflows/python-package.yml,"
Testing
","
","
"
21949,https://coveralls.io/github/datamllab/rlcard?branch=master,"
Coverage Status
","
","
"
21949,https://github.com/datamllab/rlcard-tutorial,https://github.com/datamllab/rlcard-tutorial,Tutorial in Jupyter Notebook: ,"
"
21949,https://github.com/datamllab/rlcard-showdown,RLCard-Showdown,GUI: ,"
"
21949,https://github.com/datamllab/awesome-game-ai,Awesome-Game-AI,Resources: ,"
"
21949,https://github.com/daochenzha/data-centric-AI,awesome data-centric AI resources, and ,!
21949,https://github.com/datamllab/rlcard-tutorial,RLCard Tutorial,We have updated the tutorials in Jupyter Notebook to help you walk through RLCard! Please check ,.
21949,https://github.com/datamllab/rlcard-showdown,here,"We have released RLCard-Showdown, GUI demo for RLCard. Please check out ",!
21949,https://github.com/datamllab/rlcard-showdown/,here,We also provide a GUI for easy debugging. Please check ,. Some demos:
21962,https://github.com/facebookresearch/XLM/blob/master/get-data-wiki.sh,this one,"First, we assume access to monolingual corpus such as Wikipedia for both languages. Use scripts such as ", for getting the corpus. The script 
21964,https://sites.google.com/view/calgary-campinas-dataset/home,CC359 dataset, on the test set that is compound with a subset of entries from the ,", "
21971,https://cs.jhu.edu/~noa/data/emnlp2019.tar.gz,Preprocessed data,"
",". The preprocessed data is in TFRecord format and divided into training and test splits, each divided into queries and targets."
21971,https://cs.jhu.edu/~noa/data/reddit.tar.gz,Raw comment IDs,"
",. We provide a script to download and preprocess the raw comment IDs: 
21972,https://github.com/declare-lab/M2H2-dataset,M2H2,| Date 	| Announcements 	| |-	|-	| | 03/08/2021  | 🎆 🎆 We have released a new dataset M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations. Check it out: ,. The baselines for the M2H2 dataset are created based on DialogueRNN and bcLSTM. | | 18/05/2021  | 🎆 🎆 We have released a new repo containing models to solve the problem of emotion cause recognition in conversations. Check it out: 
21972,#data-format,Data Format,"
","
"
21972,#ecpe-2d-on-reccon-dataset,ECPE-2D on RECCON dataset,"
","
"
21972,#rank-emotion-cause-on-reccon-dataset,Rank-Emotion-Cause on RECCON dataset,"
","
"
21972,#ecpe-mll-on-reccon-dataset,ECPE-MLL on RECCON dataset,"
","
"
21972,#roberta-and-spanbert-baselines-on-reccon-dataset,RoBERTa and SpanBERT Baselines on RECCON dataset,"
","
"
21972,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove,Set , path in the preprocessing files.
21987,https://github.com/besacier/mboshi-french-parallel-corpus,besacier/mboshi-french-parallel-corpus,This is an extension of the Mboshi-French parallel corpus available at ,". The French portion of the corpus was translated into four other well-resourced languages (English, German, Portuguese, Spanish) using the "
21987,https://github.com/besacier/mboshi-french-parallel-corpus/tree/master/forced_alignments_supervised_spkr/limsi-align,limsi-align,"True phones, from the ",.
21993,https://github.com/UFAL-DSG/cs_restaurant_dataset,https://github.com/UFAL-DSG/cs_restaurant_dataset,: ,"
"
22004,README_data.md,README_data,APIs for raw and Ethernet packet data over radio ,"
"
22017,https://www.geofabrik.de/data/download.html,Geofabrik's OSM catalog, as your data warehouse and , as your source of Points and Lines of interest.
22017,https://geomancer.readthedocs.io/en/latest/setup.html#setting-up-your-data-warehouse,this link,You can see the set-up instructions in ,"
"
22028,http://www2.mta.ac.il/~gideon/datasets/,MTurk-771,"
","
"
22029,https://data.noaa.gov/dataset/dataset/global-surface-summary-of-the-day-gsod,GSOD dataset, shows how the system can be used to send a collection of small data and track lineage. It parses the dataset , downloaded using 
22030,https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/human_aware_rl/static/human_data,here,. You can find some human-human and human-AI gameplay data already collected ,.
22041,https://github.com/geomdata/gda-public/,GDA Toolbax,Clone the following repository: ,"
"
22046,https://metashare.ut.ee/repository/browse/estonian-ner-corpus/88d030c0acde11e2a6e4005056b40024f1def472ed254e77a8952e1003d9f81e/,here,": Due to legal reasons, this repository only contains some dummy data. The CoNLL02/03 datasets are widely available. The Estonian NER dataset can be obtained ",. Clean and noisy training data need to be parallel (see dummy data). The data needs to be in the CoNLL BIO2 format.
22047,https://www.cityscapes-dataset.com/,CityScapes Datasets,You can download ,.Put it in data folder.
22056,https://github.com/zhou13/neurvps/blob/edac1a85d5a4f75079da6734957e3e92927e1fcc/neurvps/datasets.py#L71-L83,these lines,Uncomment , or 
22056,https://github.com/zhou13/neurvps/blob/edac1a85d5a4f75079da6734957e3e92927e1fcc/neurvps/datasets.py#L125-L127,these lines, or , to visualize vanishing points overlaid with 2D images.
22058,https://archive.ics.uci.edu/ml/datasets/Parking+Birmingham,Birmingham parking data set,"
","
"
22058,https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page,New York City (NYC) taxi data set,"
","
"
22058,https://github.com/xinychen/transdim/tree/master/datasets,../datasets/,"For example, if you want to view or use these data sets, please download them at the "," folder in advance, and then run the following codes in your Python console:"
22058,https://towardsdatascience.com/intuitive-understanding-of-randomized-singular-value-decomposition-9389e27cb9de,Intuitive understanding of randomized singular value decomposition,"
",". July 1, 2020."
22058,https://towardsdatascience.com/understand-the-lyapunov-equation-through-kronecker-product-and-linear-equation-bfff9c1e59ab,Understanding Lyapunov equation through Kronecker product and linear equation,"
",". October 8, 2021."
22058,https://towardsdatascience.com/generating-random-numbers-and-arrays-in-matlab-and-numpy-47dcc9997650,Generating random numbers and arrays in Matlab and Numpy,"
",". October 9, 2021."
22058,https://towardsdatascience.com/dynamic-mode-decomposition-for-multivariate-time-series-forecasting-415d30086b4b,Dynamic mode decomposition for multivariate time series forecasting,"
",". October 10, 2021."
22058,https://towardsdatascience.com/reduced-rank-vector-autoregressive-model-for-high-dimensional-time-series-forecasting-bdd17df6c5ab,Reduced-rank vector autoregressive model for high-dimensional time series forecasting,"
",". October 16, 2021."
22058,https://towardsdatascience.com/dynamic-mode-decomposition-for-spatiotemporal-traffic-speed-time-series-in-seattle-freeway-b0ba97e81c2c#ce4e-5f7c3f01d622,Dynamic mode decomposition for spatiotemporal traffic speed time series in Seattle freeway,"
",". October 29, 2021."
22058,https://medium.com/@xinyu.chen/analyzing-missing-data-problem-in-uber-movement-speed-data-208d7a126af5,Analyzing missing data problem in Uber movement speed data,"
",". February 14, 2022."
22058,https://medium.com/@xinyu.chen/reproducing-dynamic-mode-decomposition-on-fluid-flow-data-in-python-94b8d7e1f203,Reproducing dynamic mode decomposition on fluid flow data in Python,"
",". September 6, 2022."
22058,https://www.researchgate.net/publication/329177786_A_Bayesian_tensor_decomposition_approach_for_spatiotemporal_traffic_data_imputation,Preprint,". Transportation Research Part C: Emerging Technologies, 98: 73-84. [",] [
22082,https://www.pandas.pydata.org/,Pandas,"
","
"
22086,https://cs.nyu.edu/~roweis/data.html,https://cs.nyu.edu/~roweis/data.html, contains the USPS dataset downloaded from , (MNIST is loaded through TensorFlow)
22099,#data,Data,"
","
"
22099,#running-on-new-datasets,Running on New Datasets,"
","
"
22104,https://github.com/thunlp/FewRel/blob/master/data/pid2name.json,pid2name.json,We also provide ," to show the Wikidata PID, name and description for each relation."
22108,https://github.com/soskek/bookcorpus/issues/24#issuecomment-556024973,some discussion,": While it could be similar to the original BookCorpus, all books seemed concatenated. And, I don't know the detail. Please see ", about the dataset or ask the distributer.
22108,https://huggingface.co/datasets/bookcorpus,a dataset class by huggingface/datasets,"
",: This internally accesses the file above (by Igor) but easy to use in some cases.
22120,http://data.statmt.org/wmt17_systems/training/,wmt17_systems,"train your model following the instructions in Nematus, such as ",.
22120,http://data.statmt.org/bzhang/neurips19_rmsnorm/,dataset & training script & pretrained model,"To ease the training of RNNSearch, we also provide the used/preprocessed ",.
22124,http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/,Ubuntu Dialog Dataset,for ,"
"
22136,https://tinloaf.github.io/ygg/datastructuresoverview.html,datastructure overview,"If you're not sure whether one of these data structures is the right data structure for your application, check out the ", in the 
22141,#obtain-data,Obtain data,"
","
"
22143,https://thunlp.oss-cn-qingdao.aliyuncs.com/OpenQA_data.tar.gz,here,"We provide Quasar-T, SearchQA and TrivialQA  dataset we used for the task in data/ directory. We preprocess the original data to make it satisfy the input format of our codes, and can be download at ",.
22151,http://jmcauley.ucsd.edu/data/amazon/,here,", which can be downloaded ",.
22155,https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/,WikiQA,Implementation is now only focusing on AS task with , corpus. (I originally tried to deal with PI task with 
22160,http://corpus-texmex.irisa.fr/,TEXMEX's bvecs format,Integer sketches should be stored in ,". That is, the dimension number and feature values for each sketch are interleaved (in little endian), where the number is 4 bytes of size and each feature is 1 byte of size. To convert vectors of real numbers into sketches, you can use "
22172,https://www.vis.uni-stuttgart.de/forschung/visual-analytics/visuelle-analyse-videostroeme/stuttgart_artificial_background_subtraction_dataset/,Stuttgart University,The dataset used in this study was developed by ,. For training the models the 
22183,https://www.cityscapes-dataset.com/,Cityscapes,"
","
"
22192,http://cocodataset.org/,COCO dataset,Where the object class comes from the , and are listed in the 
22192,data/coco.names,data/coco.names file, and are listed in the ,.
22192,https://vision.in.tum.de/data/datasets/rgbd-dataset/download,https://vision.in.tum.de/data/datasets/rgbd-dataset/download, sequence of the TUM-RGBD dataset here: ,"
"
22198,https://github.com/jacobdanovitch/Trouble-With-The-Curve/tree/master/data,Data,"
","
"
22205,docs/dataset.md,docs files,. More information about dataset can be found in repository's ,. There is an additional step of transforming dataset to a pickle instance which is explained 
22227,https://github.com/warnikchow/coaudiotext/blob/master/README.md#0-problem-definition--loading-dataset,0. Problem definition & loading dataset,"
","
"
22227,https://github.com/warnikchow/coaudiotext/blob/master/README.md#4-parallel-utilization-of-audio-and-text-data,4. Parallel utilization of audio and text data,"
","
"
22227,https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0,mel spectrogram,"First, since we only utilize the utterances that can be disambiguated with speech, here we extract the acoustic features from the files. There are many ways to abstract the physical components, but here we utilize ", due to its rich acoustic information and intuitive concept. The process is done with 
22229,https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/fakenews/fakenews.zip,"
download
","
","
"
22238,https://github.com/MikulasZelinka/textworld_kg_dataset/blob/master/dataset.zip,"
dataset.zip
",We have split the dataset (,", "
22238,https://github.com/MikulasZelinka/textworld_kg_dataset/raw/master/dataset.zip,download,", ",") into training, validation, and test sets, each of them is in a "
22241,https://github.com/hardmaru/sketch-rnn-datasets/tree/master/aaron_sheep,Aaron Koblin Sheep Market,"
","
"
22241,https://github.com/hardmaru/sketch-rnn-datasets/tree/master/kanji,Kanji Stroke Data,"
","
"
22241,https://github.com/hardmaru/sketch-rnn-datasets/tree/master/omniglot,Omniglot Stroke Data,"
","
"
22250,https://github.com/mlco2/impact/tree/master/data,"
data/
",Anything to say or add? See ,"
"
22274,http://data.csail.mit.edu/soundnet/actions3/broden1_224.zip,Broden (224x224),"
","
"
22274,http://data.csail.mit.edu/soundnet/actions3/broden1_227.zip,Broden (227x227),"
","
"
22274,http://data.csail.mit.edu/soundnet/actions3/broden1_384.zip,Broden (384x384),"
", Note: these can be used with the 
22274,http://moments.csail.mit.edu/data/moments.bib,bib,", ","
"
22274,http://moments.csail.mit.edu/multi_data/multi_moments.bib,bib,", ","
"
22275,https://sigsep.github.io/datasets/musdb.html,full MUSDB18 dataset,Download the ," and extract it into a folder of your choice. It should have two subfolders: ""test"" and ""train"" as well as a README.md file."
22279,#hda-person-dataset,HDA Person Dataset,                 | 2014             | 84           | 1           |          | Pyramid Features(ACF)      | Vary      | ✔          |                    | ✔                        | | ,        | 2014             | 53           | 13          | 2976     | Hand/Pyramid Features(ACF) | Vary      | ✔          | ✔                  | ✔                        | | 
22279,#shinpuhkan-dataset,Shinpuhkan Dataset,        | 2014             | 53           | 13          | 2976     | Hand/Pyramid Features(ACF) | Vary      | ✔          | ✔                  | ✔                        | | ,        | 2014             | 24           | 16          |          | Hand                       | 128X48    | ✔          | ✔                  |                          | | 
22279,#casia-gait-database-b,CASIA Gait Database B,        | 2014             | 24           | 16          |          | Hand                       | 128X48    | ✔          | ✔                  |                          | | ,     | 2015(*see below) | 124          | 11          |          | Background subtraction     | Vary      | ✔          | ✔                  | ✔                        | | 
22279,http://homepages.dcc.ufmg.br/~william/datasets.html,"ETH1,2,3","
","
"
22279,http://www.eecs.qmul.ac.uk/~jason/data/i-LIDS_Pedestrian.tgz,QMUL iLIDS,"
","
"
22279,https://researchdatafinder.qut.edu.au/display/n27416,SAIVT-Softbio,"
","
"
22279,http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html,iLIDS-VID,"
","
"
22279,http://www.eecs.qmul.ac.uk/~rlayne/downloads_qmul_drone_dataset.html,MPR Drone,"
","
"
22279,http://vislab.isr.ist.utl.pt/hda-dataset/,HDA Person Dataset,"
","
"
22279,http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/,Shinpuhkan Dataset,"
","
"
22279,http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html,Large scale person search,"
","
"
22279,http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/,Airport,"
","
"
22279,http://liuyu.us/dataset/lpw/index.html,LPW,"
","
"
22279,https://www.pkuml.org/resources/pkusketchreid-dataset.html,PKU Sketch-ReID,"
","
"
22291,https://github.com/Jeffrey-Ede/datasets/wiki,here,The STEM Crops dataset is available ,.
22306,#dataset-preparation,Dataset Preparation,"
","
"
22306,https://www.tensorflow.org/datasets,TensorFlow Datasets (TFDS),", but we also provide simpler wrappers for datasets available in ", (a 
22306,#t5data,above,Depending on your data source (see ,"), you will need to prepare your data appropriately."
22306,https://www.tensorflow.org/datasets,TensorFlow Datasets (TFDS),s use , as their data source. When you run our training binary (see instructions 
22306,https://www.tensorflow.org/datasets/gcs,GCS bucket," flag to point to a persistent storage location, such as a ",. This is a requirement when training on TPU.
22306,https://www.tensorflow.org/datasets/catalog/c4,C4,The ," dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it requires a significant amount of bandwidth for downloading the raw "
22306,https://cloud.google.com/dataflow/,Google Cloud Dataflow," support in TFDS, which enables distributed preprocessing of the dataset and can be run on ",". With 500 workers, the job should complete in ~16 hours."
22306,https://www.tensorflow.org/datasets/beam_datasets,TFDS Beam instructions,Read more in the ,.
22306,http://data.statmt.org/news-commentary/v14/training/,News Commentary 14,"). For example, you could try one of the paired translation datasets from WMT '19 "," training set (e.g., "
22306,http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.en-fr.tsv.gz,English-French," training set (e.g., ","). When using a TSV file, you would replace the "
22306,https://console.cloud.google.com/storage/browser/t5-data/experiments,gs://t5-data/experiments, in ,. The 
22306,https://console.cloud.google.com/storage/browser/t5-data/experiments/objectives,gs://t5-data/experiments/objectives," folder has different subdirectories corresponding to the different sections in our paper. For example, "," contains the experiments from Section 3.3 (""Unsupervised objectives""). Each subdirectory of the "
22306,https://console.cloud.google.com/storage/browser/t5-data/experiments/objectives/obj-prefix_lm,gs://t5-data/experiments/objectives/obj-prefix_lm,"Let's say you want to reproduce the results for the ""Prefix language modeling"" objective (the first row in Table 4). The operative configs for that experiment live in ",". In the base directory, there is an operative config for pre-training the model ("
22306,https://console.cloud.google.com/storage/browser/t5-data/experiments/objectives/obj-prefix_lm/operative_config.gin,gs://t5-data/experiments/objectives/obj-prefix_lm/operative_config.gin,". In the base directory, there is an operative config for pre-training the model (","). Then, there are subdirectories for each of the downstream fine-tuning mixtures we consider, each of which has its own operative config (for example, "
22306,https://console.cloud.google.com/storage/browser/t5-data/experiments/objectives/obj-prefix_lm/cnn_dailymail_v002/operative_config.gin,gs://t5-data/experiments/objectives/obj-prefix_lm/cnn_dailymail_v002/operative_config.gin,"). Then, there are subdirectories for each of the downstream fine-tuning mixtures we consider, each of which has its own operative config (for example, ","). To run this experiment, first pre-train a model with the pre-training operative config:"
22306,https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/small/,gs://t5-data/pretrained_models/small, (60 million parameters): ,"
"
22306,https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/base/,gs://t5-data/pretrained_models/base, (220 million parameters): ,"
"
22306,https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/large/,gs://t5-data/pretrained_models/large, (770 million parameters): ,"
"
22306,https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/3B/,gs://t5-data/pretrained_models/3B, (3 billion parameters): ,"
"
22306,https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/11B/,gs://t5-data/pretrained_models/11B, (11 billion parameters): ,"
"
22312,https://joshua.incubator.apache.org/data/fisher-callhome-corpus,"
Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus, Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch and Sanjeev Khudanpur, IWSLT 2013","
","
"
22343,https://sleepdata.org,National Sleep Research Resource,Download all datasets from the ,", "
22343,https://sleepdata.org,PhysioNet,", ", or other sleep repositories as described and referenced in the Supplementary Material's 
22347,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d,here,Download the data from ,.
22347,https://www.cityscapes-dataset.com/,here,Download the data from ,.
22347,https://www.cityscapes-dataset.com/,here,Follow the instructions , to request for the dataset download.
22347,https://bdd-data.berkeley.edu/,here,Download the data from ,.
22348,https://filebox.ece.vt.edu/~Badour/datasets/DeepFashion.zip,here,. We provide the data in pickle format ,.
22348,https://filebox.ece.vt.edu/~Badour/datasets/NYU_RGBD_matfiles.zip,here, We use the NYU v2 dataset. We provide the mat files ,.
22358,http://data.allenai.org/downloads/qasc/qasc_dataset.tar.gz,http://data.allenai.org/downloads/qasc/qasc_dataset.tar.gz,: ,"
"
22358,http://data.allenai.org/downloads/qasc/qasc_corpus.tar.gz,http://data.allenai.org/downloads/qasc/qasc_corpus.tar.gz,:  ,"
"
22358,#downloading-data,Downloading Data,"
","
"
22358,http://data.allenai.org/downloads/qasc/qasc_dataset_1step.tar.gz,here,"To run the single-step retrieval baseline, we provide the train, dev and test files with the retrieved context ",. These JSONL files contain the retrieved context for each choice as a paragraph in the 
22371,http://www.fastcampus.co.kr/data_camp_lab/,데이터 사이언스 논문 세미나,"
", @ FastCampus
22371,http://www.fastcampus.co.kr/data_camp_dsr/,딥러닝-음성인식 CAMP,"
", @ FastCampus
22371,https://github.com/goodatlas/zeroth/blob/master/s5/data/local/lm/README.md,s5/data/local/lm/README.md , 을 발급받은 경우 오디오 데이터와 함께 자동으로 받아지는 언어모델과 발음사전의 세부사항입니다. 개인적으로 직접 특화된 언어모델과 발음사전을 만들고자 하는 경우에는 세부적인 방법이 ,에 기술되어 있으니 참조하시기 바랍니다.
22376,https://github.com/acl2017submission/event-data/releases,Download Page,"
","
"
22377,https://github.com/acl2017submission/event-data,automatically-labeled event data,We then import and extend some new event types based on an ,", from Freebase and Wikipedia, constrained to specific domains such as music, film, sports, education, etc."
22385,#data-recording,Data recording,"
","
"
22398,https://github.com/text-machine-lab/MUTT/tree/master/data/sick,SICK (Sentences Involving Compositional Knowledge),"The paper contains evaluation of eight sentence representation methods (Word2Vec, GloVe, FastText, ELMo, Flair, BERT, LASER, USE) on five polish linguistic tasks. Dataset for these tasks are distributed with the repository and two of them are released specifically for this evaluation: the ", corpus translated to Polish and 8TAGS classification dataset. Pre-trained models used in this study are available for download in separate repository: 
22400,https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/train-v1.1.json,here,"In order to evaluate on XQuAD, models should be trained on the SQuAD v1.1 training file. which can be downloaded from ",. Model validation similarly should be conducted on the SQuAD v1.1 validation file.
22403,https://github.com/bicepjai/Deep-Survey-Text-Classification/blob/master/data_prep/dataset/README.md,here,Details regarding the data used can be found  ,"
"
22404,https://github.com/cmry/amica/blob/master/README.md#data,here,. Also see ,.
22404,https://github.com/cmry/amica#including-new-data,Including New Data,"
","
"
22405,http://tst-centrale.org/nl/tst-materialen/corpora/sonar-corpus-detail,Sonar500,) | | ,      | 
22408,./data/twitter,Twitter,. The data is composed of two domains: , and 
22408,./data/europarl,Europarl, and ,". In each, we identified and annotated gold plurality labels for the ambiguous English ""you""."
22410,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html,The VoxCeleb Speaker Recognition Challenge (VoxSRC),**New challenge on speaker recognition: ,.
22410,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,"Voxceleb1, Voxceleb2","
","
"
22416,https://www.kaggle.com/c/painter-by-numbers/data,WikiArt,"
", dataset is used as the style image dataset.
22417,http://study.rsgis.whu.edu.cn/pages/download/building_dataset.html,Whu,"
","
"
22417,https://spacenetchallenge.github.io/datasets/Urban_3D_Challenge_summary.html,Urban,"
","
"
22417,https://spacenetchallenge.github.io/datasets/spacenetBuildings-V2summary.html,Space Net,"
","
"
22418,httsp://github.com/humdrum-tools/humdrum-data,humdrum-data,".  For those unable to use GitHub, the repertory submodule can be hosted by the ", repository.
22429,https://www.biendata.xyz/competition/chaindream_nd_task2/leaderboard/,leadboard,"We also apply CONNA++ to a name disambiguation competition (https://www.biendata.xyz/competition/chaindream_nd_task2/) . Now, we still maintain the 1st rank in the ",", and we will also release the code of CONNA++ at the end of the competition."
22453,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php,KITTI scene flow estimation benchmark, to generate results that can be submitted to the online ,. You should be able to get the following results.
22458,https://github.com/deepmind/gqn-datasets,here,This repo is based on the dataset loading code from the GQN datasets found ,". Note that the code in this repo can be used to load some of those datasets, but they must be downloaded separately. Follow the instructions in the original repo to do so."
22458,https://console.cloud.google.com/storage/browser/gqn-dataset,here, datasets are located , and the 
22458,https://console.cloud.google.com/storage/browser/egqn-datasets,here, datasets are located ,.
22473,tests/sample_data/,"
tests/sample_data/
",You may use the text encoders provided at , and skip this step.
22480,http://snap.stanford.edu/data/web-BeerAdvocate.html,beer review," (CAR).  To make this repo neat and light-weight, we release the core code with a single multi-aspect dataset (i.e. the ",") for the demo purpose.  If you are interested in reproducing the exact results for other datasets, please contact us, and we are very happy to provide the code and help."
22480,https://github.com/code-terminator/classwise_rationale/blob/b3f78e1da69fb778f81211f8b62d13f109c56289/download_data.sh#L8,"
download.sh
","Below is the step-by-step instruction for running our code.  Please first contact the authors for detailed instructions to obtain the access and URL to download the dataset.  After cloning the repo, you could replace the URL placeholder in ",".  Then, you are all set to download the dataset and the pre-trained word embeddings by running:"
22485,https://pandas.pydata.org/,pandas,"
","
"
22490,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5,Download the ," dataset as the source domain, and the "
22490,https://www.cityscapes-dataset.com/,Cityscapes," dataset as the source domain, and the ", dataset as the target domain.
22498,http://hunch.net/~jl/datasets/imagenet/training.txt.gz,Train,Fine-grained ImageNet-22K dataset: , / 
22498,http://hunch.net/~jl/datasets/imagenet/testing.txt.gz,Test, / ," . Yet again, the data format must be changed to match the datasets on Extreme Classification repo."
22506,https://github.com/mweiss17/SEVN-data,SEVN-data, in the , github repository.
22514,https://www2.informatik.uni-hamburg.de/wtm/software/semantic-object-accuracy/data.tar.gz,download,"
"," our preprocessed data (bounding boxes, bounding box labels, preprocessed captions), save to "
22514,http://cocodataset.org/#download,here,obtain the train and validation images from the 2014 split ,", extract and save them in "
22517,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,"
","
"
22517,https://www.cityscapes-dataset.com,CityScapes,"
","
"
22517,data/,data,"To access data of the CityScapes dataset, one has to register an account and then request special access to the ground truth disparities. When this data is retrieved the following directories should be put in the ", folder: cs_camera/ with all camera parameters. cs_disparity/ with all ground truth disparities. cs_leftImg8bit/ with all left images. cs_rightImg8bit/ with all right images.
22522,http://htmlpreview.github.com/?https://github.com/andrewzm/deepIDE/blob/master/1_Preproc_data/anim/SST_anim.html,link,An animation of the SST product in the North Atlantic (,)
22528,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2081,data archive website,Or request the original siplits of the data for the reproduction of the results on this ," (However, please take note of the licence agreement as the corpus is not publicly available)"
22528,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2081,Snopes Corpus,Ask for permission to dowload the dataset:  ,"
"
22529,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2081,data archive website,You can request the original corpus crawled by us: ," (However, please take note of the licence agreement as the corpus is not publicly available)"
22532,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, files formatted according to the ,.
22532,http://ilk.uvt.nl/conll/#dataformat,CoNLL data format, file formatted according to the , with a previously trained model is:
22534,https://www.robots.ox.ac.uk/~vgg/data/mview/,dinosaur dataset,"If you want to try the method on the public dataset like the dinosour, please first find the images from ",", then apply the "
22541,https://numba.pydata.org/numba-doc/dev/user/installing.html,Numba,"
", for speeding up graph construction using the GPU.
22543,./aux_scripts/data_prep/README.md,readme,"data_prep: several data preparation scripts, see the ",.
22543,https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/,WikiText,", "," or use you own dataset. The data should be divided in a train.txt, valid.txt and test.txt and the correct data path should be specified in the configuration file ('data_path')."
22552,demorphy/data/words.dg,demorphy/data/words.dg,Download the dictionary file , and replace it under the corresponding directory again. Then you're ready to launch the setup script:
22564,http://vipl.ict.ac.cn/view_database.php?id=3,COX-S2V,"
","
"
22566,http://nlp.stanford.edu/data/glove.840B.300d.zip,glove.840B.300d.zip,"For glove word embeddings used in our work, please download ",", and preprocess the word embedding .txt file to a glove.840B.300d_dict.npy file, making it a dict whose key is a word and the corresponding value is the 300-d word embedding."
22572,https://github.com/davide-belli/toulouse-road-network-dataset/requirements.txt,"
requirements.txt
",See ,"
"
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/dataset,"
dataset/
","
",: Should contain the output Toulouse Road Network dataset. If you run 
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/raw,"
raw/
","
",: Contains the raw files publicly available at 
22572,https://www.geofabrik.de/data/shapefiles.html,geofabrik.de,: Contains the raw files publicly available at , used as source to extract the Toulouse Road Network dataset.
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/utils,"
utils/
","
",: Contains utils for the generation of the dataset.
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/experiments,"
experiments/
","
",": Contains two experiment to study the distribution of graphs in different splits and the size of the BFS fronteer, used to linearize the dataset (see paper or blog post for info)."
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/config.py,"
config.py
","
",: Configuration used to extract our version of Toulouse Road Network dataset.
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/generate_toulouse_road_network_dataset.py,"
generate_toulouse_road_network_dataset.py
","
",: Main script to generate the dataset. It takes a few hours on a laptop. It calls three main subcomponents:
22572,https://github.com/davide-belli/toulouse-road-network-dataset/tree/master/dataset.py,"
dataset.py
","
",": PyTorch class extending Dataset Class. Includes options to load from source images, pickle files, apply BFS heuristics and others."
22573,https://github.com/davide-belli/generative-graph-transformer/tree/master/data,"
data/
","
",: Should contain the Toulouse Road Network dataset. If you run 
22573,https://github.com/davide-belli/generative-graph-transformer/blob/master/data/download_dataset.sh,"
data/download_dataset.sh
",First download Toulouse Road Network dataset using ,.
22582,https://oscar-corpus.com/,OSCAR,Deduplicated Ukrainian part from the , corpus
22583,http://mulan.sourceforge.net/datasets-mlc.html,here, directory. The bibtex/bookmarks datasets can be downloaded ,. The script 
22591,./benchmark_data,benchmark data,"
",: data used for performance benchmarking. Please see instruction to properly setup the data before use.
22600,http://www.msmarco.org/dataset.aspx,MS MARCO Reading Comprehension v2.1 training set,Download ,"
"
22614,https://console.cloud.google.com/dataflow,Google Dataflow,This can be monitored via ,". Note that ""wall time"" displayed is not the "
22616,md/formats/data_format.md,Data format,"
",.
22617,https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30,ref,. ,"
"
22617,https://github.com/mgalley/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction,DSTC-7 official repo,Please follow the ," to extract the data, and put "
22618,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction#FAQ,FAQ,7/11/2018: An , section has been added to the data extraction page.
22618,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction,Official training data,7/1/2018: , is up.
22618,https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction/trial,Trial data,6/18/2018: , is up.
22618,https://github.com/DSTC-MSR/DSTC7-End-to-End-Conversation-Modeling/tree/master/data_extraction,data extraction,Please check the , for the input data pipeline. Note: We are providing scripts to extract the data from a Reddit 
22636,/datasets,"
Chinese NLP tasks
",The library comprises several example scripts for conducting ,:
22642,https://newsela.com/data/,here,You can request the Newsela dataset and the OPUS corpus from , and 
22643,https://github.com/AbhilashaRavichander/PrivacyQA_EMNLP/tree/master/data/meta-annotations/OPP-115%20Annotations,meta-annotations folder,"), and the annotations for both train and test splits can be found in the ",".  Each column corresponding to an OPP category contains a ""1"" if a category is considered relevant to the question as described in the paper,  and ""0"" otherwise.  A brief description of OPP-115 categories is as follows:"
22644,material/data_structure.md,here,Organize data using folder structure described ,.
22649,https://www.codacy.com/manual/nacoyang/xdeep?utm_source=github.com&utm_medium=referral&utm_content=datamllab/xdeep&utm_campaign=Badge_Grade,"
Codacy Badge
","
","
"
22649,https://travis-ci.com/datamllab/xdeep,"
Build Status
","
","
"
22649,https://github.com/datamllab/xdeep/tree/master/docs,here,"For detailed tutorial, please check the docs directory of this repository ",.
22659,datasets/wiki/README.md,wikipedia dump, embeddings on the ,.
22659,datasets/wiki/README.md,its README file," for word similarity evaluation of trained spherical word embeddings on the wikipedia dump. The script will first download a zipped file of the pre-processed wikipedia dump (retrieved 2019.05; the zipped version is of ~4GB; the unzipped one is of ~13GB; for a detailed description of the dataset, see ","), and then run "
22662,./datasets,datasets,We provide the (z-scored) benchmark classification datasets used to generate the paper in the ," directory, but we omit the ICU mortality dataset (generated using the same procedures from "
22672,https://kelvins.esa.int/proba-v-super-resolution/data/,data,1. Load , and save clearance
22684,http://www.cloudsat.cira.colostate.edu/data-products/level-2b/2b-cldclass-lidar,2B-CLDCLASS-LIDAR product, and the , derived from the combination of CloudSat Cloud Profiling Radar (CPR) and CALIPSO Cloud‐Aerosol Lidar with Orthogonal Polarization (CALIOP).
22701,http://pandas.pydata.org/,pandas,"
", for logging to csv
22701,http://bokeh.pydata.org,bokeh,"
", for training visualization
22708,sample_data,sample_data,Prepare training data and word/label embeddings in ,.
22712,https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_adversarial_competition/dataset,ImageNet-Compatible images,"Evaluation on success rate, robustness and transferability on 1000 ",.
22712,https://github.com/tensorflow/cleverhans/blob/master/examples/nips17_adversarial_competition/dataset,its official repository,", including their URLs, cropping bounding boxes, classification labels and some other metadata. More details on this dataset can be found in ",.
22725,https://www.cityscapes-dataset.com/,Cityscapes,: Download website ,", see dataset preparation code in "
22725,https://github.com/yuhuayc/da-faster-rcnn/tree/master/prepare_data,DA-Faster RCNN,", see dataset preparation code in ","
"
22725,https://github.com/naoto0804/cross-domain-detection/tree/master/datasets,Cross Domain Detection,: Dataset preparation instruction link ,. 
22725,https://fcav.engin.umich.edu/sim-dataset/,Sim10k,: Website ,"
"
22725,https://github.com/harsh-99/SCL-Domain-adaptive-object-detection/tree/master/lib/datasets/data_prep,data preparation folder, and data preparation file can be found in this repository in ,.
22731,https://www.kaggle.com/quora/question-pairs-dataset,Quora Question Pairs Dataset,The Quora dataset is composed from the ,"
"
22735,https://opendatacommons.org/licenses/by/1-0/,ODC-By 1.0,"S2ORC is only for non-commercial use, and is released under the ",".  By using S2ORC, you agree to the terms in the license."
22735,https://slideslive.com/38929131/s2orc-the-semantic-scholar-open-research-corpus,our 12 min ACL 2020 talk,. You can also watch ,.
22735,https://opendatacommons.org/licenses/by/1-0/,ODC-By 1.0, under the ,". By using S2ORC, you are agreeing to its usage terms."
22743,#comparison-with-other-databases,Comparison with Other Databases,"
","
"
22743,https://www.cockroachlabs.com/docs/stable/demo-data-replication.html,Explore core features,"
",", such as data replication, automatic rebalancing, and fault tolerance and recovery."
22748,http://www.msmarco.org/dataset.aspx,msmarco.org,To Download the MSMARCO Dataset please navigate to , and agree to our Terms and Conditions. If there is some data you think we are missing and would be useful please open an issue.
22753,https://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc,https://archive.ics.uci.edu/ml/datasets/UJIIndoorLoc,"Indoor WIFI Dataset: UJIIndoorLoc, ",.
22753,http://cs.joensuu.fi/sipu/datasets/,http://cs.joensuu.fi/sipu/datasets/,"P. Fränti and S. Sieranoja: Clustering Basic Benchmarks, ",.
22781,http://corpus-db.org,Corpus-DB," contains the full corpus used for the larger ""corpus B"" analysis, created using ","
"
22784,http://www.robots.ox.ac.uk/~yshi/mmdgm/datasets/cub.zip,here,: We offer a cleaned-up version of the CUB dataset. Download the dataset ,". First, create a "
22787,applications/data_denoising/mnist_experiments/small_scale/test_bilevel_importance_learning_mnist.py,test_bilevel_importance_learning_mnist.py,Run ,  with appropriate noise level specified on line 62.
22787,applications/data_denoising/mnist_experiments/large_scale/Penalty/test_bilevel_importance_learning_mnist.py,test_bilevel_importance_learning_mnist.py,Run , with appropriate noise level specified on line 64.
22787,applications/data_denoising/cifar10_experiments/Penalty/test_bilevel_importance_learning_cifar10.py,test_bilevel_importance_learning_cifar10.py,Run , with appropriate noise level specified on line 70.
22787,applications/data_denoising/svhn_experiments/Penalty/pre_process_svhn_data.py,pre_process_svhn_data.py,"Split data into 72257 digits for training, 1000 digits for validation, 26032 digits for testing using ","
"
22787,applications/data_denoising/svhn_experiments/Penalty/test_bilevel_importance_learning_svhn.py,test_bilevel_importance_learning_svhn.py,Run , with appropriate noise level specified on line 62.
22787,applications/data_poisoning/data_augmentation_attacks/untargeted_attacks/Penalty/test_bilevel_poisoning_untargeted.py,test_bilevel_poisoning_untargeted.py,Run , by specifying number of poisoned points to add on line 15
22787,applications/data_poisoning/data_augmentation_attacks/targeted_attacks/Penalty/test_bilevel_poisoning_targeted.py,test_bilevel_poisoning_targeted.py,Run , by specifying number of poisoned points to add on line 36
22787,applications/data_poisoning/clean_label_attacks/dogfish_dataset/extract_inception_features.py,extract_inception_features.py,Run , to extract 2048 dimensional features for all the images
22787,applications/data_poisoning/clean_label_attacks/test_bilevel_clean_label_attack.py,test_bilevel_clean_label_attack.py,Run , from outside dogfish_dataset
22789,/dataset,dataset,"
","
"
22803,https://naccdata.org/,NACC,", we compared the deep learning model with volume/thickness models on external independent cohort from ",. The volume and thickness data are extracted using the Freesurfer and quality controled by radiologists.
22803,http://adni.loni.usc.edu/data-samples/access-data/,ADNI website,Request approval and register at ,"
"
22808,https://towardsdatascience.com/why-machine-learning-on-the-edge-92fac32105e6,Why Edge Computing,"
","
"
22811,https://australiav100data.blob.core.windows.net/heliang/dbt_imagenet.params,model,The ImageNet pretrained , is available.
22817,src/dataloader.py,dataloader.py,"
",": path config of data, sources of raw data, preprocessing of data"
22817,src/dataset.py,dataset.py,"
",: processing of data
22819,https://tianchi.aliyun.com/dataset/dataDetail?dataId=42,Tmall, folder. The full raw datasets are: ,", "
22819,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649,Taobao,", ", and 
22819,http://apex.sjtu.edu.cn/datasets/6,CCMR, and ,. 
22821,cleaned-data/,cleaned-data,The fully cleaned E2E NLG Challenge data can be found in ,". The training and development set are filtered so that they don't overlap the test set, hence the "
22821,partially-cleaned-data/,partially-cleaned-data,The partially cleaned data (see paper) are under ,. Do not use these unless you have a good reason to do so.
22838,https://github.com/bitextor/bitextor-data/releases/tag/bitextor-v1.0,bitextor-data repository,A source of bilingual information between these two languages: either a bilingual lexicon (such as those available at the ,"), a machine translation (MT) system, or a parallel corpus to be used to produce either a lexicon or an MT system (depending on the alignment strategy chosen, see below)"
22843,https://d3t7erp6ge410c.cloudfront.net/tanda-aaai-2020/data/asnq.tar,here,"ASNQ is used to transfer the pre-trained models in the paper, and can be downloaded ",.
22843,https://d3t7erp6ge410c.cloudfront.net/tanda-aaai-2020/data/asnq.dev%2B%2B.tar,here,ASNQ-Dev++ can be downloaded ,.
22843,#data,data,"The documentation, including the shared ", and 
22853,http://cocodataset.org/#people,here,"Mscoco is a dataset built by Microsoft, which includes detection, segmentation, keypoints and other tasks. The dataset can be download ","
"
22856,https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset,DARPA,"
","
"
22856,http://odds.cs.stonybrook.edu/twitterworldcup2014-dataset,TwitterWorldCup2014,"
","
"
22856,http://odds.cs.stonybrook.edu/twittersecurity-dataset,TwitterSecurity,"
","
"
22856,https://towardsdatascience.com/controlling-fake-news-using-graphs-and-statistics-31ed116a986f,Towards Data Science,"
","
"
22883,https://www.influxdata.com/products/influxdb-overview/,InfluxDB,You can use any database you like. I chose , for saving all results. Most of the figures (graphs) are the screenshot of 
22889,https://kaolin.readthedocs.io/en/latest/modules/kaolin.io.dataset.html#kaolin.io.dataset.CachedDataset,CachedDataset,Reformated the data preprocessing with a new , replacing 
22889,https://kaolin.readthedocs.io/en/latest/modules/kaolin.io.dataset.html#kaolin.io.dataset.ProcessedDataset,ProcessedDataset, replacing ,"
"
22894,https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/,low quality sequencing runs, samples and ,", and easy adjustments for "
22896,https://www.caida.org/data/as-relationships/,CAIDA AS relationship data, | | -c    | --caida_relations | Reads AS relationships as bzip file from , (serial-2) | | -M    | --ass_mac_mapping | AS to MAC mapping as file with following syntax 
22896,https://www.caida.org/data/as-relationships/,CAIDA AS relationship data,Add  , (serial-2)
22899,https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/train-v1.1.json,Training set,SQuAD-1.1: ,", "
22899,https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.json,Dev Set,", ","
"
22899,https://msropendata.com/datasets/939b1042-6402-4697-9c15-7a28de7e1321,CSV file,NewsQA: ,"
"
22899,http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json,Training set,CoQA: ,"
"
22899,http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-dev-v1.0.json,Dev set,"
","
"
22899,https://s3-us-west-2.amazonaws.com/allennlp/datasets/drop/drop_dataset.zip,The whole dataset,DROP: ,"
"
22905,https://blog.godatadriven.com/fairness-in-ml,Towards fairness in ML with adversarial networks,: keras & TensorFlow implementation of ,.
22905,http://blog.godatadriven.com/fairness-in-pytorch,Fairness in Machine Learning with PyTorch,: PyTorch implementation of ,.
22908,https://www.inf.ufpr.br/vri/databases/vehicle-reid/data.tgz,Dataset,"
","
"
22908,https://www.inf.ufpr.br/vri/databases/vehicle-reid/videos.tgz,Videos,"
","
"
22908,https://www.inf.ufpr.br/vri/databases/vehicle-reid/models.tgz,Models,"
","
"
22912,https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release,Human Connectome Project,We pretrain our Dense Unet model using the Freesurfer segmentations of 1113 subjects available in the , dataset and fine-tuned the model using 101 manually labeled brain scans from 
22912,https://mindboggle.info/data.html,Mindboggle, dataset and fine-tuned the model using 101 manually labeled brain scans from , dataset.
22913,./reproducing/readme-datacreation-dqg.md,Data creation for DQG,"
","
"
22913,./reproducing/readme-datacreation-ws-tb.md,Data creation for WS-TB,"
","
"
22929,table_data,table_data,Please see , for the original tables used for the experiments.
22943,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI,For ,", first download the dataset using this "
22943,http://www.cvlibs.net/download.php?file=raw_data_downloader.zip,script,", first download the dataset using this ", provided on the official website of KITTI. Placing the dataset on SSD would increase the training speed.
22943,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,the official web,NYU Depth: Download from ,"
"
22943,https://vision.in.tum.de/data/datasets/rgbd-dataset/download,the official web,RGB-D SLAM: Download from ,"
"
22943,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,the official web,7 Scenes: Download from ,"
"
22943,http://cocodataset.org/#download,the official website,"Next, download MS COCO 2014 from ",", extra annotation from "
22943,http://datasets.d2.mpi-inf.mpg.de/hosang17cvpr/coco_minival2014.tar.gz,here,", extra annotation from ", and make symbolic links as follows.
22945,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,here,Graph classification benchmarks are publicly available at ,.
22959,./data,data,"The train/test split used in the paper is the default split provided by ShapeNet. For more convenience, we provide the pre-processed train/test file lists in ",.
22962,https://github.com/cvdfoundation/ava-dataset,here,AVA	   : Download from ,"
"
22962,http://jhmdb.is.tue.mpg.de/challenge/JHMDB/datasets,here,J-HMDB-21: Download from ,"
"
22962,https://github.com/facebookresearch/SlowFast/blob/master/slowfast/datasets/DATASET.md,here,Use instructions , for the preperation of AVA dataset.
22969,https://www.dropbox.com/s/48oe7shjq0ih151/data.tar.gz?dl=0,here,NEW (20/10/06): You can download all the preprocessed datasets used in the paper from ,"
"
22981,http://www.cvlibs.net/datasets/kitti/eval_depth_all.php,KITTI dataset,"First, you need to download a stereo vision dataset. For this, we recommend ", and 
22981,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow dataset, and , and pass the path to the skeleton code.
22981,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow,"Next, you can download one of the stereo vision datasets. For the demostration purpose, we choose """,""". Download both ""RGB images"" and ""Disparity"" into the same directory "
22981,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow,"And then, download the """,""" dataset to the root directory, "
22981,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,SceneFlow,"Next, you can follow the previous instruction to download the dataset. For the demostration purpose, we choose """,""". Download both ""RGB images"" and ""Disparity"" in the same directory "
22990,http://pandas.pydata.org/,pandas,"
", for logging to csv
22990,http://bokeh.pydata.org,bokeh,"
", for training visualization
22997,https://github.com/MTG/mtg-jamendo-dataset/tree/master/data,"
data
",Metadata files in ,"
"
22997,https://github.com/MTG/mtg-jamendo-dataset/tree/master/stats,"
stats
",Statistics in ,"
"
22997,https://essentia.upf.edu/documentation/datasets/mtg-jamendo/,online at MTG UPF, music database) in JSON format. The audio files and the NPY/JSON files are split into folders packed into TAR archives. The dataset is hosted ,.
22997,data/splits/split-0/autotagging-test.tsv,split-0 test set, contains annotations for the ," according to the taxonomies of 15 existing music classification datasets including genres, moods, danceability, voice/instrumental, gender, and tonal/atonal. These labels are suitable for training individual classifiers or learning everything in a multi-label setup (auto-tagging). Most of the taxonomies were annotated by three different annotators. We provide the subset of annotations with perfect inter-annotator agreement ranging from 411 to 8756 tracks depending on the taxonomy."
23000,dataset/Imagenet,dataset/Imagenet,Please download the test images from https://drive.google.com/file/d/1Gs_Rw-BDwuEn5FcWigYP5ZM9StCufZdP/view?usp=sharing and extract it under ,"
"
23000,dataset/Imagenet,dataset/Imagenet,Please download the train images from https://drive.google.com/file/d/1R_aC1onf0Yv77cL0OHjJ2VeXjrIbgKXb/view?usp=sharing and extract it under ,"
"
23012,https://visualdialog.org/data,here,Download the VisDial v1.0 dialog json files and images from ,.
23012,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json,here,Download the word counts file for VisDial v1.0 train split from ,. They are used to build the vocabulary.
23019,./translator_data,translator_data,The instructions for Syntax-based Machine Translation are given in the , folder.
23019,./generator_data,generator_data,The instructions for AMR-to-Text Generation are given in the , folder.
23021,https://www.tensorflow.org/programmers_guide/datasets#consuming_tfrecord_data,TFRecords,Convert downloaded Stanford Dogs Dataset to TensorFlow friendly , file: 
23026,http://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest-results/,IEEE Data Fusion Contest 2018 (DFC2018),PyTorch implementation to learn both semantics and height from the following aerial images datasets: , and 
23026,./datasets,datasets,Download one or both datasets to start training/inference with this code. Our scripts expect the datasets to be placed in ,.
23026,http://www.grss-ieee.org/community/technical-committees/data-fusion/2018-ieee-grss-data-fusion-contest-results/,DFC 2018,"
",": collection of multi-source optical imagery over Houston, Texas. In particular, it contains Very High Resolution (VHR) color images resampled at 5cm / pixel, hyperspectral images and LiDAR-derived products such as DSMs and Digital Elevation Models (DEMs) at a resolution of 50cm / pixel."
23032,https://www.drone-dataset.com/,drone datasets,The goal of this repository is to make using ," as easy as possible. Therefore, we provide source code in Python for import and visualization. Thus, this source code not only allows to visualize trajectories and thus get an overview, but also serves as a template for your own projects."
23053,http://cocodataset.org/#home,MSCOCO,"
", dataset is applied for the training of the proposed image reconstruction network.
23053,http://cocodataset.org/#home,MSCOCO,Download , datasets and transfer the raw images into 
23076,https://www.eia.gov/beta/international/data/browser/#/?pa=0000000010000000000000000000000000000000000000000000000000u&c=ruvvvvvfvsujvv1vrvvvvfvvvvvvfvvvou20evvvfvrvvvvvvurs&ct=0&vs=INTL.44-2-AFG-QBTU.A&cy=2016&vo=0&v=H&start=2014&end=2016,U.S. Energy Information Administration data,We obtained international energy mix data from the ," for the year 2016. Specifically, we looked at the energy consumption of countries worldwide, broken down by energy source. For the data points labeled "
23087,https://github.com/facebookarchive/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,here,"b. For Imagenet, you need to download the dataset and follow the instructions ","
"
23096,https://www.kaggle.com/jessicali9530/celeba-dataset,this Kaggle link,You can download these dataset files from ,". The original dataset, due to Liu et al. (2015), can be found "
23096,https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz,here,You can download a tarball of this dataset ,. The Waterbirds dataset can also be accessed through the 
23096,https://nlp.stanford.edu/data/dro/multinli_bert_features.tar.gz,here, and can be downloaded ,.
23098,egs/aishell/local/data.sh,data.sh, for feature extraction. Take a look at , for how to prepare data with 
23098,docs/how_to_prepare_large_dataset.md,English,Guide to train models on more than 1500 hours of speech data: , | 
23098,docs/how_to_prepare_large_dataset_ch.md,中文, | ,"
"
23109,https://www.nvidia.com/en-us/data-center/h100/,NVIDIA H100,"CUTLASS primitives are very efficient.  When used to construct device-wide GEMM kernels, they exhibit peak performance comparable to cuBLAS for scalar GEMM computations. The above figure shows CUTLASS performance relative to cuBLAS for large matrix dimensions on an "," (NVIDIA Hopper architecture), an "
23109,https://www.nvidia.com/en-us/data-center/l40/,NVIDIA L40," (NVIDIA Hopper architecture), an "," (NVIDIA Ada architecture), an "
23109,https://www.nvidia.com/en-us/data-center/a100/,NVIDIA A100," (NVIDIA Ada architecture), an "," (NVIDIA Ampere architecture), and an "
23109,https://www.nvidia.com/en-us/data-center/a40/,NVIDIA A40," (NVIDIA Ampere architecture), and an ",  (NVIDIA Ampere architecture). CUTLASS 3.0 was compiled with the 
23109,https://www.nvidia.com/en-us/data-center/a100/,NVIDIA A100,"When using CUTLASS building blocks to construct device-wide implicit gemm (Fprop, Dgrad, and Wgrad) kernels, CUTLASS performance is also comparable to cuDNN when running Resnet-50 layers on an ", as shown in the above figure.  Tensor Core operations are still implemented using CUDA's 
23115,https://github.com/HPAI-BSC/MetH-baselines/tree/master/download_data,download_data,"First of all, we have to download the dataset that we aim to use. Downloading scripts are located in the "," module. If we plan to use the MetH-Medium dataset, for example, go to terminal and run the corresponding script like following:"
23118,https://github.com/mdda/worldtree_corpus/tree/textgraphs_2019,textgraphs_2019 branch,"the EMNLP TextGraphs 2019 Workshop, please refer to the ",  (UPDATED LINK)
23118,https://github.com/mdda/worldtree_corpus/tree/textgraphs_2020,textgraphs_2020 branch,"the COLING TextGraphs 2020 Workshop, please refer to the ","
"
23124,http://www.cbsr.ia.ac.cn/users/ynyu/dataset/,here,The original Baidu dataset link is ,"
"
23138,https://fairdatapoint.readthedocs.io/en/latest/?badge=latest,"
Documentation Status
","
","
"
23138,https://hub.docker.com/r/fairdata/fairdatapoint,"
Docker Pulls
","
","
"
23138,https://www.fairdatapoint.org,"
FAIR Data Point (FDP)","
"," is a REST API for creating, storing, and serving "
23138,https://specs.fairdatapoint.org,FAIR Data Point software specification, according to the , document.
23138,https://fairdatapoint.readthedocs.io/,FDP Deployment and REST API usage Documentation,"More information about FDP, how to deploy it and use it can be found in the ",.
23138,https://github.com/FAIRDataTeam/OpenRefine-metadata-extension,OpenRefine Metadata Extension,"
","
"
23138,https://app.fairdatapoint.org/swagger-ui.html,app.fairdatapoint.org/swagger-ui.html, for your deployment ( e.g. ,).  More detailed descriptions and examples of these API calls is available in the 
23138,https://fairdatapoint.readthedocs.io/,Deployment and Usage instructions,).  More detailed descriptions and examples of these API calls is available in the ,"
"
23138,https://fairdatapoint.readthedocs.io/en/latest/deployment/advanced-configuration.html#mongo-db,configure the mongodb address, file in the project root and ," , and then run:"
23139,data/external/msb201126-s1.csv,msb201126-s1.csv,| , | 
23139,data/rdf/predict_gold_standard_omim.nq.gz,predict_gold_standard_omim.nq.gz, | , | 
23139,data/rdf/predict_gold_standard_omim_metadata.nq,predict_gold_standard_omim_metadata.nq, | , | 
23139,data/external/pubchem.tsv,pubchem.tsv, | , | 
23139,data/rdf/pubchem_mapping.nq.gz,pubchem_mapping.nq.gz, | , | 
23139,data/rdf/pubchem_mapping_metadata.nq,pubchem_mapping_metadata.nq, | , | 
23139,data/external/human_interactome.tsv,human_interactome.tsv, | , | 
23139,data/rdf/human_interactome.nq.gz,human_interactome.nq.gz, | , | 
23139,data/rdf/human_interactome_metadata.nq,human_interactome_metadata.nq, | , | 
23139,data/rdf/hpo_annotations.nq.gz,hpo_annotations.nq.gz, | , | 
23139,data/rdf/hpo_annotations_metadata.nq,hpo_annotations_metadata.nq, | , | 
23139,data/external/mim2mesh.tsv,mim2mesh.tsv, |, | 
23139,data/rdf/omim_mesh_annotations.nq.gz,omim_mesh_annotations.nq.gz, | , | 
23139,data/rdf/omim_mesh_annotations_metadata.nq,omim_mesh_annotations_metadata.nq, | , | 
23139,data/external/meshAnnotationsFromBioPorttalUsingOMIMDesc.txt,meshAnnotationsFromBioPorttalUsingOMIMDesc.txt, | , | 
23139,data/rdf/omim_mesh_bioportal.nq.gz,omim_mesh_bioportal.nq.gz, | , | 
23139,data/rdf/omim_mesh_bioportal_metadata.nq,omim_mesh_bioportal_metadata.nq, | , | 
23140,https://datacarpentry.org/,Data Carpentry, and ,.
23150,https://github.com/ictnlp/BoN-NAT/blob/master/data.py,"
data.py
", function located in , before you run the code.
23156,https://homes.esat.kuleuven.be/~smc/daisy/daisydata.html,DaISy,"I used the Continuous Stirred Tank Reactor (CSTR) data, as well as the data from a Test Setup of an Industrial Winding Process (Winding), both from the ", dataset. The 
23157,https://dms.sztaki.hu/~fberes/ln/ln_data_2019-10-29.zip,link,You can also download the compressed data file with this ,.
23166,#datasets,Datasets,"
","
"
23166,http://pytorch.org/docs/torchvision/datasets.html,API,"
",.
23171,https://datatracker.ietf.org/doc/html/rfc7591,OAuth 2.0 Dynamic Client Registration Protocol,"
","
"
23171,https://datatracker.ietf.org/doc/html/rfc7592,OAuth 2.0 Dynamic Client Registration Management Protocol,"
","
"
23181,http://bit.ly/bias-corpus,this link to download,Click ," (100MB, expands to 500MB)."
23181,https://www.dropbox.com/s/qol3rmn0rq0dfhn/bias_data.zip?dl=0,download link,"If that link is broken, try this mirror: ","
"
23186,https://storage.cloud.google.com/mentornet_project/data.zip,data,"Alternatively, you may download the zip files: ", and 
23195,https://github.com/ntunlp/Zero-Shot-Cross-Lingual-NER/tree/master/ner_data,here,dataset for finnish (fi) and arabic (ar) is updated. Please check ,.
23195,https://github.com/mpsilfve/finer-data,this,finnish (fi) : Adapted from ," repository. However, they don't come in a form so that we can perform transfer learning experiments (from en conll NER dataset to fi dataset). We refactored the original source and corrected some tags manually for standardization."
23204,http://www.skleinberg.org/data.html,S. Kleinberg,"Financial benchmark with stock returns, taken from "," (Finance CPT) and preprocessed. Files with 'returns' in the filename are the input datasets, the other files contain the ground truth."
23204,http://www.fmrib.ox.ac.uk/datasets/netsim/,Smith et al.,"Neuroscientific FMRI benchmark with brain networks, taken from "," and preprocessed. Files with 'timeseries' in the filename are the input datasets, the other files contain the ground truth."
23216,https://www.postgresql.org/docs/9.0/sql-createdatabase.html,via postgres, databases manually ,.
23223,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php,KITTI evaluation suite,In order to use the ,", you need to install:"
23225,http://www.cs.toronto.edu/~faghri/vsepp/data.tar,[vgg_precomp],"
","
"
23226,https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/,this TopCoder post,The best description of the algorithm I'm aware of is ,", which I highly recommend should you attempt to understand what's going on."
23226,https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/,the TopCoder post,Seriously consider reading , and 
23231,https://www.cityscapes-dataset.com/method-details/?submissionID=7836,Cityscapes Score,  | ,"
"
23235,http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz,Speech Command Dataset,We used the ," [1] (2nd version) as testbed to test our approach.  This dataset consists  on  aset of WAV audio files of 30 different spoken commands. The duration of all the files is fixed to 1 second, and the sample-rate is 16kHz in all the samples, so that each audio waveform is  composed  by 16000 values,  in  the  range [−2^15 , 2^15].   In the paper,  we used a subset of ten classes, those standard labels selected in previous publications: "
23235,http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz,test set,This model achieves an an 85.2% Top One  Accuracy on the , of the Speech Command Dataset (2nd version). The training procedure is based on the code provided in 
23235,http://download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz,test set,. It obtained an 82.5% Top One  Accuracy on the , of the Speech Command Dataset (2nd version).
23244,data/gtsrb_dataset_int.h5,testing data," for traffic sign recognition in the repo, along with the "," used for reverse engineering. The sample code uses this model/dateset by default. The entire process for examining all labels in the traffic sign recognition model takes roughly 10 min. All reverse engineered triggers (mask, delta) will be stored under RESULT_DIR. You can also specify which labels you would like to focus on. You could configure it yourself by changing the "
23247,https://github.com/isayev/ANI1_dataset/blob/master/benchmark/ani1_gdb10_ts.h5,here,. The GDB10 test set can be found ,". They should be all put into a directory, eg. ANI_DATA_DIR"
23248,http://trec-dd.org/dataset.html,here,Obtain the TREC DD datasets following the instructions ,.
23263,#dataset-download-link,Dataset Download Link,"
","
"
23265,https://archive.ics.uci.edu/ml/datasets.php,here, expects the UCI datasets (,) in the 
23271,https://developer.ibm.com/exchanges/data/all/pubtabnet/,here,Images and annotations can be downloaded ,". If you want to download the data from the command line, you can use curl or wget to download the data."
23271,./exploring_PubTabNet_dataset.ipynb,Jupyter notebook,A , is provided to inspect the annotations of 20 sample tables.
23275,prep_data.py,prep_data.py, and run ,.
23281,https://towardsdatascience.com/how-to-deploy-onnx-models-in-production-60bd6abfd3ae,Cortex,"
","
"
23301,https://bdd-data.berkeley.edu,BDD100K dataset,". Also, the BDD-Anomaly dataset is sourced from the ",. Code for the multi-label out-of-distribution detection experiments is available in 
23301,https://bdd-data.berkeley.edu/portal.html,BDD website,We cannot reshare the images from BDD100K so please visit , to download them.  The images should be from the 10K set of images that they released.
23306,https://hobbitdata.informatik.uni-leipzig.de/homes/mroeder/palmetto/Wikipedia_bd.zip,The index,"
"," should be downloaded and extracted to some path (for example, "
23312,https://github.com/brightmart/nlp_chinese_corpus,NLP Chinese Corpus,We collected data from ,"
"
23312,https://github.com/brightmart/nlp_chinese_corpus,NLP Chinese Corpus,"
","
"
23329,http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html,ECSSD,"
","
"
23341,https://www.w3.org/standards/semanticweb/data,Linked Data,Cayley is an open-source database for ,. It is inspired by the graph database behind Google's 
23341,https://en.wikipedia.org/wiki/Freebase_(database),Freebase, (formerly ,).
23341,https://en.wikipedia.org/wiki/Freebase_(database),Freebase,: simplified version for , fans
23346,https://github.com/nihalsid/ViewAL/tree/master/dataset/scannet-sample,"
dataset/scannet-sample
",A small example dataset is provided with this repository in ,.
23346,https://github.com/nihalsid/ViewAL/tree/master/dataset/preprocessing-scripts,"
dataset/preprocessing-scripts
","To use this repository datasets must be in the structure described in last section. For creating the lmdb database, seed set, train / test splits and superpixel maps check helper scripts in ",. We use 
23347,https://github.com/deepmind/dsprites-dataset/raw/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz,dsprites,", "," (and the Deppmind's variants: color, noisy, scream, introduced "
23347,https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/named_data.py,here," (and the Deppmind's variants: color, noisy, scream, introduced ","), "
23347,https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/,smallnorb,"), ",", "
23347,http://www.scottreed.info/files/nips2015-analogy-data.tar.gz,cars3d,", ",", "
23347,https://storage.googleapis.com/disentanglement_dataset/data_npz/sim_toy_64x_ordered_without_heldout_factors.npz,mpi3d_toy,", ",", and "
23347,https://storage.googleapis.com/disentanglement_dataset/data_npz/sim_realistic_64x_ordered_without_heldout_factors.npz,mpi3d_realistic,", and ",", and "
23347,https://storage.googleapis.com/disentanglement_dataset/Final_Dataset/mpi3d_real.npz,mpi3d_real,", and ",.
23347,https://github.com/rr-learning/disentanglement_dataset,repository,Please check the , for the mpi3d datasets for license agreements and consider citing their work.
23377,https://www.cityscapes-dataset.com/,Cityscapes dataset,First download the ," and then downsample the images and label maps to 1024x512 using bilinear and nearest-neighbor interpolation, respectively ("
23378,https://www.kaggle.com/bryanpark/jit-dataset,JIT Dataset,"
","
"
23378,https://www.kaggle.com/bryanpark/jejueo-single-speaker-speech-dataset,JSS Dataset,"
","
"
23381,https://github.com/UniversalDependencies/UD_Norwegian-Bokmaal/blob/master/README.md#data-splits,documentation,Extended with entity annotations. More details on the splits can be found in the , of the Norwegian Bokmål UD project.
23403,https://github.com/memray/seq2seq-keyphrase#data,KP20k,The name of the fields were chosen to match ,.
23410,https://travis-ci.com/ismir/mir-datasets,"
Build Status
","
","
"
23410,http://audiocontentanalysis.org/data-sets/,audiocontentanalysis,"
","
"
23410,http://www.ismir.net/resources/datasets/,ISMIR datasets page,"
","
"
23410,https://github.com/ismir/mir-datasets/blob/master/mir-datasets.yaml,mir-datasets.yaml,"Importantly, ", file is the 
23410,https://github.com/ismir/mir-datasets/pulls,pull request," – all other formats and representations of this data (markdown, HTML, latex tables, what have you) should be derived from this master object. If there is some format for this table you would like to see, feel free to submit a ",!
23410,https://www.audiocontentanalysis.org/datasets.html,website, to maintain a rolling list of MIR datasets on his ,. Many thanks and high fives for the diligent work and foresight to recognize the value this collection has to the ISMIR community and beyond!
23410,https://github.com/ismir/mir-datasets/blob/master/mir-datasets.yaml,mir-datasets.yaml,Each record in the , file adheres to one of the following formats:
23410,https://github.com/ismir/mir-datasets/issues/1,will, We (,") have tests, and they will (eventually) fail on Travis."
23410,outputs/mir-datasets.md,mir-datasets.md,Which will produce the output , and 
23410,outputs/mir-datasets.js,mir-datasets.js, and , files contained in this repository. Note that this output file 
23410,https://github.com/ismir/mir-datasets/issues,open an issue,"If the formatting is wrong, please help fix the script (or ",)
23415,https://datatracker.ietf.org/doc/html/draft-pbryan-zyp-json-ref-03,JSON Reference, is a library for automatic dereferencing of , objects for Python (supporting Python 3.7+).
23416,https://dataplatform.cloud.ibm.com/exchange/public/entry/view/8bddf7f7e5d004a009c643750b16d0c0,notebook," uses Lale, see demo ","
"
23428,https://druid.apache.org/docs/latest/ingestion/data-management.html,"
management
","
","
"
23428,https://druid.apache.org/docs/latest/querying/sql.html#metadata-tables,SQL systems tables, from one convenient location. All powered by ,", allowing you to see the underlying query for each view."
23428,https://github.com/apache/druid-website-src/tree/master/_data,"
apache/druid-website-src
", - contribute your own events and articles by submitting a PR in the , repository.
23444,https://homes.esat.kuleuven.be/~tokka/daisydata.html,DaISy,: CSTR system from the , dataset
23463,https://github.com/MLRG-CEFET-RJ/stconvs2s/tree/master/data,data,All datasets are publicly available at http://doi.org/10.5281/zenodo.3558773. Datasets must be placed in the , folder.
23463,https://github.com/MLRG-CEFET-RJ/stconvs2s/blob/master/tool/dataset.py#L24,here,", # of latitude, # of longitude, 1). However, in the code, we force the correct length of the input sequence in the tensor as shown ",.
23480,https://github.com/huggingface/datasets,Huggingface's datasets,"
", library includes BERTScore in their metric collection.
23482,https://www.cityscapes-dataset.com,link,Cityscapes : ,"
"
23486,docs/demo-dataset.md,Demo Dataset,"
","
"
23487,http://www.cs.virginia.edu/~lg5bt/files/jnet_data.zip,data,Download the , to the directory that 
23494,http://wellyzhang.github.io/project/raven.html#dataset,chizhang's project page,"To download the dataset, please check ",.
23497,data,data,We expect that most users will be interested in the ," directory, that contains our training corpora, pre-trained models and agreement test sets."
23507,http://www.cs.york.ac.uk/semeval-2013/task2/index.php?id=data,Semeval Twitter sentiment analysis dataset,For downloading tweets distributed using IDs to protect privacy.  Uses the format of the ,"
"
23510,https://allenai.org/data/data-all.html,AI2 Arithmetic Questions,Data collected is from ,", "
23513,log/parsed_data.txt,log/parsed_data.txt,"After four randomized runs, which took a couple of days, I produced the file ", with
23513,log/parsed_data.txt,log/parsed_data.txt,"I did four randomized trials for each target algorithm, running each component for at least 10 seconds in each (typically tens or hundreds of iterations), and the results are quite consistent. You may look at the semi-processed data ", if you like.
23517,https://ai.stanford.edu/~amaas/data/sentiment/,IMDB movies review dataset,"
","
"
23522,http://www.anc.org/data/masc/,MASC,The raw data of EmoBank is gathered from ,", the manually annotated subcorpus of the ANC (Ide et al., 2010) and the "
23529,./ivpgan/utils/sim_data.py,sim_data.py," in their names are used for evaluating a trained model. We use a resource tree structure to aggregate all training and evaluation statistics which are then saved as JSON files for later analysis. For more on the resource tree structure, you can examine ", and its usage in 
23532,dataset_generation,dataset_generation,"
","
"
23532,dataset_generation/crawler.py,dataset_generation/crawler.py,"
","
"
23532,dataset_generation/random_paper_sampler.py,dataset_generation/random_paper_sampler.py,"
","
"
23532,dataset_generation/downloader.py,dataset_generation/downloader.py,"
","
"
23532,dataset_generation/latex_input_resolver.py,dataset_generation/latex_input_resolver.py,"
","
"
23532,dataset_generation/complete_dataset.py,dataset_generation/complete_dataset.py,"
","
"
23532,dataset_generation/renumber_paper.py,dataset_generation/renumber_paper.py,"
","
"
23536,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ,. To download the ," dataset as multi-resolution TFRecords, run:"
23554,EventGAN/data,EventGAN/data,The default option is for the data to be downloaded into ,", e.g. EventGAN/data/event_gan_data/ and EventGAN/data/mvsec/hdf5/."
23554,EventGAN/data/comb_train_files.txt,EventGAN/data/comb_train_files.txt," To modify the training data, you can modify ",", with a separate sequence and starting timestamp per line."
23567,http://www.cs.york.ac.uk/semeval-2013/task2/index.php?id=data,Semeval Twitter sentiment analysis dataset,For downloading tweets distributed using IDs to protect privacy.  Uses the format of the ,"
"
23569,https://github.com/ianseifs/Treepedia_Public/blob/master/Treepedia/metadataCollector.py,here,"Example: You can just run the code of ""metadataCollector.py"" ","
"
23570,/data,~/data, are included under ,.
23571,https://www.kdnuggets.com/2020/05/evidence-counterfactuals-predictive-models-big-data.html,here, for explaining document classifications. (Read , our blogpost on the use of Evidence Counterfactuals.)
23573,"http://didt.inictel-uni.edu.pe/dataset/MauFlex_Dataset.rar,",MauFlex, and ," datasets. To train a model on the full dataset, download datasets from official websites."
23579,https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq,AFHQ, To download the ," dataset and the pre-trained network, run the following commands:"
23581,https://pandas.pydata.org,Pandas,"
", library to be able to save the dataset as a dataframe compatible with 
23587,http://www.cvlibs.net/datasets/kitti/index.php,KITTI, achieves state-of-the-art results for both tasks on established benchmarks (, & 
23587,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,FlyingChairs dataset,Download all the relevant datasets including the ,", the "
23587,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D dataset,", the ", (we use 
23587,http://www.cvlibs.net/datasets/kitti/index.php,KITTI dataset," following the practice of FlowNet 2.0), the ",", and the "
23593,#233-hiro-data-efficient-hierarchical-reinforcement-learning,HIRO (Data Efficient Hierarchical Reinforcement Learning),        2.3.3. ,        2.3.4. 
23593,#233-hiro-data-efficient-hierarchical-reinforcement-learning,section on HIRO, is set to True. This attribute is described in the ,.
23593,#233-hiro-data-efficient-hierarchical-reinforcement-learning,section on HIRO, is set to True. This attribute is described in the ,.
23593,#233-hiro-data-efficient-hierarchical-reinforcement-learning,section on HIRO, is set to True. This attribute is described in the ,.
23596,#datasets,Datasets,"
","
"
23603,https://www.anaconda.com/blog/sustaining-our-stewardship-of-the-open-source-data-science-community,commercial license of Anaconda,"If you choose to install Miniconda (a minimal subset of Anaconda), you must take care to not violate the "," introduced in Sep 2020. Miniconda is not bound by this licence, but downloading packages through the default channel pointing to anaconda seems to be. All commands in this instruction use the community-driven channel "
23640,https://sites.google.com/corp/view/streetlearn/dataset,StreetLearn, and ,. Such tasks require agents to interpret natural language instructions/dialog to navigate in photo-realistic environments in order to achieve prescribed navigation goals. We have added a minimal set of abstractions on top of SEED RL allowing us to generalize the architecture to solve a variety of other RL problems.
23647,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,here,Download the KITTI odometry dataset from , - get the color images and ground truth poses. Unzip the data in 
23658,#caveats-in-distributeddataparallel,caveat in using distributed training, instead. This trains using the DistributedDataParallel mode. (Also see , below)
23664,https://github.com/anguelos/wi19_evaluate/blob/master/test_data/test_leaderboard/team_1/0004200862.csv,here,Submissions must be in csv form as ,.
23667,https://github.com/NYUMedML/GNN_for_EHR/tree/master/data,data,The preprocessing tools that extracts medical code for datasets are enclosed in ,. Run the command:
23684,https://wyqdatabase.s3-us-west-1.amazonaws.com/DeOccNet_codes.rar,"
this link
", can be downloaded via ,. 
23684,https://wyqdatabase.s3-us-west-1.amazonaws.com/Mask_embedding.zip,here,"
","
"
23684,https://wyqdatabase.s3-us-west-1.amazonaws.com/Synscenes.zip,download,"
","
"
23684,https://wyqdatabase.s3-us-west-1.amazonaws.com/Realscenes.zip,download,"
","
"
23688,https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f,A2C algorithm,We use the , for all our experiments.
23696,https://sites.google.com/site/sznitr/code-and-datasets,RMIT dataset,", Right: ",)
23702,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI,Download and unzip the full , detection dataset to the folder 
23707,https://www.dropbox.com/s/288hotqkig7e3w9/dataset.zip?dl=0,Dropbox link,Download the data through this ,. Unzip the downloaded file in the project directory and check the following subfolders:
23708,http://vis.mit.edu/embedding-comparator/raw_data/,here," directory of this repository. Due to file size constraints, raw data for these demos (including original embeddings and words in tsv format) can be downloaded ",.
23708,preprocess_data.py,preprocess_data.py,Preprocess each model with the , Python script (details and example in script docstring).
23710,#download-datasets,Download the datasets,"
","
"
23713,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,leaderboard, for 3D object detection by using only the raw point cloud as input. PointRCNN is evaluated on the KITTI dataset and achieves state-of-the-art performance on the KITTI 3D object detection , among all published works at the time of submission.
23713,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI 3D object detection,Please download the official , dataset and organize the downloaded files as follows:
23714,http://cocodataset.org/#download,cocodataset,Please download the COCO dataset from ,. If you use 
23717,https://towardsdatascience.com/biomedical-image-segmentation-unet-991d075a3a4b,PyTorch,[,] (by Hong Jing)
23719,https://www.dropbox.com/s/minpyv59crdifk9/datasets.zip,here,Download VG annotation files from ,. Put the zip file under 
23721,python/dpu_utils/utils/dataloading.py,"
{load,save}_json[l]_gz
","
", convenience API for loading and writing 
23723,https://storage.googleapis.com/rl-reliability-metrics/data/tf_agents_full_dataset.tgz,this URL," paper (TF-Agents algorithm implementations evaluated on OpenAI MuJoCo baselines), please download using ",.
23727,https://github.com/laiguokun/multivariate-time-series-data/tree/master/traffic,LSTNet data repository," dataset, you can find it in ",.
23729,./datasets/ChnSentiCorp_htl_all/intro.ipynb,点击查看,| 数据集 | 数据概览 | 下载地址 | | ----- | -------- | ------- | | ChnSentiCorp_htl_all | 7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论 | , | | waimai_10k | 某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条 | 
23729,./datasets/waimai_10k/intro.ipynb,点击查看, | | waimai_10k | 某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条 | , | | online_shopping_10_cats | 10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条，
23729,./datasets/online_shopping_10_cats/intro.ipynb,点击查看, 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店 | , | | weibo_senti_100k | 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条 | 
23729,./datasets/weibo_senti_100k/intro.ipynb,点击查看, | | weibo_senti_100k | 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条 | , | | simplifyweibo_4_moods | 36 万多条，带情感标注 新浪微博，包含 4 种情感，
23729,./datasets/simplifyweibo_4_moods/intro.ipynb,点击查看, 其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条 | , | | dmsc_v2 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据 | 
23729,./datasets/dmsc_v2/intro.ipynb,点击查看, | | dmsc_v2 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据 | , | | yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据 | 
23729,./datasets/yf_dianping/intro.ipynb,点击查看, | | yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据 | , | | yf_amazon | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | 
23729,./datasets/yf_amazon/intro.ipynb,点击查看, | | yf_amazon | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | , |
23729,./datasets/dh_msra/intro.ipynb,点击查看,| 数据集 | 数据概览 | 下载地址 | | ----- | -------- | ------- | | dh_msra | 5 万多条中文命名实体识别标注数据（包括地点、机构、人物） | , |
23729,./datasets/ez_douban/intro.ipynb,点击查看,| 数据集 | 数据概览 | 下载地址 | | ----- | -------- | ------- | | ez_douban | 5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据 | , | | dmsc_v2 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据 | 
23729,./datasets/dmsc_v2/intro.ipynb,点击查看, | | dmsc_v2 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据 | , | | yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据 | 
23729,./datasets/yf_dianping/intro.ipynb,点击查看, | | yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据 | , | | yf_amazon | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | 
23729,./datasets/yf_amazon/intro.ipynb,点击查看, | | yf_amazon | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 | , |
23729,./datasets/baoxianzhidao/intro.ipynb,点击查看,| 数据集 | 数据概览 | 下载地址 | | ----- | -------- | ------- | | 保险知道 | 8000 多条保险行业问答数据，包括用户提问、网友回答、最佳回答 | , | | 安徽电信知道 | 15.6 万条电信问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/anhuidianxinzhidao/intro.ipynb,点击查看, | | 安徽电信知道 | 15.6 万条电信问答数据，包括用户提问、网友回答、最佳回答 | , | | 金融知道 | 77 万条金融行业问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/financezhidao/intro.ipynb,点击查看, | | 金融知道 | 77 万条金融行业问答数据，包括用户提问、网友回答、最佳回答 | , | | 法律知道 | 3.6 万条法律问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/lawzhidao/intro.ipynb,点击查看, | | 法律知道 | 3.6 万条法律问答数据，包括用户提问、网友回答、最佳回答 | , | | 联通知道 | 20.3 万条联通问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/liantongzhidao/intro.ipynb,点击查看, | | 联通知道 | 20.3 万条联通问答数据，包括用户提问、网友回答、最佳回答 | , | | 农行知道 | 4 万条农业银行问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/nonghangzhidao/intro.ipynb,点击查看, | | 农行知道 | 4 万条农业银行问答数据，包括用户提问、网友回答、最佳回答 | , | | 保险知道 | 58.8 万条保险行业问答数据，包括用户提问、网友回答、最佳回答 | 
23729,./datasets/baoxianzhidao/intro.ipynb,点击查看, | | 保险知道 | 58.8 万条保险行业问答数据，包括用户提问、网友回答、最佳回答 | , |
23736,https://github.com/google-research-datasets/natural-questions,Natural Questions,"Currently, the machine translation component is an English to Spanish translator applied to the SQuAD dataset to generate the corresponding SQuAD-es datasets, namely version 1.1 and 2.0. However, it can be extended for the other target languages and the different QA datasets, such as ",.
23737,https://github.com/kishansharma3012/FlyLarvae_dataset/blob/master/FlyLarvae_dataset.zip,Download,-,"
"
23740,codes/data_scripts/generate_2D_val.m,"
codes/data_scripts/generate_2D_val.m
",Generate LQ images with different combinations of degradations using matlab ,", "
23740,codes/data_scripts/generate_3D_val.m,"
codes/data_scripts/generate_3D_val.m
",", ",.
23740,codes/data,"
codes/data
","Prepare datasets, usually the DIV2K dataset. More details are in ",.
23740,codes/data,"
codes/data
","Prepare datasets, usually the DIV2K dataset. More details are in ",.
23743,https://medium.com/@pierre_guillou/document-ai-document-understanding-model-at-line-level-with-lilt-tesseract-and-doclaynet-dataset-347107a643b8,"Document AI | Document Understanding model at line level with LiLT, Tesseract and DocLayNet dataset",Blog Post: ,"
"
23743,https://medium.com/@pierre_guillou/document-ai-processing-of-doclaynet-dataset-to-be-used-by-layout-models-of-the-hugging-face-hub-308d8bd81cdb,"Document AI | Processing of DocLayNet dataset to be used by layout models of the Hugging Face hub (finetuning, inference)",Blog post: ,"
"
23743,processing_DocLayNet_dataset_to_be_used_by_layout_models_of_HF_hub.ipynb,"Processing of DocLayNet dataset to be used by layout models of the Hugging Face hub (finetuning, inference)",Notebook ,"
"
23743,https://huggingface.co/datasets/pierreguillou/lener_br_finetuning_language_model,pierreguillou/lener_br_finetuning_language_model,dataset: ,"
"
23745,#31-data,Data,     3.1. ,     3.2. 
23779,https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f,Training a text classifier with Flair,"
","
"
23779,https://towardsdatascience.com/zero-and-few-shot-learning-c08e145dc4ed,Zero and few-shot learning,"
","
"
23779,https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3,Benchmarking NER algorithms,"
","
"
23779,https://towardsdatascience.com/clinical-natural-language-processing-5c7b3d17e137,Clinical NLP,"
","
"
23779,https://towardsdatascience.com/docker-image-for-nlp-5402c9a9069e,A docker image for Flair,"
","
"
23789,https://github.com/common-voice/common-voice/tree/main/server/data,language specific folder,The resulting output text file is added to the ," in the Common Voice repository through a PR. This is manual, but reflects the file from the previous step. No changes are done to this file to keep legal requirements fulfilled. Therefore this steps can only be performed by staff or a designated contributor."
23789,https://discourse.mozilla.org/t/using-the-europarl-dataset-with-sentences-from-speeches-from-the-european-parliament/50184/36,Example,If you find a new open data source that provides a lot of sentences (,"), we suggest to not go through through the Sentence Collector but rather adding a scrape target here. Before you do so, let's discuss it on "
23801,https://github.com/JunweiLiang/next-prediction/blob/master/code/prepare_data/README.md,found here,Instructions for extracting features can be ,.
23805,https://sites.google.com/site/shinnosuketakamichi/research-topics/jsss_corpus,JSSS,2020/10/07 , recipe is available!
23805,https://sites.google.com/site/shinnosuketakamichi/research-topics/jsss_corpus,JSSS,"
",: Japanese female speaker
23805,https://www.data-baker.com/open_source.html,CSMSC,"
",: Mandarin female speaker
23805,https://www.kaggle.com/bryanpark/korean-single-speaker-speech-dataset,KSS,"
",: Single Korean female speaker
23805,http://shijt.site/index.php/2021/05/16/kising-the-first-open-source-mandarin-singing-voice-synthesis-corpus/,KiSing,"
",: Single Mandarin female singer (singing voice)
23811,https://datadoghq.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor,"
<img src=""https://openapi-generator.tech/img/companies/datadog.png"" width=""128"" height=""128"">
","
","
"
23811,https://bimdata.io,BIMData.io,"
","
"
23811,https://www.datadoghq.com,Datadog,"
","
"
23811,https://www.datadoghq.com/blog/java-go-libraries/,Datadog API client libraries now available for Java and Go,2020-07-20 - , by Jordan Obey at 
23812,https://github.com/Ha0Tang/GestureGAN/tree/master/datasets/ntu_split,here, to generate hand skeletons and use them as training and testing data in our experiments. Note that we filter out failure cases in hand gesture estimation for training and testing. Please cite their papers if you use this dataset. Train/Test splits for Creative Senz3D dataset can be downloaded from ,.
23812,https://github.com/Ha0Tang/GestureGAN/tree/master/datasets/senz3d_split,here, to generate hand skeletons and use them as training data in our experiments. Note that we filter out failure cases in hand gesture estimation for training and testing. Please cite their papers if you use this dataset. Train/Test splits for Creative Senz3D dataset can be downloaded from ,.
23813,http://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html,Set1 of the Rendered WB dataset,We used images from , to build our method.
23842,http://archive.ics.uci.edu/ml/datasets/Multiple+Features,UCI digit,The datasets used in the paper can be found in the 'datasets' directory. , and 
23842,"https://archive.ics.uci.edu/ml/datasets/Reuters+RCV1+RCV2+Multilingual,+Multiview+Text+Categorization+Test+collection#",Reuters, and ," datasets are from the UCI Machine Learning Repository. For Reuters dataset random sample of 100 documents per class is generated, resulting in a dataset consisting of 600 documents. "
23842,http://mlg.ucd.ie/datasets/3sources.html,3-sources," datasets are from the UCI Machine Learning Repository. For Reuters dataset random sample of 100 documents per class is generated, resulting in a dataset consisting of 600 documents. "," dataset is from the University College Dublin. Of 948 articles, we used 169 articles available in all three sources. "
23848,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense,here,", the Reddit dialogue dataset associated with ConceptNet can be downloaded from ",". Also, please download the Freebase metadata from "
23867,./data_preparation/README.md,downloaded and prepared here,"This repository contains code and models associated with the Libri-Light dataset, which can be ",. More information about dataset creation and baselines can be found in this 
23867,./data_preparation/README.md,"
data_preparation
",Documentation for downloading Libri-Light or preparing the source files from scratch can be found in ,.
23881,http://en.wikipedia.org/wiki/Linked_data,linked data,Semargl is a modular framework for crawling , from structured documents. The main goal of the project is to provide lightweight and performant tool without excess dependencies.
23883,https://github.com/tensorflow/models/blob/master/research/slim/datasets/preprocess_imagenet_validation_data.py,script,"Either you can write your own script to achive that, or use the "," provided in TensorFlow repo to do it. Step 2: We use 8 GPUs to train the models on ImageNet. To train ResNet-50 with DCL, run"
23887,/sampling_data.py/,sampling_data.py,"Before running any files, execute "," to obtain train, validation and test splits with 64 x 64 image chips."
23896,https://github.com/datafuselabs/databend,production systems,Bloom filters are used to quickly check whether an element is part of a set. Xor filters and binary fuse filters are faster and more concise alternative to Bloom filters. They are also smaller than cuckoo filters. They are used in ,.
23897,https://github.com/datafuselabs/databend,production systems,Bloom filters are used to quickly check whether an element is part of a set. Xor and binary fuse filters are a faster and more concise alternative to Bloom filters. They are also smaller than cuckoo filters. They are used in ,.
23901,https://visualdialog.org/data,here,Download the VisDial dialog json files from , and keep it under 
23901,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_train.h5,"
features_faster_rcnn_x101_train.h5
","
",: Bottom-up features of 36 proposals from images of 
23901,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_val.h5,"
features_faster_rcnn_x101_val.h5
","
",: Bottom-up features of 36 proposals from images of 
23901,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_test.h5,"
features_faster_rcnn_x101_test.h5
","
",: Bottom-up features of 36 proposals from images of 
23901,http://nlp.stanford.edu/data/glove.6B.zip,here,Download the GloVe pretrained word vectors from ,", and keep "
23911,https://www.kaggle.com/c/champs-scalar-coupling/data,Predicting Molecular Properties dataset from Kaggle.,Data preprocessing classes for ,"
"
23913,http://morpho.aalto.fi/events/morphochallenge2010/datasets.shtml,morpho.aalto.fi,. The used dataset is from ,". You should train mofessor model first, and then use it to process the target words to get the corresponding roots and affixes."
23936,https://github.com/mahmoodlab/PathomicFusion/tree/master/data/TCGA_GBMLGG,./data/TCGA_GBMLGG/,"To reporduce the results in our paper and for exact data preprocessing, implementation, and experimental details please follow the instructions here: ",. Processed data and trained models can be downloaded 
23951,https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5898cd6f_traffic-signs-data/traffic-signs-data.zip,here,Training data can be downloaded by clicking ,.
23961,https://github.com/gulvarol/grocerydataset,TobaccoShelves,"                 | This repo                                                 | 3153    | 118,388  | 37       | | "," | This repo                                                 | 354     | 13,184   | 37       | | "
23961,https://sites.google.com/view/mariangeorge/datasets,GP," | This repo                                             | 295     | 10,036   | 34       | | ",    | This repo                                                 | 680     | 9184     | 13       | | 
23961,https://github.com/skrish13/datasets-list#grocery-datasets,here,More related datasets listed ,", in case anyone is interested. [1] Precise Detection in Densely Packed Scenes, CVPR 2019 [2] https://arxiv.org/abs/1912.09476 [3] https://github.com/eg4000/SKU110K_CVPR19/issues/9 [4] https://github.com/ParallelDots/generic-sku-detection-benchmark/issues/3"
23962,https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz,part1,Here are , and 
23962,https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz,part2, and , for downloading the image files. The total disk space is around 3GB. You can equally find these links under the releases of this repository.
23970,https://ivdnt.org/downloads/taalmaterialen/tstc-sonar-corpus,SoNaR-1,                                  | ,                   | spaCy UD LassySmall                                                                             | | ---------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | | 
23970,http://textdata.nl,BERT-NL, | | ,                                                | 85.05                                                                                         | 80.45                                                                                     | 81.62                                                                                           | | 
23970,http://textdata.nl,BERT-NL, | 96.20                                                                                     | | ,                                                | 96.10                                                                                     | | 
23980,https://s3.eu-central-1.amazonaws.com/datasouls/public/sdsj2017_sberquad.csv,"Dataset: sdsj2017_sberquad.csv
","
","
"
23996,https://paperswithcode.com/sota/data-to-text-generation-on-rotowire?p=a-hierarchical-model-for-data-to-text,"
PWC
",Data-to-Text-Hierarchical ,"
"
23996,https://github.com/harvardnlp/boxscore-data,here,The dataset used in the paper can be downloaded ,". More specifically, you just need to download the "
23996,https://github.com/harvardnlp/boxscore-data/blob/master/rotowire.tar.bz2,RotoWire dataset,". More specifically, you just need to download the ",:
23996,https://github.com/harvardnlp/data2text,see here,"RG, CS and CO metrics were originaly (",") coded in Lua and python2. Because of compatibility issues with modern hardware, I have re-implemented RG in PyTorch and python3:"
24007,scripts/1_download_dataset.sh,scripts/1_download_dataset.sh,The ," script is used for downloading all audio and metadata from the internet. The total size of AudioSet is around 1.1 TB. Notice there can be missing files on YouTube, so the numebr of files downloaded by users can be different from time to time. Our downloaded version contains 20550 / 22160 of the balaned training subset, 1913637 / 2041789 of the unbalanced training subset, and 18887 / 20371 of the evaluation subset."
24010,./datasets/infohealth.demographic/,EHR-Demographics,"
", (de-identifyed clinical dataset)
24010,./datasets/infohealth.pregnancy/,EHR-Pregnancy,"
", (de-identifyed clinical dataset)
24010,./datasets/uci.mushroom/,UCI-Mushroom,"
","
"
24010,./datasets/BPA,BPA,"
", (taxonomy of medical procedures associated with ICD-10 restrictions)
24010,./datasets/freebase.FB55T,Freebase FB55T,"
", (a typed-version of Freebase)
24026,https://github.com/daltonj/treccastweb/tree/master/2019/data,GitHub repository, and ,.
24027,https://trec.nist.gov/data/wapost/,Washington Post 2020,"
", - Same from Year 3.
24027,https://microsoft.github.io/msmarco/TREC-Deep-Learning-2021#document-ranking-dataset,MS MARCO V2 (Documents),"
", - MS MARCO V2 document corpus used in 2021 TREC Deep Learning Track.
24027,https://trec.nist.gov/data/wapost/,Washington Post 2020,"
", - Note: This is a new dump including January 2012 through December 2020.
24027,http://trec-car.cs.unh.edu/datareleases/v2.0/paragraphCorpus.v2.0.tar.xz,TREC CAR paragraph collection v2.0,The ,"
"
24027,https://github.com/daltonj/treccastweb/tree/master/2019/data/training,Training topics year 1 V1.0,"
", - 30 example training topics
24027,https://github.com/daltonj/treccastweb/blob/master/2019/data/training/train_topics_mod.qrel,Training judgments,"
"," - We provide limited (incomplete) training data for 5 topics (approximately 50 turns). These are judged from the baseline retrieval run (below).  The judgments are graded on a three point scale (2 very relevant, 1 relevant, and 0 not relevant)."
24027,https://github.com/daltonj/treccastweb/tree/master/2019/data/evaluation,Evaluation topics year 1 V1.0,"
", - 50 evaluation topics
24027,https://github.com/daltonj/treccastweb/blob/master/2019/data/training/train_topic_sample_annotated_resolved_v1.0.tsv,two training queries,TRAIN: Sample annotations on , (for exemplars)
24027,https://github.com/daltonj/treccastweb/blob/master/2019/data/evaluation/evaluation_topics_annotated_resolved_v1.0.tsv,evaluation topics,EVALUTION: Complete annotations on the , for the year 1 evaluation queries.
24027,https://github.com/daltonj/treccastweb/blob/master/2019/data/training/train_topics.query,train queries, format: ,", "
24027,https://github.com/daltonj/treccastweb/blob/master/2019/data/test_topics.query,test queries,", ",", "
24027,http://trec-car.cs.unh.edu/datareleases/v2.0/paragraphCorpus.v2.0.tar.xz,TREC CAR paragraph collection v2.0,The ,"
"
24048,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D subset,"
","
"
24048,https://download.visinf.tu-darmstadt.de/data/flyingchairs_occ/occlusions_rev.zip,revised occlusion GT, + ,"
"
24048,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI Optical Flow 2015,"
", and 
24048,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow,KITTI Optical Flow 2012, and ,"
"
24051,https://www.cityscapes-dataset.com/method-details/?submissionID=5674,here,see our Cityscapes submission ,.
24051,https://www.cityscapes-dataset.com/file-handling/?packageID=3,leftImg8bit_trainvaltest.zip,Download the , and 
24051,https://www.cityscapes-dataset.com/file-handling/?packageID=1,gtFine_trainvaltest.zip, and , from the Cityscapes.
24051,tools/datasets/cityscapes/,file of image list,Put the , into where you save the dataset.
24051,https://www.cityscapes-dataset.com/login/,Cityscapes submission page,Simply zip the prediction folder and submit to the ,.
24055,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYUv2,Download , and extract it to 
24055,https://github.com/ankurhanda/nyuv2-meta-data,labels (13 classes),Download , and extract it to 
24056,dataset/FaceShifter/README.md,dataset page, for more information! See its ," for updated numbers as well as an example video. If you want to access the new data and have already applied for our download script, simply reuse the original download link to get the updated script. Otherwise, please fill out "
24056,https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html,Deep Fake Detection Dataset,"
",:
24056,dataset,dataset, We are hosting the Deep Fake Detection Dataset provided by Google & JigSaw. The dataset contains over 3000 manipulated videos from 28 actors in various scenes. The dataset has a similar file structure and is downloaded by default together with the regular dataset. See the , page for more information.
24056,dataset/README.md,download section,"Once, you obtain the download link, please head to the ",. You can also find details about the generation of the dataset there.
24057,https://storage.googleapis.com/isl-datasets/SID/Sony.zip,Sony,You can download it directly from Google drive for the , (25 GB)  and 
24057,https://storage.googleapis.com/isl-datasets/SID/Fuji.zip,Fuji, (25 GB)  and , (52 GB) sets.
24058,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K, dataset (900 HR images) from the link ,.
24059,https://download.visinf.tu-darmstadt.de/data/from_games/,website, or you can download it from the official ," and run ""python preprocess.py"" to process the images."
24063,http://files.grouplens.org/datasets/movielens/ml-20m.zip,download link,In order to train RecVAE on MovieLens-20M dataset (,"), preprocess it using following script:"
24065,http://www.cs.sfu.ca/~colour/data/shi_gehler/,Shi's Re-processing of Gehler's Raw Dataset:,"
","
"
24073,https://seaborn.pydata.org/,seaborn," (the experiments were performed with version 1.2.0).   Additionally, using the ", library will make all your plot's nicer!
24076,https://github.com/googlecreativelab/quickdraw-dataset,"
Quick, Draw！
",Many thanks to the great sketch dataset , released by Google.
24078,https://discourse.ros.org/t/introducing-the-robot-vulnerability-database/11105/7?u=vmayoral,this ROS Discourse thread,") by tackling aspects such scope and impact of the flaws (through a proper severity scoring mechanism for robots), information for facilitating mitigation, detailed technical information, etc. For a more detailed discussion, see ",.
24080,http://pandas.pydata.org,Pandas,"
","
"
24080,https://seaborn.pydata.org,Seaborn,"
","
"
24085,https://github.com/hamidkarimi/DeepDIG/wiki/Run-DeepDIG-against-a-new-dataset-or-model,here,Please refer to , to see how you can run DeepDIG against your new dataset/model.
24104,http://101.32.75.151:8181/dataset/,"ped1.tar.gz, ped2.tar.gz, avenue.tar.gz and shanghaitech.tar.gz",Please manually download all datasets from ," and tar each tar.gz file, and move them in to "
24104,http://101.32.75.151:8181/dataset/,"pretrains.tar.gz, avenue, ped1, ped2, flownet","Download the trained models (There are the pretrained FlowNet and the trained models of the papers, such as ped1, ped2 and avenue). Please manually download pretrained models from "," and tar -xvf pretrains.tar.gz, and move pretrains into "
24116,https://github.com/daerduoCarey/partnet_dataset,[data],"
","
"
24116,http://www.cvlibs.net/datasets/kitti/eval_3dobject.php,[project page],"
","
"
24116,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,[data],"
","
"
24116,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,[results],"
","
"
24116,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev,[data],"
","
"
24116,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev,[results],"
","
"
24116,http://apolloscape.auto/tracking.html#to_data_href,[data],"
","
"
24116,https://www.argoverse.org/data.html#download-link,[data],"
","
"
24116,http://buildingparser.stanford.edu/dataset.html#Download,[project page],"
","
"
24116,http://semantic-kitti.org/dataset.html#download,[data],"
","
"
24116,https://udayton.edu/engineering/research/centers/vision_lab/research/was_data_analysis_and_processing/dale.php,[project page],"
","
"
24116,https://3d.dataset.site/,[data],"
","
"
24134,https://mapster.csail.mit.edu/data/roadtagger/model.zip,link,Add download , to the pretrained model.
24134,https://mapster.csail.mit.edu/data/roadtagger/dataset_testing.zip,link,Add download , to the testing dataset and 
24134,https://mapster.csail.mit.edu/data/roadtagger/validation.p,link, to the testing dataset and , to the dataset partition.
24146,https://csegroups.case.edu/bearingdatacenter/pages/download-data-file/,CWRU Bearing Dataset,"
","
"
24146,https://mb.uni-paderborn.de/kat/forschung/datacenter/bearing-datacenter/,PU Bearing Dataset,"
","
"
24146,https://github.com/cathysiyu/Mechanical-datasets,SEU Gearbox Dataset,"
","
"
24146,https://github.com/ZhaoZhibin/DL-based-Intelligent-Diagnosis-Benchmark/tree/master/datasets,datasets,"
", contains the data augmentation methods and the Pytorch datasets for time and frequency domains.
24166,https://pandas.pydata.org/,Pandas,"
", version 0.25.3
24167,#loading-a-previously-transformed-dataset,Loading a Previously Transformed Dataset,"
","
"
24167,#Make-your-own-data-transformer,Make Your Own Data Transformer,"
","
"
24173,https://github.com/izmailovpavel/flowgmm/blob/public/experiments/synthetic_data/synthetic.ipynb,this ipython notebook,The experiments on synthetic data are implemented in ,. We additionaly provide 
24173,https://github.com/izmailovpavel/flowgmm/blob/public/experiments/synthetic_data/synthetic-labeled-only.ipynb,another ipython notebook,. We additionaly provide , applying FlowGMM to labeled data only.
24173,http://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification,here,"The training script for the UCI dataset will automatically download the relevant MINIBOONE or HEPMASS datasets and unpack them into ~/datasets/UCI/., but for reference they come from ", and 
24173,http://archive.ics.uci.edu/ml/datasets/HEPMASS,here, and ,. We follow the preprocessing (where sensible) from 
24182,https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pair website, to prepare our code base. The first thing you need to do is to download the Quora Question Pairs dataset from the , and put the same in the 
24196,./data/README.md,Dataset README,Chatbot NLU Evaluation Benchmark dataset with missing/incorrect data (STT errors) and Twitter Sentiment dataset (Check ,)
24205,https://github.com/netdata/netdata,netdata/netdata, (112986) Linux kernel source tree|," (54811) Real-time performance monitoring, done right! https://www.netdata.cloud|"
24205,https://github.com/Rdatatable/data.table,Rdatatable/data.table, (2931) R for data science: a book|, (2730) R's data.table package extends data.frame:|
24205,https://github.com/datacharmer/test_db,datacharmer/test_db," (3067) Macros in Python: quasiquotes, case classes, LINQ and more!|"," (2589) A sample MySQL database with an integrated test suite, used to test your applications and database servers|"
24211,https://salu133445.github.io/lakh-pianoroll-dataset/,Lakh Pianoroll Dataset,We train the model with training data collected from ," to generate pop song phrases consisting of bass, drums, guitar, piano and strings tracks."
24211,https://salu133445.github.io/lakh-pianoroll-dataset/,Lakh Pianoroll Dataset,The training data is collected from ," (LPD), a new multitrack pianoroll dataset."
24221,dataset.csv,Dataset CSV,"
","
"
24227,http://archive.ics.uci.edu/ml/datasets/thyroid+disease,hypothyroid dataset,Example outliers from the ,:
24227,http://htmlpreview.github.io/?https://github.com/david-cortes/outliertree/blob/master/vignettes/Explainable_Outlier_Detection_in_Titanic_dataset.html,RMarkdown,Example , using the Titanic dataset.
24236,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,Get some datasets from , .
24236,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,Run on a dataset from , using
24236,https://vision.in.tum.de/mono-dataset,https://vision.in.tum.de/mono-dataset,The format assumed is that of ,". However, it should be easy to adapt it to your needs, if required. The binary is run with:"
24236,https://github.com/tum-vision/mono_dataset_code,https://github.com/tum-vision/mono_dataset_code,Use a photometric calibration (e.g. using , ).
24237,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI dataset, cameras that computes the camera trajectory and a sparse 3D reconstruction (in the stereo and RGB-D case with true scale). It is able to detect loops and relocalize the camera in real time. We provide examples to run the SLAM system in the ," as stereo or monocular, in the "
24237,http://vision.in.tum.de/data/datasets/rgbd-dataset,TUM dataset," as stereo or monocular, in the "," as RGB-D or monocular, and in the "
24237,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC dataset," as RGB-D or monocular, and in the "," as stereo or monocular. We also provide a ROS node to process live monocular, stereo or RGB-D streams. "
24237,http://vision.in.tum.de/data/datasets/rgbd-dataset/tools,associate.py,Associate RGB images and depth images using the python script ,. We already provide associations for some of the sequences in 
24255,https://github.com/opendatacube/datacube-core,Datacube-core,"
", - Open Data Cube analyses continental scale Earth Observation data through time
24259,https://github.com/makinacorpus/easydict,easydict,"
","
"
24262,https://www.mn.uio.no/math/english/people/aca/vegarant/data/storage3.zip,https://www.mn.uio.no/math/english/people/aca/vegarant/data/storage3.zip,This repository does only contains the source code. The data (3.7 GB) can be downloaded here ,". Note that you will most likely have to change all paths so that they point to the correct data, in order for the scripts to run smoothly. In most cases, this can be done in the configuration files."
24264,https://github.com/JRC1995/Tweet-Disaster-Keyphrase/blob/master/process_data.py,process_data.py,"
"," for initial pre-processing (pos-tagging, annotating based on lexicon, preparing labels etc.)"
24264,https://github.com/JRC1995/Tweet-Disaster-Keyphrase/blob/master/vectorize_data.py,vectorize_data.py,"
", to prepare the final vectorized data for training/testing/validating after steps 1-3 are done.
24264,https://github.com/JRC1995/Tweet-Disaster-Keyphrase/blob/master/process_data.py,process_data.py,"Nevertheless, the bulk of the work in ", is done in the imported (from Utils) preprocess function which only uses a list of tweets as its main argument. So any format of data can be used if one is able to prepare a list of raw tweets to feed into the preprocess function within 
24264,https://github.com/JRC1995/Tweet-Disaster-Keyphrase/blob/master/process_data.py,process_data.py, is done in the imported (from Utils) preprocess function which only uses a list of tweets as its main argument. So any format of data can be used if one is able to prepare a list of raw tweets to feed into the preprocess function within ,.
24274,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2848,here,"The BERT and MT-DNN weights, fine-tuned on all ten stance detection datasets, can be downloaded ",. The models can be placed in folder 
24277,https://github.com/datanucleus/type-converter-kryo,DataNucleus,"
", (JDO/JPA persistence framework)
24285,https://github.com/Travelogues/travelogues-corpus,corpus repository,"To test the code yourself, you will need the corpus available in our ",.
24286,http://data.onb.ac.at/ABO/+Z180627808,http://data.onb.ac.at/ABO/+Z180627808,: ,.
24350,https://github.com/thierry-tct/SEMu-Experiement-data,here, and the data-set ,"), and cited as following:"
24353,http://www.vision.ee.ethz.ch/en/datasets/,ETH Dataset," ETH or UCY datasets, you can find them here: ", and 
24353,https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data,UCY Dataset, and ,.
24354,http://cocodataset.org/#download,MS-COCO 2014,"
", and 
24356,http://images.cocodataset.org/annotations/annotations_trainval2017.zip,here,Download the training/validation split we use in our paper from ,"
"
24356,http://cocodataset.org/#download,here,Download the images (2017 Train and 2017 Val) from ,"
"
24358,#train-deepbgc-on-your-own-data,Train DeepBGC on your own data,See , section below for more information about training a custom detector or classifier.
24364,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI 3D object detection,Please download the official , dataset and organize the downloaded files as follows:
24366,https://www.crcv.ucf.edu/data/ucf-qnrf/,Link,1、 Dowload Dataset UCF-QNRF ,"
"
24368,#datasets,Datasets,"
","
"
24368,https://github.com/aachenhang/crowdcount-mcnn/tree/master/data_preparation,Matlab Code,Density Map Generation from Key Points [,] [
24368,https://github.com/leeyeehoo/CSRNet-pytorch/blob/master/make_dataset.ipynb,Python Code,] [,] [
24374,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,here,(1) Please download the KITTI dataset and create the model folders. KITTI dataset is avaible ,. Download KITTI 
24374,https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip,point clouds,. Download KITTI ,", "
24374,https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip,left images,", ",", "
24374,https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_3.zip,right images,", ",", "
24374,https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip,calibrations matrices,", ", and 
24374,https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip,object labels, and ,.
24392,http://pandas.pydata.org/,pandas,"
","
"
24395,https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers/blob/master/ner_dataset.md,NER Datasets 🔽,Mainstream ,"
"
24395,https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers/tree/master/ner_dataset,"
ner_data
"," your dataset (If possible, with brief description) into ", and 
24416,#download-event2012-dataset,Download Event2012 dataset,"
","
"
24416,#download-event2018-dataset,Download Event2018 dataset,"
","
"
24416,#use-first-story-detection-on-you-own-dataset,Use 'First Story Detection' on you own dataset,"
","
"
24416,#tf-idf-tfidf_dataset-or-tfidf_all_tweets,tf-idf,"
","
"
24416,https://dataset.ina.fr/corpus/index.action?request_locale=en,here,"In compliance with Twitter terms of use, we do not share the tweets content, but only the tweets IDs. The corpus is available ",". Please fill-in the agreement form and indicate the name of the corpus (Event2018) in your application. Untar the folder, the labeled tweets are in the "
24417,https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/images.html,here,Dataset of images for mixing is available ,.
24426,https://github.com/google/trax/blob/master/trax/examples/trax_data_Explained.ipynb,"
trax.data API explained
","
", : Explains some of the major functions in the 
24426,https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus,Kaggle dataset, : Uses a , for implementing Named Entity Recognition using the 
24426,https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize,trax.data.tokenize,tokenize your input sentence to input into the model with ,"
"
24426,https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize,trax.data.detokenize,de-tokenize the decoded result to get the translation with ,"
"
24426,https://www.tensorflow.org/datasets/catalog/overview,TensorFlow datasets, and ,.
24426,https://www.tensorflow.org/datasets,TensorFlow Datasets,. Trax allows you to use , easily and you can also get an iterator from your own text file using the standard 
24434,./data/charades/charades_r2d_tc3_f32.pkl,Link,     | R2D   | 3 | 32  | 30.37  | ,   | | 
24434,./data/charades/charades_r2d_tc3_f64.pkl,Link,     | R2D   | 3 | 64  | 31.25  | ,   | | 
24434,./data/charades/charades_r2d_tc4_f128.pkl,Link,   | R2D   | 4 | 128 | 31.82  | ,  |
24434,./data/charades/charades_i3d_tc3_f256.pkl,Link,    | I3D  | 3 | 256  | 33.89  | ,   | | 
24434,./data/charades/charades_i3d_tc3_f512.pkl,Link,    | I3D  | 3 | 512  | 35.46  | ,   | | 
24434,./data/charades/charades_i3d_tc4_f1024.pkl,Link,  | I3D  | 4 | 1024 | 37.20  | ,  |
24434,./data/charades/charades_r3d_tc4_f1024.pkl,Link,  | R3D  | 4 | 1024 |  41.1  | ,  |
24444,http://www.cvlibs.net/datasets/kitti/index.php,KITTI,"
","
"
24444,http://make3d.cs.cornell.edu/data.html,Make3D,"
","
"
24444,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU-V2,"
","
"
24444,https://github.com/janivanecky/Depth-Estimation/tree/master/dataset,the following,Download the official dataset and use , preprocessing instructions.
24444,https://github.com/marcelampc/d3net_depth_estimation/tree/master/dfd_datasets,here,Download the official dataset from ,.
24445,https://github.com/ScanNet/ScanNet#scannet-data,authors's website, by following instructions on the ,", which involve sending a signed TOS agreement via email. You should receive an email giving you access to the "
24445,https://vision.in.tum.de/data/datasets/rgbd-dataset/download,dataset website,Download selected sequences from the , and put them into some directory. You also need to download the 
24453,https://github.com/teddyauthors/teddy/blob/readme/data/tripadvisor_hotels.zip,reviews we provide,The , in order to demonstrate the application are provided by Trip Advisor under the 
24461,https://github.com/edgi-govdata-archiving/awesome-website-change-monitoring,Awesome Website Change Monitoring,"
","
"
24461,https://envirodatagov.org/archiving/,EDGI, to a multi-disciplinary effort for archiving projects run in affiliation with , and 
24461,http://datatogether.org/,Data Together, and ,.
24472,http://datashare.is.ed.ac.uk/handle/10283/1942,Edinburgh DataShare,The speech enhancement dataset used in the work can be found in ,. 
24478,#data,Data,"
","
"
24478,#data,Data section,Download files in the ,.
24481,#download-data,Download Data,"
","
"
24481,https://visualdialog.org/data,here,Download the VisDial v1.0 dialog json files from , and keep it under 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json,here,Get the word counts for VisDial v1.0 train split ,. They are used to build the vocabulary.
24481,https://visualdialog.org/data,here,"We also provide pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome. If you wish to extract your own image features, skip this step and download VIsDial v1.0 images from "," instead. Extracted features for v1.0 train, val and test are available for download at these links."
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_train.h5,"
features_faster_rcnn_x101_train.h5
","
",: Bottom-up features of 36 proposals from images of 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_val.h5,"
features_faster_rcnn_x101_val.h5
","
",: Bottom-up features of 36 proposals from images of 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_test.h5,"
features_faster_rcnn_x101_test.h5
","
",: Bottom-up features of 36 proposals from images of 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_train.h5,"
features_vgg16_fc7_train.h5
","
",: VGG16 FC7 features from images of 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_val.h5,"
features_vgg16_fc7_val.h5
","
",: VGG16 FC7 features from images of 
24481,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_vgg16_fc7_test.h5,"
features_vgg16_fc7_test.h5
","
",: VGG16 FC7 features from images of 
24486,https://www.nuscenes.org/data/detection-megvii.zip,tracking webpage,[3]'s detection results from the NuScenes ,.
24500,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,dataset,", closeness centrality and also filtration functions on edges such as jaccard similarity, edge probability for commonly used graph classification ",.
24511,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,source,  | Reddit-Binary Graphs (,). | | Mutagenicity*      | 
24511,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,source,  | Predicting the mutagenicity of molecules (,). | | Tox 21*      | 
24511,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,source,  | Predicting a compound's toxicity (,). |
24512,https://github.com/simnalamburt/models/tree/clovaai-assembled-cnn/research/slim#an-automated-script-for-processing-imagenet-data,here,"ImageNet2012 raw images and tfrecord. For this data, please refer to ","
"
24512,./datasets,here,"For transfer learing datasets, refer to scripts in ","
"
24512,./datasets/CE_dataset/README.MD,here,To make mCE evaluation dataset. refer to ,"
"
24513,#finetuning-on-whole-dataset,Finetuning on whole dataset,"
","
"
24513,#sentiment-analysis-using-the-dutch-book-review-dataset,Sentiment analysis using the Dutch Book Review Dataset,"
","
"
24513,https://oscar-corpus.com/,OSCAR corpus,We pre-trained RobBERT using the RoBERTa training regime. We pre-trained our model on the Dutch section of the ,", a large multilingual corpus which was obtained by language classification in the Common Crawl corpus. This Dutch corpus is 39GB large, with 6.6 billion words spread over 126 million lines of text, where each line could contain multiple sentences, thus using more data than concurrently developed Dutch BERT models."
24538,http://calvin-vision.net/datasets/youtube-objects-dataset/,here,The youtube-objects dataset can be downloaded from , and annotation can be found 
24538,http://vision.cs.utexas.edu/projects/videoseg/data_download_register.html,here, and annotation can be found ,.
24538,https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html,here,The FBMS dataset can be downloaded from ,.
24589,https://www.cs.upc.edu/~nlp/wikicorpus/,Wikicorpus, contains a vocabulary of nouns extracted from ,"
"
24597,https://github.com/castor-software/depclean-experiments/tree/master/data-collector,data-collector,The , tool mines the Maven Central repository to obtain usage information about artifacts and their dependencies. The analysis relies on fine-grained static bytecode analysis with 
24597,https://github.com/castor-software/depclean-experiments/tree/master/data-collector,data-collector, folder contains a set of R scripts supporting the analysis of the data collected with the , tool.
24635,http://bit.ly/ted-data-zhijing,this dataset," For research use (strictly not commercial), download ", of all the transcripts of TED talks.
24650,https://data.mendeley.com/datasets/zh5g5wbcj9/1,here,The dataset is described in the paper and its NetVLAD descriptors for both the reference and the query set are available for download ,.
24665,https://api.semanticscholar.org/corpus/download/,OpenCorpus,'s , dataset (released on 
24665,http://labs.semanticscholar.org/corpus/corpus/archive#,here,You can download the associated OpenCorpus dataset ,.
24688,./data/,TVR dataset,Data: ,"
"
24690,https://pandas.pydata.org,https://pandas.pydata.org,Pandas (see ,)
24701,https://paperswithcode.com/sota/synthetic-data-generation-on-uci-epileptic?p=cor-gan-correlation-capturing-convolutional,"
PWC
","
","
"
24701,#data-fidelity,Data Fidelity,"
","
"
24718,https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.PascalVOCKeypoints,PascalVOC with Berkely annotations,We provide training and evaluation procedures for the ," dataset, the "
24718,https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.WILLOWObjectClass,WILLOW-ObjectClass," dataset, the "," dataset, the "
24718,https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.PascalPF,PascalPF," dataset, the "," dataset, and the "
24718,https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.DBP15K,DBP15K," dataset, and the ", dataset. Experiments can be run via:
24732,https://cs.brown.edu/people/epavlick/data.html,The Pavlick Dictionaries,"
", Crowd-sourced dictionaries available in many languages.
24739,https://pandas.pydata.org/,Pandas,      | For TwinSVM-based models evaluation and selection.             | BSD 3-Clause | | ,           | For reading and processing datasets.                           | BSD 3-Clause | | 
24739,https://github.com/mir-am/LIBTwinSVM/tree/master/dataset,here,"To help you prepare your dataset and test the program, three datasets are included ",.
24758,./data,format,"First, please prepare your data in recommended ",". The MS MARCO or ClueWeb09 corpus may be too large for your machine, you can change "
24759,documentation/data_format.md,data_format.md,For a detailed overview of commonly needed data input files see: ,"
"
24760,task_executor/config/data.yaml,"
data.yaml
", are generally available in ,"
"
24764,https://www.kaggle.com/c/imagenet-object-localization-challenge/data,ImageNet, and , must be manually downloaded and placed in the 
24770,https://spectrum.ieee.org/tech-talk/computing/networks/pay-cloud-services-data-tool-news,Cloud Services Tool Lets You Pay for Data You Use—Not Data You Store,IEEE Spectrum: ,"
"
24772,http://snap.stanford.edu/biodata/index.html,BioSNAP, folder. The raw datasets are available on the ,. See 
24802,https://www.rstudio.com/resources/videos/tidy-spatial-data-analysis/,video, (,"), "
24809,benchmark_dataset.zip,benchmark_dataset,The benchmark dataset can be downloaded from this Github repository: ,. Be aware that it expands to over 300 MB during decompression. The structure of the files is as follows.
24814,https://github.com/Valentyn1997/xray/wiki/First-look-at-dataset,First look at dataset,) and includes x-ray images of hands. The data seems not to be very clean (see ,). To handle this problem a data cleaning pipeline was implemented (see 
24818,https://github.com/rikdz/GraphWriter/tree/master/data,AGENDA,"In our experiments, we use the following datasets:  ", and 
24821,http://vowl.visualdataweb.org/webvowl/,WebVOWL,Visualize the ontology using ,. 
24821,http://visualdataweb.de/webvowl/#iri=http://ethon.consensys.net/EthOn.ttl,It should be enough to click this link,. ,. WebVOWL is also developed on GitHub: https://github.com/VisualDataWeb/WebVOWL
24843,https://nbviewer.jupyter.org/github/alartum/ncvis-examples/blob/master/big-data.ipynb,big-data.ipynb,  | Introduction to NCVis | |,| Large-scale application case |
24843,https://archive.ics.uci.edu/ml/datasets/Iris,Iris,|fmnist| FMNIST| |,|iris|Iris| |
24843,https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits,Handwritten Digits,|iris|Iris| |,|pendigits|PenDigits| |
24843,https://hemberg-lab.github.io/scRNA.seq.datasets/mouse/brain/,Mouse scRNA-seq,|coil100|COIL100| |,|scrna|ScRNA| |
24843,https://archive.ics.uci.edu/ml/datasets/Statlog+(Shuttle),Statlog (Shuttle),|scrna|ScRNA| |,|shuttle|Shuttle|
24844,https://www.kaggle.com/therohk/india-headlines-news-dataset,News Headlines Of India dataset,We use preprocessed samples from the ," to perform the comparison. Test cases are generated by taking the first 1000, 2 · 1000, . . . , 2¹⁰ · 1000 samples from the dataset. Given the same amount of time "
24844,https://www.kaggle.com/therohk/india-headlines-news-dataset,News Headlines Of India dataset, does not achieve this limit but signifi- cantly outperforms other methods. We used 10000 samples from the ,.
24844,https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits,Optical Recognition of Handwritten Digits Data Set,It is important that the proposed method has predictable behavior on simple datasets. We used the , which comprised 5620 preprocessed handwritten digits and thus has a simple structure that is assumed to be revealed by visualization. 
24845,./Tutorial%203%20Scenario%20from%20data.ipynb,Creating a scenario from data,"
","
"
24845,./Tutorial%204%20Scenario%20database.ipynb,Scenario database,"
","
"
24849,https://github.com/luizaugustogarcia/tdp1375/tree/master/src/main/resources/datasets,here, (longer random permutations ," — disregard the first zeros). Be aware that, before executing the algorithm itself, a large dataset generated from a (huge) case analysis has to be loaded into memory. This demands a significant amount of memory (~2GB) and it takes about 15 seconds to complete on a computer equipped with an 11th Gen Intel Core i9-11950H processor with 32GB DDR4 RAM and a NVMe SSD hard disk. Make sure your computer has enough memory available."
24871,https://github.com/MeMory-of-MOtion/docker-loco3d#generated-data,second section, of the guide describe the various scenarios used. The , describe the data stored and how to use them.
24872,https://github.com/Gepetto/example-robot-data,example-robot-data,"
", (only for running the examples)
24873,https://medium.com/@hbjenssen/covid-19-radiology-data-collection-and-preparation-for-artificial-intelligence-4ecece97bb5b,blog post,"The regular U-net(R231) model works very well for COVID-19 CT scans. However, collections of slices and case reports from the web are often cropped, annotated or encoded in regular image formats so that the original hounsfield unit (HU) values can only be estimated. The training data of the U-net(R231CovidWeb) model was augmented with COVID-19 slices that were mapped back from regular imaging formats to HU. The data was collected and prepared by MedSeg (http://medicalsegmentation.com/covid19/). While the regular U-net(R231) showed very good results for these images there may be cases for which this model will yield slighty improved segmentations. Note that you have to map images back to HU when using images from the web. This ", describes how you can do that. Alternatively you can set the 
24875,https://www.kaggle.com/datasnaek/youtube-new,trending YouTube videos,We use a bag-of-words dataset constructed from ,", and predict the "
24876,https://socceraction.readthedocs.io/en/latest/documentation/data/index.html,Read more »," from StatsBomb, Opta, Wyscout, Stats Perform and WhoScored as Pandas DataFrames using a unified data model. ","
"
24911,https://cocodataset.org/,MSCOCO captioning dataset,"Dataset is developed at the Language Processing Laboratory, Uka Tarsadia University, Gujarat, India. It was part of ongoing research on Natural Lanugage Processing and Machine Translation. This dataset contains around 65000 english sentiences from ", that are translated to Gujarati and converted to parallel format.
24912,https://ls7-data.cs.tu-dortmund.de/shape_net/ShapeNet_SDF.tar.gz,download,"We provide readily prepared datasets for the Chairs, Airplanes and Sofas categories of Shapenet as a ",. The size of that dataset is 71 GB.
24914,#data,Data,"
","
"
24914,#domain-specific-data,Domain-specific Data,"
","
"
24914,#knowledge-graph-database,Knowledge Graph Database,"
","
"
24914,./papers/KG-database.md,KG Database Systems,"
","
"
24914,http://www.openkg.cn/dataset/2019-ncov-baike,链接,新冠百科图谱 [,] Knowledge graph from encyclopedia[
24914,http://www.openkg.cn/dataset/2019-ncov-baike,Link,] Knowledge graph from encyclopedia[,]
24914,http://www.openkg.cn/dataset/2019-ncov-research,链接,新冠科研图谱 [,] Knowledge graph of COVID-19 research [
24914,http://www.openkg.cn/dataset/2019-ncov-research,Link,] Knowledge graph of COVID-19 research [,]
24914,http://www.openkg.cn/dataset/2019-ncov-clinic,链接,新冠临床图谱 [,] Clinical knowledge graph [
24914,http://www.openkg.cn/dataset/2019-ncov-clinic,Link,] Clinical knowledge graph [,]
24914,http://www.openkg.cn/dataset/2019-ncov-hero,链接,新冠英雄图谱 [,"] Knowledge graph of people, experts, and heroes ["
24914,http://www.openkg.cn/dataset/2019-ncov-hero,Link,"] Knowledge graph of people, experts, and heroes [",]
24914,http://www.openkg.cn/dataset/2019-ncov-event,链接,新冠热点事件图谱 [,] Knowledge graph of public events [
24914,http://www.openkg.cn/dataset/2019-ncov-event,Link,] Knowledge graph of public events [,]
24917,https://www.kaggle.com/c/allstate-purchase-prediction-challenge/data,here,The Allstate dataset can be downloaded from Kaggle ,. Place the uncompressed 
24932,https://github.com/tensorflow/cleverhans/blob/master/examples/nips17_adversarial_competition/dataset,its official repository,", including their URLs, cropping bounding boxes, classification labels and some other metadata. More details on this dataset can be found in ",.
24936,https://www.dropbox.com/s/kkj73eklp5fnut0/data.zip?dl=0,here,Labeled faces in the wild. Get it from ,.
24937,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,as described here,If the data is not preprocessed ,", then copy the scripts in "
24946,https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/LICENSE,MIT licensed,"
",.
24952,data,data directory, inside the datasets' directories in order to prepare the json files from the original sources. Please see the instructions in the , for more details.
24953,https://github.com/googlecreativelab/quickdraw-dataset,"
Quick, Draw
",We use ," dataset for our experiment. The dataset is splited into pretraining dataset and retrevial dataset by 10,000 samples per class and 1,100 samples per class. Image datasets and stroke datasets are used for traning CNN and TCN."
24961,dataset.md,Datasets,"
","
"
24964,https://github.com/deepmind/gqn-datasets,Generative Query Networks Mujoco environment,This dataset was created using the ,. This is not an official Google product.
24965,https://www.di.ens.fr/willow/research/netvlad/data/netvlad_v100_datasets.tar.gz,here,"), and the dataset specifications for the Pittsburgh dataset (available ",). 
24966,http://dobigeon.perso.enseeiht.fr/data/EELS/data_EELS_2020.zip,here,Download the data ,","
24973,https://commonvoice.mozilla.org/en/datasets,Common Voice audio clips and transcripts,Download , (version 4).
24973,https://commonvoice.mozilla.org/en/datasets,Common Voice audio clips and transcripts,Download , (version 3).
25006,https://github.com/HorizonRobotics/alf/blob/911d9573866df41e9e3adf6cdd94ee03016bf5a8/alf/algorithms/data_transformer.py#L672,HER,| |,"|Off-policy RL|Andrychowicz et al. ""Hindsight Experience Replay"" "
25006,alf/examples/hybrid_rl/replay_buffer_data/pendulum_replay_buffer_from_sac_10k,offline demonstrations,. Learning a control policy from ,.
25011,https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data,Acquire Valued Shoppers Challenge,a) Put the , data in 
25011,https://www.kaggle.com/c/home-credit-default-risk/data,Home Credit Default Risk,b) Put the , data in 
25011,https://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose/data,KDD Cup 2014,c) Put the , data in 
25024,https://github.com/akelleh/causality/blob/master/causality/analysis/dataframe.py#L8,"
causality.analysis.CausalDataFrame
", object in ,. This is just an extension of the 
25031,https://github.com/rfordatascience/tidytuesday/issues,Issues," is built around open datasets that are found in the ""wild"" or submitted as ", on our GitHub.
25031,https://github.com/rfordatascience/tidytuesday/issues,Issue,Submit the dataset as an ,"
"
25031,https://github.com/rfordatascience/tidytuesday/issues,Issue," a. Find an interesting dataset b. Find a report, blog post, article etc relevant to the data c. Submit the dataset as an ", along with a link to the article
25031,https://github.com/rfordatascience/tidytuesday/issues,Issue," a. Find an interesting dataset b. Find a report, blog post, article etc relevant to the data (or create one yourself!) c. Let us know you've found something interesting and are working on it by filing an ", on our GitHub d. Provide a link or the raw data and a cleaning script for the data e. Write a basic 
25031,https://github.com/rfordatascience/tidytuesday/tree/master/community_resources/code_chunks,Pull Request,Want to submit a useful code-chunk? Please submit as a , and follow the 
25031,https://github.com/rfordatascience/tidytuesday/blob/master/community_resources/code_chunks/readme.md,guide, and follow the ,.
25031,https://medium.com/nightingale/writing-alt-text-for-data-visualization-2a218ef43f81,article,The DataViz Society/Nightingale has an , on writing 
25031,data/2018,2018,"
", | 
25031,data/2019,2019, | , | 
25031,data/2020,2020, | ,  | 
25031,data/2021,2021,  | , | 
25031,data/2022,2022, | , | 
25031,data/2023,2023, | ,"
"
25031,data/2023/2023-01-10/readme.md,Bird FeederWatch data, | , | 
25031,https://feederwatch.org/explore/raw-dataset-requests/,FeederWatch, | , | 
25031,data/2023/2023-01-17/readme.md,Art history data, | , | 
25031,data/2023/2023-01-24/readme.md,Alone data, | , | 
25031,https://gradientdescending.com/alone-r-package-datasets-from-the-survival-tv-series/,Alone R package: Datasets from the survival TV series, | , | | 5 | 
25031,data/2023/2023-01-31/readme.md,Pet Cats UK, | , | 
25031,https://www.datarepository.movebank.org/handle/10255/move.882,Movebank for Animal Tracking Data, | , | 
25031,"https://themarkup.org/data-is-plural/2023/01/25/from-jazz-solos-to-cats-on-the-move#:~:text=Giuseppe%20Sollazzo%5D-,Cats%20on%20the%20move,-.%20Between%202013",Cats on the Move, | , | | 6 | 
25031,data/2023/2023-02-07/readme.md,Big Tech Stock Prices, | , | 
25031,https://www.kaggle.com/datasets/evangower/big-tech-stock-prices,Big Tech Stock Prices on Kaggle, | , | 
25031,data/2023/2023-02-14/readme.md,Hollywood Age Gaps, | , | 
25031,data/2023/2023-02-21/readme.md,Bob Ross Paintings, | , | 
25031,https://github.com/jwilber/Bob_Ross_Paintings/blob/master/data/bob_ross_paintings.csv,Bob Ross Paintings data, | , | 
25031,data/2023/2023-02-28/readme.md,African Language Sentiment, | , | 
25031,data/2023/2023-03-07/readme.md,Numbats in Australia, | , | 
25031,data/2023/2023-03-14/readme.md,European Drug Development, | , | 
25031,https://www.ema.europa.eu/en/medicines/download-medicine-data,European Medicines Agency, | , | 
25031,https://towardsdatascience.com/dissecting-28-years-of-european-pharmaceutical-development-3affd8f87dc0,Dissecting 28 years of European pharmaceutical development, | , | | 12 | 
25031,data/2023/2023-03-21/readme.md,Programming Languages, | , | 
25031,data/2023/2023-03-28/readme.md,Time Zones, | , | 
25031,https://data.iana.org/time-zones/tz-link.html,IANA tz database, | , | 
25031,data/2023/2023-04-04/readme.md,Premier League Match Data, | , | 
25031,https://www.kaggle.com/datasets/evangower/premier-league-match-data,Premier League Match Data 2021-2022, | , | 
25031,https://www.rfordatasci.com,Link,| Link | Description | | --- | --- | | , | The R4DS Online Learning Community Website| | 
25031,https://github.com/baltimore-sun-data,Link, | The Upshot by NY Times | | , | The Baltimore Sun Data Desk | | 
25031,https://github.com/datadesk,Link, | The Baltimore Sun Data Desk | | , | The LA Times Data Desk | | 
25031,https://serialmentor.com/dataviz/,Link,| Link | Description | | --- | --- | | , | Fundamentals of Data Viz by Claus Wilke | | 
25031,https://bookdown.org/rdpeng/artofdatascience/,Link, | Fundamentals of Data Viz by Claus Wilke | | , | The Art of Data Science by Roger D. Peng & Elizabeth Matsui | | 
25031,https://medium.com/bbc-visual-and-data-journalism/how-the-bbc-visual-and-data-journalism-team-works-with-graphics-in-r-ed0b35693535,Link, cookbook by Winston Chang | , | BBC Data Journalism team |
25033,https://codecov.io/gh/ropensci/dataspice?branch=main,"
Codecov test coverage
","
","
"
25033,https://github.com/amoeba/dataspice-example/,https://github.com/amoeba/dataspice-example, might look like can be found at ,". From there, you can also check out a preview of the HTML "
25033,https://amoeba.github.io/dataspice-example/,https://amoeba.github.io/dataspice-example, generates at , and how Google sees it at 
25033,https://search.google.com/test/rich-results?url=https%3A%2F%2Famoeba.github.io%2Fdataspice-example%2F,https://search.google.com/test/rich-results?url=https%3A%2F%2Famoeba.github.io%2Fdataspice-example%2F, and how Google sees it at ,.
25033,https://annakrystalli.me/dataspice-tutorial/,https://annakrystalli.me/dataspice-tutorial/, at , (
25033,https://github.com/annakrystalli/dataspice-tutorial,GitHub repo, (,).
25033,https://developers.google.com/search/docs/data-types/dataset,dataset discovery, generates a json-ld file (“linked data”) to aid in ,", creation of more extensive metadata (e.g. "
25033,https://github.com/amoeba/dataspice-example/,repository," folder with a simple view of the dataset with the metadata and an interactive map. For example, this ", results in this 
25033,https://amoeba.github.io/dataspice-example/,website, results in this ,"
"
25033,http://rd-alliance.github.io/metadata-directory/standards/,RDA metadata directory, & the ,.
25035,https://github.com/the-pudding/data/tree/master/nba-unexpected,repo,"                                                         | February 2023         | Never                 | basketball, NBA                                           | ",                                       | | 
25035,https://pudding.cool/2022/04/random-data/raw.json,json,"                                                         | April 2022         | Daily                 | replication, randomness                                           | ",                                       | | 
25035,https://github.com/the-pudding/banknotes/tree/master/src/data,repo," | April 2022 | Never | banknotes, currency | ", | | 
25035,https://github.com/the-pudding/data/tree/master/women-in-headlines,repo,"                                                         | February 2022         | Never                 | headlines, women                                           | ",                                       | | 
25035,https://github.com/the-pudding/data/tree/master/foundation-names,repo,"                                                         | March 2021         | Never                 | beauty, makeup, diversity, US                                           | ",                                       | | 
25035,https://github.com/the-pudding/data/tree/master/winning-the-internet,repo,                                                        | July 2020          | Sometimes             | newsletter                                                              | ,                                   | | 
25035,https://github.com/the-pudding/police_misconduct_pudding/tree/main/src/assets/data,data,                                                     | October 2020       | Never                 | social                                                                  | ,                     | | 
25035,https://github.com/the-pudding/data/tree/master/campaign-colors,repo,"                         | August 2020        | Never                 | politics, design, color                                                 | ",                                        | | 
25035,https://github.com/the-pudding/song-decay-clean/tree/master/src/assets/data,repo,                                                              | July 2020          | Never                 | music                                                                   | ,                            | | 
25035,https://pudding.cool/2020/04/infinite-data/data.json,json,"                                         | Apr 2020           | Daily                 | piano, probability                                                      | ",                                                   | | 
25035,https://github.com/the-pudding/data/tree/master/kidz-bop,repo,"                                           | Apr 2020           | Never                 | Kidz Bop, music, censorship                                             | ",                                               | | 
25035,https://github.com/the-pudding/data/tree/master/census-history,repo,"                                      | Mar 2020           | Never                 | US, census                                                              | ",                                         | | 
25035,https://github.com/the-pudding/data/tree/master/rain,repo,"                                                  | Feb 2020           | Never                 | weather, precipitation                                                  | ",                                                   | | 
25035,https://github.com/the-pudding/data/tree/master/laugh,repo,"                                                                    | Oct 2019           | Never                 | laughter, Reddit                                                        | ",                                                  | | 
25035,https://github.com/the-pudding/data/tree/master/dog-shelters,repo,"                                                           | Oct 2019           | Never                 | dogs, PetFinder, US                                                     | ",                                           | | 
25035,https://github.com/the-pudding/data/tree/master/summer-reading,repo,"                                              | June 2019          | Never                 | books, library, Seattle                                                 | ",                                         | | 
25035,https://github.com/the-pudding/data/tree/master/people-map-uk,repo,"                                                     | June 2019          | Never                 | wiki, names, UK                                                         | ",                                          | | 
25035,https://github.com/the-pudding/data/tree/master/people-map,repo,"                                                        | May 2019           | Never                 | wiki, names, US                                                         | ",                                             | | 
25035,https://github.com/the-pudding/data/tree/master/names-in-songs,repo,"                                                              | May 2019           | Never                 | music, names, US                                                        | ",                                         | | 
25035,https://github.com/the-pudding/data/tree/master/three-seconds,repo,"                              | May 2019           | Never                 | basketball, NBA                                                         | ",                                          | | 
25035,https://github.com/the-pudding/data/tree/master/vogue,repo,"                                                           | April 2019         | Never                 | fashion, diversity, US                                                  | ",                                                  | | 
25035,https://github.com/the-pudding/eu-regions/tree/master/src/assets/data,data and analysis,"                                                                   | April 2019         | Daily                 | european union, eu                                                      | ",                     | | 
25035,https://pudding.cool/2019/03/sankey-nba-data/data-all.json,data,"                                                              | March 2019         | Daily                 | spelling, names, sankey, NBA                                            | ",                                             | | 
25035,https://github.com/the-pudding/telephone/tree/master/src/assets/data,repo,                                                  | March 2019         | Never                 | music                                                                   | ,                                   | | 
25035,https://github.com/the-pudding/data/tree/master/hype,repo,"                                     | March 2019         | Never                 | US, basketball, NBA                                                     | ",                                                   | | 
25035,https://pudding.cool/2019/02/sankey-data/data-all.json,data,"                                                    | February 2019      | Never                 | spelling, names, sankey                                                 | ",                                                 | | 
25035,https://github.com/the-pudding/data/tree/master/dress-codes,repo,"    | February 2019      | Never                 | US, dress code, high school                                             | ",                                            | | 
25035,https://github.com/the-pudding/data/tree/master/boybands,repo,"                                                    | November 2018      | Never                 | music, boybands, dance, US                                              | ",                                               | | 
25035,https://github.com/the-pudding/data/tree/master/dearabby,repo,"                                             | November 2018      | Never                 | advice, anxiety                                                         | ",                                               | | 
25035,https://github.com/the-pudding/data/tree/master/titletowns,repo,"                               | November 2018      | Never                 | sports, championships, rankings, basketball, football, hockey, baseball | ",                                             | | 
25035,https://github.com/the-pudding/wiki-billboard-data#historical-data,repo,"                                     | October 2018       | Never                 | celebrities, culture                                                    | ",                                     | | 
25035,https://github.com/the-pudding/wiki-billboard-data,repo,"                                          | September 2018     | Daily                 | celebrities, culture                                                    | ",                                                     | | 
25035,https://github.com/the-pudding/data/tree/master/filmordigital,repo,"                                                          | August 2018        | Never                 | film, movies                                                            | ",                                          | | 
25035,https://github.com/the-pudding/data/tree/master/pockets,repo,"                                                     | August 2018        | Never                 | pockets, fashion, equality, women                                       | ",                                                | | 
25035,https://github.com/the-pudding/wiki-death-data,repo,"                                                 | August 2018        | Never                 | Wikipedia, pageviews, death                                             | ",                                                         | | 
25035,https://github.com/the-pudding/data/tree/master/birth-control,repo,"                                             | July 2018          | Never                 | contraception, US, birth control, health, survey, CDC                   | ",                                          | | 
25035,https://github.com/the-pudding/data/tree/master/gayborhoods,"Men are from Chelsea, Women are from Park Slope",                                          | | ,"           | June 2018          | Never                 | gayborhoods, gay, lesbian, queer, LGBTQ, neighborhoods, pride, gender   | "
25035,https://github.com/the-pudding/data/tree/master/gayborhoods,repo,"           | June 2018          | Never                 | gayborhoods, gay, lesbian, queer, LGBTQ, neighborhoods, pride, gender   | ",                                            | | 
25035,https://github.com/polygraph-cool/data/tree/master/makeup-shades,repo,"                                             | June 2018          | Never                 | beauty, makeup, Fenty, diversity, US, global                            | ",                                       | | 
25035,https://github.com/polygraph-cool/data/tree/master/skate-music,repo,"                                           | June 2018          | Never                 | skateboard, music, genre, gnarly                                        | ",                                         | | 
25035,https://github.com/polygraph-cool/data/tree/master/cookies,repo,"                                    | May 2018           | Never                 | baking, cookies, machine learning, NLP                                  | ",                                             | | 
25035,https://github.com/polygraph-cool/data/tree/master/one-hit-wonders,repo,"                                                | April 2018         | Never                 | sports, ranking, basketball, golf, tennis, baseball, hockey             | ",                                     | | 
25035,https://pudding.cool/2018/04/birthday-data/data.json,json,"                                         | April 2018         | Daily                 | math, paradox, birthday                                                 | ",                                                   | | 
25035,https://github.com/polygraph-cool/data/tree/master/neighborhoods,repo,"                                                       | March 2018         | Never                 | neighborhoods, business, Yelp                                           | ",                                       | | 
25035,https://github.com/polygraph-cool/data/tree/master/stand-up,repo,"                                                | February 2018      | Never                 | stand-up, comedy, Ali Wong, US                                          | ",                                            | | 
25035,https://github.com/polygraph-cool/data/tree/master/mars-weather,repo,"                                                        | February 2018      | Daily                 | mars, weather, Curiosity Rover, global                                  | ",                                        | | 
25035,https://github.com/polygraph-cool/data/tree/master/clinics,repo," | September 2017     | Never                 | abortion, clinics, duration, access, US                                 | ",                                             | | 
25035,https://github.com/polygraph-cool/data/tree/master/cetaceans,repo,"                                           | July 2017          | Never                 | whales, dolphins, captivity, US                                         | ",                                           | | 
25035,https://github.com/polygraph-cool/data/tree/master/births,repo,"                                                        | May 2017           | Never                 | births, babies, county, US                                              | ",                                              | | 
25037,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,VoxCeleb1,"
", and 
25037,https://datashare.is.ed.ac.uk/handle/10283/3336,ASVSpoof19, and , (logical access)  datasets
25043,https://github.com/udacity/self-driving-car/tree/master/datasets/CH2,Udacity,The dataset used in this project is ,", which contains real-word condition images collected by a front camera installed in a vehicle. The training and testing dataset contains 33805 and 5614 frames, speriately. The steer angle of each frame is converted from a degree to a calue in the range (-1,1)"
25043,https://github.com/udacity/self-driving-car/tree/master/datasets/CH2#ch2_001,here, directories from , for both training (
25052,https://github.com/illacceptanything/illacceptanything/blob/75109b706351420c31999915d4d54b0b4ab12df7/data/text/EICAR.COM.TXT,may trigger antivirus software,; ,!
25071,https://sigmathling.kwarc.info/resources/arxmliv-dataset-082018/,arXMLiv 08/2018, for the datasets , and 
25077,https://voice.mozilla.org/en/datasets,common voices datasets,"Transfer learning on other languages, using the ","
"
25077,https://voice.mozilla.org/en/datasets,here,To begin download the common voices datasets ,", you will also need to download our phonem annotations and our train / val / test splits for each language "
25077,https://voice.mozilla.org/en/datasets,here,To begin download the common voices datasets ,", you will also need to download our phonem annotations and our train / val / test splits for each language "
25091,https://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Oxford 102 Flowers,"
","
"
25100,https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions/tree/master/Data/Model_train_dev_test_dataset/Other_model_train_dev_test_dataset,Data,] [,]
25100,https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions/tree/master/Data/Model_train_dev_test_dataset/Other_model_train_dev_test_dataset,Data,] [,]
25100,https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions/tree/master/Data/Model_train_dev_test_dataset/Other_model_train_dev_test_dataset,Data,] [,]
25100,https://github.com/14H034160212/HHH-An-Online-Question-Answering-System-for-Medical-Questions/tree/master/Data/Model_train_dev_test_dataset/BERT_train_dev_test_dataset,Data,] [,]
25103,https://deepmind.com/research/open-source/open-source-datasets/kinetics/,Kinetics dataset, of ," consists of the 200 categories with most training examples; for each category, we randomly sample 400 examples from the training set, and 25 examples from the validation set, resulting in 80K training examples and 5K validation examples in total."
25108,data/,"
data
"," which can be modified for other architectures and datasets. To support more datasets, please add new dataloaders to ", folder.
25120,https://www.ino.ca/en/solutions/video-analytics-dataset/,Internet,"The dataset in VIFB is a test set, which is collected by the authors from the "," and from fusion tracking datasets [1,2,3]. Image registration between RGB and infrared images are also conducted by corresponding authors. We appreciate the authors of these datasets very much for making these images publicly available for research. "
25120,https://www.ino.ca/en/solutions/video-analytics-dataset/,link,Please also cite these papers and the , if you use VIFB
25138,https://ibug.doc.ic.ac.uk/resources/afew-va-database/,AFEW-VA," dataset for EXPR, and the ", dataset for VA; (2) resampling the minority class and the majority class. Our purpose is to create a more balanced data distribution for each individual class.
25156,https://github.com/raosudha89/GYAFC-corpus,official repo, folder. For the formality transfer dataset please see its ,.
25168,#data,Data,"
","
"
25168,https://medium.com/@jeffrey.lancaster/the-ultimate-game-of-thrones-dataset-a100c0cf35fb,"""The Ultimate Game of Thrones Dataset""","
","
"
25168,https://medium.com/@jeffrey.lancaster/32-game-of-thrones-data-visualizations-f4ab6bc978d8,"""32 Game of Thrones Data Visualizations""","
","
"
25168,https://medium.com/@jeffrey.lancaster/19-more-game-of-thrones-data-visualizations-bb2d2c0981f9,"""19 More Game of Thrones Data Visualizations""","
","
"
25168,https://medium.com/@jeffrey.lancaster/game-of-thrones-s8-e1-data-visualization-recap-9656a97c3df3,"""Winterfell""","
"," (Season 8, Episode 1)"
25168,https://medium.com/@jeffrey.lancaster/game-of-thrones-s8-e2-data-visualization-recap-3d7336ca975e,"""A Knight of the Seven Kingdoms""","
"," (Season 8, Episode 2)"
25168,https://medium.com/@jeffrey.lancaster/game-of-thrones-s8-e3-data-visualization-recap-7843e1d24282,"""The Long Night""","
"," (Season 8, Episode 3)"
25168,https://medium.com/@jeffrey.lancaster/game-of-thrones-s8-e4-data-visualization-recap-20ee28cf2af1,"""The Last of the Starks""","
"," (Season 8, Episode 4)"
25179,#how-can-i-use-my-own-dataset,How can I use my own dataset?,"
","
"
25198,https://www.med.upenn.edu/sbia/brats2018/data.html,[HERE],[1] BraTs 2018: ,"
"
25198,https://www.med.upenn.edu/cbica/brats2019/data.html,[HERE],[2] BraTs 2019: ,"
"
25211,https://github.com/tensorflow/data-validation,"
Tensorflow Data Validation (TFDV)
","
", - analyze the distribution of your dataset
25211,https://www.tensorflow.org/tfx/data_validation/get_started,TensorFlow Data Validation, and how to use it with a real dataset. This notebook also goes over , and 
25212,https://seaborn.pydata.org,seaborn.pydata.org,Online documentation is available at ,.
25212,https://seaborn.pydata.org/tutorial.html,tutorial,The docs include a ,", "
25212,https://seaborn.pydata.org/examples/index.html,example gallery,", ",", "
25212,https://seaborn.pydata.org/api.html,API reference,", ",", and other useful information."
25212,https://pandas.pydata.org/,pandas,", ",", and "
25215,https://www.comet.ml/site/data-scientists/,Comet," - An example of how to log metrics, hyperparameters and more from Catalyst runs to ","
"
25220,https://towardsdatascience.com/once-upon-a-repository-how-to-write-readable-maintainable-code-with-pytorch-951f03f6a829,"Once Upon a Repository: How to Write Readable, Maintainable Code with PyTorch","
","
"
25220,https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f,PyTorch Ignite - Classifying Tiny ImageNet with EfficientNet,"
","
"
25235,https://github.com/hsp-iit/GRASPA-benchmark#how-to-collect-the-data,How to collect data,"
","
"
25235,https://github.com/hsp-iit/GRASPA-benchmark#how-to-collect-the-data,here,Once the data required by the benchmark are available (see ," instructions on how to collect them), all the scores can be computed by:"
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data,"
data/
","
", collects the:
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/objects/YCB,"
YCB objects models
","
", used in the grasping layouts[1];
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/scenes/grasping/3D_scenes,"
3D renders of the grasping layouts
",the ,;
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/scenes/grasping/printable_layouts,"
printable versions
",the ,;
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/scenes/reachability,"
reachability test
",the poses to be reached for the , and the 
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/scenes/camera_calibration,"
camera-calibration test
", and the ,.
25235,https://github.com/hsp-iit/GRASPA-benchmark/tree/master/data/template_files,"
template files
","
",", the "
25235,/data_collection,How to collect the data,"
","
"
25236,https://github.com/robotology-playground/GRASPA-test/tree/master/experiment_data,"
experiment_data
",The , directory contains the 
25250,https://github.com/spdx/license-list-data,license-list-data,The license texts are taken directly from , repository. The detection algorithm is 
25250,dataset.zip,dataset,On the , of ~1000 most starred repositories on GitHub as of early February 2018 (
25250,dataset.projects.gz,list, of ~1000 most starred repositories on GitHub as of early February 2018 (,"), "
25256,https://github.com/salesforce/awd-lstm-lm/blob/master/getdata.sh,here,Instructions for acquiring PTB is ,". While CIFAR-10, CIFAR-100 and SVHN can be automatically downloaded by torchvision. To use NAS-Bench-1Shot1 to monitor the anytime test accuracy of NAS algorithms, please download this tfrecord: "
25258,https://github.com/NVlabs/ffhq-dataset,FFHQ, and ,; and a toy dataset: 
25258,/data/MixtureGaussian3By3.pk,Mixture of Gaussians,; and a toy dataset: ,.
25263,./parallel-corpus/,the Readme in this folder,"| directory | description | |---        |---          | | parallel-corpus | Main parallel corpus with a canonical split in  109108 training triples, 2000 validation triples and 2000 test triples. Each triple is annotated by metadata (repository owner, repository name, source file and line number). Also two versions of the above corpus reassembled into pairs: (declaration+body, docstring) and (declaration+docstring, body), for  code documentation tasks and code generation tasks, respectively. You may refer to "," for descriptions about escape tokens| | code-only-corpus | A code-only corpus of 161630 pairs of function declarations and function bodies, annotated with metadata. | | backtranslations-corpus | A corpus of docstrings automatically generated from the code-only corpus using Neural Machine Translation, to enable data augmentation by ""backtranslation"" | | nmt-outputs | Test and validation outputs of the baseline Neural Machine Translation models. | | repo_split.parallel-corpus | An alternate train/validation/test split of the parallel corpus which is ""repository-consistent"": no repository is split between training, validation or test sets. | | repo_split.code-only-corpus | A ""repository-consistent"" filtered version of the code-only corpus: it only contains fragments which appear in the training set of the above repository. | | scripts | Preprocessing scripts used to generate the corpora. | | V2 | code-docstring-corpus version 2, with class declarations, class methods, module docstrings and commit SHAs. |"
25266,https://datasets.simula.no/kvasir/,The Kvasir Dataset,Our dataset is a part of ,.
25268,http://people.ee.ethz.ch/~ihnatova/pynet.html#dataset,Zurich RAW to RGB mapping dataset,Download , and extract it into 
25275,docs/datasets.md,datasets.md,Please refer to , for details.
25282,https://www.tensorflow.org/datasets/catalog/imagenet2012,tensorflow_datasets,"Once you have created virtual machine with Cloud TPUs, and pre-downloaded the ImageNet data for ",", please set the following enviroment variables:"
25282,https://www.tensorflow.org/datasets/catalog/imagenet2012_subset,tensorflow datasets,You can access 1% and 10% ImageNet subsets used for semi-supervised learning via ,: simply set 
25308,data,data/,The synthetic data used in the paper in ,", as well as the "
25308,data_generator/data_generator.py,script used to generate the data,", as well as the ",.
25308,results/synthetic_data_results.xlsx,More detailed results than the ones shown in the paper for the synthetic data,"
",.
25308,data/synthetic_data_train.csv,data/synthetic_data_train.csv, train dataset file; check , for the correct format.
25308,data/synthetic_data_priorities.csv,data/synthetic_data_priorities.csv, priorities file; check , for the correct format.
25308,data/loss_function.txt,data/loss_function.txt, loss function file; check , for the correct format.
25308,data/best_genetic_w_arp_config,data/best_genetic_w_arp_config, evaluate a rules configuration; check ,"
"
25308,data/best_genetic_config,"""best_random_config"""," obtained the best results in the validation set. Then, by checking the ""removed rules"", we created a rule configuration file ", and evaluated that rule configuration on the test set.
25325,http://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014,UCI machine learning database," corresponds to average power consumption by 321 Portuguese households  between 2012 and 2014, in units of kilowatts consumed in fifteen minute increments. This dataset is from the ",.
25325,https://developer.ibm.com/exchanges/data/all/double-pendulum-chaotic/,Asseman et al.," correspond to two different double pendulum experiments, taken from a series of experiments by ",". In Asseman et al.'s original study, pendula were filmed and segmented to produce (x, y) positions of centroids over time. Here, we have converted the dataset into the four canonical Hamiltonian coordinates (theta1, theta2, p1, p2)."
25325,http://crcns.org/data-sets/thalamus/th-1/about-th-1,CRCNS, A time series of spiking rates for a neuron in a mouse thalamus. Raw spike data was obtained from , and processed with the authors' code in order to generate a spike rate time series.
25325,https://www.mad.tf.fau.de/research/activitynet/gaitphase-database/,the GaitPhase database," are marker positions and force recordings for a patient running on a treadmill, from ","
"
25329,http:/people.aifb.kit.edu/mfa/hybridcite2020/ACLMAG/acl_training_data.txt.gz,ACL-MAG training data,"
","
"
25329,http:/people.aifb.kit.edu/mfa/hybridcite2020/MAG/Training/mag_training_data.txt.gz,MAG training data,"
","
"
25329,http:/people.aifb.kit.edu/mfa/hybridcite2020/MAG/Training/mag_training_data_50citationsmin.txt.gz,MAG50 training data,"
","
"
25329,http:/people.aifb.kit.edu/mfa/hybridcite2020/MAG/Training/mag_training_data_cited_contexts.txt.gz,MAG-Cited training data,"
","
"
25332,https://github.com/TalwalkarLab/leaf/tree/master/data/shakespeare,instructions to prepare Shakespeare dataset,. Following the ,", we choose to use non-i.i.d., full-size dataset, and split 80% of the data points into the training dataset. Moreover, we set minimum number of samples per user at 9K. Thus, the following command returns our data partitioning:"
25365,https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet,GitHub link, (,). 
25380,examples/translation/makedataforbert.sh,makedataforbert.sh,Then you can use  , to get input file for BERT model (please note that the path is correct). You can get
25383,https://github.com/tensorflow/tensor2tensor/blob/e1f0e3a746bb322f4bf3975fad2c8105b3a43a49/tensor2tensor/data_generators/wikisum/wikisum.py#L335,_normalize_text, have been preprocessed with ,.
25383,https://github.com/tensorflow/tensor2tensor/blob/e1f0e3a746bb322f4bf3975fad2c8105b3a43a49/tensor2tensor/data_generators/wikisum/wikisum.py#L335,_normalize_text," tags, normalized with ",", and filtered with "
25383,https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/utils.py#L214,filter_paragraph,", and filtered with ",.
25393,http://people.sc.fsu.edu/~jburkardt/data/metis_graph/metis_graph.html,METIS graph format, This executable can be used to compute the minimum cut of a given graph with different algorithms. We use the , for all graphs. Run any minimum cut algorithm using the following command
25396,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,here,We support LIBSVM datasets which can be downloaded ,. The downloaded file should be unzipped and put in the following folder
25406,https://github.com/AlfredXiangWu/LightCNN#datasets,LightCNN,"We include an identity loss in our code, which is refered to ",". The pretrained LightCNN model can be downloaded from https://drive.google.com/file/d/1Jn6aXtQ84WY-7J3Tpr2_j6sX0ch9yucS/view. After downloading, save the model under the folder "
25406,https://github.com/AlfredXiangWu/LightCNN#datasets,LightCNN,. The identity loss module is brought from ,.
25411,#creating-the-dataset,Creating the dataset,"
","
"
25411,#reanalysis-with-cleaned-data,Reanalysis with cleaned data,"
","
"
25411,#creating-the-dataset-1,Creating the dataset,"
","
"
25411,#reanalysis-with-cleaned-data-1,Reanalysis with cleaned data,"
","
"
25412,#preprocessing-of-data,Preprocessing of data,"
","
"
25424,dataset/,dataset/,"
", - contains files associated with preparation and loading the dataset of convex quadrangles
25424,data_inv/,data_inv/,"
", - contains a dataset used in the experiments (convex quadrangle estimation only)
25443,https://github.com/NVlabs/ffhq-dataset,FFHQ,For the experiments presented in this repository we used the , dataset.
25447,http://academic.oup.com/mnras/article/472/4/4508/4104651/21SSD-a-public-data-base-of-simulated-21cm-signals,"MNRAS 472, 4508",", 2017, ",).
25456,http://cocodataset.org/#download,here, Download the COCO train2014 and val2014 data ,. Put the COCO train2014 images in the folder 
25467,data/conf.json.example,An example is provided,A configuration JSON file is needed to configure the scripts. ,. It should contains:
25468,#available-datasets,Available Datasets, - the directory with datasets. See , for summary.
25468,#available-datasets,Available Datasets, data! To use the clean dataset refer to ,. For the list of finally used predictors and the applied postprocessing refer to 
25479,http://www.grss-ieee.org/community/technical-committees/data-fusion,IEEE GRSS Data Fusion Contest 2020 (DFC2020),This repository provides a simple baseline for the current ," based on state-of-the-art CNNs for semantic segmentation, "
25479,#unlabeled-datasets,dataloader for unlabeled data,"In the meantime, please use our ", to infer labels for the contest.
25483,http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/,Aircraft,"
","
"
25483,http://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Oxford Flower 102,"
","
"
25483,https://www.vision.ee.ethz.ch/datasets_extra/food-101/,Food 101,"
","
"
25483,https://ai.stanford.edu/~jkrause/cars/car_dataset.html,Stanford Cars,"
","
"
25499,http://codh.rois.ac.jp/face/dataset/demo/learning-to-painting_drawings.zip,Learning to painting,"
","
"
25499,http://codh.rois.ac.jp/face/dataset/demo/intrinsic-style-transfer_drawings.zip,Intrinsic style transfer drawing,"
","
"
25509,https://datashare.is.ed.ac.uk/handle/10283/2791,here,"The experiments are conducted on a dataset from Valentini et. al.,  and are downloaded from ",. The following script can be used to download the dataset. 
25511,#data,Data,"
","
"
25513,examples/toy_dataset.py,toy data example,". To get started, check out the ",.
25515,https://hub.docker.com/r/sage2/jupyterlab-datascience-notebook,"
Docker Cloud Build Status
Docker Pulls
","
","
"
25524,timm/data/_info/,"
timm/data/_info
", to ,.
25524,https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055,Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide, on his blog yesterday. Well worth a read. ,"
"
25524,https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055,Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide,"
", by 
25528,https://github.com/jthsieh/DDPAE-video-prediction/blob/master/data/moving_mnist.py,link,Generator [,]
25532,https://archive.ics.uci.edu/ml/datasets/letter+recognition,letters dataset, provides an example for binary classification of the ,. Simply specify the data file location as well as Liblinear library location and run it.
25532,https://archive.ics.uci.edu/ml/datasets/MiniBooNE+particle+identification,MiniBooNE dataset, provides an example for binary classification of the , There are 3 inputs to run this program and several others to be supplied in a 
25538,https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel,"
torch.nn.parallel.DistributedDataParallel
", is deprecated. Use ,"
"
25539,https://data.lip6.fr/srvp/,Pretrained Models,"
","
"
25539,https://github.com/edenton/svg/blob/master/data/download_kth.sh,https://github.com/edenton/svg/blob/master/data/download_kth.sh,(see also , from the official implementation of 
25539,https://github.com/edenton/svg/blob/master/data/download_bair.sh,https://github.com/edenton/svg/blob/master/data/download_bair.sh,(see also , from the official implementation of 
25545,#datasets,Datasets,"
","
"
25545,#sequential-data-and-time-series,Sequential Data and Time Series,"
","
"
25545,https://storage.cloud.google.com/seldon-datasets/genome/readme.docx?organizationId=156002945562,README,". There are respectively 1, 7 and again 7 million sequences in the training, validation and test sets. For detailed info on the dataset check the ",.
25545,http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html,here, allows you to select a subset of network intrusions as targets or pick only specified features. The original data can be found ,.
25574,https://github.com/google-research/disentanglement_lib#downloading-the-data-sets,here,Follow the instructions ," to download the publicly available datasets, and store them inside "
25574,https://github.com/rr-learning/disentanglement_dataset,here, datasets can be obtained by following the instructions ,.
25593,https://github.com/youansheng/torchcv/tree/master/data,data,TorchCV has defined the dataset format of all the tasks which you could check in the subdirs of ,. Following is an example dataset directory trees for training semantic segmentation. You could preprocess the open datasets with the scripts in folder 
25593,https://github.com/youansheng/torchcv/tree/master/data/seg/preprocess,data/seg/preprocess,. Following is an example dataset directory trees for training semantic segmentation. You could preprocess the open datasets with the scripts in folder ,"
"
25595,http://robotcar-dataset.robots.ox.ac.uk,Oxford Robotcar Dataset,This repo contains sample MATLAB and Python code for viewing and manipulating data from the , and 
25595,http://ori.ox.ac.uk/datasets/radar-robotcar-dataset,Oxford Radar Robotcar Dataset, and ,.
25595,http://robotcar-dataset.robots.ox.ac.uk,website,"To obtain the data, please visit the dataset ",. Downloads are chunked into 
25595,http://ori.ox.ac.uk/datasets/radar-robotcar-dataset,website,"To obtain the data, please visit the dataset ",". Downloads are separated into individual zip files for each sensor, for each traversal."
25595,https://github.com/dbarnes/radar-robotcar-dataset-sdk,radar-robotcar-dataset-sdk,An example script for scraping the dataset website can be found at ,.
25596,https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/,https://lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/,Dataset: ,"
"
25596,lcas_simple_data.zip,lcas_simple_data.zip,"
", contains 172 consecutive frames (in .pcd file) with 2 fully annotated pedestrians.
25605,https://github.com/SsnL/dataset-distillation,Dataset Distillation,"
","
"
25630,https://www.cityscapes-dataset.com/,Cityscapes,Get dataset from ,", and from "
25630,http://www.6d-vision.com/lostandfounddataset,Lost and Found,", and from ",.
25639,https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local,Google Local ratings data, : Custom relevance function on ,.
25639,https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local,Google Local ratings data, : Relevance scores from latent embedding based factorization on ,.
25639,https://grouplens.org/datasets/hetrec-2011/,Last.fm data, : Relevance scores from latent embedding based factorization on ,.
25648,http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,Blitzer Dataset,Results on ,"
"
25648,https://www.yelp.com/dataset/challenge,Yelp,Results on very divergent datasets such as , and 
25648,https://ai.stanford.edu/~amaas/data/sentiment/,IMDb, and ,", in addition to Amazon Reviews. (3 randomly selected combinations)."
25648,https://github.com/jitinkrishnan/Diversity-Based-Generalization/tree/master/raw_data,raw_data,"
","
"
25648,https://github.com/jitinkrishnan/Diversity-Based-Generalization/tree/master/raw_data,raw_data,"
","
"
25652,http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/,Anime Face, and , datasets
25652,https://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html,Oxford Flower,Comparison of fine-tuning (left) and freeze D (right) under ,", "
25655,https://github.com/ai-se/tech-debt/tree/master/data,Original,"
"," from Maldonado and Shihab ""Detecting and quantifying different types of self-admitted  technical  debt,"" in 2015 IEEE 7th InternationalWorkshop on Managing Technical Debt (MTD). IEEE, 2015, pp. 9–15."
25655,https://github.com/ai-se/tech-debt/tree/master/new_data/corrected,Corrected,"
",": 439 labels checked, 431 labels corrected."
25655,https://github.com/ai-se/tech-debt/tree/master/new_data/conflicts,conflicts,"Find conflicting labels (GT=no AND Easy=yes), save as csv files under the ", directory:
25655,https://github.com/ai-se/tech-debt/tree/master/new_data/validate,validate,"Validate the conflicting labels manually, results are under the ", directory.
25655,https://github.com/ai-se/tech-debt/tree/master/new_data/corrected,corrected,"Correct ground truth labels with the validation results, new data saved under ", directory:
25655,https://github.com/ai-se/tech-debt/tree/master/new_data/rest,rest,", also output the data with the ""easy to find"" SATDs removed to the ", directory:
25658,https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html,Google speech commands data set,Convolutional neural networks for , with 
25661,https://cdn.commonvoice.mozilla.org/cv-corpus-5.1-2020-06-22/zh-HK.tar.gz,Common Voice ZH (TW),"
","
"
25661,https://cdn.commonvoice.mozilla.org/cv-corpus-5.1-2020-06-22/pt.tar.gz,Common Voice PT,"
","
"
25661,https://cdn.commonvoice.mozilla.org/cv-corpus-5.1-2020-06-22/es.tar.gz,Common Voice ES,"
","
"
25674,#classes-and-data-types,Classes and Data Types,"
","
"
25683,./docs/data_types_order_transfer.md,"Data Types, Order and Transfer of Parameters","
","
"
25695,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/satimage.scale,satimage,"Before executing the program, download datasets, ",", "
25695,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/vehicle.scale,vehicle,", ",", and "
25695,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits,pendigits,", and ",.
25699,data/,"
data
","
",: Dir with the trees.
25704,http://mscoco.org/dataset/#download,download,Visit MS COCO , page for more details.
25704,http://mscoco.org/dataset/#format,format,Visit MS COCO , page for more details.
25720,./util/data/make_data.py,"
util/data/make_data.py
","If one needs to run experiments on other datasets, please refer to ", to build TFRecords.
25727,#querying-the-data-schema-of-models,"Querying the ""data schema"" of models","
","
"
25730,https://github.com/kimiyoung/planetoid/tree/master/data,"Cora, Citeseer, Pubmed",We used three public data sets in our experiments: ,.
25731,https://github.com/fusion-jena/QuestionsMetadataBiodiv/tree/master/questions,Questions,"
","
"
25731,https://github.com/fusion-jena/QuestionsMetadataBiodiv/tree/master/data_repositories,Data_Repositories,"
","
"
25733,https://paperswithcode.com/sota/image-classification-on-cifar-10?p=understanding-and-enhancing-mixed-sample-data,"
PWC
","
","
"
25733,https://paperswithcode.com/sota/image-classification-on-fashion-mnist?p=understanding-and-enhancing-mixed-sample-data,"
PWC
","
","
"
25736,https://github.com/jracp/NCBIdataPrep,code,", and datasets can be cleaned by this ",.
25739,#about-dataset,About Dataset,"
","
"
25739,#dataset--preparation,Dataset Preparation,"
","
"
25739,http://cs.uky.edu/~jacobs/datasets/cvusa/,CVUSA,Or download , / 
25741,#perform-training-on-coco-dataset,Perform Training on COCO Dataset,"
","
"
25741,#add-your-customized-dataset,Add your Customized Dataset,"
","
"
25741,#perform-training-on-coco-dataset,Perform Training on COCO Dataset,"
","
"
25741,#add-your-customized-dataset,Add your Customized Dataset,"
","
"
25741,#perform-training-on-coco-dataset,Perform Training on COCO Dataset,"
","
"
25741,maskrcnn_benchmark/data/datasets/__init__.py,"
vc_rcnn/data/datasets/__init__.py
","
",: add it to 
25742,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,Download training dataset ,.
25745,http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip,download link,"
", [82G]
25752,#datasets,Datasets, // , // 
25752,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI website,The KITTI (raw) dataset used in our experiments can be downloaded from the ,". For convenience, we provide the standard splits used for training and evaluation: "
25752,https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar.gz,here,". The full KITTI_raw dataset, as used in our experiments, can be directly downloaded ", or with the following command:
25752,https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/DDAD_tiny.tar,DDAD,"For simple tests, we also provide a ""tiny"" version of ", and 
25752,https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_tiny.tar,KITTI, and ,:
25757,http://www.cvlibs.net/datasets/kitti/raw_data.php,here,We trained and tested on the KITTI dataset. Download the raw dataset ,". We provide a dataloader, but we first require that the data be preprocessed. To do so, run "
25772,https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip,this file,"Or, more precisely, ",.
25772,https://youtube-vos.org/dataset/,YouTubeVOS 2018,"To test our validation split and the YouTubeVOS challenge 'valid' split, download ", and place it in this directory structure:
25779,https://datashare.is.ed.ac.uk/handle/10283/3257,here, The listening test results can be downloaded from ,"
"
25779,https://datashare.is.ed.ac.uk/handle/10283/3061,here, The databases and results (submitted speech) can be downloaded from ,"
"
25783,https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_Veracity_Classification/6392078,PHEME corpus,set symlink for social context directory (organised in , structure) in 
25806,https://github.com/red-data-tools/GR.rb,Ruby package GR,"
","
"
25806,https://github.com/red-data-tools/GR.rb/issues/new,Ruby,", ",).
25817,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json,Introduction to Tweet JSON,Format: JSON Keys: see ,"
"
25818,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data,data,"All data, including Sequential Data, Industry Relation, and Wiki Relation, are under the ", folder.
25818,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data/google_finance,google_finance,Raw data: files under the ," folder are the historical (30 years) End-of-day data (i.e., open, high, low, close prices and trading volume) of more than 8,000 stocks traded in US stock market collected from Google Finance."
25818,https://github.com/hennande/Temporal_Relational_Stock_Ranking/tree/master/data/2013-01-01,2013-01-01,Processed data: , is the dataset used to conducted experiments in our paper.
25821,https://vision.cs.ubc.ca/datasets/fashion/,from,. Follow the instruction on dataset downloading ,.
25821,data/taichi-loading/README.md,data/taichi-loading,. Follow the instructions in , or instructions from https://github.com/AliaksandrSiarohin/video-preprocessing.
25826,https://www.nlm.nih.gov/databases/umls.html,Access to NLM UMLS Metathesaurus/Ontology,"
","
"
25827,https://vincentarelbundock.github.io/Rdatasets/doc/survival/flchain.html,Flchain,"
","
"
25827,https://sleepdata.org/datasets/shhs,SLEEP,"
",": A subset of the Sleep Heart Health Study (SHHS), a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing."
25827,./data,"
data
","). In addition, the ", directory contains downloaded 
25827,https://vincentarelbundock.github.io/Rdatasets/doc/survival/flchain.html,Flchain, directory contains downloaded , and 
25843,https://github.com/soumith/imagenet-multiGPU.torch#data-processing,here,": To run it on ImageNet, you need to follow the guidelines ",. You should copy the ImageNet images into the 
25843,https://github.com/fMoW/dataset,here,: You can find the instructions to download images ,". After downloading the images, you need to crop the images from the large satellite images based on the bounding boxes provided in the '.json' files. The original fMoW paper adaptively determines the context and add it to the bounding box to find the final area of interest. We follow their strategy to preprocess the images. After preprocessing images, you need to create a "
25849,http://semantic-kitti.org/dataset.html#download,here,SemanticKITTI dataset can be found ,. Download the files related to semantic segmentation and extract everything into the same folder.
25862,"https://twitter.com/intent/tweet?text=Build%20your%20robust%20machine%20learning%20models%20with%20DeepRobust%20in%2060%20seconds&url=https://github.com/DSE-MSU/DeepRobust&via=dse_msu&hashtags=MachineLearning,DeepLearning,secruity,data,developers","
Tweet
","
","
"
25862,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/#supported-datasets,datasets,[12/2020] [Graph Package] Add four more , and one defense algorithm. More details can be found 
25874,#data,Data,"
","
"
25874,#datasets,Datasets,"
","
"
25890,https://github.com/yinleon/links-as-data,Links as Data,"
","
"
25890,https://github.com/yinleon/links-as-data/blob/master/nbs/congress-links.ipynb,GitHub," How to extract links from congressional Tweets, preprocess them, and use them as features to predict poltical affiliation. View the Notebook on ", | 
25890,http://bit.ly/links-as-data,NbViewer, | , | 
25890,http://bit.ly/links-as-data-slides,Slides, | ,| 
25890,https://mybinder.org/v2/gh/yinleon/links-as-data/master?filepath=nbs%2Fcongress-links.ipynb,Binder,| ,"
"
25891,https://pypi.org/project/youtube-data-api/,install this module using pip,We recommend you ,:
25892,./data/script_download_molecules.sh,ZINC-full,Added , dataset (249K molecular graphs) with 
25892,./docs/02_download_datasets.md,Proceed as follows,"
", to download the benchmark datasets.
25892,./docs/04_add_dataset.md,Instructions,"
", to add a dataset to the benchmark.
25893,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,TU,COLORS and TRIANGLES datasets are now also available in the ," format, so that you can use a general TU datareader. See PyTorch Geometric examples for "
25893,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,"COLLAB, PROTEINS and D&D",We validate our weakly-supervised approach on three common graph classification benchmarks: ,.
25895,http://data.csail.mit.edu/places/places365/filelist_places365-standard.tar,Places365-Standard,First download the image list and annotations for , and the image list and annotations for 
25895,http://data.csail.mit.edu/places/places365/filelist_places365-challenge.tar,Place365-Challenge, and the image list and annotations for ,", and decompress the files in the data folder. This file only contains image list, without actual images."
25898,https://github.com/Jeffrey-Ede/datasets/wiki,here,"There are three main datasets containing 19769 experimental STEM images, 17266 experimental TEM images and 98340 simulated TEM exit wavefunctions. Datasets are available ",.
25898,https://github.com/Jeffrey-Ede/datasets/wiki,main page,"SAVE_DATA: Full save location of a NumPy file containing a dataset. For example, from the datasets ",". SAVE_FILE: Full save location of a NumPy file containing tSNE map points. Files for each visualization are in this repository and have filenames in the form ""tsne_*.npy"" for PCA and "
25908,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
25916,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,"CoNLL 2003 POS, Chunking and NER datasets","
",.
25923,https://github.com/MohitLamba94/Iterative-Pooling/blob/master/make_dataset.m,MATLAB file,Our code does not uses the validation dataset during the training phase and so the validation dataset can also be used for evaluation. We additionally provide a , which illustrates how this dataste was created for image resolution of 1024x1024 and also describes how to add rotation to create 
25949,https://www.cs.ubc.ca/~kmyi/imw2020/data.html,Data download link,"
","
"
25950,https://www.cs.ubc.ca/~kmyi/imw2020/data.html,here,Data can be downloaded ,: you may want to download the images for validation and testing. Most of the scripts assume that the images are in 
25952,http://www.vovakim.com/projects/CorrsTmplt/doc_data.html,bhcp, and ," respectively, and you can download the preprocessed training data "
25969,gamma-dataset,toy dataset,The respective datasets are: ,",  "
25969,/theta-dataset,theta-dataset,"Here, an additional visual examples on another dataset with GT (we call ",) for our 3 methods 
25973,https://www.dropbox.com/s/fjmncwhsnfgkmzk/data.tar.gz?dl=0,here,Download and extract our certification logs from ,. You can instead simply run the following from within the root directory of this repository
25978,https://data.vision.ee.ethz.ch/reyang/HLVC_model.zip,Download link,Pre-trained models (,)
25980,latentspace/datasets.py,latentspace/datasets.py,"To work with a custom dataset, you need to implement a new dataset wrapper in ",. Add support to this dataset in 
25981,http://www.robotmultimodal.com/datasets/,http://www.robotmultimodal.com/datasets/,"This repository will host the machine learning models to benchmark the iCub multisensor datasets. To follow up-to-date information, visit ",.
25982,data/labels%20table.txt,labels table.txt,The complete list of segmented structures is available in , along with their corresponding values. This table also details the order in which the posteriors maps are sorted.
25982,data/training_label_maps,data,"This repository contains all the code and data necessary to train, validate, and test your own network. Importantly, the proposed method only requires a set of anatomical segmentations to be trained (no images), which we include in ",". While the provided functions are thoroughly documented, we highly recommend to start with the following tutorials:"
25982,data,data,"
",: this folder contains some examples of brain label maps if you wish to train your own SynthSeg model.
25989,https://github.com/1024er/cbert_aug/tree/crayon/datasets/stsa.binary,https://github.com/1024er/cbert_aug/tree/crayon/datasets/stsa.binary,STSA-2 : ,"
"
25989,https://github.com/1024er/cbert_aug/tree/crayon/datasets/TREC,https://github.com/1024er/cbert_aug/tree/crayon/datasets/TREC,TREC : ,"
"
25989,https://github.com/MiuLab/SlotGated-SLU/tree/master/data/snips,https://github.com/MiuLab/SlotGated-SLU/tree/master/data/snips,SNIPS : ,"
"
26013,data,data, to reproduce the training step. I have also included pre-trained model weights and verification results in , so this step can be skipped.
26018,lolip/dataset/__init__.py,lolip/dataset/__init__.py," with torchvision ImageFolder readable format. For more detail, please refer to ",.
26018,notebooks/dataset_dist.ipynb,notebooks/dataset_dist.ipynb,The code for getting train-train separation and test-train separation ,"
"
26018,notebooks/dataset_dist.ipynb,notebooks/dataset_dist.ipynb,Table1 and Figures in Appendix D: ,"
"
26020,http://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/?_r=0,on the New York Times Open blog,"We use a conditional random field model (CRF) to extract tags from labelled training data, which was tagged by human news assistants. We wrote about our approach ",. More information about CRFs can be found 
26040,https://github.com/northeastern-datalab/factorized-graphs/blob/master/reproducibility.md,/reproducibility.md,"
", contains a detailed description to reproduce the experimental results reported in the paper (as submitted to the 
26056,https://multiqa.s3.amazonaws.com/data/SQuAD1-1_train.jsonl.gz,train,| Dataset | MultiQA format |  SQuAD2.0 format (GZipped) | | :----- | :-----:|  :------------------: | | SQuAD-1.1 | ," , "
26056,https://multiqa.s3.amazonaws.com/data/SQuAD1-1_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SQuAD1-1_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SQuAD1-1_train.json.gz,dev," , ", | SQuAD-2.0 | 
26056,https://multiqa.s3.amazonaws.com/data/SQuAD2-0_train.jsonl.gz,train, | SQuAD-2.0 | ," , "
26056,https://multiqa.s3.amazonaws.com/data/SQuAD2-0_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SQuAD2-0_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SQuAD2-0_train.json.gz,dev," , ", | NewsQA | 
26056,https://multiqa.s3.amazonaws.com/data/NewsQA_train.jsonl.gz,train, | NewsQA | ," , "
26056,https://multiqa.s3.amazonaws.com/data/NewsQA_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/NewsQA_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/NewsQA_dev.json.gz,dev," , ", | HotpotQA | 
26056,https://multiqa.s3.amazonaws.com/data/HotpotQA_train.jsonl.gz,train, | HotpotQA | ," , "
26056,https://multiqa.s3.amazonaws.com/data/HotpotQA_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/HotpotQA_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/HotpotQA_dev.json.gz,dev," , ", | TriviaQA-unfiltered |  
26056,https://multiqa.s3.amazonaws.com/data/TriviaQA_unfiltered_dev.jsonl.gz,dev, | TriviaQA-unfiltered |  , |  
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/TriviaQA_unfiltered_dev.json.gz,dev, |  , | TriviaQA-wiki | 
26056,https://multiqa.s3.amazonaws.com/data/TriviaQA_wiki_train.jsonl.gz,train, | TriviaQA-wiki | ," , "
26056,https://multiqa.s3.amazonaws.com/data/TriviaQA_wiki_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/TriviaQA_wiki_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/TriviaQA_wiki_dev.json.gz,dev," , ", | SearchQA | 
26056,https://multiqa.s3.amazonaws.com/data/SearchQA_train.jsonl.gz,train, | SearchQA | ," , "
26056,https://multiqa.s3.amazonaws.com/data/SearchQA_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SearchQA_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/SearchQA_dev.json.gz,dev," , ", | BoolQ | 
26056,https://multiqa.s3.amazonaws.com/data/BoolQ_train.jsonl.gz,train, | BoolQ | ," , "
26056,https://multiqa.s3.amazonaws.com/data/BoolQ_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/BoolQ_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/BoolQ_dev.json.gz,dev," , ", | ComplexWebQuestions | 
26056,https://multiqa.s3.amazonaws.com/data/ComplexWebQuestions_train.jsonl.gz,train, | ComplexWebQuestions | ," , "
26056,https://multiqa.s3.amazonaws.com/data/ComplexWebQuestions_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComplexWebQuestions_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComplexWebQuestions_dev.json.gz,dev," , ", | | DROP | 
26056,https://multiqa.s3.amazonaws.com/data/DROP_train.jsonl.gz,train, | | DROP | ," , "
26056,https://multiqa.s3.amazonaws.com/data/DROP_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DROP_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DROP_dev.json.gz,dev," , ", | WikiHop | 
26056,https://multiqa.s3.amazonaws.com/data/WikiHop_train.jsonl.gz,train, | WikiHop | ," , "
26056,https://multiqa.s3.amazonaws.com/data/WikiHop_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/WikiHop_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/WikiHop_dev.json.gz,dev," , ", | DuoRC Paraphrase| 
26056,https://multiqa.s3.amazonaws.com/data/DuoRC_Paraphrase_train.jsonl.gz,train, | DuoRC Paraphrase| ," , "
26056,https://multiqa.s3.amazonaws.com/data/DuoRC_Paraphrase_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DuoRC_Paraphrase_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DuoRC_Paraphrase_dev.json.gz,dev," , ", | DuoRC Self| 
26056,https://multiqa.s3.amazonaws.com/data/DuoRC_Self_train.jsonl.gz,train, | DuoRC Self| ," , "
26056,https://multiqa.s3.amazonaws.com/data/DuoRC_Self_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DuoRC_Self_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/DuoRC_Self_dev.json.gz,dev," , ", | ComplexQuestions | 
26056,https://multiqa.s3.amazonaws.com/data/ComplexQuestions_train.jsonl.gz,train, | ComplexQuestions | ," , "
26056,https://multiqa.s3.amazonaws.com/data/ComplexQuestions_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComplexQuestions_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComplexQuestions_dev.json.gz,dev," , ", | ComQA | 
26056,https://multiqa.s3.amazonaws.com/data/ComQA_train.jsonl.gz,train, | ComQA | ," , "
26056,https://multiqa.s3.amazonaws.com/data/ComQA_dev.jsonl.gz,dev," , ", | 
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComQA_train.json.gz,train, | ," , "
26056,https://multiqa.s3.amazonaws.com/squad2-0_format_data/ComQA_dev.json.gz,dev," , ","
"
26056,https://github.com/alontalmor/multiqa/blob/master/datasets/README.md,Readme,see , in the datasets folder. A 
26056,https://github.com/alontalmor/multiqa/blob/master/models/datasets/multiqa_train.jsonschema.json,here, for a single context in multiqa is available ,.
26061,https://github.com/GuillaumeGomez/mp3-metadata/pull/9,Multiple panics, mp3-metadata | , | afl | 
26069,https://csegroups.case.edu/bearingdatacenter/pages/download-data-file/,CWRU Bearing Dataset,"
","
"
26069,https://mfpt.org/fault-data-sets/,MFPT Bearing Dataset,"
","
"
26069,https://mb.uni-paderborn.de/kat/forschung/datacenter/bearing-datacenter/,PU Bearing Dataset,"
","
"
26069,http://biaowang.tech/xjtu-sy-bearing-datasets/,XJTU-SY Bearing Dataset,"
","
"
26069,https://github.com/cathysiyu/Mechanical-datasets,SEU Gearbox Dataset,"
","
"
26069,https://github.com/ZhaoZhibin/DL-based-Intelligent-Diagnosis-Benchmark/tree/master/datasets,datasets,"
", contains the data augmentation methods and the Pytorch datasets for 1D and 2D signals.
26098,data,data/,"
", contains raw and processed datasets that we include in this repository for testing.
26101,https://github.com/cocodataset/cocoapi,pycocotools,"
","
"
26101,https://github.com/kuanghuei/SCAN/blob/master/README.md#data-pre-processing-optional,here, to obtain image features for fair comparison. More details about data pre-processing (optional) can be found ,". All the data needed for reproducing the experiments in the paper, including image features and vocabularies, can be downloaded from "
26106,https://github.com/pydata/pandas,Pandas,"
", >= 1.3.0
26106,https://github.com/pydata/pandas-datareader,pandas_datareader,"
", >= 0.4.0
26107,https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#,UCI electricity dataset,"This repository contains two sets of experiments, one for the ",", and one on historical S&P500 stock prices."
26115,demo_data,"
demo_data
",. The outputs for example videos saved in the folder , are visualized in this notebook.
26115,train_data,"
train_data
",All data required for training is contained in the , folder:
26115,train_data/ATL,"
train_data/ATL
","The major video database consists of center-field and side-view videos, one for each play (6 seconds length). The videos are sorted by data, and are stored in ",.
26115,train_data/batter_runs/videos,"
train_data/batter_runs/videos
","For the batter's first step, a separate folder of videos was created that only contains videos (and the belonging joint trajectories) in which the batter starts to run. This folder is ","
"
26115,train_data/high_quality_videos,"
train_data/high_quality_videos
","For bat detection, high quality videos from YouTube are used. They are stored in ",.
26115,train_data/short_ball_speed_videos,"
train_data/short_ball_speed_videos
","For ball speed I use shorter videos from a public database, that show only the ball trajectory (<1 second). There are two side-view and one center-field camera for each play (folders of 6-digit-play id). They are stored in ",.
26115,train_data/batter_hq_joints,"
train_data/batter_hq_joints
",Joint trajectories for high quality videos can be found in the folder ,.
26115,train_data/speed_labels_sv.json,"
train_data/speed_labels_sv.json
","In order to estimate the ball release frame, the ball speed is estimated as well. To compare it to ground truth speed from Statcast, these Statcast values are saved in a separate json file (containing the speed for each side-view video): ","
"
26115,train_data/batter_runs/labels_first_batter_train,"
train_data/batter_runs/labels_first_batter_train
",](train_data/batter_runs /labels_first_batter_test) and ,.
26115,train_data/batter_first_data,"
train_data/batter_first_data
","When a model is trained to find the batter's first step, the training data can be saved and visualized. An example is saved in ", and 
26115,train_data/batter_first_label,"
train_data/batter_first_label
", and ,"
"
26117,https://github.com/gpapamak/maf#how-to-get-the-datasets,here,Datasets and preprocessing code are forked from the MAF authors' implementation ,. The unzipped datasets should be symlinked into the 
26124,http://cocodataset.org/#keypoints-leaderboard,"
winner
","
","
"
26124,http://cocodataset.org/#keypoints-leaderboard,COCO leaderboard,which won 2019 COCO Keypoint Challenge and ranks 1st place on both COCO test-dev and test-challenge datasets as shown in ,"
"
26124,https://github.com/cocodataset/cocoapi,cocoapi website,Install COCOAPI referring to ,", or:"
26124,http://cocodataset.org/#download,COCO website,Download images from ,", and put train2014/val2014 splits into "
26135,https://github.com/ElementAI/TADAM/tree/master/datasets,pre-processing, (,)
26143,http://picdataset.com/challenge/leaderboard/hoi2019,"
Human-Object Interaction in the Wild (HOIW)
"," place in ICCV-2019 Person in Context Challenge (PIC19 Challenge), on both ", and 
26143,http://picdataset.com/challenge/leaderboard/pic2019,"
Person in Context (PIC)
", and , tracks.
26143,http://picdataset.com:8000/challenge/task/download/,http://picdataset.com:8000/challenge/task/download/,Please find the dataset from the PIC challenge website: ,"
"
26156,http://cocodataset.org/,MS-COCO, folder which are taken from ,.
26159,https://github.com/facebookresearch/habitat-sim#datasets,habitat-sim,"We use the data sources linked from the public habitat-api repository. You will need to individually download MP3D, and Gibson from their sources. ", and 
26159,https://github.com/facebookresearch/habitat-api#data,habitat-api, and ," share the links to the files. We additionally use the Point-Nav datasets from habitat-api, but we also provide a script for generating new datasets."
26180,https://mcl.korea.ac.kr/research/Submitted/jtlee_slnet/ICCV2017_JTLEE_dataset.7z,here,Download original SEL dataset from , and extract to 
26189,https://github.com/CenekAlbl/drone-tracking-datasets,Here,"
", you can find the dataset we used for our experiments.
26191,https://snap.stanford.edu/data/#disjointgraphs,SNAP,The newly introduced graph classification datasets are available at ,", "
26191,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,TUD Graph Kernel Datasets,", ",", and "
26191,https://chrsmrrs.github.io/datasets/,GraphLearning.io,", and ",.
26192,https://github.com/benedekrozemberczki/datasets/archive/master.zip,"
repo size
","
","
"
26192,https://graphmining.ai/datasets/twitch_gamers.zip,Twitch Gamers,"
","
"
26192,https://graphmining.ai/datasets/lasftm_asia.zip,LastFM Asia Social Network,"
","
"
26192,https://snap.stanford.edu/data/feather-deezer-social.html,Deezer Europe Social Network,"
","
"
26192,https://graphmining.ai/datasets/graph_classification/git_stargazers.zip,GitHub StarGazer Graphs + Target,"
","
"
26192,https://graphmining.ai/datasets/graph_classification/twitch_egos_1.zip,Twitch Ego Nets Part I,"
","
"
26192,https://graphmining.ai/datasets/graph_classification/twitch_egos_2.zip,Twitch Ego Nets Part II + Target,"
","
"
26192,https://graphmining.ai/datasets/graph_classification/reddit_threads.zip,Reddit Thread Graphs + Target,"
","
"
26192,https://graphmining.ai/datasets/graph_classification/deezer_egos.zip,Deezer Ego Nets + Target,"
","
"
26192,https://snap.stanford.edu/data/github-social.html,GitHub Web-ML,"
","
"
26192,https://graphmining.ai/datasets/deezer/RO.zip,Romania,"
","
"
26192,https://graphmining.ai/datasets/deezer/HR.zip,Croatia,"
","
"
26192,https://graphmining.ai/datasets/deezer/HU.zip,Hungary,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/politician_edges.zip,Politicians,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/company_edges.zip,Companies,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/athletes_edges.zip,Athletes,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/new_sites_edges.zip,Media,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/public_figure_edges.zip,Public Figures,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/artist_edges.zip,Artists,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/government_edges.zip,Government,"
","
"
26192,https://graphmining.ai/datasets/facebook_page_page/tvshow_edges.zip,TV Shows,"
","
"
26192,https://graphmining.ai/datasets/wikipedia/chameleon.zip,Wikipedia Chameleons,"
","
"
26192,https://graphmining.ai/datasets/wikipedia/crocodile.zip,Wikipedia Crocodiles,"
","
"
26192,https://graphmining.ai/datasets/wikipedia/squirrel.zip,Wikipedia Squirrels,"
","
"
26192,https://graphmining.ai/datasets/twitch/DE.zip,Germany,"
","
"
26192,https://graphmining.ai/datasets/twitch/ENGB.zip,England,"
","
"
26192,https://graphmining.ai/datasets/twitch/ES.zip,Spain,"
","
"
26192,https://graphmining.ai/datasets/twitch/FR.zip,France,"
","
"
26192,https://graphmining.ai/datasets/twitch/PTBR.zip,Porutgal,"
","
"
26192,https://graphmining.ai/datasets/twitch/RU.zip,Russia,"
","
"
26192,https://graphmining.ai/datasets/twitch/TW.zip,Taiwan,"
","
"
26192,https://graphmining.ai/datasets/facebook_large.zip,Facebook Large Page-Page,"
","
"
26214,https://figshare.com/articles/dataset/OpenEA_dataset_v1_1/19258760/3,v2.0 dataset, issue. It is strongly recommended to use the ," for evaluating attribute-based entity alignment methods, such that the results can better reflect the robustness of these methods in real-world situation."
26214,#kg-sampling-method-and-datasets,KG Sampling Method and Datasets,"
","
"
26214,#dataset-overview,Dataset Overview,"
","
"
26214,#dataset-description,Dataset Description,"
","
"
26214,https://figshare.com/articles/dataset/OpenEA_dataset_v1_1/19258760/2,figshare,The v1.1 datasets used in this paper can be downloaded from ,", "
26214,https://www.dropbox.com/s/nzjxbam47f9yk3d/OpenEA_dataset_v1.1.zip?dl=0,Dropbox,", ", or 
26214,https://figshare.com/articles/dataset/OpenEA_dataset_v1_1/19258760/3,figshare,) The v2.0 datasets can be downloaded from ,", "
26214,https://www.dropbox.com/s/xfehqm4pcd9yw0v/OpenEA_dataset_v2.0.zip?dl=0,Dropbox,", ", or 
26217,https://www.cityscapes-dataset.com/anonymous-results/?id=9a8b7333dcb66360b4f38ba00db7c84e7997f7203084bf6e92ca9bbabbc34640,Benchmark,| Models | Data |  Crop Size | Batch Size | Output Stride | mIoU | External Link | |:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:| | HANet (ResNext-101) | Fine train/val + Coarse | 864X864 | 12 | 8 | 83.2% | , | | HANet (ResNet-101) | Fine train/val |  864X864 | 12 | 8 | 82.1% | 
26217,https://www.cityscapes-dataset.com/anonymous-results/?id=f96818d678c67c82449323203d144e530fb66102a5b5a101f599a96cc62458e7,Benchmark, | | HANet (ResNet-101) | Fine train/val |  864X864 | 12 | 8 | 82.1% | , | | HANet (ResNet-101) | Fine train | 768X768 | 8 | 8 | 80.9% | 
26217,https://www.cityscapes-dataset.com/anonymous-results/?id=1e5e85818e439332fdae01037259706d9091be2b9fca850eb4a851805f5ed44d,Benchmark, | | HANet (ResNet-101) | Fine train | 768X768 | 8 | 8 | 80.9% | , |
26217,https://www.cityscapes-dataset.com/,Cityscapes,We evaludated HANet on , and 
26219,https://github.com/LauraRuis/multimodal_seq2seq_gSCAN#demo-training-a-model-on-a-dummy-dataset,here,See an demonstration of training a multi-modal neural model on this demo dataset ,.
26220,https://github.com/LauraRuis/multimodal_seq2seq_gSCAN#demo-training-a-model-on-a-dummy-dataset,the last section of this readme,". For a demo on training a small model on a dummy dataset, refer to ",.
26235,https://github.com/samuelbroscheit/wikiextractor-wikimentions/blob/master/Load%20wikiextractor%20data.ipynb,this notebook,then each articles dictionary contains an additional field 'internal_links'. Please see , for a HOWTO and code snippet for reading the data.
26270,https://neo4j.com/graph-data-science-library/,Data Science ecosystem, and its ,.
26270,https://www.dropbox.com/s/38t2e11n4w6xv6w/data.zip?dl=1,here,All experiments reported in the paper is carried out on AWS EC2 r5.2xlarge instances. Please download experiment datasets ,", and unzip into "
26281,https://doi.org/10.3389/fdata.2019.00008,doi: 10.3389/fdata.2019.00008,", IAAA ICWSM International Workshop on Modeling and Mining Socia-Media Driven Complex Networks (Soc2Net), Munich, DE, Frontiers in Big Data 2:8, 2019. ", - 
26283,./demo_data,demo_data, demo datasets (included in ,) through: Admin Panel -> Add dataset. You should then be able to follow the introduction to the app (available from the landing page).
26283,./demo_data,demo_data, demo datasets (included in ,) through: Admin Panel -> Add dataset. You should then be able to follow the introduction to the app (available from the landing page).
26283,app/utils/dataset_schema.json,utils/dataset_schema.json,All datasets must adhere to a specific dataset schema (see ,"). See the files in [demo_data] for examples, as well as those in "
26285,tree/master/utils/plot_dataset.py,utils/plot_dataset.py, is also provided in ,.
26287,https://github.com/alan-turing-institute/TCPD/blob/master/datasets/nile/nile.json#L8,nile dataset," field to mark the indices of each data point. At the moment, these indices need to be consecutive integers. This entry mainly exist for a future scenario where we may want to consider non-consecutive timesteps. If the time axis can be mapped to a date or time, then a type and format of this field can be specified (see e.g. the ",", which has year labels)."
26287,https://github.com/alan-turing-institute/TCPD/blob/master/datasets/uk_coal_employ/uk_coal_employ.json#L236,uk_coal_employ, (see e.g. , for an example).
26287,https://github.com/alan-turing-institute/TCPD/blob/master/datasets/brent_spot/brent_spot.json#L511,brent_spot dataset," object if possible (see, e.g., the ","). If this is not available, the time series name should be added to the "
26293,https://docs.python.org/3/tutorial/datastructures.html#dictionaries,plain python dictionary, for ,"
"
26298,http://vipl.ict.ac.cn/view_database.php?id=14,LRW-1000 dataset,"
"," had cropped the mouth ROI, we directly sent them to the model."
26304,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/mancini_piano.tar.gz,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/mancini_piano.tar.gz,Download and make 'PIANO' dataset (,)
26304,https://magenta.tensorflow.org/datasets/maestro,https://magenta.tensorflow.org/datasets/maestro,Download 'MAESTRO' dataset (,)
26304,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/mancini_piano.tar.gz,Bach piano performances,"
","
"
26304,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/sc09.tar.gz,Speech Commands Zero through Nine (SC09),"
","
"
26304,http://deepyeti.ucsd.edu/cdonahue/wavegan/data/drums.tar.gz,Drum sound effects,"
","
"
26304,https://magenta.tensorflow.org/datasets/nsynth,Nsynth dataset,"
","
"
26310,https://www.dropbox.com/s/8n02xqv3l9q18r1/datasets.zip?dl=0,ETH-UCY Dataset,"
", Provided by 
26310,https://github.com/agrimgupta92/sgan/blob/master/scripts/download_data.sh,SGAN, Provided by ,"
"
26310,https://github.com/vineetsk1/cs231a-project/tree/master/data/challenges/3,Processed,", ","
"
26311,https://pytorch.org/docs/0.3.1/data.html?highlight=dataset#torch.utils.data.DataLoader,"
""num_workers""
","To transform an image with size (H:64, W:200), it takes less than 3ms using a 2.0GHz CPU. It is possible to accelerate the process by calling multi-process batch samplers in an on-the-fly manner, such as setting ", in 
26311,https://pytorch.org/docs/0.3.1/data.html?highlight=dataset#torch.utils.data.DataLoader,PyTorch, in ,.
26312,https://github.com/me-box/databox,Databox,Git clone , into 
26326,https://www.eecs.yorku.ca/~kamel/sidd/dataset.php,here,Download the SIDD-Medium dataset from ,"
"
26328,os2d/data/dataloader.py,os2d/data/dataloader.py, from ,. See 
26328,os2d/data/dataset.py,os2d/data/dataset.py,. See , for docs and examples.
26328,#rerunning-experiments-on-retail-and-instre-datasets,below,For the rest of the training scripts see ,.
26334,#data-preparation,Data Preparation,"In addition to the benchmarks used by previous works, we introduce new benchmarks on three datasets: PASCAL VOC, COCO, and LVIS. We sample multiple groups of few-shot training examples for multiple runs of the experiments and report evaluation results on both the base classes and the novel classes. These are described in more detail in ",.
26334,#data-preparation,Data Preparation,"
","
"
26334,#data-preparation,Data Preparation,: Dataset files (see , for more details)
26334,fsdet/data/builtin_meta.py,fsdet/data/builtin_meta.py,": We use the train/val sets of PASCAL VOC 2007+2012 for training and the test set of PASCAL VOC 2007 for evaluation. We randomly split the 20 object classes into 15 base classes and 5 novel classes, and we consider 3 random splits. The splits can be found in ",.
26334,http://cocodataset.org/,COCO,"
",: We use COCO 2014 and extract 5k images from the val set for evaluation and use the rest for training. We use the 20 object classes that are the same with PASCAL VOC as novel classes and use the rest as base classes.
26334,https://www.lvisdataset.org/,LVIS,"
",: We treat the frequent and common classes as the base classes and the rare categories as the novel classes.
26334,datasets/README.md,datasets/README.md,See , for more details.
26336,http://tacodataset.org,tacodataset.org,"TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labeled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. Currently, images are hosted on Flickr and we have a server that is collecting more images and annotations @ ","
"
26336,https://github.com/cocodataset/cocoapi,coco python api,", you will also need ",. You can get this using
26336,http://tacodataset.org/stats,here,As you can see ,", most of the original classes of TACO have very few annotations, therefore these must be either left out or merged together. Depending on the problem, "
26339,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,We use DF2K dataset (the combination of , and 
26345,full_conv/Exp4_data_efficiency/imagenet_classification,Imagenet Experiment, with , and 
26345,full_conv/Exp4_data_efficiency/patch_matching,Patch Matching Experiment, and ,"
"
26345,#5-Small-datasets,Small datasets,"
", with 
26345,full_conv/Exp5_small_datasets/action_recognition,Action Recognition, with ,"
"
26345,full_conv/Exp4_data_efficiency/imagenet_classification,here,The experiment folder is ,.
26345,full_conv/Exp4_data_efficiency/patch_matching,here,The experiment folder is ,.
26345,full_conv/Exp5_small_datasets/action_recognition,here,The experiment folder is ,.
26355,https://github.com/facebookresearch/clevr-dataset-gen/,CLEVR generator,Our code to generate CLEVR-XAI is built upon the original ,.
26365,#data-prep,Data Prep, | , | 
26405,#database-logging,Database logging,"
","
"
26412,https://github.com/datadesk/california-2016-election-precinct-maps/tree/master/final-results,final-results,"If you want precinct-level results for all of California for statewide races in the Nov. 8, 2016, election, look in the ", directory.
26412,https://github.com/datadesk/california-2016-election-precinct-maps/tree/master/shapefiles,shapefiles,"If you want California precinct shapefiles by county, look in the ", directory.
26412,http://statewidedatabase.org,Statewide Database at U.C. Berkeley Law,We at the Los Angeles Times Data Viz team wanted to make the most detailed California election maps ever. To do that we had to work with each of the 58 counties. The secretary of state DOES NOT keep precinct-level results. The good folks at ," do organize these results, but not until at least six months after the election. We wanted to publish as soon as possible."
26412,https://github.com/datadesk/california-2016-election-precinct-maps/blob/master/final-results/all_precinct_results.csv,all_precinct_results.csv,"After that, you should be able to join with ",.
26428,#data-preparation,Data Preparation,"
","
"
26430,https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs,Quora Question Pairs," (MNLI). For paraphrase detection, we use ", (QQP) and 
26437,https://www.jstor.org/stable/1970637?seq=1#metadata_info_tab_contents,"Ann. Math., 91, 550-569, 1970.",AnnM_1970_550_569.csv: ,"
"
26437,https://www.jstor.org/stable/2118576?seq=1#metadata_info_tab_contents,"Ann. Math., 140, 703-722, 1994.",Alford94.csv: ,"
"
26437,https://www.jstor.org/stable/2044999?seq=1#metadata_info_tab_contents,"Proc. Amer. Math. Soc., 88(3) 486-490, 1983.",Brezis83.csv: ,"
"
26437,https://www.jstor.org/stable/2160465?seq=1#metadata_info_tab_contents,"Proc. Amer. Math. Soc., 120(3) 743-748, 1994.",Erbe94.csv: ,"
"
26437,https://www.jstor.org/stable/2318254?seq=1#metadata_info_tab_contents,"Amer. Math. Mon., 82(10), 985-992, 1975.",Li75.csv: ,"
"
26437,https://www.jstor.org/stable/1990945?seq=1#metadata_info_tab_contents,"J. Amer. Math. Soc., 2(3), 599-635, 1989.",Lusztig89.csv: ,"
"
26456,https://huggingface.co/nlp/viewer/?dataset=x_stance,"
Live Viewer
","
","
"
26457,https://github.com/hjwdzh/AdversarialTexture/raw/master/data/,"
data
",Please refer to , directory for details.
26458,https://omnomnom.vision.rwth-aachen.de/data/trackrcnn/trackrcnn_init.zip,pretrained model, data directory and the path to the ,", respectively. Logs, checkpoints and summaries are stored in the "
26458,https://omnomnom.vision.rwth-aachen.de/data/trackrcnn/conv3d_sep2-00000005.zip,our model,"Either first train your own model as described above, or download ", and extract the files into models/conv3d_sep2/
26459,http://cocodataset.org/#download,MS-COCO 2014 training set,Download the , and unzip it at path 
26463,https://tiles-data.isi.edu,https://tiles-data.isi.edu,Code accompanying the TILES 2018 data set publication and data release.  The data set can be obtained from ,"
"
26469,https://github.com/gokyildirim/salmon_dataset/blob/master/evaluation_code.ipynb,evaluation code,Additional files are added to run our ,. Please re-download the dataset if you have an older copy.
26469,https://github.com/gokyildirim/salmon_dataset/blob/master/evaluation_code.ipynb,ipython notebook,Please refer to the , to use/modify the evaluation code.
26484,https://eurocity-dataset.tudelft.nl/eval/benchmarks/detection,EuroCity Persons,"
","
"
26484,https://eurocity-dataset.tudelft.nl/,EuroCity Persons,"
","
"
26485,https://github.com/luogen1996/MCN/blob/master/data/README.md,DATA_PRE_README.md, Follow the instructions of  ," to generate training data and testing data of RefCOCO, RefCOCO+ and RefCOCOg."
26485,https://github.com/luogen1996/MCN/blob/master/data/README.md,DATA_PRE_README.md," sets of RefCOCO, RefCOCO+ and RefCOCOg (nearly 6500 images).  Please follow the instructions of  ", to download them.
26489,https://archive.ics.uci.edu/ml/datasets/spambase,here,The MNIST and CIFAR-10 datasets will be downloaded at running time. The spam classification dataset can be downloaded , and put the 
26489,https://www.kaggle.com/c/dogs-vs-cats/data,here," folder. For the ImageNet dataset, the dog vs. cat dataset can be downloaded ",", and one can extract the "
26504,https://www.med.upenn.edu/cbica/brats2020/data.html,Brain Tumour Segmentation (BraTS) Challenge 2020,The data used are the ones provided by the ,.
26505,https://github.com/bark-simulator/bark-databasse/,BARK-DB,"
",: Provides a framework to integrate multiple BARK scenario sets into a database. The database module supports binary serialization of randomly generated scenarios to ensure exact reproducibility of behavior benchmarks across systems.
26512,http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,CUB-200-2011,Download the , datasets and copy the contents of the extracted 
26512,http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz,FGVC-Aircraft,Download the , datasets and copy the contents of the extracted 
26515,https://github.com/memray/OpenNMT-kpg-release/tree/master/script/transfer/train_fulldata,script/,). Config files can be found at ,.
26515,https://huggingface.co/datasets/memray/keyphrase/,repo," from disk, without any hassle of tokenization or conversion to tensor files. Please check out Huggingface "," for all resources. ~~- Paper datasets and DUC: KP20k/Inspec/Krapivin/NUS/SemEval2010/DUC2001.~~ ~~- 4 large annotated datasets: KP20k, OpenKP, KPTimes+JPTimes, StackExchange.~~"
26515,https://github.com/memray/OpenNMT-kpg-release/tree/master/script/transfer/train_fulldata,Configs,"
", using RoBERTa subword tokenization. Vocab (including merges.txt/vocab.json/tokenizer.json) can be found 
26516,https://github.com/wayne391/Lead-Sheet-Analysis/tree/master/lead_sheet_dataset/datasets/event/,event,"
","
"
26516,https://github.com/wayne391/Lead-Sheet-Analysis/tree/master/lead_sheet_dataset/datasets/pianoroll/,pianoroll,"
","
"
26527,https://pandas.pydata.org,Pandas,", as well as ", and 
26527,https://data61.csiro.au/,CSIRO's Data61,"StellarGraph is designed, developed and supported by ",". If you use any part of this library in your research, please cite it using the following BibTex entry"
26529,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GW9GDM,data,|,]
26529,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GW9GDM,Dataverse,We release 2 pairs of complete/sampled retweet cascades on Cyberbullying (sampling rate: 0.5272) and YouTube (sampling rate: 0.9153). The data is hosted on ,.
26529,/data,data,This script can generate all the data files to run the experiments under the ,", "
26531,https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html,Rendered Handpose Dataset,"
","
"
26544,http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset,GTSRB,"
",: we provide scripts to download it.
26544,http://horatio.cs.nyu.edu/mit/tiny/data/tiny_images.bin,80 Million Tiny Images,"
","
"
26544,https://www.robots.ox.ac.uk/~vgg/data/dtd/,Textures,"
","
"
26546,http://opendatagroup.github.io/hadrian/hadrian-0.8.3/index.html#com.opendatagroup.hadrian.jvmcompiler.PFAEngine,API, (,) is 
26546,http://www.opendatagroup.com,Open Data,) is ,"'s complete implementation of PFA for the Java Virtual Machine (JVM). Hadrian is designed as a library to be embedded in applications or used as a scoring engine container. To make Hadrian immediately usable, we provide containers that allow Hadrian to be dropped into an existing workflow. Hadrian can currently be used as a "
26546,https://github.com/opendatagroup/hadrian/wiki/Hadrian-Standalone,standard-input/standard-output process,"'s complete implementation of PFA for the Java Virtual Machine (JVM). Hadrian is designed as a library to be embedded in applications or used as a scoring engine container. To make Hadrian immediately usable, we provide containers that allow Hadrian to be dropped into an existing workflow. Hadrian can currently be used as a ",", a "
26546,https://github.com/opendatagroup/hadrian/wiki/Hadrian-MR,Hadoop map-reduce workflow,", a ",", an "
26546,https://github.com/opendatagroup/hadrian/wiki/Hadrian-Actors,actor-based workflow,", an "," of interacting scoring engines, or as a "
26546,https://github.com/opendatagroup/hadrian/wiki/Hadrian-GAE,servlet in a Java Servlet container," of interacting scoring engines, or as a ",", including Google App Engine."
26546,http://opendatagroup.github.io/hadrian/titus-0.8.3/titus.genpy.PFAEngine,API, (,") is Open Data's complete implementation of PFA for Python. Hadrian and Titus both execute the same scoring engines, but while Hadrian's focus is speed and portability, Titus's focus is on model development. Included with Titus are standard model producers, a "
26546,https://github.com/opendatagroup/hadrian/wiki/PrettyPFA,PrettyPFA,") is Open Data's complete implementation of PFA for Python. Hadrian and Titus both execute the same scoring engines, but while Hadrian's focus is speed and portability, Titus's focus is on model development. Included with Titus are standard model producers, a "," parser for easier editing, a "
26546,https://github.com/opendatagroup/hadrian/wiki/PFA-Inspector,PFA-Inspector," parser for easier editing, a "," commandline for interactive analysis of a PFA document, and many other tools and scripts."
26546,https://github.com/opendatagroup/hadrian/wiki,Hadrian wiki,See the ," for more information, including "
26546,https://github.com/opendatagroup/hadrian/wiki/Installation,installation instructions," for more information, including ", and tutorials.
26546,mailto:licensing@opendatagroup.com,licensing@opendatagroup.com,Contact , to see how Hadrian can fit into your environment.
26546,https://github.com/opendatagroup/augustus,Augustus,The Roman emperor naming convention is continued from ,", Open Data's producer and consumer of the "
26547,https://github.com/opendatagroup/hadrian,Hadrian project,"To score exported models, use a reference PFA scoring engine in Java, Python or R from the ",. 
26549,http://www.cvlibs.net/datasets/kitti/raw_data.php,raw KITTI dataset,You can download the entire , by running:
26549,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,benchmark split," of the standard Eigen split of KITTI, which is designed for monocular training. You can also train a model using the new ", or the 
26549,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,odometry split, or the , by setting the 
26549,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,new KITTI depth benchmark,  | Evaluate with the improved ground truth from the , | | 
26549,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,new KITTI depth benchmark,        | The , test files. |
26549,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI odometry dataset,"For this evaluation, the ","
"
26550,http://ai.stanford.edu/~amaas/data/sentiment/,IMDB dataset, on the polluted , (please refer to the paper for details) for the demo purpose.
26550,http://snap.stanford.edu/data/web-BeerAdvocate.html,beer review dataset,The original ," has been removed by the dataset’s original author, at the request of the data owner, BeerAdvocate.  To respect the wishes and legal rights of the data owner, we do not include the data in our repo.  If you are interested in beer review, please first obtain the dataset from the original authors who released the dataset. We will then be happy to provide our data and environment partitions to whoever is granted rights to the data.  Once you have access to the dataset and have prepared it in the desired format, please refer to the "
26550,https://www.tensorflow.org/datasets/catalog/imdb_reviews,build-in functions,.  You could of course directly using the , in TensorFlow with some minor tweaks.
26551,#data-preparation,Data Preparation,"
","
"
26555,https://towardsdatascience.com/b051d28f3d2e,Word Beam Search: A CTC Decoding Algorithm,"
","
"
26555,https://towardsdatascience.com/5a889a3d85a7,Beam Search Decoding in CTC-trained Neural Networks,"
","
"
26570,https://numba.pydata.org,"
numba
","
", 0.36.2
26577,./data/README.md,DATA_README,Download Data (~530gb) (See , for more details)
26577,./data/README.md,DATA_README,"If you just want to use ASRL, you can refer to   ",. It contains direct links to download ASRL
26581,https://research.donorschoose.org/t/download-opendata/33,Donorschoose.org,. We used datasets provided generaously by ,", "
26581,https://www.yelp.com/dataset,Yelp Review,", ",", and "
26581,https://research.donorschoose.org/t/download-opendata/33,"""Project Essays""",Download , provided by Donorschoose.org
26583,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7scenes,Sample results on , and 
26583,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7scenes,Intermediate optical flow results on ,", "
26583,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7scenes, comprising the sequential full image paths in lines. Please go to the , dataset to download the source images.
26583,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7scenes," function of numpy matrices. They have a channel number of 4, with 3 for scene coordinates and 1 for binary masks of pixels. The mask for one pixel is 1 if its label scene coordinates are valid and 0 otherwise. Their resolutions are 8 times lower than the images. For example, for the "," dataset, the images have a resolution of 480x640, while the label maps have a resolution of 60x80."
26583,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,7scenes,You can download the trained models of , from the 
26603,http://cocodataset.org/#download,this link,"For COCO 2017, visit "," and download the 2017 validation images, which is about 1.1GB. Un-tar this file. It will create a folder called "
26605,#train-the-2-fold-ecan-with-aeca-on-mnist-data-set,Train the 2-fold ECAN with AECA on MNIST data set,"
","
"
26605,#train-the-2-fold-ecan-with-veca-on-mnist-data-set,Train the 2-fold ECAN with VECA on MNIST data set,"
","
"
26611,https://www.dropbox.com/s/ufiu97sn4j4h9z0/dataset.zip?dl=0,here,training data is available ,.
26611,https://github.com/DifanLiu/NeuralContours/blob/master/data/README.md,here, to compute all the input geometric feature maps and lines. See , for details.
26617,https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz,model download,"
","
"
26623,https://storage.googleapis.com/electra-data/electra_small.zip,link,| Model | Layers | Hidden Size | Params | GLUE score (test set) | Download | | --- | --- | --- | --- | ---  | --- | | ELECTRA-Small | 12 | 256 | 14M | 77.4  | , | | ELECTRA-Base | 12 | 768 | 110M | 82.7 | 
26623,https://storage.googleapis.com/electra-data/electra_base.zip,link, | | ELECTRA-Base | 12 | 768 | 110M | 82.7 | , | | ELECTRA-Large | 24 | 1024 | 335M |  85.2 | 
26623,https://storage.googleapis.com/electra-data/electra_large.zip,link, | | ELECTRA-Large | 24 | 1024 | 335M |  85.2 | , |
26623,https://storage.googleapis.com/electra-data/vocab.txt,here,". Our ELECTRA models all used the exact same vocabulary as English uncased BERT, which you can download ",.
26623,https://www.tensorflow.org/tutorials/load_data/tfrecord,tfrecord,. It pre-processes/tokenizes the data and outputs examples as , files under 
26623,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json,train,: Download the , and 
26623,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json,dev, and , datasets and move them under 
26623,https://github.com/mrqa/MRQA-Shared-Task-2019#datasets,here,: Download the data from ,. Move the data to 
26628,http://www.cvlibs.net/datasets/kitti/eval_stereo.php,KITTI Stereo,"
","
"
26628,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow,"
","
"
26629,https://warwick.ac.uk/fac/sci/dcs/research/tia/data/pannuke,here,The PanNuke dataset can be downloaded ,. 
26634,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,FlyingChairs,"
","
"
26634,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D,"
","
"
26634,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow,KITTI 2012,"
", & 
26634,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI 2015, & ,"
"
26636,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K," are diverse images from ImageNet, ", (used in publication) or similar image sets. To see all options run 
26636,http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip,Download link,DIV2K source image set used for dataset generation for ML-SIM. ,.
26638,#download-the-data,"
Download
", | , | 
26640,https://github.com/voloshinov/multidimensional-segmented-regression/blob/master/src/testing/synthetic_data_experiments_constant.ipynb,/testing/synthetic_data_experiments_constant.ipynb,The notebook , contains our synthetic data experiments.
26640,https://github.com/voloshinov/multidimensional-segmented-regression/blob/master/src/testing/real_data_experiments_boston_constant.ipynb,/testing/real_data_experiments_boston_constant.ipynb,The notebook ," contains real data experiments with constant fit, and the notebooks "
26640,https://github.com/voloshinov/multidimensional-segmented-regression/blob/master/src/testing/real_data_experiments_boston_linear_2.ipynb,testing/real_data_experiments_boston_linear_2.ipynb," contains real data experiments with constant fit, and the notebooks ", and 
26640,https://github.com/voloshinov/multidimensional-segmented-regression/blob/master/src/testing/real_data_experiments_boston_linear_3.ipynb,testing/real_data_experiments_boston_linear_3.ipynb, and ," contain real data experiments with linear fit, splitting in 2 and 3 dimensions respectively."
26641,http://ptak.felk.cvut.cz/6DB/public/bop_datasets/lm_test_all.zip,here,"Please note, we only provide 20 images for quick test, if you need to test the whole sequence, you can download the LINEMOD dataset ",.
26674,metadata.csv,metadata, and , and 
26674,https://github.com/mlmed/torchxrayvision/blob/master/torchxrayvision/datasets.py#L867,here,"Current stats of PA, AP, and AP Supine views. Labels 0=No or 1=Yes. Data loader is ","
"
26674,https://github.com/GeneralBlockchain/covid-19-chest-xray-lung-bounding-boxes-dataset,Lung Bounding Boxes,"
", and 
26674,https://github.com/GeneralBlockchain/covid-19-chest-xray-segmentations-dataset,Chest X-ray Segmentation, and , (license: CC BY 4.0) contributed by 
26674,https://github.com/v7labs/covid-19-xray-dataset/tree/master/annotations,Lung and other segmentations for 517 images,"
", (license: CC BY) in COCO and raster formats by 
26674,https://github.com/v7labs/covid-19-xray-dataset,v7labs, (license: CC BY) in COCO and raster formats by ,"
"
26679,https://datascience.jpl.nasa.gov/poster-39,Poster, | , at the Second AI and Data Science 
26679,https://datascience.jpl.nasa.gov/aiworkshop,Workshop, at the Second AI and Data Science , at JPL.
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data,datafolder,Datasets used in the project are availale in the ,.
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data/se_ner_annotated.tsv,CR annotated dataset,"
","
"
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data/acronyms.txt,Accronyms dataset,"
","
"
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data/definitions.txt,Definitions dataset,"
","
"
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data/keywords2annotate.txt,Keyword annotated dataset,"
","
"
26679,https://github.com/jitinkrishnan/NASA-SE/blob/master/se_data/seva-toie-sentences.txt,sentences,Try with more example/template ,.
26687,http://numba.pydata.org/,Numba, and ,"
"
26688,http://www.bioinf.jku.at/people/klambauer/sars_cov_dataset.zip,Data set,"
"," (if you use this data set, please do not forget to cite this work!)"
26693,http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits,"
handwritten digits
",Now we load the classic , datasets. It contains 1797 images with 
26694,pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb,"
PyTorch
", | TBD | ,  | | ResNet-34 Digit Classifier | 
26694,pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb,"
PyTorch
", | TBD | ,  | | ResNet-50 Digit Classifier| 
26694,pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb,"
PyTorch
", | TBD | ,  | | ResNet-101 Gender Classifier| 
26694,https://ai.stanford.edu/~amaas/data/sentiment/,IMDB movie review,  | | DistilBERT as feature extractor | , | DistilBERT classifier with sklearn random forest and logistic regression | 
26694,https://ai.stanford.edu/~amaas/data/sentiment/,IMDB movie review, | , | DistilBERT classifier with sklearn random forest and logistic regression using the scikit-learn 
26694,https://ai.stanford.edu/~amaas/data/sentiment/,IMDB movie review,  | | Fine-tune DistilBERT I | , | Fine-tune only the last 2 layers of DistilBERT classifier |  
26694,https://ai.stanford.edu/~amaas/data/sentiment/,IMDB movie review, | | Fine-tune DistilBERT II | , | Fine-tune the whole DistilBERT classifier | 
26694,./pytorch-lightning_ipynb/data-augmentation/autoaugment,"
PyTorch Lightning
",| Title                      | Dataset | Description | Notebooks                                                    | | -------------------------- | ------- | ----------- | ------------------------------------------------------------ | | AutoAugment & TrivialAugment for Image Data | CIFAR-10     | Trains a ResNet-18 using AutoAugment and TrivialAugment         | , |
26694,pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb,"
PyTorch
",|Title | Dataset | Description | Notebooks | | --- | --- | --- | --- | | Custom Data Loader Example for PNG Files | TBD | TBD | ,  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5 | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb,"
PyTorch
",  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5 | TBD | TBD | ,  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb,"
PyTorch
",  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA | TBD | TBD | , | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb,"
PyTorch
", | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw | TBD | TBD | , | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb,"
PyTorch
", | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset | TBD | TBD | , | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD) | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb,"
PyTorch
", | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD) | TBD | TBD | ,  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb,"
PyTorch
",  | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images | TBD | TBD | , | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Fashion MNIST | TBD | TBD | 
26694,pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb,"
PyTorch
", | | Using PyTorch Dataset Loading Utilities for Custom Datasets -- Fashion MNIST | TBD | TBD | ,  |
26694,pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb,"
PyTorch
",|Title | Description | Notebooks | | --- | --- | --- | | Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA | TBD | ,  | | Distribute a Model Across Multiple GPUs with Pipeline Parallelism (VGG-16 Example) | TBD | 
26694,tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb,"
TensorFlow
",|Title | Description | Notebooks | | --- | --- | --- | | Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives | TBD | , | | Storing an Image Dataset for Minibatch Training using HDF5 | TBD | 
26694,tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb,"
TensorFlow
", | | Storing an Image Dataset for Minibatch Training using HDF5 | TBD | , | | Using Input Pipelines to Read Data from TFRecords Files | TBD | 
26694,tensorflow1_ipynb/mechanics/dataset-api.ipynb,"
TensorFlow
", | | Using TensorFlow's Dataset API | TBD | , |
26698,https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition,Packt Page,"
","
"
26700,https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28,Data Augmentation in NLP,This python library helps you with augmenting nlp for your machine learning projects. Visit this introduction to understand about ,. 
26700,https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb,BERT," | insert, substitute, swap, delete | Apply augmentation randomly | |Textual| Word | AntonymAug | substitute | Substitute opposite meaning word according to WordNet antonym| |Textual| | ContextualWordEmbsAug | insert, substitute | Feeding surroundings word to ",", DistilBERT, "
26700,https://medium.com/dataseries/why-does-xlnet-outperform-bert-da98a8503d5b,XLNet, or ," language model to find out the most suitlabe word for augmentation| |Textual| | RandomWordAug | swap, crop, delete | Apply augmentation randomly | |Textual| | SpellingAug | substitute | Substitute word according to spelling mistake dictionary | |Textual| | SplitAug | split | Split one word to two words randomly| |Textual| | SynonymAug | substitute | Substitute similar word according to WordNet/ PPDB synonym | |Textual| | "
26700,https://medium.com/towards-artificial-intelligence/unsupervised-data-augmentation-6760456db143,TfIdfAug," language model to find out the most suitlabe word for augmentation| |Textual| | RandomWordAug | swap, crop, delete | Apply augmentation randomly | |Textual| | SpellingAug | substitute | Substitute word according to spelling mistake dictionary | |Textual| | SplitAug | split | Split one word to two words randomly| |Textual| | SynonymAug | substitute | Substitute similar word according to WordNet/ PPDB synonym | |Textual| | "," | insert, substitute | Use TF-IDF to find out how word should be augmented | |Textual| | WordEmbsAug | insert, substitute | Leverage  "
26700,https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a,word2vec," | insert, substitute | Use TF-IDF to find out how word should be augmented | |Textual| | WordEmbsAug | insert, substitute | Leverage  ",", "
26700,https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a,GloVe,", ", or 
26700,https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a,fasttext, or , embeddings to apply augmentation| |Textual| | 
26700,https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28,BackTranslationAug, embeddings to apply augmentation| |Textual| | , | substitute | Leverage two translation models for augmentation | |Textual| | ReservedAug | substitute | Replace reserved words | |Textual| Sentence | ContextualWordEmbsForSentenceAug | insert | Insert sentence according to 
26700,https://medium.com/dataseries/why-does-xlnet-outperform-bert-da98a8503d5b,XLNet, | substitute | Leverage two translation models for augmentation | |Textual| | ReservedAug | substitute | Replace reserved words | |Textual| Sentence | ContextualWordEmbsForSentenceAug | insert | Insert sentence according to ,", "
26700,https://towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655,GPT2,", ", or DistilGPT2 prediction | |Textual| | AbstSummAug | substitute | Summarize article by abstractive summarization method | |Textual| | LambadaAug | substitute | Using language model to generate text and then using classification model to retain high quality results | |Signal| Audio | CropAug | delete | Delete audio's segment | |Signal| | LoudnessAug|substitute | Adjust audio's volume | |Signal| | MaskAug | substitute | Mask audio's segment | |Signal| | NoiseAug | substitute | Inject noise | |Signal| | PitchAug | substitute | Adjust audio's pitch | |Signal| | ShiftAug | substitute | Shift time dimension forward/ backward | |Signal| | SpeedAug | substitute | Adjust audio's speed | |Signal| | VtlpAug | substitute | Change vocal tract | |Signal| | NormalizeAug | substitute | Normalize audio | |Signal| | PolarityInverseAug | substitute | Swap positive and negative for audio | |Signal| Spectrogram | FrequencyMaskingAug | substitute | Set block of values to zero according to frequency dimension | |Signal| | TimeMaskingAug | substitute | Set block of values to zero according to time dimension | |Signal| | LoudnessAug | substitute | Adjust volume |
26700,https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff,Data Augmentation library for Text,"
","
"
26700,https://medium.com/towards-artificial-intelligence/how-does-data-noising-help-to-improve-your-nlp-model-480619f9fb10,How does Data Noising Help to Improve your NLP Model?,"
","
"
26700,https://towardsdatascience.com/data-augmentation-for-speech-recognition-e7c607482e78,Data Augmentation library for Speech Recognition,"
","
"
26700,https://towardsdatascience.com/data-augmentation-for-audio-76912b01fdf6,Data Augmentation library for Audio,"
","
"
26700,https://medium.com/towards-artificial-intelligence/unsupervised-data-augmentation-6760456db143,Unsupervied Data Augmentation,"
","
"
26700,https://amitness.com/2020/05/data-augmentation-for-nlp/,A Visual Survey of Data Augmentation in NLP,"
","
"
26701,https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192,Interpretable or Accurate? Why Not Both?,"
","
"
26701,https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb,"The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.","
","
"
26701,https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8,InterpretML: Another Way to Explain Your Model,"
","
"
26701,https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254,The right way to compute your Shapley Values,"
","
"
26701,https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727,The Art of Sprezzatura for Machine Learning,"
","
"
26701,https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95,Mixing Art into the Science of Model Explainability,"
","
"
26717,http://www.robots.ox.ac.uk/~joon/data/baseline_lite_ap.model,here,"A pretrained model, described in [1], can be downloaded from ",.
26717,http://www.robots.ox.ac.uk/~joon/data/baseline_v2_smproto.model,here,"A larger model trained with online data augmentation, described in [2], can be downloaded from ",.
26717,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb,The , datasets are used for these experiments.
26717,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/train_list.txt,here,The train list for VoxCeleb2 can be download from ,. The test lists for VoxCeleb1 can be downloaded from 
26717,https://mm.kaist.ac.kr/datasets/voxceleb/index.html#testlist,here,. The test lists for VoxCeleb1 can be downloaded from ,.
26720,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs,FlyingChairs,"
","
"
26720,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D,"
","
"
26720,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI,"
","
"
26721,https://journals.aps.org/datasets,here,You can access the original APS dataset ,. (Released by 
26721,input_data.md,here,Detailed pre-process files information can be found ,.
26736,https://github.com/cuishuhao/BNM/tree/master/DA/data/Balance_Domainnet,Balance domainnet,[x] ,.(New dataset)
26739,https://download.visinf.tu-darmstadt.de/data/from_games/,gta5(G),The training procedure needs ,", "
26739,https://synthia-dataset.net/downloads/,synthia_rand_citys(S),", ",", "
26739,https://www.mapillary.com/dataset/vistas?pKey=1GyeWFxH_NPIQwgl0onILw,mapillary(M),", ", and 
26739,https://www.cityscapes-dataset.com/,cityscapes(C), and ,. Please download them and put to the same folder which can be specified in 
26754,http://rocdatascience.com/,[Rochester Data Science Consortium],"
","
"
26757,https://figshare.com/articles/dataset/Mining_Coronavirus_COVID-19_Posts_in_Social_Media/12597755,here,You can access our dataset and pre-trained BERT model ,.
26757,https://figshare.com/articles/dataset/Mining_Coronavirus_COVID-19_Posts_in_Social_Media/12597755,here,"We have uploaded the updated dataset,  ",". It contains about 9 million tweets published between Jan 27 and April 20, see the paper for more details."
26758,https://www.cityscapes-dataset.com/,here, The Cityscapes dataset can be downloaded at ,"
"
26760,https://grouplens.org/datasets/movielens/,Movielens,You can find the full version of recommendation datasets via ,", "
26760,http://www.cp.jku.at/datasets/LFM-1b/,Last-FM,", ",", "
26760,http://jmcauley.ucsd.edu/data/amazon,Amazon-book,", ",.
26765,http://maxwell.cs.umass.edu/mvcnn-data/modelnet40v1.tar,tarball,modelnet40v1 (12 views w/ upright assumption): , (204M)
26765,http://maxwell.cs.umass.edu/mvcnn-data/modelnet40v2.tar,tarball,modelnet40v2 (80 views w/o upright assumption): , (1.3G)
26765,http://maxwell.cs.umass.edu/mvcnn-data/shapenet55v1.tar,tarball,shapenet55v1 (12 views w/ upright assumption): , (2.4G)
26765,http://maxwell.cs.umass.edu/mvcnn-data/shapenet55v2.tar,tarball,shapenet55v2 (80 views w/o upright assumption): , (15G)
26767,http://nlp.stanford.edu/data/glove.6B.zip,"
Pre-trained glove embedding
","
",: 
26774,https://projects.asl.ethz.ch/datasets/doku.php?id=laserregistration:laserregistration,the ETH datasets,"
",;
26774,http://asrl.utias.utoronto.ca/datasets/3dmap/index.html,the Canadian Planetary Emulation Terrain 3D Mapping datasets,"
",;
26774,https://vision.in.tum.de/data/datasets/rgbd-dataset,the TUM Vision Groud RGBD datasets,"
",;
26774,https://irap.kaist.ac.kr/dataset/,the KAIST Urban datasets,"
",.
26775,https://www.cityscapes-dataset.com/,Cityscapes & Foggy Cityscapes,"
","
"
26775,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d,KITTI,"
","
"
26791,http://jaina.cs.ucdavis.edu/datasets/adv/imagenet/img.tar.gz,ImageNet Test Set,"
","
"
26793,https://huggingface.co/datasets/elenanereiss/german-ler,https://huggingface.co/datasets/elenanereiss/german-ler,Now available on Huggingface ,"
"
26794,http://roboimagedata2.compute.dtu.dk/data/MVS/Rectified.zip,rectified images,Download the , from 
26794,http://roboimagedata.compute.dtu.dk/?page_id=36,DTU benchmark, from , and unzip it to 
26799,https://github.com/xinntao/BasicSR/wiki/Prepare-datasets-in-LMDB-format,here,Commonly used training and testing datasets can be downloaded ,.
26799,https://github.com/Maclory/SPSR/tree/master/code/data/README.md,More details, paths for the data loader (,)
26808,http://www.nersc.gov/about/nersc-staff/data-analytics-services/wahid-bhimji/,Wahid Bhimji,"
","
"
26817,http://campar.in.tum.de/files/SacrumNavDataset/dataset.zip,this link.,The data set is available in ,"
"
26819,https://github.com/hhase/sacrum_data-set,dataset,: corresponds to the location of the ,.
26820,process_data/categories.txt,ShapeNetCore categories, specify one of the ,.
26835,https://github.com/uzh-rpg/vilib/blob/master/visual_lib/test/images/create_feature_detector_evaluation_data.sh,custom script,The EuRoC Machine Hall dataset mentioned in the paper for feature detection and tracking can be downloaded through our ,". This is the dataset, that is used by default in the test code. Please note, that in our online example, the test image count has been reduced from the original 3682 to 100 for a quicker evaluation, but this may be readjusted any time "
26862,https://github.com/lattas/avatarme#public-dataset,"
RealFaceDB
","
","
"
26862,https://www.micc.unifi.it/resources/datasets/florence-superface/,Superface,", ", and subjects captured with a 
26870,http://pandas.pydata.org/,pandas, Events and summaries are utilizing , for data structures and analysis. New metrics can reuse already computed values from depending metrics.
26879,#Supported-dataset-names,"
Supported dataset names
",. Check the ,"
"
26879,#Supported-dataset-names,"
Supported dataset names
",. Check the ,"
"
26889,https://motchallenge.net/data/MOT17Det/,MOT Challenge dataset,Prepare synthesized raw video denoising dataset (SRVD dataset) to pretrain RViDeNet. Please download ," and select four videos (02, 09, 10, 11) from train set. To convert sRGB clean videos to raw clean videos, run:"
26893,https://github.com/M3SOulu/MozillaApacheDataset/blob/master/data.md,doc folder,The full documentation of the dataset can be found in the , of this repository.
26894,https://github.com/cocodataset/cocoapi,COCOAPI,Install ,:
26897,http://www.semantic-kitti.org/dataset.html#overview,here,"2, Download Velodyne point clouds and label data in SemanticKITTI dataset ",.
26903,https://github.com/dgedon/DeepSSM_SysID/tree/master/data,/data,The used data files are stored in ,. For the Wiener Hammerstein system we refer to the original website (see readme in the folder) since the data files are rather large. In order to extend for more datasets the dataset has to be provided in a specific format and added in the 
26903,https://github.com/dgedon/DeepSSM_SysID/blob/master/data/loader.py,/data/loader.py,. For the Wiener Hammerstein system we refer to the original website (see readme in the folder) since the data files are rather large. In order to extend for more datasets the dataset has to be provided in a specific format and added in the ,". A training, validation and test dataset has to be provided as numpy arrays of shape (sequence length, signal dimension). The sequence length is defined in the file "
26903,https://github.com/dgedon/DeepSSM_SysID/blob/master/options/dataset_options.py,/options/dataset_options.py,". A training, validation and test dataset has to be provided as numpy arrays of shape (sequence length, signal dimension). The sequence length is defined in the file ",.
26904,http://www2.isprs.org/commissions/comm3/wg4/data-request-form2.html,ISPRS Potsdam,Download a segmentation dataset such as , or 
26930,https://github.com/ipfs/go-datastore,go-datastore,A distributed , implementation using Merkle-CRDTs.
26930,https://pkg.go.dev/github.com/ipfs/go-datastore#Datastore,"
Datastore
",. It satisfies the , and 
26930,https://pkg.go.dev/github.com/ipfs/go-datastore#Batching,"
Batching
", and , interfaces from 
26930,https://github.com/ipfs/go-datastore,"
go-datastore
","A user-provided, thread-safe, ", implementation to be used as permanent storage. We recommend using the 
26931,https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type,CRDT,", an immutable, cryptographically verifiable, operation-based conflict-free replicated data structure (",) for distributed systems. ipfs-log is formalized in the paper 
26931,#database-browser-ui,Database browser UI,"
","
"
26932,https://webchat.freenode.net/?channels=ipfs-dynamic-data,"
#ipfs-dynamic-data
","
","
"
26932,https://github.com/ipfs/dynamic-data-and-capabilities/issues/36,Issue describing next meeting,. ,.
26932,https://waffle.io/ipfs/dynamic-data-and-capabilities,"
Waffle.io - Columns and their card count
","
","
"
26932,https://github.com/ipfs/dynamic-data-and-capabilities,Dynamic Data and Capabilities Working Group, , was born.
26932,https://webchat.freenode.net/?channels=ipfs-dynamic-data,#ipfs-dynamic-data IRC Channel,Join our ,"
"
26932,https://github.com/ipfs/dynamic-data-and-capabilities/issues/36,Issue describing next meeting,. ,.
26932,https://github.com/ipfs/dynamic-data-and-capabilities/issues,the Special Interest Group challenges,Check out , and see how you can help contribute.
26935,https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type,CRDT," is an immutable, operation-based conflict-free replicated data structure (",") for distributed systems. It's an append-only log that can be used to model a mutable, shared state between peers in p2p applications."
26935,https://github.com/orbitdb/ipfs-log/tree/master/API.md##appenddata,append(data),"
","
"
26943,http://vowl.visualdataweb.org/webvowl.html,WebVOWL,Open ontology online by importing the ontology on ,.
26945,https://frictionlessdata.io/data-package/,data packaged it,"We have cleaned and normalized that data, for example tidying dates and consolidating several files into normalized time series. We have also added some metadata such as column descriptions and ",.
26945,https://pandas.pydata.org,Pandas,This repository uses , to process and normalize the data.
26945,https://www.opendatacommons.org/licenses/pddl/1-0/,Public Domain and Dedication License,This dataset is licensed under the Open Data Commons ,.
26954,https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets,"Occluded-REID, P-DukeMTMC-reID",Download the raw datasets ,", and "
26954,https://kaiyangzhou.github.io/deep-person-reid/datasets.html,here, datasets can be found ,. And then place them under the directory like:
26957,https://www.azdhs.gov/covid19/data/index.php,Department of Health Services,Arizona: ,"
"
26957,https://www.mendocinocounty.org/community/novel-coronavirus/covid-19-case-data,Mendocino County,"
","
"
26957,https://covid19.colorado.gov/covid-19-data,Department of Public Health and Environment,Colorado: ,"
"
26957,https://data-cdphe.opendata.arcgis.com/datasets/CDPHE::colorado-covid-19-positive-cases-and-rates-of-infection-by-county-of-identification/explore?location=38.980312%2C-105.550873%2C7.30,Colorado Department of Public Health and Environment Open Data Portal,"
","
"
26957,https://data.ct.gov/stories/s/COVID-19-data/wa3g-tfvc/,Department of Public Health,Connecticut: ,"
"
26957,https://coronavirus.dc.gov/page/coronavirus-data,Government of The District of Columbia,District of Columbia: ,"
"
26957,https://protect-public.hhs.gov/datasets/HHSGOV::community-profile-report-counties/about,U.S. Department of Health & Human Services, & ,"
"
26957,https://hub.mph.in.gov/dataset?q=COVID,Department of Health,Indiana: ,"
"
26957,https://www.maine.gov/dhhs/mecdc/infectious-disease/epi/airborne/coronavirus/data.shtml,Department of Health and Human Services,Maine: ,"
"
26957,https://www.stlouis-mo.gov/covid-19/data/#totalsByDate,St. Louis City,"
","
"
26957,https://datanexus-dhhs.ne.gov/views/Covid/1_CountyStatisticsMap?%3AisGuestRedirectFromVizportal=y&%3Aembed=y,Nebraska Department of Health and Human Services,"
","
"
26957,https://health.data.ny.gov/Health/New-York-State-Statewide-COVID-19-Testing/xdss-u53e/data,State Department of Health,New York: ,"
"
26957,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,New York City Health Department,"
","
"
26957,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,NYC Department of Health and Mental Hygiene,"
", & 
26957,https://github.com/nychealth/coronavirus-data,Github Repo, & ,"
"
26957,https://chcc.datadriven.health/ui/99/dashboard/cbaeede2-4f75-11eb-b380-0242ac1d004a,Northern Mariana Islands Commonwealth Dept of Public Health,Northern Mariana Islands: ,"
"
26957,https://ri-department-of-health-covid-19-data-rihealth.hub.arcgis.com/,Department of Health,Rhode Island: ,"
"
26957,https://scdhec.gov/covid19/covid-19-data,Department of Health and Environmental Control,South Carolina: ,"
"
26957,https://covid-data-amarillo.hub.arcgis.com/,Amarillo County,"
","
"
26957,https://covid-data-amarillo.hub.arcgis.com/,Potter County,"
","
"
26957,https://www.dhs.wisconsin.gov/covid-19/data.htm,Department of Health Services,Wisconsin: , (https://data.dhsgis.wi.gov/datasets/wi-dhs::covid-19-data-by-county-v2/about)
26976,https://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Flowers-102," , ",", "
26980,https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635?source=friends_link&sk=03e1a2de548367b22139568a7c798180&gi=85b436f7c556,Blog Post,"
","
"
26980,#training-on-synthetic-composite-adobe-dataset,Training code on synthetic-composite Adobe dataset,"
","
"
26980,#dataset,Captured Data,"
","
"
26980,#training-on-synthetic-composite-adobe-dataset,supervised training on synthetic-composite Adobe dataset,Training code for , and 
26980,http://images.cocodataset.org/zips/test2017.zip,"
bg_train
",Download background images: Download MS-COCO images and place it in , and in 
26980,http://images.cocodataset.org/zips/val2017.zip,"
bg_test
", and in ,.
26988,http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/,FGVC-Aircraft,", ", and 
26988,http://www.robots.ox.ac.uk/~vgg/data/oid/,OID-Aircraft, and ,.
26988,https://github.com/Tsingularity/PoseNorm_Fewshot/blob/master/dataset/download.sh#L3,line,". If you wanna download the dataset from google drive via command line directly, please refer to this ", of code in 
26989,http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,EuRoC MAV Dataset,Download ,". Although it contains stereo cameras, we only use one camera. Before testing, copy the new "
26994,https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/tree/master/data,here," that summarize population estimates, demographics, ethnicity, housing, education, employment and income, climate, transit scores, and healthcare system-related metrics. A detailed description of all variables can be found ",.
26994,https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/tree/master/data,./data,"
"," folder contains aggregated machine-readable file counties.csv with demographic, socioeconomic, health care, and education data for each county in the 50 states and Washington DC. Data is organized by FIPS codes - unambiguous identifiers for each county, since the same county name may appear in many states."
26994,https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/tree/master/raw_data,./raw_data,"
", contains raw datasets that were used to create 
26994,https://github.com/JieYingWu/disease_spread/raw_data,./raw_data,Please create a new directory in , with a sensible name based on the type of data you are adding.
26994,https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/tree/master/data,data README,"Additionally, we would like to thank our sources, which can be found in the ",.
26997,http://vision.middlebury.edu/flow/data/,Middlebury-OTHERS dataset,[ , ] - download 
26997,https://github.com/baowenbo/MEMC-Net#hd-dataset-results,HD dataset,[ , ] - download the original ground truth videos [
27001,http://snap.stanford.edu/data/higgs-twitter.html,Twitter,"In this resposity, we provide EComm dataset as an example, you can also download all the other datasets from the SNAP platform (",", and "
27001,http://snap.stanford.edu/data/sx-mathoverflow.html,Math-Overflow,", and ","). Besides, you can also use your own Dynamic Heterogeneous Networks dateset, as long as it fits the following template."
27005,https://datastudio.google.com/reporting/b669b26c-4f49-4415-b734-0e5f90976169/page/VIwK,Africa COVID-19 Dashboard,"
","
"
27005,data/templates/,data/templates/,See the , if you want to start a new line list for a country that is not yet included. You must upload csv though.
27005,https://sites.google.com/view/data-science-covid-19,Stanford <> CS472 Data science and AI for COVID-19,"
","
"
27005,/data,/data,Data Available [,]
27005,data/line_lists/line-list-africa.csv,line-list-africa,| dataset         | url | raw_url[file] | |-----------------|-----|---------------| |    line-list-africa             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-africa.csv,line-list-africa.csv,   |       ,        | |    line-list-algeria            |  
27005,data/line_lists/line-list-algeria.csv,line-list-algeria,        | |    line-list-algeria            |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-algeria.csv,line-list-algeria.csv,   |       ,        | |    line-list-benin          |  
27005,data/line_lists/line-list-benin.csv,line-list-benin,        | |    line-list-benin          |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-benin.csv,line-list-benin.csv,   |       ,        | |    line-list-egypt           |  
27005,data/line_lists/line-list-egypt.csv,line-list-egypt,        | |    line-list-egypt           |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-egypt.csv,line-list-egypt.csv,   |       ,        | |    line-list-ethiopia            |  
27005,data/line_lists/line-list-ethiopia.csv,line-list-ethiopia,        | |    line-list-ethiopia            |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-ethiopia.csv,line-list-ethiopia.csv,   |       ,        | |    line-list-gambia             |  
27005,data/line_lists/line-list-gambia.csv,line-list-gambia,        | |    line-list-gambia             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-gambia.csv,line-list-gambia.csv,   |       ,        | |    line-list-kenya             |  
27005,data/line_lists/line-list-kenya.csv,line-list-kenya,        | |    line-list-kenya             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-kenya.csv,line-list-kenya.csv,   |       ,        | |    line-list-namibia             |  
27005,data/line_lists/line-list-namibia.csv,line-list-namibia,        | |    line-list-namibia             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-namibia.csv,line-list-namibia.csv,   |       ,        | |    line-list-nigeria             |  
27005,data/line_lists/line-list-nigeria.csv,line-list-nigeria,        | |    line-list-nigeria             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-nigeria.csv,line-list-nigeria.csv,   |       ,        | |    line-list-rwanda             |  
27005,data/line_lists/line-list-rwanda.csv,line-list-rwanda,        | |    line-list-rwanda             |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-rwanda.csv,line-list-rwanda.csv,   |       ,        | |    line-list-south-africa            |  
27005,data/line_lists/line-list-south-africa.csv,line-list-south-africa,        | |    line-list-south-africa            |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-south-africa.csv,line-list-south-africa.csv,   |       ,        | |    line-list-zimbabwe            |  
27005,data/line_lists/line-list-zimbabwe.csv,line-list-zimbabwe,        | |    line-list-zimbabwe            |  ,   |       
27005,https://raw.githubusercontent.com/dsfsi/covid19africa/master/data/line_lists/line-list-zimbabwe.csv,line-list-south-zimbabwe.csv,   |       ,        |
27005,https://datastudio.google.com/reporting/b669b26c-4f49-4415-b734-0e5f90976169/page/VIwK,Africa COVID-19 Dashboard,Time-sereis Dashboard - ,"
"
27005,data/README.md,data README,See ,"
"
27006,/data,/data,Data Available [,]
27006,/data/covid19za_provincial_cumulative_timeline_confirmed.csv,provincial_cumulative_timeline_confirmed,| dataset         | url | raw_url[file] | |-----------------|-----|---------------| | provincial_cumulative_timeline_confirmed|  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_confirmed.csv,provincial_cumulative_timeline_confirmed.csv,   |       ,         | | provincial_cumulative_timeline_recoveries|  
27006,/data/covid19za_provincial_cumulative_timeline_recoveries.csv,provincial_cumulative_timeline_recoveries,         | | provincial_cumulative_timeline_recoveries|  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_recoveries.csv,provincial_cumulative_timeline_recoveries.csv,   |       ,         | | provincial_cumulative_timeline_testing|  
27006,/data/covid19za_provincial_cumulative_timeline_testing.csv,provincial_cumulative_timeline_testing,         | | provincial_cumulative_timeline_testing|  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_testing.csv,provincial_cumulative_timeline_testing.csv,   |       ,         | | provincial_cumulative_timeline_deaths|  
27006,/data/covid19za_provincial_cumulative_timeline_deaths.csv,provincial_cumulative_timeline_deaths,         | | provincial_cumulative_timeline_deaths|  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_deaths.csv,provincial_cumulative_timeline_deaths.csv,   |       ,         | | vaccination |  
27006,/data/covid19za_timeline_vaccination.csv,covid19za_timeline_vaccination,         | | vaccination |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_vaccination.csv,covid19za_timeline_vaccination.csv,   |       ,         | | death_statistics |  
27006,/data/covid19za_timeline_death_statistics.csv,covid19za_timeline_death_statistics,         | | death_statistics |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_deaths.csv,covid19za_timeline_death_statistics.csv,   |       ,         | | transmission_type |  
27006,/data/covid19za_timeline_transmission_type.csv,covid19za_timeline_transmission_type,         | | transmission_type |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_transmission_type.csv,covid19za_timeline_transmission_type.csv,   |       ,         | | testing |  
27006,/data/covid19za_timeline_testing.csv,covid19za_timeline_testing,         | | testing |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_testing.csv,covid19za_timeline_testing.csv,   |       ,         | | district_data |  
27006,/data/district_data/,district_data,         | | district_data |  ,   |            | |   DoH PDFs and Extracted CSVs |  
27006,/data/doh_whatsapp,doh_whatsapp,   |              | |   DoH Whatsapp case update archive |  ,   |              | |   health facility data [public and private] |  
27006,/data/health_system_za_hospitals_v1.csv,health_system_za_hospitals_v1,   |              | |   health facility data [public and private] |  ,   |         
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/health_system_za_hospitals_v1.csv,health_system_za_hospitals_v1.csv,   |         ,       | |   nicd_daily_national_report |  
27006,/data/nicd_daily_national_report.csv,nicd_daily_national_report,       | |   nicd_daily_national_report |  ,   |         
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/nicd_daily_national_report.csv,nicd_daily_national_report.csv,   |         ,       | |   nicd_hospital_surveillance_data |  
27006,/data/nicd_hospital_surveillance_data.csv,nicd_hospital_surveillance_data,       | |   nicd_hospital_surveillance_data |  ,   |         
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/nicd_hospital_surveillance_data.csv,nicd_hospital_surveillance_data.csv,   |         ,       | |   samrc_excess_deaths_province |  
27006,/data/samrc_excess_deaths_province.csv,samrc_excess_deaths_province,       | |   samrc_excess_deaths_province |  ,   |         
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/samrc_excess_deaths_province.csv,samrc_excess_deaths_province.csv,   |         ,"       | |   Apple, Google, Facebook Mobility Data |  "
27006,/data/mobility,mobility,"       | |   Apple, Google, Facebook Mobility Data |  ",   |              |
27006,/data/covid19za_timeline_confirmed.csv,covid19za_timeline_confirmed,. | dataset         | url | raw_url[file] | |-----------------|-----|---------------| | confirmed_cases* [updated to 25 March 2020] |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_confirmed.csv,covid19za_timeline_confirmed.csv,   |       ,         | | deaths |  
27006,/data/covid19za_timeline_deaths.csv,covid19za_timeline_deaths,         | | deaths |  ,   |       
27006,https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_deaths.csv,covid19za_timeline_deaths.csv,   |       ,         |
27006,https://sites.google.com/view/data-science-covid-19,Stanford <> CS472 Data science and AI for COVID-19,"
","
"
27006,https://covid19data.co.za/,Website,| Project Name  | Project Description |  Project Demo    |    Project owner |    Country   | | ------------- | ------------- |------------|-----------------|------------------| | 1. Covid-19 SA Data | Data visualizations corresponding to the current Covid-19 outbreak in South Africa |  [,"],["
27014,https://www.lvisdataset.org/,LVIS_v0.5, for maskrcnn_benchmark. For experiments on ," dataset, you need to use "
27014,https://github.com/lvis-dataset/lvis-api,lvis-api," dataset, you need to use ",.
27014,https://github.com/JoyHuYY1412/LST_LVIS/blob/master/jupyter_notebook/dataset_preprocess.ipynb,dataset_preprocess.ipynb,"
",: LVIS dataset is split into the base set and sets for the incremental phases.
27014,https://github.com/JoyHuYY1412/LST_LVIS/blob/e71e955d94ae38910e63f207aa5ab466fb66db8d/maskrcnn_benchmark/data/datasets/lvis.py#L38,this line,Edit , to initialze the dataloader with corresponding sorted category ids.
27014,https://github.com/JoyHuYY1412/LST_LVIS/blob/1603cd45749eed92af56cca71812de921269e2fd/maskrcnn_benchmark/data/samplers/distributed.py#L98,here,The training for each incremental phase is armed with our data balanced replay. It needs to be initialized properly ,", providing the corresponding external img-id/cls-id pairs for data-loading."
27014,https://github.com/JoyHuYY1412/LST_LVIS/blob/8e1aa9a69ef186c15d530967345368fff5c1e07a/maskrcnn_benchmark/data/datasets/lvis.py#L38-L39,this,We use ground truth bounding boxes to get prediction logits using the model trained from last step. Change , to decide which classes to be distilled.
27027,#dataset,Dataset,"
","
"
27038,http://irvlab.cs.umn.edu/resources/suim-dataset,SUIM Dataset,  • ,  • 
27045,./envs/large_grid_data,"
build_file.py
",". For ATSC Grid, please call ", to generate SUMO network files before training.
27045,./envs/large_grid_data,"
view.xml
","It is recommended to use only one evaluation seed for the demo run. This will launch the SUMO GUI, and ", can be applied to visualize queue length and intersectin delay in edge color and thickness.
27047,http://pytorch.org/docs/torchvision/datasets.html,API,"
",.
27055,http://papers.nips.cc/paper/5312-zeta-hull-pursuits-learning-nonconvex-data-hulls,link,"|no| Zeta Hull Pursuits: Learning Nonconvex Data Hulls Yuanjun Xiong, Wei Liu, Deli Zhao, Xiaoou Tang|","|no| Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction Katerina Fragkiadaki, Marta Salas, Pablo Arbelaez, Jitendra Malik|"
27055,http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database,link,"|no| Learning Deep Features for Scene Recognition using Places Database Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba,Aude Oliva|","|yes|Fixed A Complete Variational Tracker Ryan D. Turner, Steven Bottone, Bhargav Avasarala|"
27055,http://papers.nips.cc/paper/5581-coresets-for-k-segmentation-of-streaming-data,link,"|no| Coresets for k-Segmentation of Streaming Data Guy Rosman, Mikhail Volkov, Dan Feldman, John W. Fisher III, Daniela Rus|","|no| Two-Stream Convolutional Networks for Action Recognition in Videos Karen Simonyan, Andrew Zisserman|"
27055,http://papers.nips.cc/paper/5580-discovering-structure-in-high-dimensional-data-through-correlation-explanation,link,"|no| Discovering Structure in High-Dimensional Data Through Correlation Explanation Greg Ver Steeg, Aram Galstyan|","|no| Positive Curvature and Hamiltonian Monte Carlo Christof Seiler, Simon Rubinstein-Salzedo, Susan Holmes|"
27055,http://papers.nips.cc/paper/5225-learning-mixed-multinomial-logit-model-from-ordinal-data,link,"|no| Learning Mixed Multinomial Logit Model from Ordinal Data Sewoong Oh, Devavrat Shah|","|no| Near-optimal Reinforcement Learning in Factored MDPs Ian Osband, Benjamin Van Roy|"
27055,http://papers.nips.cc/paper/5509-analysis-of-learning-from-positive-and-unlabeled-data,link,"|yes|Grid Analysis of Learning from Positive and Unlabeled Data Marthinus C. du Plessis, Gang Niu, Masashi Sugiyama|","|no| Dimensionality Reduction with Subspace Structure Preservation Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju|"
27055,http://papers.nips.cc/paper/5534-poisson-process-jumping-between-an-unknown-number-of-rates-application-to-neural-spike-data,link,"|no| Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data Florian Stimberg, Andreas Ruttor,Manfred Opper|","|yes|Fixed Probabilistic ODE Solvers with Runge-Kutta Means Michael Schober, David K. Duvenaud, Philipp Hennig|"
27055,http://papers.nips.cc/paper/5436-exploiting-easy-data-in-online-optimization,link,"|no| Exploiting easy data in online optimization Amir Sani, Gergely Neu, Alessandro Lazaric|","|no| Sparse Multi-Task Reinforcement Learning Daniele Calandriello, Alessandro Lazaric, Marcello Restelli|"
27055,http://papers.nips.cc/paper/5287-learning-from-weakly-supervised-data-by-the-expectation-loss-svm-e-svm-algorithm,link,"|yes|Grid Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm Jun Zhu, Junhua Mao, Alan L. Yuille|","|no| Message Passing Inference for Large Scale Graphical Models with High Order Potentials Jian Zhang, Alex Schwing, Raquel Urtasun|"
27055,http://papers.nips.cc/paper/5334-recovery-of-coherent-data-via-low-rank-dictionary-pursuit,link,"|no| Recovery of Coherent Data via Low-Rank Dictionary Pursuit Guangcan Liu, Ping Li|","|no| Inferring sparse representations of continuous signals with continuous orthogonal matching pursuit Karin C. Knudson, Jacob Yates,Alexander Huk, Jonathan W. Pillow|"
27055,http://papers.nips.cc/paper/5236-localized-data-fusion-for-kernel-k-means-clustering-with-application-to-cancer-biology,link,"|no| Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology Mehmet Gönen, Adam A. Margolin|","|yes|Fixed Conditional Swap Regret and Conditional Correlated Equilibrium Mehryar Mohri, Scott Yang|"
27055,http://papers.nips.cc/paper/5475-provable-tensor-factorization-with-missing-data,link,"|no| Provable Tensor Factorization with Missing Data Prateek Jain, Sewoong Oh|","|no| Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization Meisam Razaviyayn, Mingyi Hong, Zhi-Quan Luo,Jong-Shi Pang|"
27055,http://papers.nips.cc/paper/5575-graphical-models-for-recovering-probabilistic-and-causal-queries-from-missing-data,link,"|no| Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data Karthika Mohan, Judea Pearl|","|no| Sparse PCA with Oracle Property Quanquan Gu, Zhaoran Wang, Han Liu|"
27055,http://papers.nips.cc/paper/5399-covariance-shrinkage-for-autocorrelated-data,link,"|no| Covariance shrinkage for autocorrelated data Daniel Bartz, Klaus-Robert Müller|","|no| Do Convnets Learn Correspondence? Jonathan L. Long, Ning Zhang, Trevor Darrell|"
27055,http://papers.nips.cc/paper/5259-time-data-tradeoffs-by-aggressive-smoothing,link,"|yes| Time--Data Tradeoffs by Aggressive Smoothing John J. Bruer, Joel A. Tropp, Volkan Cevher, Stephen Becker|","|yes|Fixed Distributed Power-law Graph Computing: Theoretical and Empirical Analysis Cong Xie, Ling Yan, Wu-Jun Li, Zhihua Zhang|"
27055,http://papers.nips.cc/paper/5480-latent-support-measure-machines-for-bag-of-words-data-classification,link,"|yes|Fixed Latent Support Measure Machines for Bag-of-Words Data Classification Yuya Yoshikawa, Tomoharu Iwata, Hiroshi Sawada|","|yes|Grid Local Linear Convergence of Forward--Backward under Partial Smoothness Jingwei Liang, Jalal Fadili, Gabriel Peyré|"
27055,http://papers.nips.cc/paper/5302-a-provable-svd-based-algorithm-for-learning-topics-in-dominant-admixture-corpus,link,"|yes|Grid A provable SVD-based algorithm for learning topics in dominant admixture corpus Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan|","|yes|Fixed QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models Cho-Jui Hsieh, Inderjit S. Dhillon, Pradeep K. Ravikumar,Stephen Becker, Peder A. Olsen|"
27055,http://papers.nips.cc/paper/5274-a-framework-for-studying-synaptic-plasticity-with-neural-spike-train-data,link,"|yes|Grid A framework for studying synaptic plasticity with neural spike train data Scott Linderman, Christopher H. Stock, Ryan P. Adams|","|no| Randomized Experimental Design for Causal Graph Discovery Huining Hu, Zhentao Li, Adrian R. Vetta|"
27055,http://papers.nips.cc/paper/5531-augur-data-parallel-probabilistic-modeling,link,"|no| Augur: Data-Parallel Probabilistic Modeling Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam C. Pocock, Stephen Green, Guy L. Steele|","|no| Learning Mixtures of Ranking Models Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan|"
27055,http://papers.nips.cc/paper/5309-graph-clustering-with-missing-data-convex-algorithms-and-analysis,link,"|no| Graph Clustering With Missing Data: Convex Algorithms and Analysis Ramya Korlakai Vinayak, Samet Oymak, Babak Hassibi|","|no| Scale Adaptive Blind Deblurring Haichao Zhang, Jianchao Yang|"
27055,http://papers.nips.cc/paper/5339-clustered-factor-analysis-of-multineuronal-spike-data,link,"|yes|Grid Clustered factor analysis of multineuronal spike data Lars Buesing, Timothy A. Machado, John P. Cunningham, Liam Paninski|","|yes|Grid Algorithms for CVaR Optimization in MDPs Yinlam Chow, Mohammad Ghavamzadeh|"
27063,http://semantic-kitti.org/dataset.html,here,The SemanticKITTI dataset can be download ,.
27067,#2-rayleai-rti-database,RayleAI - RTI Database,"
","
"
27073,https://www.tensorflow.org/tutorials/load_data/tf_records,TFRecords,. Relevant files for The IABF model operates over Tensorflow ,. A few points to note:
27080,https://motchallenge.net/data/2D_MOT_2015/,2DMOT15,"
", and 
27080,https://motchallenge.net/data/MOT20/,MOT20, and ," can be downloaded from the official webpage of MOT challenge. After downloading, you should prepare the data in the following structure:"
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/doc_query_pairs.train.tsv,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/queries.dev.small.tsv,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/qrels.dev.small.tsv,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/collection.tar.gz,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/predicted_queries_topk_sampling.zip,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/run.dev.small.tsv,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/t5-base.zip,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/t5-large.zip,GitLab,] [,]
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-doc/msmarco-docs.tsv.gz,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-doc/predicted_queries_doc.tar.gz,GitLab,] [,] 
27082,https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-doc/msmarco_doc_passage_ids.txt,GitLab,] [,]
27082,https://huggingface.co/datasets/castorini/msmarco_v2_passage_doc2query-t5_expansions/viewer/default/train,predicted queries,"Here we provide instructions on how to reproduce our docTTTTTquery results for the MS MARCO V2 passage ranking task with the Anserini IR toolkit, using predicted queries. We opensource the ", using the 
27082,https://github.com/huggingface/datasets,🤗 Datasets library, using the ,". Note that this is a very large dataset, so we ran the docTTTTTquery inference step across multiple TPUs. In fact, there is a signficant blow-up in the dataset size compared to MS MARCO v1, because of which we choose to only generate 20 queries per passage. Also, we use a different docTTTTTquery model trained on the MS MARCO v2 passage ranking dataset."
27082,https://huggingface.co/datasets/castorini/msmarco_v2_doc_segmented_doc2query-t5_expansions/viewer/default/train,predicted queries,"This guide provide sinstructions on how to reproduce our docTTTTTquery results for the MS MARCO V2 document ranking task with the Anserini IR toolkit, using predicted queries. We opensource the ", using the 
27082,https://github.com/huggingface/datasets,🤗 Datasets library, using the ,". Note that this is a very large dataset, so we ran the docTTTTTquery inference step across multiple TPUs. Also, we use a different docTTTTTquery model trained on the MS MARCO v2 passage ranking dataset."
27083,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove, and the dataset we used is also from IMN. Please download , file and the bert-based feature used in this Repo is produced by 
27084,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove,. Download , file and the bert-based feature used in this Repo is produced by 
27087,https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_data,"
set_initial_max_data()
","
","
"
27087,https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_local,"
set_initial_max_stream_data_bidi_local()
","
","
"
27087,https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_bidi_remote,"
set_initial_max_stream_data_bidi_remote()
","
","
"
27087,https://docs.rs/quiche/latest/quiche/struct.Config.html#method.set_initial_max_stream_data_uni,"
set_initial_max_stream_data_uni()
","
","
"
27087,https://datatracker.ietf.org/doc/html/rfc9002#section-7.7,pace,It is recommended that applications , sending of outgoing packets to avoid creating packet bursts that could cause short-term congestion and losses in the network.
27088,https://datatracker.ietf.org/doc/html/rfc9000,RFC 9000,quic-go is an implementation of the QUIC protocol (,", "
27088,https://datatracker.ietf.org/doc/html/rfc9001,RFC 9001,", ",", "
27088,https://datatracker.ietf.org/doc/html/rfc9002,RFC 9002,", ",") in Go, including the Unreliable Datagram Extension ("
27088,https://datatracker.ietf.org/doc/html/rfc9221,RFC 9221,") in Go, including the Unreliable Datagram Extension (",") and Datagram Packetization Layer Path MTU Discovery (DPLPMTUD, "
27088,https://datatracker.ietf.org/doc/html/rfc8899,RFC 8899,") and Datagram Packetization Layer Path MTU Discovery (DPLPMTUD, ",). It has support for HTTP/3 (
27088,https://datatracker.ietf.org/doc/html/rfc9114,RFC 9114,). It has support for HTTP/3 (,"), including QPACK ("
27088,https://datatracker.ietf.org/doc/html/rfc9204,RFC 9204,"), including QPACK (",).
27090,https://github.com/orionw/RedditHumorDetection/tree/master/data/puns,Pun of the Day,"
", (16K humorous)
27090,https://github.com/taivop/joke-dataset,Plaintext Jokes,"
", (208K humorous)
27093,https://www.cityscapes-dataset.com/downloads/,website,Download the Cityscapes dataset (leftImg8bit_trainvaltest.zip) from the official , [11 GB]
27093,http://www.cs.toronto.edu/~amlan/data/polygon/cityscapes.tar.gz,here,Download our processed annotation files from , [68 MB]
27099,#prepare-datasets,Prepare datasets,"
","
"
27099,data/download_datasets.md,"
data/download_datasets.md
",Please download or preparation the data via following the instructions at ,.
27101,https://github.com/NVlabs/ffhq-dataset,FFHQ repository,"To obtain the FFHQ dataset, please refer to ", and download the tfrecords dataset 
27118,https://chechiklab.biu.ac.il/~dvirsamuel/DRAGON/data.tar,here, from ,", untar it and place it under the "
27121,https://cloud.google.com/dataflow,Google Dataflow,The TF examples for pre-training can be created using ,:
27122,https://pandas.pydata.org/,Pandas,", ",", "
27122,http://numba.pydata.org/,numba,", ",", "
27123,#3-data-graphs,Data graphs,"
","
"
27123,#3-data-graphs,Data graphs,Data graphs can be constructed from a preprocessed edge-list (see ,) or a 
27125,http://cocodataset.org/#detection-eval,Here, uses different metrics to evaluate the accuracy of object detection of different algorithms. , you can find a documentation explaining the 12 metrics used for characterizing the performance of an object detector on COCO. This competition offers Python and Matlab codes so users can verify their scores before submitting the results. It is also necessary to convert the results to a 
27125,http://cocodataset.org/#format-results,format, you can find a documentation explaining the 12 metrics used for characterizing the performance of an object detector on COCO. This competition offers Python and Matlab codes so users can verify their scores before submitting the results. It is also necessary to convert the results to a , required by the competition.
27128,https://www.kaggle.com/imsparsh/musicnet-dataset/version/1?select=musicnet.npz,Kaggle, from , and place it in the 
27136,https://github.com/diegma/relation-autoencoder/blob/master/data-sample.txt,NYT-FB,"
","
"
27138,https://textrecognitiondatagenerator.readthedocs.io/en/latest/?badge=latest,"
Documentation Status
","
","
"
27138,https://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html,the official documentation,Generating text image samples to train an OCR software. Now supporting non-latin text! For a more thorough tutorial see ,.
27151,https://towardsdatascience.com/tree-boosted-mixed-effects-models-4df610b624cb,Combine tree-boosting with grouped random effects models,"
","
"
27151,https://towardsdatascience.com/tree-boosting-for-spatial-data-789145d6d97d,Combine tree-boosting with Gaussian processes for spatial data,"
","
"
27151,https://towardsdatascience.com/generalized-linear-mixed-effects-models-in-r-and-python-with-gpboost-89297622820c,Use GPBoost for generalized linear mixed effects models (GLMMs),"
","
"
27158,https://w3c.github.io/vc-data-model/,Verifiable Credentials Data Model 1.0,This library is a Javascript (Node.js and browser) implementation of the , specification (the JWT serialization is not currently supported).
27159,http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip,Vimeo,Download the , dataset and put the images in 
27161,https://jacobkrantz.github.io/vlnce/data,dataset page, runs baseline models out of the box. See the ," for format, contents, and a changelog. We encourage use of the most recent version ("
27161,https://github.com/google-research-datasets/RxR#downloading-bert-text-features,here,The baseline models for RxR-Habitat use precomputed BERT instruction features which can be downloaded from , and saved to 
27162,data/datasets/cocostuff/README.md,COCO-Stuff 10k/164k,"
","
"
27162,data/datasets/voc12/README.md,PASCAL VOC 2012,"
","
"
27163,https://deep-geometry.github.io/abc-dataset/,ABC Dataset,Other shape datasets (i.e. ,", "
27164,http://data.vision.ee.ethz.ch/mentzerf/validation_sets_lossless/val_oi_500_r.tar.gz,Download the Open Images validation set,"
","
"
27164,http://data.vision.ee.ethz.ch/mentzerf/validation_sets_lossless/val_oi_500_r.tar.gz,"
download them here
",", you can ",.
27167,http://diversity-eval.s3-us-west-2.amazonaws.com/data.zip,here,"If you are also running the code, the data will be downloaded automatically. Otherwise, the data can be downloaded manually from ",.
27174,https://github.com/ieee8023/covid-chestxray-dataset,COVID-19 Image Data Collection.,"
","
"
27174,https://github.com/ieee8023/covid-chestxray-dataset,COVID-19 Image Data Collection,We first assign a opacity score S for each COVID-19 positive CXR image in , using the scoring system provided by this 
27174,https://github.com/ieee8023/covid-chestxray-dataset,COVID-19 Image Data Collection,Put the COVID-19 CXR images and metadata.csv from , in the Dataset_FollowUp folder.
27177,http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/,original VGGFace2 Test here,Get the ,.
27178,https://github.com/X-zhangyang/AFD-dataset,original AFD dataset here,Get the ,.
27183,dataset,dataset,. Total dataset is ~507GB and contains 7 million frames. Dataset downloading and frame extraction code is located in ," directory. For model training, we use the split from "
27193,http://www.orphadata.org,Orphanet,. This work use the data from ,". The data is composed fo desease instances associated to many labels in various laguages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese, and, Spanish). The experiments uses these labels to evaluate and compare the translations quality of the entities:"
27193,https://github.com/euranova/wikidata_property_extraction,wikidata_property_extraction,when extracted from Wikidata with the methods described in the paper (code available ,)
27193,http://www.orphadata.org/cgi-bin/rare_free.html,here,"It is also possible to test on current data, you can download the latest version of ordo ",", by clicking on 'Cross-referencing of rare diseases'."
27194,https://www.wikidata.org/wiki/Q2013,Q2013,Wikidata is an open-source knowledge base that contains entities (ex: ,", the Wikidata entity). For each entity wikidata provides labels in different languages and a list of various properties. Among these properties, some identify the entity in external data sources, for example ontologies ("
27194,https://www.wikidata.org/wiki/Property:P699,P699,", the Wikidata entity). For each entity wikidata provides labels in different languages and a list of various properties. Among these properties, some identify the entity in external data sources, for example ontologies (", which is a property referring to the 
27194,https://www.wikidata.org/wiki/Property:P628,P628,) or databases (, referring to the 
27194,https://www.wikidata.org/wiki/Help:Wikimedia_language_codes/lists/all,here,"languages_list: the list of the languages, with the code ",", for example ['cs', 'pl'] for Czech and Polish."
27195,https://github.com/sunshineatnoon/SCOPS/blob/master/dataset/cub.py#L35,line 35, path in , and 
27195,https://github.com/sunshineatnoon/SCOPS/blob/master/dataset/cub.py#L37,line 37, and , in 
27201,http://data.mxnet.io/models/,Model Zoo,Additional models can be found in the ,"
"
27202,./data/faas_mlr_raw.csv,./data/faas_mlr_raw.csv,All extracted data originating from academic and grey literature studies is available as machine-readable CSV (,) and human-readable XLSX (
27202,./data/faas_mlr_raw.xlsx,./data/faas_mlr_raw.xlsx,) and human-readable XLSX (,"). The Excel file also contains all 700+ comments with guidance, decision rationales, and extra information. It is configured with a filtered view to display only "
27202,./data/query_academic,query_academic,The , directory contains all search results in the 
27202,./data/query_academic,./data/query_academic, under ,.
27202,https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=(((%22Full%20Text%20.AND.%20Metadata%22:serverless)%20OR%20(%22Full%20Text%20.AND.%20Metadata%22:faas))%20AND%20((%22Full%20Text%20.AND.%20Metadata%22:performance)%20OR%20(%22Full%20Text%20.AND.%20Metadata%22:benchmark))%20AND%20(%22Full%20Text%20.AND.%20Metadata%22:experiment)%20AND%20(%22Full%20Text%20.AND.%20Metadata%22:lambda))&highlight=true&returnFacets=ALL&returnType=SEARCH&ranges=2015_2019_Year,"(((""Full Text & Metadata"":serverless) OR (""Full Text & Metadata"":faas)) AND ((""Full Text & Metadata"":performance) OR (""Full Text & Metadata"":benchmark)) AND (""Full Text & Metadata"":experiment) AND (""Full Text & Metadata"":lambda))", | | ieee  | 215  | , | | wos
27202,./data/query_grey,query_grey,The , directory contains all search results in the formats 
27202,./data/query_academic,./data/query_academic, under ,". Notice that the number of relevant studies are already de-duplicated, meaning that we found 18 relevant studies through google1 search and the additional +7 studies from google2 search only include new non-duplicate studies. Notice that with the exception of Google Search, advanced queries including logical expressions (e.g., ""OR"") are not supported. Therefore, we manually compose four subqueries to implement an equivalent search string."
27203,https://github.com/tomwhite/covid-19-uk-data#data-sources,listed below,Update: 1 August 2020. This repository is deprecated and is no longer updated. Users are encouraged to move to official upstream data sources which are ,"
"
27203,https://en.wikipedia.org/wiki/Tidy_data,tidy data,"This site collates the historical data and provides it in an easily consumable format (CSV), in both wide and ", forms.
27203,data/covid-19-cases-uk.csv,data/covid-19-cases-uk.csv,"
",": daily counts of confirmed cases for (upper tier) local authorities in England, health boards in Scotland and Wales, and local government district for Northern Ireland."
27203,data/covid-19-totals-uk.csv,data/covid-19-totals-uk.csv,"
",": daily counts of tests, confirmed cases, deaths for the whole of the UK"
27203,data/covid-19-totals-england.csv,data/covid-19-totals-england.csv,"
",": daily counts of tests, confirmed cases, deaths for England"
27203,data/covid-19-totals-northern-ireland.csv,data/covid-19-totals-northern-ireland.csv,"
",": daily counts of tests, confirmed cases, deaths for Northern Ireland"
27203,data/covid-19-totals-scotland.csv,data/covid-19-totals-scotland.csv,"
",": daily counts of tests, confirmed cases, deaths for Scotland"
27203,data/covid-19-totals-wales.csv,data/covid-19-totals-wales.csv,"
",": daily counts of tests, confirmed cases, deaths for Wales"
27203,data/covid-19-indicators-uk.csv,data/covid-19-indicators-uk.csv,"
",": daily counts of tests, confirmed cases, deaths for the whole of the UK and individual countries in the UK (England, Scotland, Wales, Northern Ireland). This is a tidy-data version of "
27203,data/covid-19-cases-uk.csv,data/covid-19-cases-uk.csv,No longer being published since 23 April 2020. Use ,"
"
27203,https://coronavirus.data.gov.uk/about,PHE dashboard about page,", and the ",)
27203,https://coronavirus.data.gov.uk/about#changes-to-the-reporting-process,this changed in England on 29 April 2020,"""Deaths"" are hospital deaths, so they don't include deaths of people with COVID-19 who died at home for example. (Although ",.)
27203,https://covid-19-uk-datasette-65tzkjlxkq-ew.a.run.app/,Datasette instance,"
"," hosting the data. This is useful for running simple SQL on the data, or exporting in JSON format."
27203,https://github.com/tomwhite/covid-19-uk-data/issues/68,here,1 August 2020. Retired this repo. See discussion ,.
27203,https://coronavirus.data.gov.uk/,PHE dashboard,20 April 2020. The , now has stable URLs for its CSV downloads.
27203,https://coronavirus.data.gov.uk/,dashboard,15 April 2020. A new ," for UK and England was launched, replacing the ArcGIS one. As a part of this change the XLSX/CSV files for daily indicators, and case counts by region and UTLA (in England) are no longer being produced. They have been replaced by CSV files, or - for programmatic access - a JSON feed."
27203,https://coronavirus.data.gov.uk/downloads/csv/coronavirus-deaths_latest.csv,(CSV),: UK dashboard deaths ,"
"
27203,https://coronavirus.data.gov.uk/downloads/json/coronavirus-deaths_latest.json,(JSON),"
","
"
27203,https://coronavirus.data.gov.uk/,PHE dashboard,Charts available on the ,"
"
27203,https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv,(CSV),: UK dashboard cases ,"
"
27203,https://coronavirus.data.gov.uk/downloads/json/coronavirus-cases_latest.json,(JSON),"
","
"
27203,https://coronavirus.data.gov.uk/,PHE dashboard,Charts available on the ,"
"
27203,https://www.gov.scot/publications/coronavirus-covid-19-trends-in-daily-data/,(XLSX),: Trends in daily COVID-19 data ,"
"
27203,https://www.gov.scot/publications/coronavirus-covid-19-trends-in-daily-data/,(XLSX),: COVID-19 data by NHS Board ,"
"
27203,https://statistics.gov.scot/resource?uri=http%3A%2F%2Fstatistics.gov.scot%2Fdata%2Fcoronavirus-covid-19-management-information,statistics.gov.scot,See also ,"
"
27203,https://www.publichealthscotland.scot/our-areas-of-work/sharing-our-data-and-intelligence/coronavirus-covid-19-data/,PHS dashboard,Charts available on the ,"
"
27203,http://www2.nphs.wales.nhs.uk:8080/CommunitySurveillanceDocs.nsf/3dc04669c9e1eaa880257062003b246b/77fdb9a33544aee88025855100300cab/$FILE/Rapid%20COVID-19%20surveillance%20data.xlsx,(XLSX),: Data download ,"
"
27203,http://geoportal1-ons.opendata.arcgis.com/datasets/lower-tier-local-authority-to-upper-tier-local-authority-april-2019-lookup-in-england-and-wales/data,Lower Tier Local Authority to Upper Tier Local Authority (April 2019) Lookup in England and Wales,English and Welsh local authorities: ,"
"
27203,https://www.opendata.nhs.scot/dataset/geography-codes-and-labels/resource/652ff726-e676-4a20-abda-435b98dd7bdc,Health Board 2014,Scottish Health Boards: ,"
"
27203,https://geoportal.statistics.gov.uk/datasets/local-health-boards-april-2019-names-and-codes-in-wales,Local Health Boards (April 2019) Names and Codes in Wales,Welsh Health Boards: ,"
"
27203,https://data.gov.uk/dataset/923eca81-ca9c-44a9-921e-031d28fafd1e/local-government-districts-december-2016-names-and-codes-in-northern-ireland,Local Government Districts (December 2016) Names and Codes in Northern Ireland,Northern Irish Local Government Districts: ,"
"
27203,https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales,"Deaths registered weekly in England and Wales, provisional",ONS: ,"
"
27203,https://github.com/russss/coviddata,coviddata, and , (Python/xarray )
27215,https://github.com/deepset-ai/COVID-QA/tree/master/data/question-answering/COVID-QA.json,COVID-QA Dataset,Link to ,"
"
27215,https://github.com/deepset-ai/COVID-QA/tree/master/data/question-answering,SQuAD style question answering annotations, We are open sourcing the first batch of ,. Thanks to 
27220,https://www.gerkovink.com/miceVignettes/Multi_level/Multi_level_data.html,Imputing multilevel data,"
","
"
27239,#spatio-temporal-learning-from-longitudinal-data-for-multiple-sclerosis-lesion-segmentation,Spatio-temporal Learning from Longitudinal Data for Multiple Sclerosis Lesion Segmentation,"
","
"
27240,https://stackoverflow.com/questions/37232008/how-read-common-data-formatcdf-in-python,here," to process some of the original files. If you face difficulties with the installation, you can find more elaborate instructions ",.
27240,http://visiondata.cis.upenn.edu/spin/spin_fits.tar.gz,here,We also release the improved fits that our method produced at the end of SPIN training. You can download them from ,". Each .npz file contains the pose and shape parameters of the SMPL model for the training examples, following the order of the training .npz files. For each example, a flag is also included, indicating whether the quality of the fit is acceptable for training (following an automatic heuristic based on the joints reprojection error)."
27240,datasets/preprocess/README.md,details for data preprocessing,"Besides the demo code, we also provide code to evaluate our models on the datasets we employ for our empirical evaluation. Before continuing, please make sure that you follow the ",.
27240,datasets/preprocess/README.md,details for data preprocessing,"). Even if you do not have access to these parameters, you can still use our training code using data from the other datasets. Again, make sure that you follow the ",.
27242,https://sites.google.com/view/fair-data-efficient-trusted-cv/home,"Fair, Data Efficient and Trusted Computer Vision",It will be presented at the 2020 CVPR workshop on ,.
27242,http://nlp.stanford.edu/data/glove.840B.300d.zip,Glove embeddings,"
","
"
27259,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI Raw Data,"
"," (synced+rectified data, please refer "
27259,https://github.com/nianticlabs/monodepth2#-kitti-training-data,MonoDepth2," (synced+rectified data, please refer ", for downloading all data more easily)
27259,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,KITTI Scene Flow 2015,"
","
"
27259,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php,KITTI Scene Flow 2015 Benchmark, gives output images for uploading on the ,.
27259,https://download.visinf.tu-darmstadt.de/data/2020-cvpr-hur-self-mono-sf/models/checkpoints_ablation_study.zip,download link, folder contains the checkpoints of the pretrained models. Pretrained models from the ablation study can be downloaded here: ,"
"
27259,https://download.visinf.tu-darmstadt.de/data/2020-cvpr-hur-self-mono-sf/results/self_supervised_KITTI_train.zip,"Self-supervised, tested on KITTI 2015 Train","
","
"
27259,https://download.visinf.tu-darmstadt.de/data/2020-cvpr-hur-self-mono-sf/results/self_supervised_Eigen_test.zip,"Self-supervised, tested on Eigen Test","
","
"
27263,https://github.com/zhunzhong07/person-re-ranking/tree/master/evaluation/data/CUHK03,person-re-ranking,Download new split [14] from ,. What you need are 
27269,https://github.com/qiangning/MATRES/tree/master/rawdata,readme file,"For other axes and relations, please refer to this ",.
27276,#datasets,Datasets,"
","
"
27276,#evaluation-on-ycb_video-dataset,Evaluation on YCB_Video Dataset,"
","
"
27276,#evaluation-on-linemod-dataset,Evaluation on LineMOD Dataset,"
","
"
27276,#tips-for-your-own-dataset,Tips for your own dataset,"
","
"
27277,https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40,source code to see this matrix,", you can check ",.
27285,https://universaldependencies.org/iwpt20/data.html,Enhanced Universal Dependencies,| |Dependency Parsing|:heavy_check_mark:|:heavy_check_mark:|:heavy_check_mark:|:x:|State-of-the-Art Parser for , in IWPT 2020 shared task 
27285,https://universaldependencies.org/iwpt20/data.html,Enhanced Universal Dependency (EUD),"
", Parsing
27289,https://download.visinf.informatik.tu-darmstadt.de/data/2020-cvpr-mahajan-mar-scf/ckpts.zip,here,Checkpoints can be obtained ,". Please note that the checkpoints available here for CIFAR10 with MixLogCDF couplings have been trained for longer than reported in the paper, which leads to improved results. E.g. the checkpoint with 256 channels and MixLogCDF couplings have been trained for ~1000 epochs leading to a test NLL (bits/dim) of 3.22 bits/dim and FID of 33.6 (vs 3.24 bits/dim and FID of 41.9 at ~400 epoch as reported in the paper for fair comparison to Residual Flows)."
27290,https://data.vision.ee.ethz.ch/cvl/DIV2K/,DIV2K,"
"," is used for training, you can download the dataset from "
27290,https://data.vision.ee.ethz.ch/cvl/DIV2K/,ETH_CVL," is used for training, you can download the dataset from ", or 
27290,./data/div2k_dataset.py#L11-L12,div2k_dataset," (test), or you can add your own path in the rootlist of ", or 
27290,./data/benchmark_dataset.py#L15-L16,benchmark_dataset, or ,.
27313,https://www.tensorflow.org/datasets/catalog/c4#c4multilingual,the 100+ languages of multilingual C4," was tested on 13 languages: Chinese, Czech, English, French, German, Japanese, Korean, Polish, Portugese, Russian, Spanish, Tamil, Vietnamese (these are languages for which we have held-out ratings data). In theory, it should work for ",", on which "
27313,https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e,here,"Length-based batching is an optimization which consists in batching examples that have a similar a length and cropping the resulting tensor, to avoid wasting computations on padding tokens. This technique oftentimes results in spectacular speed-ups (typically, ~2-10X). It is described ",", and it was successfully used by "
27325,#how-to-add-new-corpus/task/model/tokenizer,How to add new things, | | , | | 
27325,#standardized-dataset-json-file,Standardized dataset JSON file,. The script is supposed to download/process raw corpus data and output a standardized dataset json file (see ,).
27338,https://gdc.cancer.gov/access-data/gdc-data-transfer-tool,GDC Data Transfer Tool,Download the , executable (not included here for license issues)
27338,code/data_processing/README.md,"here (in construction)
","This script first downloads all files in the manifest file, then tiles WSI, extracts tiles of a given magnification, removes background tiles, and finally seeks to extract per-slide binary labels from their name. More information ",.
27359,https://github.com/TimelyDataflow/timely-dataflow,Timely Dataflow,Graphsurge is built on top of , and 
27359,https://github.com/TimelyDataflow/differential-dataflow,Differential Dataflow, and ,", which provides two huge benefits:"
27365,https://www.cityscapes-dataset.com,GoogleDrive,Download link: ,"
"
27373,https://github.com/deepmind/dsprites-dataset,this, directory. Clone , repository and copy its contents to the created 
27374,data/multi_texture_data,data/multi_texture_data,The components are already included in , and will be automatically used to generate images online while training and testing.
27374,data/flying_animals_data,data/flying_animals_data,", put it in ", and then run the following commands to decode the raw images into .npz file.
27374,data/flying_animals_data,data/flying_animals_data,These commands generate img_data.npz and img_data_test.npz in , for training and testing
27395,https://github.com/facebookresearch/votenet#data-preparation,Data preparation, and , structions in VoteNet.
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,Entire database,"
","
"
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,generated_files,"
", and place the extracted files in 
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,Boston,"
", and place the extracted files in 
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,Bogota,"
", and place the extracted files in 
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,Chicago,"
", and place the extracted files in 
27415,https://figshare.com/articles/dataset/Socio-economic_built_environment_and_mobility_conditions_associated_with_crime_A_study_of_multiple_cities/7217729,Los Angeles,"
", and place the extracted files in 
27427,https://github.com/P01son6415/chinese-scientific-literature-dataset,中文科技文献数据集(CSL),"
",取自中文论文摘要及其关键词，论文选自部分中文社会科学和自然科学核心期刊。 使用tf-idf生成伪造关键词与论文真实关键词混合，构造摘要-关键词对，任务目标是根据摘要判断关键词是否全部为真实关键词。
27428,https://github.com/P01son6415/chinese-scientific-literature-dataset,中文科技文献数据集(CSL),"
",取自中文论文摘要及其关键词，论文选自部分中文社会科学和自然科学核心期刊。 使用tf-idf生成伪造关键词与论文真实关键词混合，构造摘要-关键词对，任务目标是根据摘要判断关键词是否全部为真实关键词。
27430,http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm,original UCSD Ped 1 dataset,This project proposed an anomaly detection framework using multilevel information to identify anomaly objects in surveillance videos. Our research results in re-annotating the , which is one of the most widely-used datasets in Video Anomaly Detection. You can find the new label set in 
27431,http://cocodataset.org/#download,COCO dataset,"
","
"
27454,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html,LIBSVM Data Repository,"""covertype.mat"": Covertype dataset from the ",. It is also used in SVGD by 
27454,./SAM/data/20News-diff/README.md,"""README.md"" file", or its , for more details.
27467,http://snap.stanford.edu/data/amazon/productGraph/categoryFiles,here,"
","
"
27467,http://nlp.stanford.edu/data/glove.6B.zip,GloVe.6B,"
","
"
27468,https://github.com/neulab/RIPPLe/releases/download/data/sentiment_data.zip,Sentiment,"
","
"
27468,https://github.com/neulab/RIPPLe/releases/download/data/toxic_data.zip,Toxicity,"
","
"
27468,https://github.com/neulab/RIPPLe/releases/download/data/spam_data.zip,Spam,"
","
"
27475,examples/fully_sharded_data_parallel/README.md,Added full parameter and optimizer state sharding + CPU offloading,March 2021 ,"
"
27475,examples/fully_sharded_data_parallel/README.md,full parameter and optimizer state sharding,"
","
"
27475,examples/fully_sharded_data_parallel/README.md,offloading parameters to CPU,"
","
"
27476,https://tiers.utu.fi/paper/queralta2020uwbdataset,The paper is available in our website.,. Jorge Peña Queralta and Carmen Martínez Almansa and Fabrizio Schiano and Dario Floreano and Tomi Westerlund. , and also in 
27476,https://star-history.com/#TIERS/uwb-drone-dataset&Date,"
Star History Chart
","
","
"
27478,#data-scheme,Data scheme,"
","
"
27478,#dataset,dataset,"
","
"
27478,data/prostate.lineage,prostate.lineage,Instruction to download the data are contained in the lineage files , and 
27478,data/atlas.lineage,atlas.lineage, and ,. They are text files containing the md5sum of the original zip.
27485,https://github.com/JordanMicahBennett/SMART-CT-SCAN_BASED-COVID19_VIRUS_DETECTOR/blob/master/data/screenshots/Drag%20and%20Drop%20Version_INSTRUCTIONS_SmartCovid19Detector.md,here,"Note: The animation above represents a Drag&Drop version, separate from the instance discussed on this page. The Drag&Drop version version does the same thing as the non-Drag&Drop version, with the exception of the Drag&Drop feature. The Drag&Drop version is available ",.
27485,https://github.com/JordanMicahBennett/SMART-CT-SCAN_BASED-COVID19_VIRUS_DETECTOR/blob/master/README.md#covid-19-ai-datacall-on-the-ministry-of-health,I still call to have code/data released for enhanced covid19 spread control,"
",.
27485,https://github.com/ieee8023/covid-chestxray-dataset,covid19 xray dataset released 4 days ago,", from a recent ",.
27485,https://github.com/ieee8023/covid-chestxray-dataset,Dr. Cohen's collation, from ,. Ensure the extracted 
27485,https://github.com/JordanMicahBennett/SMART-CT-SCAN_BASED-COVID19_VIRUS_DETECTOR/blob/master/data/ct-scans/covid-19-positive/coronavirus_positive_WeifangKong_et-al.png,seen in this repository," on any of the test data from the 2 gigabytes kaggle directory, or on the single positive covid-19 example ",", that was taken from "
27485,https://github.com/ieee8023/covid-chestxray-dataset,"+21 axial lung images, +11 lateral view lung images, and about +118 coronal view lung images, re Covid19 positive cases","
",", collated by "
27485,https://github.com/JordanMicahBennett/SMART-CT-SCAN_BASED-COVID19_VIRUS_DETECTOR/blob/master/data/screenshots/Drag%20and%20Drop%20Version_INSTRUCTIONS_SmartCovid19Detector.md,"A separate instance of the Smart Covid19 Detector, that includes Drag and drop functionality, has been produced","
",.
27497,asset-transfer-private-data,Private data," | Go, JavaScript | Java, JavaScript | | "," | This sample demonstrates the use of private data collections, how to manage private data collections with the chaincode lifecycle, and how the private data hash can be used to verify private data on the ledger. It also demonstrates how to control asset updates and transfers using client-based ownership and access control. | "
27497,https://hyperledger-fabric.readthedocs.io/en/latest/private_data_tutorial.html,Using Private Data," | This sample demonstrates the use of private data collections, how to manage private data collections with the chaincode lifecycle, and how the private data hash can be used to verify private data on the ledger. It also demonstrates how to control asset updates and transfers using client-based ownership and access control. | "," | Go, Java | JavaScript | | "
27497,off_chain_data,Off chain data, | | -------------|------------------------------|------------------| | , | Learn how to use block events to build an off-chain database for reporting and analytics. | 
27501,http://cocodataset.org/#home,COCO, and ," to train fcot. Before running the training scripts, you should download the datasets and set the correct datasets path in "
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/Profiles_train_gen.npy,Training set, here [,] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/Profiles_test_gen.npy,Validation set,] [,"], and the "
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/Profiles_train_et.npy,Training set, here [,] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/Profiles_test_et.npy,Validation set,] [,]. The following example illustrates how to load the information in python:
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_blind_feat_gender.npy,Gender results," was trained with the candidates competencies, the demographic attributes and the Unbiased scores. You can download the results here [",] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_blind_feat_ethnicity.npy,Ethnicity results,] [,].
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_feat_gender.npy,Gender results," was trained with the candidates competencies, the demographic attributes and the Gender/Ethinicty Biased scores. You can download the results here [",] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_feat_ethnicity.npy,Ethnicity results,] [,].
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_gender.npy,Gender results, was trained with the candidates competencies and the Gender/Ethnicity Biased scores. You can download the results here [,] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_ethnicity.npy,Ethnicity results,] [,].
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_facial_gender.npy,Gender results," was trained with the candidates competencies, the face embeddings and the Gender/Ethnicity Biased scores. You can download the results here [",] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_facial_ethnicity.npy,Ethnicity results,] [,].
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_facial_ag_gender.npy,Gender results," was trained with the candidates competencies, the agnostic face embeddings and the Gender/Ethnicity Biased Scores. You can download the results here [",] [
27507,https://github.com/BiDAlab/FairCVtest/blob/master/data/predictions_biased_facial_ag_ethnicity.npy,Ethnicity results,] [,].
27515,http://konect.uni-koblenz.de/networks/ucidata-zachary,Zachary's Karate Club,"
"," network is a well known network, which consists of 34 nodes and 78 edges. It shows the relationships between Zachary Karate Club members. Each node represents a member of the karate club and each edge represents a ties between two members of the club."
27519,https://vision.princeton.edu/projects/2012/SUN360/data/,SUN360 dataset, These images are extracted from the ,"
"
27527,https://www.kaggle.com/netflix-inc/netflix-prize-data,Netflix Prize dataset,This repository contains information about the genres of the movies of the ,.
27528,meta_dataset/data/tfds/README.md,the documentation page," at NeurIPS 2021's Datasets and Benchmarks track, we are releasing a TensorFlow Datasets-based implementation of Meta-Dataset's input pipeline which is compatible with both the original Meta-Dataset protocol (MD-v1) and the updated protocol designed for VTAB+MD (MD-v2). See ", for more information and example code snippets.
27528,#downloading-and-converting-datasets,download and convert,"
"," the data, and"
27528,https://github.com/google-research/meta-dataset/blob/main/Intro_to_Metadataset.ipynb,introduction notebook,See this , for a demonstration of how to sample data from the pipeline (episodes or batches).
27528,https://github.com/google-research/meta-dataset/tree/arxiv_v1,arxiv_v1,", please use the instructions, code, and configuration files at version ", of this repository.
27528,https://github.com/google-research/meta-dataset/tree/arxiv_v2_dev,arxiv_v2_dev,. You can follow the progess in branch , of this repository.
27528,https://github.com/google-research/meta-dataset/blob/main/Leaderboard.ipynb,this notebook,The tables below were generated by ,.
27528,https://github.com/google-research/meta-dataset/issues/54,#54,If you were affected by ,", make sure the evaluation on Traffic Sign is done on shuffled samples. We encourage you to re-train your best model (or at least perform validation again) as well."
27528,https://github.com/google-research/meta-dataset/issues/new,issue,Create an ,", with the name of the model, results, as well as the article to cite or any other relevant information to include, and label it ""leaderboard"". Alternatively, submit a PR with an update to the notebook."
27528,https://github.com/google-research/meta-dataset/tree/main/meta_dataset/dataset_conversion/splits,canonical splits," contains information about which classes are part of the meta-training, meta-validation, and meta-test set. These files are only used during the dataset conversion phase, but can help troubleshooting later. To re-use the "," instead of re-generating them, you can make it point to "
27528,doc/dataset_conversion.md#ilsvrc_2012,instructions,"Dataset (other names)                                                                                                                        | Number of classes (train/valid/test)    | Size on disk                 | Conversion time -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- | ---------------------------- | --------------- ilsvrc_2012 (ImageNet, ILSVRC) [","]                                                  | 1000 (712/158/130, hierarchical)        | ~140 GiB                    | 5 to 13 hours omniglot ["
27528,doc/dataset_conversion.md#omniglot,instructions,"]                                                  | 1000 (712/158/130, hierarchical)        | ~140 GiB                    | 5 to 13 hours omniglot [","]                                                                            | 1623 (883/81/659, by alphabet: 25/5/20) | ~60 MiB                     | few seconds aircraft (FGVC-Aircraft) ["
27528,doc/dataset_conversion.md#aircraft,instructions,"]                                                                            | 1623 (883/81/659, by alphabet: 25/5/20) | ~60 MiB                     | few seconds aircraft (FGVC-Aircraft) [","]                                                            | 100 (70/15/15)                          | ~470 MiB (2.6 GiB download) | 5 to 10 minutes cu_birds (Birds, CUB-200-2011) ["
27528,doc/dataset_conversion.md#cu_birds,instructions,"]                                                            | 100 (70/15/15)                          | ~470 MiB (2.6 GiB download) | 5 to 10 minutes cu_birds (Birds, CUB-200-2011) [","]                                                     | 200 (140/30/30)                         | ~1.1 GiB                    | ~1 minute dtd (Describable Textures, DTD) ["
27528,doc/dataset_conversion.md#dtd,instructions,"]                                                     | 200 (140/30/30)                         | ~1.1 GiB                    | ~1 minute dtd (Describable Textures, DTD) [","]                                                          | 47 (33/7/7)                             | ~600 MiB                    | few seconds quickdraw (Quick, Draw!) ["
27528,doc/dataset_conversion.md#quickdraw,instructions,"]                                                          | 47 (33/7/7)                             | ~600 MiB                    | few seconds quickdraw (Quick, Draw!) [",]                                                           | 345 (241/52/52)                         | ~50 GiB                     | 3 to 4 hours fungi (FGVCx Fungi) [
27528,doc/dataset_conversion.md#fungi,instructions,]                                                           | 345 (241/52/52)                         | ~50 GiB                     | 3 to 4 hours fungi (FGVCx Fungi) [,]                                                                    | 1394 (994/200/200)                      | ~13 GiB                     | 5 to 15 minutes vgg_flower (VGG Flower) [
27528,doc/dataset_conversion.md#vgg_flower,instructions,]                                                                    | 1394 (994/200/200)                      | ~13 GiB                     | 5 to 15 minutes vgg_flower (VGG Flower) [,"]                                                          | 102 (71/15/16)                          | ~330 MiB                    | ~1 minute traffic_sign (Traffic Signs, German Traffic Sign Recognition Benchmark, GTSRB) ["
27528,doc/dataset_conversion.md#traffic_sign,instructions,"]                                                          | 102 (71/15/16)                          | ~330 MiB                    | ~1 minute traffic_sign (Traffic Signs, German Traffic Sign Recognition Benchmark, GTSRB) [","] | 43 (0/0/43, test only)                  | ~50 MiB (263 MiB download)  | ~1 minute mscoco (Common Objects in Context, COCO) ["
27528,doc/dataset_conversion.md#mscoco,instructions,"] | 43 (0/0/43, test only)                  | ~50 MiB (263 MiB download)  | ~1 minute mscoco (Common Objects in Context, COCO) [","]                                              | 80 (0/40/40, validation and test only)  | ~5.3 GiB (18 GiB download)  | 4 hours "
27528,doc/dataset_conversion.md#ilsvrc_2012,instructions, (,") in order to make it a training only dataset. Also,"
27537,data/LICENSE.txt,License,"The dataset needs to be installed in the data folder. We provide a script for automatically downloading the dataset from a Google Drive repository. The dataset is shared only for scientific research purposes. Please, see the ", file that comes along the dataset.
27544,#datasets,datasets,Download and unzip the ,"
"
27544,scripts/make_dataset.sh,make_dataset.sh,You need to preprocess the datasets in order to index all the samples and extract faces. Just run the script ,"
"
27544,https://www.kaggle.com/c/deepfake-detection-challenge/data,Facebook's DeepFake Detection Challenge (DFDC) train dataset,"
", | 
27544,https://github.com/ondyari/FaceForensics/blob/master/dataset/README.md,FaceForensics++,"
", | 
27545,https://download.visinf.tu-darmstadt.de/data/from_games/,here,: Please follow the instructions , to download images and semantic segmentation annotations. The GTA5 dataset directory should have this basic structure:
27545,https://www.cityscapes-dataset.com/,Cityscape,: Please follow the instructions in , to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:
27555,#dataset,Dataset,"
","
"
27557,#data-handling,Data Preprocessing,"
","
"
27564,https://cocodataset.org/#download,2014 COCO dataset,Download the , as well as 
27564,https://github.com/uclanlp/reducingbias/tree/master/data/COCO,gender annotations, as well as ,", and place them in customizable filepaths specified in the code "
27564,https://github.com/princetonvisualai/revise-tool/blob/master/datasets.py#L383,here,", and place them in customizable filepaths specified in the code ",.
27564,https://raw.githubusercontent.com/matplotlib/basemap/master/lib/mpl_toolkits/basemap_data/epsg,here,"If the epsg file is still not found, it can be downloaded manually from ",", with the path locaation set as mentioned."
27567,#dataset,Dataset,"
","
"
27570,https://catalog.ldc.upenn.edu/docs/LDC93S4B/corpus.html,ATIS,"
"," (Airline Travel Information System) is NLU dataset in airline travel domain. The dataset contains 4978 train and 893 test utterances classified into one of 26 intents, and each token in utterance is labeled with tags from 128 slot filling tags in IOB format."
27571,docs/getting_data.md,here,") to request the MongoDB dump that contains the dataset. Alternatively, see "," for instructions on how to get the data from scratch, which will take a few days."
27571,tell/data/dataset_readers/nytimes_faces_ner_matched.py,here,You can see an example of how we read the NYTimes800k samples from the MongoDB database ,. Here's a minimum working example in Python:
27576,http://physionet.incor.usp.br/physiobank/database/stdb/,MIT-BIH ST Change Database,"
","
"
27578,#supported-datasets,datasets,: create the probing task datasets from the original ,.
27594,#get-the-processed-train/val/test-datasets,Get the processed train/val/test datasets,"
","
"
27594,#process-training-and-validation-data-to-onmt-style,Process training and validation data to ONMT style,"
","
"
27594,https://www.dropbox.com/s/tavebz23va1hvrd/ExHiRD_test_datasets.zip?dl=1,processed testing datasets,Download the ,. Download the 
27594,https://www.dropbox.com/s/5sbwt2k66nly1ib/ExHiRD_train_valid_dataset.zip?dl=1,processed train_valid_dataset,. Download the ,.
27594,https://www.dropbox.com/s/5sbwt2k66nly1ib/ExHiRD_train_valid_dataset.zip?dl=1,processed train_valid_dataset," under the home folder (i.e., ExHiRD-DKG), download the ", into 
27597,https://www.donneesquebec.ca/recherche/fr/dataset/empreintes-des-batiments,website,"We would like to thank the City of Quebec for providing and maintaining this dataset. To download these GT footprints, please refer to their ",.
27597,https://blogs.bing.com/maps/2019-03/microsoft-releases-12-million-canadian-building-footprints-as-open-data,blog entry,We compared the SRSM results with the open Canada building footprint datasets carried out by Microsoft Bing maps team (see their , and 
27598,https://opendatacommons.org/licenses/odbl/,Open Data Commons Open Database License (ODbL),This data is licensed by Microsoft under the ,"
"
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Alberta.zip,Alberta,| Province/Territory         | Number of Buildings  | Unzipped MB | | ------------- |:-------------:| -----:| | ,"|1,777,439|389| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/BritishColumbia.zip,British Columbia,"|1,777,439|389| | ","|1,359,628|301| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Manitoba.zip,Manitoba,"|1,359,628|301| | ","|632,982|135| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/NewBrunswick.zip,New Brunswick,"|632,982|135| | ","|350,989|71| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/NewfoundlandAndLabrador.zip,Newfoundland and Labrador,"|350,989|71| | ","|255,568|51| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/NorthwestTerritories.zip,Northwest Territories,"|255,568|51| | ","|13,161|3| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/NovaScotia.zip,Nova Scotia,"|13,161|3| | ","|402,358|81| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Nunavut.zip,Nunavut,"|402,358|81| | ","|2,875|1| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Ontario.zip,Ontario,"|2,875|1| | ","|3,781,847|808| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/PrinceEdwardIsland.zip,Prince Edward Island,"|3,781,847|808| | ","|76,590|16| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Quebec.zip,Quebec,"|76,590|16| | ","|2,495,801|512| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/Saskatchewan.zip,Saskatchewan,"|2,495,801|512| | ","|681,553|146| | "
27598,https://usbuildingdata.blob.core.windows.net/canadian-buildings-v2/YukonTerritory.zip,Yukon,"|681,553|146| | ","|11,395|3|"
27612,https://datadoghq.com,"Datadog, Inc.", from ,  and make use of the 
27613,https://github.com/feathers-dataset/feathersv1-dataset,FeathersV1 Dataset,This repository contains a Jupyter notebooks and scripts for solving sparse classification task based on ,.
27613,https://github.com/feathers-dataset/feathersv1-dataset,FeathersV1 Dataset,Download , into 
27613,https://github.com/feathers-dataset/feathersv1-classification/blob/master/requirements.txt,requirements.txt,We recommend using the versions of packages provided in the , file.
27613,https://github.com/feathers-dataset/feathersv1-classification/blob/master/LICENSE,LICENSE,Please refer to the FeathersV1 Classification , file.
27614,https://github.com/feathers-dataset/feathersv1-classification,FeathersV1 Classification notebooks,"
","
"
27614,https://github.com/feathers-dataset/feathersv1-dataset/blob/master/AUTHORS,AUTHORS,Disclaimer: all dataset images were taken from Internet open collections and public or private collections with the consent of the authors. Please check , file for complete information about licenses and authors.
27614,https://github.com/feathers-dataset/feathersv1-dataset/blob/master/LICENSE,"
LICENSE
",In addition please refer to the FeathersV1 Dataset , file.
27616,#dataset,Dataset,"
","
"
27621,https://www.entsoe.eu/data/map/,ENTSO-E Transmission System Map, extraction of the ,. The grid model contains 6763 lines (alternating current lines at and above 220kV voltage level and all high voltage direct current lines) and 3642 substations.
27621,https://open-power-system-data.org/,OPSD project,Electrical demand time series from the ,.
27622,http://pandas.pydata.org/,pandas,"
", for storing data about components and time series
27629,http://www.phontron.com/download/conala-corpus-v1.1.zip,conala-corpus-v1.1.zip,Download , and unzip the content into 
27629,https://conala-corpus.github.io/,CoNaLa,"Our paper's training strategy is basically 3-step: pretrain on mined + API data, finetune on "," dataset, and rerank."
27629,https://conala-corpus.github.io/,CoNaLa challenge dataset," software, and the ",.
27630,#simulation-data,Simulation Data,"
","
"
27630,#real-data,Real Data,"
","
"
27630,#uci-datasets,UCI Datasets,"
","
"
27630,#image-datasets,Image Datasets,"
","
"
27630,http://archive.ics.uci.edu/ml/datasets.php,UCI machine learning repository,The original UCI datasets were from ,". As the real data has no groud truth for density, we evaluate Roundtrip by calculating the average log likelihood on the test data. Similar to the simulation data, we take "
27632,https://www.dlology.com/blog/tutorial-chinese-sentiment-analysis-with-hotel-review-data/,write up,You may also read my , including the key difference compared to processing english data and final results.
27632,https://www.dlology.com/blog/tutorial-chinese-sentiment-analysis-with-hotel-review-data/,blog,"Enjoy, leave a comment in my ", if you have any question.
27633,https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-05-learning_from_unlabeled_text_data.ipynb,Learning from Unlabeled Text Data,Tutorial 5: ,"
"
27633,https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-A4-customdata-text_regression_with_extra_regressors.ipynb,Using Custom Data Formats and Models: Text Regression with Extra Regressors,Tutorial A4: ,"
"
27633,https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c,"
ktrain: A Lightweight Wrapper for Keras to Help Train Neural Networks
","
","
"
27633,https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358,"
BERT Text Classification in 3 Lines of Code
","
","
"
27633,https://towardsdatascience.com/build-an-open-domain-question-answering-system-with-bert-in-3-lines-of-code-da0131bc516b,"
Build an Open-Domain Question-Answering System With BERT in 3 Lines of Code
","
","
"
27633,https://ai.stanford.edu/~amaas/data/sentiment/,IMDb Movie Reviews,Example: Text Classification of , Using 
27633,https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2,Named Entity Recognition,Example: Sequence Labeling for , using a randomly initialized 
27633,https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz,Cora Citation Graph,Example: Node Classification on , using a 
27633,https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html,20 Newsgroups Dataset, on , Using 
27633,https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c,training,| Feature  | TensorFlow |  PyTorch | Sklearn | --- | :-: | :-: | :-: | | ," any neural network (e.g., text or image classification)  |  ✅  | ❌  | ❌  | | "
27633,https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-05-learning_from_unlabeled_text_data.ipynb,Topic Modeling, (pretrained)     |  ❌  | ✅  |❌   | | , (sklearn)  |  ❌  | ❌  | ✅  | | 
27638,https://sibeiyang.github.io/dataset/ref-reasoning,Ref-Reasoning Dataset,"
","
"
27638,https://nlp.stanford.edu/data/gqa/allImages.zip,training images, and share the same , with GQA. We generate referring expressions according to the 
27638,https://nlp.stanford.edu/data/gqa/sceneGraphs.zip,image scene graph annotations, with GQA. We generate referring expressions according to the , provided by the 
27649,https://towardsdatascience.com/drug-discovery-with-deep-learning-under-10-lines-of-codes-742ee306732a,blog,[07/20] A ," is posted on the Towards Data Science Medium column, check this out!"
27649,https://www.aicures.mit.edu/data,open task,[05/20] Support drug property prediction for screening data that does not have target proteins such as bacteria! An example using RDKit2D with DNN for training and repurposing for pseudomonas aeruginosa (MIT AI Cures's ,) is provided as a 
27649,DEMO/load_data_tutorial.ipynb,Dataset Tutorial,| Name | Description | |-----------------|-------------| | , | Tutorial on how to use the dataset loader and read customized data| | 
27649,http://staff.cs.utu.fi/~aatapa/data/DrugTarget/,DAVIS, to process the data| |,|
27649,https://www.aicures.mit.edu/data,MIT AI Cures, (Thanks to ,) | Data  | Function | |-------|----------| |
27649,DEMO/load_data_tutorial.ipynb,Dataset Tutorial,Checkout ,.
27650,https://gluon-cv.mxnet.io/model_zoo/segmentation.html#ade20k-dataset,GluonCV Toolkit,Gluon models and training: Please visit ,.
27650,https://gluon-cv.mxnet.io/build/examples_datasets/recordio.html,GluonCV tutorial,"Here we use raw image data format for simplicity, please follow ", if you would like to use RecordIO format.
27650,https://gluon-cv.mxnet.io/model_zoo/segmentation.html#ade20k-dataset,GluonCV Toolkit,Training with MXNet: ,.
27654,http://papers.nips.cc/paper/8374-data-differentiable-architecture-approximation,DATA: Differentiable ArchiTecture Approximation, | ICLR | - | - | | , | NeurIPS | - | - | | 
27668,https://mapzen.com/data/metro-extracts/,Mapzen,. Another one is ,.
27670,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html,VoxCeleb2, path to the directory of your dataset. The format of the directory is expected to be similar to the one used in , dataset. The 
27678,https://www.jstor.org/stable/2334380?seq=1#metadata_info_tab_contents,"Kettenring, 1971",; ,] based approach to construct these answer reference sets at-scale across the whole dataset.
27681,https://www.dropbox.com/s/9w8nmj791c9ogsx/data_upload_v3.zip?dl=0,dataset_link,"We prepared a dataset of around 5000 images, which can be downloaded from here: ","
"
27681,https://github.com/ieee8023/covid-chestxray-dataset,Covid-Chestxray-Dataset,"
",", for COVID-19 X-ray samples"
27682,#1-naver-clovacall-dataset-contribution,1. Naver ClovaCall dataset contribution,"
","
"
27682,#the-dataset-statistics,The dataset statistics,"
","
"
27682,#the-dataset-structure,The dataset structure,"
","
"
27682,#2-dataset-downloading-and-license,2. Dataset downloading and license,"
","
"
27682,http://www.aihub.or.kr/aidata/105,here, dataset can be download from , (
27683,#1-naver-clovacall-dataset-contribution,1. Naver ClovaCall dataset contribution,"
","
"
27683,#the-dataset-statistics,The dataset statistics,"
","
"
27683,#the-dataset-structure,The dataset structure,"
","
"
27683,#2-dataset-downloading-and-license,2. Dataset downloading and license,"
","
"
27683,http://www.aihub.or.kr/aidata/105,here, dataset can be download from , (
27687,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow,Download ,", "
27687,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo,KITTI 2012,", ", and 
27687,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI 2015, and , datasets.
27687,dataloader/dataloader.py,dataloader/dataloader.py," (an example on KITTI dataset is provided), and then create a new dataset dictionary in ",.
27704,https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html,dataset, [,]
27708,https://github.com/ieee8023/covid-chestxray-dataset,covid-chestxray-dataset, uses the , for COVID-19 X-Ray images and 
27710,https://data.broadinstitute.org/bbbc/BBBC006/,bbbc006,BBBC006: a hunman U2OS cell dataset ,"
"
27712,http://camma.u-strasbg.fr/datasets,Cholec80,The LRTD repository contains the codes of our LRTD paper. We validate our approach on a large surgical video dataset ," by performing surgical workflow recognition task. By using our LRTD based selection strategy, we can outperform other state-ofthe-art active learning methods who only consider neighbor-frame information. Using only up to "
27712,http://camma.u-strasbg.fr/datasets,Cholec80,download data from , and then split the data into 25fps using 
27713,datasets,datasets," and can be downloaded by following the links below. First, download the ", and unpack into 
27713,https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8,Link,| Dataset | Download | | ------- | :--------: | | SST-2| [,] | | MNLI | [
27713,https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce,Link,] | | MNLI | [,] | | CoLA | [
27718,#data,Data,"
","
"
27721,docs/datasets/README.md,here,It is also important that you have a recent version of the database which can be obtained by contacting us directly. You can also generate your own database by following the instructions ,.
27721,https://dblp.uni-trier.de/faq/How+can+I+download+the+whole+dblp+dataset,DBLP,"AIP combines three data sources, namely ",", "
27721,https://api.semanticscholar.org/corpus/download/,Semantic Scholar,", ",", and "
27727,https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py,this, using , script.
27730,https://s3.amazonaws.com/fairseq-py/data/wmt14.v2.en-fr.newstest2014.tar.bz2,download (.tar.bz2),"
","
"
27730,https://s3.amazonaws.com/fairseq-py/data/wmt14.v2.en-fr.ntst1213.tar.bz2,download (.tar.bz2),"
", Convolutional 
27730,https://s3.amazonaws.com/fairseq-py/data/wmt14.en-de.newstest2014.tar.bz2,download (.tar.bz2),"
", Convolutional 
27730,https://s3.amazonaws.com/fairseq-py/data/wmt17.v2.en-de.newstest2014.tar.bz2,download (.tar.bz2),"
", Transformer 
27730,https://s3.amazonaws.com/fairseq-py/data/wmt14.en-fr.joined-dict.newstest2014.tar.bz2,download (.tar.bz2),"
", Transformer 
27730,https://s3.amazonaws.com/fairseq-py/data/wmt16.en-de.joined-dict.newstest2014.tar.bz2,download (.tar.bz2),"
", Transformer 
27730,https://s3.amazonaws.com/fairseq-py/data/gbw_test_lm.tar.bz2,download (.tar.bz2), | , Convolutional 
27730,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WikiText-103,) | , | 
27730,https://s3.amazonaws.com/fairseq-py/data/wiki103_test_lm.tar.bz2,download (.tar.bz2), | ,"
"
27730,https://s3.amazonaws.com/fairseq-py/data/stories_test.tar.bz2,download (.tar.bz2), | ,"
"
27751,https://github.com/wenhuchen/LogicNLG/blob/master/data,data,The data used for LogicNLG is provided in ," folder, the details are described in "
27751,https://github.com/wenhuchen/LogicNLG/blob/master/data/README.md,README," folder, the details are described in ","
"
27760,https://github.com/JDAI-CV/VeRidataset,VeRi,If  you want to use our baseline on public datasets (such as , datasets).
27760,https://github.com/JDAI-CV/VeRidataset,VeRi,| Backbone (baseline)        | , | download                                                     | | -------------------------- | ---------------------------------------------- | ------------------------------------------------------------ | | ResNet50 (batch 48)        | 79.8/95.0                                      | 
27763,#datasets,datasets, See , for an overview of the datasets (daily updates).
27763,#cord-19-dataset,CORD-19 dataset,"
","
"
27763,#covid19-preprints-dataset,COVID19 preprints dataset,"
","
"
27763,https://github.com/nicholasmfraser/covid19_preprints/blob/master/data/covid19_preprints.csv,https://github.com/nicholasmfraser/covid19_preprints/blob/master/data/covid19_preprints.csv,The most recent version of the dataset can be found here:,.
27781,https://github.com/rodrigoberriel/ego-lane-analysis-system/tree/master/datasets,ELAS,"
","
"
27785,#4-covid-semiseg-dataset,4. COVID-SemiSeg Dataset,"
","
"
27787,https://sobigdata.d4science.org/group/sobigdatalab/method-engine,"
SoBigData Method Engine
",BoilerNet is now integrated into the SoBigData platform! Use your own or a pre-trained model to extract text from HTML pages or annotate them directly. Available in the ,.
27788,https://criteostorage.blob.core.windows.net/criteo-research-datasets/criteo-continuous-offline-dataset.csv.gz,here,Details on the dataset can be found in the paper. You can also download it directly from , (2.3GB zipped CSV).
27790,http://nlp.stanford.edu/data/glove.6B.zip,here, from , and unzip to the repository.
27812,https://pandas.pydata.org,pandas,Python 3 for code generation and with , installed for the evaluation scripts
27819,datasets/,datasets,the , used for assessing our proposal
27823,https://github.com/huggingface/datasets,datasets,": loads data from plaintext, tsv, and huggingface's ","
"
27823,https://github.com/masakhane-io/masakhane-mt/blob/master/starter_notebook-custom-data.ipynb,starter notebook,"
", Masakhane - Machine Translation for African Languages in 
27825,https://datasetops.readthedocs.io/en/latest/?badge=latest,"
Documentation Status
","
","
"
27825,https://codecov.io/gh/LukasHedegaard/datasetops,"
codecov
","
","
"
27825,https://pypi.org/project/datasetops/,Python package index,Binary installers available at the ,"
"
27825,https://www.tensorflow.org/datasets,Tensorflow,Collecting and preprocessing datasets is tiresome and often takes upwards of 50% of the effort spent in the data science and machine learning lifecycle. While , and 
27825,https://www.tensorflow.org/datasets,PyTorch, and ," have some useful datasets utilities available, they are designed specifically with the respective frameworks in mind. Unsuprisingly, this makes it hard to switch between them, and training-ready dataset definitions are bound to one or the other. Moreover, they do not aid you in standard scenarios where you want to:"
27825,https://pytorch.org/docs/stable/torchvision/datasets.html#mnist,torchvision datasets," aims to make these processing steps easier, faster, and more intuitive to perform, while retaining full compatibility to and from the leading libraries. This also means you can grab a dataset from ", and use it directly with tensorflow:
27827,https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html,CDC webpage,. Our forecast appears on the official ,".  Our model can consider the effect of many complexities of the epidemic process and yet be simplified to a few parameters that are learned using fast linear regressions. Therefore, our approach can learn and generate forecasts extremely quickly. On a 2 core desktop machine, our approach takes only 3.18s to tune hyper-parameters, learn parameters and generate 100 days of forecasts of reported cases and deaths for all the states in the US. The total execution time for 184 countries is 11.83s and for more than 3000 US counties is around 30s. For around 20,000 locations data for which are made available by "
27827,https://github.com/GoogleCloudPlatform/covid-19-open-data,Google,".  Our model can consider the effect of many complexities of the epidemic process and yet be simplified to a few parameters that are learned using fast linear regressions. Therefore, our approach can learn and generate forecasts extremely quickly. On a 2 core desktop machine, our approach takes only 3.18s to tune hyper-parameters, learn parameters and generate 100 days of forecasts of reported cases and deaths for all the states in the US. The total execution time for 184 countries is 11.83s and for more than 3000 US counties is around 30s. For around 20,000 locations data for which are made available by ",", our approch takes around 10 mins. Despite being fast, the accuracy of our forecasts is on par with the state-of-the-art as demonstrated on the "
27840,http://www.cvlibs.net/datasets/kitti/eval_odometry.php,KITTI Odometry website,Follow the instruction on , to download the KITTI odometry train set. Then train with
27847,https://github.com/huggingface/datasets,"
datasets package", and datasets from the ,! Here's an example of loading and attacking a pre-trained model and dataset:
27847,https://textattack.readthedocs.io/en/latest/api/datasets.html,more details at here,"Dataset loading via other mechanism, see: ","
"
27853,https://github.com/jiachenwestlake/Cross-Domain_NER/tree/master/unsupervised_domain_adaptation/data/news_tech,CBS SciTech News NER, We take CoNLL-2003 English named entity recognition (NER) dataset as the source domain and the , dataset as the target domain.
27862,http://eventcqa.l3s.uni-hannover.de/dataset/input.zip,input.zip,The input data files are compressed in one file ,". Extract them to your home folder and specify the path in the config.properties file. The files are composed of DBpedia predicates and relations, where:"
27864,https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset,WIKITEXT-103,| | , | 1147M | 22.2 (PPL) | 
27871,https://slideslive.com/38928832/syntactic-data-augmentation-increases-robustness-to-inference-heuristics,here,. A 7 minnute presentation on the paper can be accessed ,.
27871,https://github.com/Aatlantise/syntactic-augmentation-nli/tree/master/datasets,"
datasets
",Augmentation datasets are in the , folder. Each file is named using the following abbreviations:
27871,https://github.com/Aatlantise/syntactic-augmentation-nli/tree/master/datasets/pass_trsf_pos_small.tsv,"
pass_trsf_pos_small.tsv
","For example, "," is an set of 101 passivization with transformed hypothesis examples whose labels are entailment. Also, please note that the negative combined transformed-hypothesis nonentailed datasets ("
27871,https://github.com/Aatlantise/syntactic-augmentation-nli/tree/master/generate_dataset.py,"
generate_dataset.py
", data files were used to augment the MultiNLI training set in our experiments. They are randomly selected subsets or unions of subsets of transformations created by running ,", which requires MultiNLI's json file "
27871,https://github.com/nyu-mll/GLUE-baselines/blob/master/download_glue_data.py,download_glue_data.py,", and MNLI data from running ",. It includes files mentioned below like 
27872,https://raw.githubusercontent.com/nikhgarg/EmbeddingDynamicStereotypes/master/data/mturk_stereotypes.csv,MTurk data from Garg et al. (2018), is a direct copy of the ,". If you use this data, please cite their paper as follows:"
27872,https://github.com/oagarwal/personality-bias/tree/master/data/raw,the raw data directory from the personality-based surveys of Agarwal et al. (2018), is a direct copy of ,". If you use this data, please cite their paper as follows:"
27877,data/README.md,database,This is an ongoing ultrasound data collection initiative for COVID-19. Please help growing the ,.
27877,./data/dataset_metadata.csv,dataset_metadata.csv,"Feel free to use (and cite) our dataset. We currently have >200 LUS videos labelled with a diagnostic outcome. Moreover, lung severity scores for 136 videos are made available in the ", under the column 
27877,data/README.md,data/README.md,". Further clinical information (symptoms, visible LUS patterns etc) are provided for some videos. For details see ",.
27877,https://towardsdatascience.com/ultrasound-for-covid-19-a-deep-learning-approach-f7906002892a,blogpost,Read our ,"
"
27877,data,data,Find all details on the current state of the database in the , folder.
27888,https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews,here,", IMDB ",.
27895,#metadata-api,Metadata API,"
","
"
27895,https://graphics.stanford.edu/data/3Dscanrep/,Bunny," will read OBJ, STL or PLY files as input, and output Draco-encoded files. We have included Stanford's ", mesh for testing. The basic command line looks like this:
27895,src/draco/metadata,src/draco/metadata,Please see , and 
27895,https://graphics.stanford.edu/data/3Dscanrep/,https://graphics.stanford.edu/data/3Dscanrep/,Bunny model from Stanford's graphic department ,"
"
27898,https://www.khronos.org/blog/ubers-vis.gl-brings-gltf-to-geospatial-data-visualization,Uber’s vis.gl brings glTF to geospatial data visualization,"
"," by Georgios Karnas, Ib Green, Travis Gorkin, and Xintong Xia. June 2019"
27898,https://constructingdata.wordpress.com/2018/09/07/gltf-and-construction-part-1-secrets-of-the-cloud/,glTF and Construction – Part 1: Secrets of the Cloud,"
", by Tim Davies. September 2018
27898,https://constructingdata.wordpress.com/2018/09/08/gltf-and-construction-part-2-3d-for-everyone/,glTF and Construction – Part 2: 3D for Everyone,"
", by Tim Davies. September 2018
27901,https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/all_sources_metadata_2020-03-13.csv,metadata file,Specific licensing information for individual articles in the dataset is available in the ,.  See also the 
27901,https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/all_sources_metadata_2020-03-13.readme,readme for the metadata file,.  See also the ,.
27908,https://github.com/InitialBug/MarCo-Dialog/tree/master/data,data/,The dataset is already preprocessed and put in ," folder (train.json, val.json and test.json). We have also uploaded the model checkpoints in "
27915,/data-preparation.ipynb,here,"After downloading, we created article-title pairs, saved in tabular datset format (.csv) and extracted a sample subset (80,000 for training & 20,000 for validation). This data preparation can be found ",.
27920,https://www.embs.org/ai-driven-informatics-sensing-imaging-and-big-data-analytics-for-fighting-the-covid-19-pandemic/,"Special Issue on ""AI-driven Informatics, Sensing, Imaging and Big Data Analytics for Fighting the COVID-19 Pandemic""","
","
"
27931,http://cvgl.stanford.edu/projects/uav_data/,Stanford Drone Dataset, dataset and the ,". For each dataset, we provide, in the "
27940,https://github.com/EdinburghNLP/csi-corpus,"
CSI dataset
","In this work, we summarize screenplays by taking into account their underlying narrative structure. We formalize screenplay summarization as scene selection, where we want to select an optimal subsequence of scenes that describe the story from beginning to end. We conduct experiments on the ", that contains gold-standard scene-level summary annotations.
27940,https://github.com/EdinburghNLP/csi-corpus,"
CSI dataset
","
",: Summarization dataset containing 39 episodes with scene-level binary annotations indicating whether each scene belongs to the summary.
27948,https://data.vision.ee.ethz.ch/cvl/ntire20/,NTIRE2020 Real Image Super-Resolution Challenge,We participate CVPRW ,"
"
27960,https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics-aida,Artificial Intelligence for Data Analytics (AIDA) project,. This work was carried out under the ," at the Alan Turing Institute (London, UK)."
27960,https://github.com/alan-turing-institute/aida-data-engineering-issues/blob/master/Broadband_analysis.ipynb,broadband_analysis,The analysis is summarized in the , notebook.
27960,https://ropensci.github.io/cleanEHR/data_clean.html,blog post,"The CleanEHR anonymized and public dataset can be requested online. It contains records for 1978 patients (one record each) who died at a hospital (or in some cases arrived dead), with 263 fields, including 154 longitudinal fields (time-series). These fields cover patient demographics, physiology, laboratory, and medication information for each patient/record, recorded in intensive care units across several NHS trusts in England. The dataset comes as an R data object, which can be most profitably accessed with an accompanying R package (https://cran.r-project.org/src/contrib/Archive/cleanEHR/). The R package is available at https://github.com/ropensci/cleanEHR. There is also a ", on how to use it. A detailed explanation of each field is found in https://github.com/ropensci/cleanEHR/wiki under Data-set-1.0 and CCHIC-Data-Fields.
27960,https://github.com/alan-turing-institute/aida-data-engineering-issues/blob/master/cleanEHR_analysis.ipynb,cleanEHR_analysis,"The analysis was adapted from work performed by Giovanni Colavizza and Camila Rangel Smith, and is further summarized in the ", notebook.
27960,https://github.com/alan-turing-institute/aida-data-engineering-issues/blob/master/HES_analysis.ipynb,HES_analysis,"The analysis was adapted from work performed by Giovanni Colavizza and Angus Williams, and is further summarized in the ", notebook.
27964,https://github.com/switchablenorms/CelebAMask-HQ#dataset-agreement,CC BY-NC-SA 4.0, | , | CelebAMask-HQ | | 
27965,https://github.com/twtygqyy/pytorch-SRResNet/tree/master/data,Code for Data Generation,Please refer , for creating training files.
27965,http://cv.snu.ac.kr/research/VDSR/train_data.zip,291,We provide a pretrained model trained on , images with data augmentation
27968,http://robotics.ethz.ch/~asl-datasets/2020_voxgraph_arche,here,We provide a demo dataset ,", which features a hexacopter flying through an indoor-outdoor search and rescue training site. The rosbag includes the robot's visual, inertial and LiDAR sensor data, and the GPS RTK measurements used for the evaluations in the paper."
27970,https://github.com/ethz-asl/voxblox_ground_truth/blob/8f868dc4290ebaffa8b4c6435491f3cfa386783d/sample_data/gazebo/worlds/burning_building_rubble.world#L4-L5,sample.world,"For an example, see the provided ",.
27970,http://graphics.stanford.edu/data/3Dscanrep/,here," meshes, you could download samples from Stanford's 3D Scanning Repository ",.
28023,#data,Data,"
","
"
28032,http://nlp.stanford.edu/data/glove.840B.300d.zip,here, directory. Download the word embeddings from , and place it to 
28033,https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data,Twitter Streaming API,"We are continuously collecting the data since March 5, 2020 and will keep fetching the tweets using ",". We have collected around 700GB of raw data until April 24, 2020 and saved this data as JSON files. However, we dynamically process this data in real-time for the "
28035,https://pandas.pydata.org/,pandas,"
","
"
28038,http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html,Mall dataset,: ,"
"
28041,https://visualdialog.org/data,here,Download the VisDial v0.9 and v1.0 dialog json files from , and keep it under 
28041,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json,"
visdial_1.0_word_counts_train.json
", provides the word counts for VisDial v1.0 train split ,. They are used to build the vocabulary. Keep it under 
28041,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_train.h5,"
features_faster_rcnn_x101_train.h5
","
",: Bottom-up features of 36 proposals from images of 
28041,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_val.h5,"
features_faster_rcnn_x101_val.h5
","
",: Bottom-up features of 36 proposals from images of 
28041,https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/features_faster_rcnn_x101_test.h5,"
features_faster_rcnn_x101_test.h5
","
",: Bottom-up features of 36 proposals from images of 
28041,http://nlp.stanford.edu/data/glove.6B.zip,here,Download the GloVe pretrained word vectors from ,", and keep "
28048,http://www.msmarco.org/dataset.aspx,msmarco.org,To Download the MSMARCO Dataset please navigate to , and agree to our Terms and Conditions. If there is some data you think we are missing and would be useful please open an issue.
28048,https://ai.baidu.com/broad/subordinate?dataset=dureader,DuReader,"
", is a Chinese dataset focused on machine reading comprehension and question answering. Its design and area of focus is very similar to that of MSMARCO. The DuReader team has created scripts to allow DuReader system to use msmarco data and we have created scripts to allow MSMARCO teams to use DuReader data. We Strongly recommend training and testing your system with both datasets. We are in the process of creating an analysis tool that would take results to both systems and debug the wins/losses.
28052,http://moa.cms.waikato.ac.nz/datasets/,here,"This problem is often used as a benchmark for concept drift classification. Initially described by Harris et al. it was used thereafter for several performance comparisons. A critical note to its suitability as a benchmark can be found in. The dataset holds information of the Australian New South Wales Electricity Market, whose prices are affected by supply and demand. Each sample, characterized by attributes such as day of week, time stamp, market demand etc., refers to a period of 30 minutes and the class label identifies the relative change (higher or lower) compared to the last 24 hours. We used the normalized version as it also can be found ",.
28052,https://archive.ics.uci.edu/ml/datasets/Covertype,original source, (,)
28052,http://moa.cms.waikato.ac.nz/datasets/,here,"Assigns cartographic variables such as elevation, slope, soil type, ... of 30 x 30 meter cells to different forest cover types. Only forests with minimal human-caused disturbances were used, so that resulting forest cover types are more a result of ecological processes. It is often used as a benchmark for drift algorithms. We used the normalized version as it also can be found ",.
28052,https://archive.ics.uci.edu/ml/datasets/Poker+Hand,original source, (,)
28052,http://moa.cms.waikato.ac.nz/datasets/,here,", in which virtual drift is introduced via sorting the instances by rank and suit. Duplicate hands were also removed. We used the normalized version as it also can be found ",.
28066,data/,data,"
"," contains a very small sample data for both scripts, just to make the formatting clear."
28067,https://github.com/mitre-attack/attack-stix-data,attack-stix-data,"If you are looking for ATT&CK represented in STIX 2.1, please see the "," GitHub repository. Both MITRE/CTI (this repository) and attack-stix-data will be maintained and updated with new ATT&CK releases for the foreseeable future, but the data model of attack-stix-data includes quality-of-life improvements not found on MITRE/CTI. Please see the "
28067,https://github.com/mitre-attack/attack-stix-data,attack-stix-data USAGE document," GitHub repository. Both MITRE/CTI (this repository) and attack-stix-data will be maintained and updated with new ATT&CK releases for the foreseeable future, but the data model of attack-stix-data includes quality-of-life improvements not found on MITRE/CTI. Please see the ", for more information on the improved data model of that repository.
28071,notebooks/datasets.ipynb,datasets,See , notebook for an example of how to load the datasets provided below. The 
28075,https://github.com/google-research-datasets/ToTTo#leaderboard-submission,below,. You can find more submission information ,". By emailing us or by submitting prediction files, you consent to being contacted by Google about your submission, this dataset or any related competitions."
28089,https://blog.nelsonliu.me/2017/09/23/flattening-the-gigaword-corpus/,blog post on flattening the Gigaword corpus,See my , for more information about how the code in this repo works.
28097,https://data.bris.ac.uk/data/dataset/2tw6gdvmfj3f12papdy24flvmo,data.bris.ac.uk,Models are available to downloaded from ,.
28104,./10-data-protection,10-data-protection,Folder ,"
"
28110,https://github.com/NorskRegnesentral/weak-supervision-for-NER/releases/download/acl2020/wikidata.json,"
wikidata.json
","
","
"
28110,https://github.com/NorskRegnesentral/weak-supervision-for-NER/releases/download/acl2020/wikidata_small.json,"
wikidata_small.json
","
","
"
28115,https://github.com/LiyingCheng95/EntityDescriptionGeneration/tree/master/sockeye/data/ENT-DESC%20dataset,ENT-DESC,". For more details regarding the data preparation step, please refer to ",.
28117,https://github.com/verypluming/systematicity/releases/tag/dataset,release," directory. If you fail to install tools etc, please use our dataset generated by the above instruction: ",.
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE,"
", (
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE, | | , | 0.313 |  0.221 |  0.347 |   0.497 | 
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE, | | , | 0.228 |  0.053 |  0.368 |   0.520 |   
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE, | | , | 0.676 | 0.542  | 0.787  |   0.875 |   
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE, | | , | 0.553 | 0.315  | 0.764  |   0.924 |   
28118,http://web.informatik.uni-mannheim.de/pi1/libkge-models/wikidata5m-complex.yaml,config.yaml," | Random, 600 epochs | 200 epochs      | 0.301 |  0.245 |  0.331 |   0.397 | ", | 
28118,http://web.informatik.uni-mannheim.de/pi1/libkge-models/wikidata5m-complex.pt,NegSamp-kl, | , | | 
28118,https://github.com/uma-pi1/GraSH/blob/main/examples/experiments/selected_trials/wikidata5m/complex-wikidata-combined.yaml,config.yaml," | GraSH, 192 epochs  | 64 epochs       | 0.300 |  0.247 |  0.328  |   0.390 | ", | - |
28118,https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data,TransE,                                                    | 0.613 |  0.578 |  0.637 |   0.669 | | , | 0.553 |  0.520 |  0.571 |   0.614 |
28118,https://github.com/uma-pi1/kge-iclr20/blob/master/data_dumps/iclr2020-fb15k-237-all-trials.csv,this output for a search job, or ,. Additional configuration options or metrics can be added to the CSV files as needed (using a 
28120,https://medium.com/@uwdata/errudite-55d5fbf3232e,our blog post,Read , which explains the core idea of Errudite.
28121,dataset/,dataset/,| Path     	               | Description                         	| |------------------------- |------------------------------	| | ,     | The experiment notebooks expect the patched TACRED dataset splits to be stored here. | | 
28131,https://scifact.s3-us-west-2.amazonaws.com/release/latest/data.tar.gz,"
Download the dataset here
",⬇️,.
28131,#claim-generation-data,Claim generation data,: Claim / citation context data now available to train claim generation models. See ,.
28131,#dataset,Dataset,"
","
"
28131,https://scifact.s3-us-west-2.amazonaws.com/release/latest/data.tar.gz,click here,"Or, ", to download the tarball.
28131,doc/data.md,data.md,See , for descriptions of the schemas for each file type.
28134,https://github.com/LuminosoInsight/exquisite-corpus,Exquisite Corpus,This data comes from a Luminoso project called ,", whose goal is to download good, varied, multilingual corpus data, process it appropriately, and combine it into unified resources such as wordfreq."
28134,http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html,http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html,) and Google Books Syntactic Ngrams (,). The terms of use of this data are:
28134,http://corpus.leeds.ac.uk/list.html,http://corpus.leeds.ac.uk/list.html,"The Leeds Internet Corpus, from the University of Leeds Centre for Translation Studies (",)
28134,http://crr.ugent.be/programs-data/subtitle-frequencies,http://crr.ugent.be/programs-data/subtitle-frequencies,"It contains data from various SUBTLEX word lists: SUBTLEX-US, SUBTLEX-UK, SUBTLEX-CH, SUBTLEX-DE, and SUBTLEX-NL, created by Marc Brysbaert et al. (see citations below) and available at ",.
28134,http://mokk.bme.hu/resources/webcorpus/,http://mokk.bme.hu/resources/webcorpus/,"Halácsy, P., Kornai, A., Németh, L., Rung, A., Szakadát, I., & Trón, V. (2004). Creating open language resources for Hungarian. In Proceedings of the 4th international conference on Language Resources and Evaluation (LREC2004). ","
"
28134,https://oscar-corpus.com/publication/2019/clmc7/asynchronous/,https://oscar-corpus.com/publication/2019/clmc7/asynchronous/,"Ortiz Suárez, P. J., Sagot, B., and Romary, L. (2019). Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. In Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. ","
"
28148,data,data,We expect that most users will be interested in the ," directory, that contains our training corpora, pre-trained models and agreement test sets."
28149,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2329,here, dataset can be downloaded from , (
28149,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2329,here,Download datasets from ,. You can download the following files:
28149,#b-use-given-training-data-to-reproducefine-tune-the-model,B. Use available training data to reproduce/fine-tune the model,"
","
"
28149,#c-use-your-own-data-to-fine-tune-a-new-aspect-controlled-neural-argument-generation-model,C. Use your own data to fine-tune a new aspect-controlled neural argument generation model,"
","
"
28149,#b-use-given-training-data-to-reproducefine-tune-the-model,B. Use given training data to reproduce/fine-tune the model,) and follow ,", Steps 4-5."
28149,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2329,Argument Aspect Detection dataset,"To train an aspect detection model, please download our "," (due to license reasons, it is necessary to fill the form with your name and email). As a model, we suggest BERT from "
28150,https://slideslive.com/38931238/ai4bharatindicnlp-dataset-monolingual-corpora-and-word-embeddings-for-indic-languages,VIDEO,. You can see the talk here: ,.
28150,#indicnlp-news-article-classification-dataset,IndicNLP News Article Classification Dataset,"
","
"
28150,#publicly-available-classification-datasets,Publicly available Classification Datasets,"
","
"
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/pa.vocabfreq.tsv.gz,link,| Language | Sentences | Tokens  | Types | Vocab Frequency | Corpus | | -------- | --------- | ------- | ----- | --------------- | ------ | | pa       | 6.5M      | 179.4M  | 0.5M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/pa.txt.gz,link, | , | | hi       | 62.9M     | 1199.8M | 5.3M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/hi.vocabfreq.tsv.gz,link, | | hi       | 62.9M     | 1199.8M | 5.3M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/hi.txt.gz,link, | , | | bn       | 7.2M      | 100.1M  | 1.5M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/bn.vocabfreq.tsv.gz,link, | | bn       | 7.2M      | 100.1M  | 1.5M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/bn.txt.gz,link, | , | | or       | 3.5M      | 51.5M   | 0.7M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/or.vocabfreq.tsv.gz,link, | | or       | 3.5M      | 51.5M   | 0.7M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/or.txt.gz,link, | , | | gu       | 7.8M      | 129.7M  | 2.4M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/gu.vocabfreq.tsv.gz,link, | | gu       | 7.8M      | 129.7M  | 2.4M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/gu.txt.gz,link, | , | | mr       | 9.9M      | 142.4M  | 2.6M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/mr.vocabfreq.tsv.gz,link, | | mr       | 9.9M      | 142.4M  | 2.6M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/mr.txt.gz,link, | , | | kn       | 14.7M     | 174.9M  | 3.0M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/kn.vocabfreq.tsv.gz,link, | | kn       | 14.7M     | 174.9M  | 3.0M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/kn.txt.gz,link, | , | | te       | 15.1M     | 190.2M  | 4.1M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/te.vocabfreq.tsv.gz,link, | | te       | 15.1M     | 190.2M  | 4.1M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/te.txt.gz,link, | , | | ml       | 11.6M     | 167.4M  | 8.8M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/ml.vocabfreq.tsv.gz,link, | | ml       | 11.6M     | 167.4M  | 8.8M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/ml.txt.gz,link, | , | | ta       | 20.9M     | 362.8M  | 9.4M  | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/corpus_stats/ta.vocabfreq.tsv.gz,link, | | ta       | 20.9M     | 362.8M  | 9.4M  | , | 
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/ta.txt.gz,link, | , |
28150,https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/evaluations/classification/classification_public_datasets.tgz,"
DOWNLOAD
","
","
"
28151,https://github.com/diegma/relation-autoencoder/blob/master/data-sample.txt,sample, Input format: same as ,"
"
28157,https://towardsdatascience.com/exploring-auto-sklearn-models-with-pipelineprofiler-5b2c54136044,Medium post,: ,"
"
28158,http://clic.ub.edu/corpus/,AnCora,Spanish data from the , corpus.
28158,http://clic.ub.edu/corpus/,AnCora,The original annotation was done in a constituency framework as a part of the , project at the University of Barcelona. It was converted to dependencies and used in the 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/train.txt,train,English: , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/valid.txt,valid, / , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/test.txt,test, / , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/vocab.txt,vocab, / ,"
"
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Hebrew/train.txt,train,Hebrew: , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Hebrew/valid.txt,valid, / , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Hebrew/test.txt,test, / , / 
28163,https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/Hebrew/vocab.txt,vocab, / ,"
"
28166,http://www.nersc.gov/about/nersc-staff/data-analytics-services/karthik-kashinath/,Karthik Kashinath,", ",", "
28166,https://www.nersc.gov/about/nersc-staff/data-analytics-services/mustafa-mustafa/,Mustafa Mustafa,", ",", "
28166,http://www.nersc.gov/about/nersc-staff/data-analytics-services/prabhat/,Prabhat,", ",", "
28169,https://github.com/dmis-lab/BioSyn#datasets,here,. Note that the initial run will take some time to embed the whole dictionary. You can download the dictionary file ,.
28185,http://visionandlanguage.net/VIST/dataset.html,Visual Storytelling,The SIND dataset can be downloaded from the , website.
28186,data,data,Prepare training data and word embeddings in ,.
28191,https://github.com/oana-inel/FAIRView-VideoSummaryExplanations/blob/master/data/video_dataset_content.csv,videos,200 , machine annotated with key concepts extracted from video subtitles and video frames;
28191,https://github.com/oana-inel/FAIRView-VideoSummaryExplanations/blob/master/data/video_summaries,video summaries,800 ,  (four summaries per video) machine annotated with concepts extracted from video subtitles and frames;
28191,https://github.com/oana-inel/FAIRView-VideoSummaryExplanations/blob/master/data/visual_explanations,visual explanations,3200 , (four explanations per video summary with different levels of 
28191,https://github.com/oana-inel/FAIRView-VideoSummaryExplanations/blob/master/data,data folder,"If you want to regenerate the results of the machine enrichment tools used for the machine annotation of both original videos and video summaries, which can be found in the "," of this repository, you need to register for the following Google Cloud APIs and follow their instructions. A schelethon notebook for running each of the APIs below is provided in the "
28197,./dataset/,"
dataset
",. The documentation for the dataset can be found in ,.
28199,https://production-media.paperswithcode.com/about/datasets.json.gz,Datasets,"
","
"
28202,https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/,Thread on Reddit,"Reddit user Stuck_In_The_Matrix has created a very large archive of public Reddit comments and put them up for downloading, see: ","
"
28202,http://www.nltk.org/data.html,NLTK.org,Data was created by using the Vader dataset from ,.
28205,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,GloVe embeddings,Download the pretrained , and put it into the 
28208,data/PREPARE_DATA.md,PREPARE_DATA.md,See ,.
28209,#data,Data,"
","
"
28209,#running-on-new-datasets,Running on New Datasets,"
","
"
28209,http://jmcauley.ucsd.edu/data/amazon/index.html,Amazon," | 1,596 | 14 | Image Generation (215), Object Detection (296), Image Classification (361), Semantic Segmentation (170), Pose Estimation (96), Super Resolution (75), Text Generation (24), Text Classification (26), Named Entity Recognition (22), Question Answering (102), Machine Translation (117), Language Modeling (44), Speech Synthesis (27), Speech Recognition (21) | | ", | 
28210,https://github.com/datamllab/pyodds,PyODD: An End-to-end Outlier Detection System,"
","
"
28210,#datasets,Datasets,"
","
"
28210,http://odds.cs.stonybrook.edu/yelpchi-dataset/,Yelp Spam Review Dataset,", we preprocessed ", with reviews as nodes and three relations as edges.
28227,#dataset-processing,Data Processing,"
","
"
28227,#dataset-processing,download dataset,"
","
"
28227,#dataset-processing,convert dataset,"
","
"
28227,#dataset-processing,generate examples,"
","
"
28227,https://gist.githubusercontent.com/csarron/2a7f5da27f45e7e0795c9946f7c95f76/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py,link,GLUE: ,"
"
28227,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json,train-v1.1.json,SQuAD v1.1: , and 
28227,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json,dev-v1.1.json, and ,"
"
28227,https://www.cs.cmu.edu/~glai1/data/race/,RACE dataset,"
","
"
28228,encode_datasets.py,script,"If you wanna see how this encoding is done on our datasets, check out this ",.
28228,https://console.cloud.google.com/storage/browser/unifiedqa/data,this Google Cloud bucket,"While the datasets we used are all public, it could be a bit time-confusing to convert them all into text-to-text format. We're releasing the already-proccessed text-to-text datasets based on the encoding used in this work. Files are included in ",. 
28228,encode_datasets.py,Here,. , is the script we used in order to convert each dataset into text-in-text-out format.
28228,#feeding-data-into-unifiedqa,the earlier section,) when encoding encoding your inputs. See , where we delineate how to encode the inputs.
28250,https://github.com/nyu-dl/dl4mt-nonauto#downloading-datasets--pre-trained-models,here,WMT'16 Romania to English (RO-EN) can be obtained from ,.
28263,./data,data,) and save them into , folder.
28263,./data,data,Special word lists: You can find all word lists used in this project in , folder.
28271,https://github.com/butsugiri/gec-pseudodata,gec-pseudodata,"
","
"
28272,https://github.com/butsugiri/gec-pseudodata/blob/master/bpe/bpe_code.trg.dict_bpe8000,bpe code file (compatible with subword-nmt),"
","
"
28272,https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.pret.checkpoint_last.pt,pretlarge (pre-train only),"
","
"
28272,https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.spell_error.pretrain.checkpoint_last.pt,pretlarge+SSE (pre-train only),"
","
"
28272,https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.finetune.checkpoint_best.pt,pretlarge (finetuned),"
","
"
28272,https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.spell_error.finetune.checkpoint_best.pt,pretlarge+SSE (finetuned),"
","
"
28272,https://github.com/butsugiri/gec-pseudodata/tree/master/vocab,vocabulary files,"
","
"
28272,https://github.com/butsugiri/gec-pseudodata/tree/master/outputs,outputs,Model outputs Table 5 are available in ,.
28272,https://github.com/butsugiri/gec-pseudodata/blob/master/bpe/bpe_code.trg.dict_bpe8000,this,Split source sentence into subwords using , bpe code file.
28276,./download_data_and_test/download_Sintel_test_set.m,this code,"To reproduce the numbers in Table I, run "," to download the data, and "
28276,./download_data_and_test/test_table_I_Sintel.txt,this code," to download the data, and ", to produce the outputs.
28276,./download_data_and_test/download_Adobe_MIT_test_set.m,this code,"To reproduce the numbers in Table II, run "," to download the data, and "
28276,./download_data_and_test/test_table_II_Adobe_MIT.txt,this code," to download the data, and ", to produce the outputs.
28276,./download_data_and_test/download_TESTIMAGES_dataset.txt,these instructions,"To reproduce the numbers in Table III, follow "," to download the data, and run "
28276,./download_data_and_test/test_table_III_TESTIMAGES_1200.txt,this code," to download the data, and run ", to produce the outputs.
28276,./download_data_and_test/test_table_IV_Kodak.txt,this code," which has already been downloaded to ./data/Test/Kodak, run ",.
28276,./download_data_and_test/download_ESPL_v2_dataset.txt,these instructions,"To reproduce the numbers in Table V, follow "," to download the data, and run "
28276,./download_data_and_test/test_table_V_ESPL_v2.txt,this code," to download the data, and run ", to produce the outputs.
28276,./download_data_and_test/download_MS_COCO_dataset.txt,these instructions,"To reproduce the numbers in Table S4, follow "," to download the data, and run "
28276,./download_data_and_test/test_table_S4_MS_COCO.txt,this code," to download the data, and run ", to produce the outputs.
28276,./download_data_and_test/download_TESTIMAGES_dataset.txt,these instructions,"To reproduce the numbers in Table S5, follow "," to download the data, and run "
28276,./download_data_and_test/test_table_S5_TESTIMAGES_800.txt,this code," to download the data, and run ", to produce the outputs.
28276,./download_data_and_test/test_table_S6.txt,this code,"To reproduce the numbers in Table S6, and run ",.
28276,./download_data_and_test/test_table_S7_Adobe_MIT.txt,this code,"To reproduce the numbers in Table S7, and run ",.
28276,./download_data_and_test/download_BSD_dataset.txt,these instructions,"To reproduce the numbers in Table S8, follow "," to download the data, and run "
28276,./download_data_and_test/test_table_S8.txt,this code," to download the data, and run ", to produce the outputs.
28276,./download_data_and_test/download_train_val_data.m,this code,Run , to download training data.
28286,data,data/,"
", contains raw and processed datasets that we include in this repository for testing.
28304,#examples-and-datasets,Examples and datasets,Dump data from the SQL database to a JSON file (see , for the JSON structure).
28305,https://github.com/wwzjer/Semi-supervised-IRR/tree/master/data/rainy_image_dataset/real_input,SIRR,147 unlabeled rainy images from ,"
"
28309,#download-data,Download Data,"
","
"
28309,#data-preparation,Data Preparation,"
","
"
28309,https://sheffieldnlp.github.io/fever/data.html,our website,Download the FEVER dataset from , into the data directory:
28309,#data-preparation,Data Preparation,Run the oracle evaluation for the Decomposable Attention model on the dev set (requires sampling the NEI class for the dev dataset - see ,)
28317,https://github.com/LibraryOfCongress/newspaper-navigator/tree/master/beyond_words_data,/beyond_words_data/," dataset consists of crowdsourced locations of photographs, illustrations, comics, cartoons, and maps in World War I era newspapers, as well as corresponding textual content (titles, captions, artists, etc.). In order to utilize this dataset to train a visual content recognition model for historical newspaper scans, a copy of the dataset can be found in this repo (in ",) formatted according to the 
28317,http://cocodataset.org/#format-data,COCO,) formatted according to the , standard for object detection. The images are stored in 
28317,https://github.com/LibraryOfCongress/newspaper-navigator/tree/master/beyond_words_data/images,/beyond_words_data/images/, standard for object detection. The images are stored in ,", and the JSON can be found in "
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/beyond_words_data/trainval.json,/beyond_words_data/trainval.json,", and the JSON can be found in ",". The JSON also includes annotations of headlines and advertisements, as well as annotations for additional pages with maps to boost the number of maps in the dataset. These annotations were all done by one person (myself) and thus are unverified. The breakdown is as follows:"
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/beyond_words_data/train_80_percent.json,/beyond_words_data/train_80_percent.json,"For an 80%-20% split of the dataset, see ", and 
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/beyond_words_data/val_20_percent.json,/beyond_words_data/val_20_percent.json, and ,".  Lastly, the original verified annotations from the "
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/beyond_words_data/beyond_words.txt,beyond_words_data/beyond_words.txt, site can be found at ,.
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/process_beyond_words_dataset.py,process_beyond_words_dataset.py," annotations added since 12/01/2019, first update the annotations file from the Beyond Words website, then run the script ",".  To add the additional headline and advertisement annotations, you can retrieve them from "
28317,https://github.com/LibraryOfCongress/newspaper-navigator/blob/master/beyond_words_data/trainval.json,/beyond_words_data/trainval.json,".  To add the additional headline and advertisement annotations, you can retrieve them from ", and add them to your dataset.
28324,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json,train-v1.1.json,"
","
"
28324,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json,dev-v1.1.json,"
","
"
28324,https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json,train-v2.0.json,"
","
"
28324,https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json,dev-v2.0.json,"
","
"
28324,https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py,WordPiece,": Apply whitespace tokenization to the output of the above procedure, and apply ", tokenization to each token separately. (Our implementation is directly based on the one from 
28324,https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html,Project Guttenberg Dataset, no longer have it available for public download. The , is a somewhat smaller (200M word) collection of older books that are public domain.
28324,https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py,tensor2tensor's WordPiece generation script,"
","
"
28325,http://bvisionweb1.cs.unc.edu/licheng/referit/data/refcocog.zip,RefCOCOg dataset,This folder contains adversarial annotations for part of the test images from ,.
28325,http://mscoco.org/dataset/#overview,mscoco," folder, which can be from ", Bounding box annotations are also from mscoco dataset
28339,https://github.com/fladhak/creative-summ-data/tree/main/booksum,repository, workshop at COLING 2022. A revised version of the novel chapter summarization task can be found at the associated ,.
28345,https://www.yelp.com/dataset,Yelp,For our experiment on ," dataset, we used "
28352,https://github.com/afshinrahimi/wikiumls/blob/master/data/meshdesc_wiki.json,here,Gold values Mesh Descriptor ID:WikiTitle collected from wikidata annotated by wikidata contributors are available ,. To convert MeSH descriptor ids into UMLS CUIs use MRCONSO.RRF file which is available 
28358,./deeplesion/dataset/generate_mask_with_grabcut.md,mask generation,"Before training, mask should be generated from bounding box and recists. ","
"
28365,https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks#datasets,Released Datasets,"
","
"
28365,https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/newsph/newsph-nli.zip,"
download
","
","
"
28365,https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/wikitext-tl-39/wikitext-tl-39.zip,"
download
","
","
"
28365,https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/,WikiText Long Term Dependency dataset," Large scale, unlabeled text dataset with 39 Million tokens in the training set. Inspired by the original "," (Merity et al., 2016). TL means ""Tagalog."" Originally published in "
28365,https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/hatenonhate/hatespeech_raw.zip,"
download
","
","
"
28365,https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/dengue/dengue_raw.zip,"
download
","
","
"
28366,https://github.com/luuleitner/deepMTJ/blob/master/license_datasets,Creative Commons Attribution 4.0 International License,The provided dataset and models are licensed under a ,.
28370,https://20bn.com/datasets/something-something/v1,something-something-v1,Download the , and 
28370,https://20bn.com/datasets/something-something/v2,something-something-v2, and ,", and extract frames for videos."
28374,https://github.com/leondz/hatespeechdata/blob/master/README.md,source code,Please send contributions via github pull request. You can do this by visiting the ," on github and clicking the edit icon (a pencil, above the text, on the right) - more details "
28374,https://huggingface.co/datasets/strombergnlp/shaj,strombergnlp/shaj,Dataset reader: 🤗 ,"
"
28374,https://huggingface.co/datasets/strombergnlp/offenseval_2020,strombergnlp/offenseval_2020,Dataset reader: 🤗 ,"
"
28374,https://www.kaggle.com/naurosromim/bengali-hate-speech-dataset,https://www.kaggle.com/naurosromim/bengali-hate-speech-dataset,Link to data: ,"
"
28374,https://huggingface.co/datasets/DDSC/dkhate,DDSC/dkhate,Dataset reader: 🤗 ,"
"
28374,https://huggingface.co/datasets/strombergnlp/bajer_danish_misogyny,strombergnlp/bajer_danish_misogyny,Dataset reader: 🤗 ,"
"
28374,https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech,https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech,Link to data: ,"
"
28374,https://www.ims.uni-stuttgart.de/data/stance_hof_us2020,https://www.ims.uni-stuttgart.de/data/stance_hof_us2020,Link to data: ,"
"
28374,https://github.com/paul-rottger/hatecheck-data,https://github.com/paul-rottger/hatecheck-data,Link to data: ,"
"
28374,https://github.com/networkdynamics/slur-corpus,https://github.com/networkdynamics/slur-corpus,Link to data: ,"
"
28374,https://www.tensorflow.org/datasets/catalog/civil_comments,https://www.tensorflow.org/datasets/catalog/civil_comments,Link to data: ,"
"
28374,https://github.com/Vicomtech/hate-speech-dataset,https://github.com/Vicomtech/hate-speech-dataset,Link to data: ,"
"
28374,https://amiibereval2018.wordpress.com/important-dates/data/,https://amiibereval2018.wordpress.com/im nt-dates/data/,Link to data: ,"
"
28374,https://hasocfire.github.io/hasoc/2019/dataset.html,https://hasocfire.github.io/hasoc/2019/dataset.html,Link to data: ,"
"
28374,https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN,https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN,Link to data: ,"
"
28374,https://hasocfire.github.io/hasoc/2019/dataset.html,https://hasocfire.github.io/hasoc/2019/dataset.html,Link to data: ,"
"
28374,http://www.straintek.com/data/,http://www.straintek.com/data/,Link to data: ,"
"
28374,http://www.straintek.com/data/,http://www.straintek.com/data/,Link to data: ,"
"
28374,https://huggingface.co/datasets/strombergnlp/offenseval_2020,strombergnlp/offenseval_2020,Dataset reader: 🤗 ,"
"
28374,https://hasocfire.github.io/hasoc/2019/dataset.html,https://hasocfire.github.io/hasoc/2019/dataset.htm,Link to data: ,"
"
28374,https://github.com/msang/hate-speech-corpus,https://github.com/msang/hate-speech-corpus,Link to data: ,"
"
28374,http://www.di.unito.it/~tutreeb/haspeede-evalita18/data.html,http://www.di.unito.it/~tutreeb/haspeede-evalita18/data.html,Link to data: ,"
"
28374,http://www.di.unito.it/~tutreeb/haspeede-evalita18/data.html,http://www.di.unito.it/~tutreeb/haspeede-evalita18/data.html,Link to data: ,"
"
28374,https://amiibereval2018.wordpress.com/important-dates/data/,https://amiibereval2018.wordpress.com/important-dates/data/,Link to data: ,"
"
28374,https://huggingface.co/datasets/strombergnlp/offenseval_2020,strombergnlp/offenseval_2020,Dataset reader: 🤗 ,"
"
28374,http://hatespeechdata.com/,http://hatespeechdata.com/,This page is ,.
28378,#preprocessing-a-dataset,Preprocessing a Dataset,"
","
"
28378,#an-example-preprocessing-a-new-dataset,An Example: Preprocessing a New Dataset,"
","
"
28388,data/ru.tgz,"
ru.tgz
","
"," contains the full preprocessed romanized Russian dataset, including the symbol tables and priors. The language model data file is a preprocessed version of the "
28388,data/ar.tgz,"
ar.tgz
","
", contains only the files for the symbol tables and priors. The 
28390,docs/dataset.md,BIG Dataset and Relabeled PASCAL VOC 2012,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/predictions,predictions/, are available in ,.
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/data,data/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/AID1706_binarized_sars.csv,AID1706_binarized_sars.csv,"
"," - (N = 290,726; hits = 405) In-vitro assay that detects inhibition of SARS-CoV 3CL protease via fluorescence from "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/evaluation_set_v2.csv,evaluation_set_v2.csv,"
"," - (N = 5,671; hits = 41) An evaluation set for SARS-CoV 3CL protease containing 41 experimentally validated hits along with 5630 molecules from the "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/AID1706_binarized_sars_full_eval_actives.csv,AID1706_binarized_sars_full_eval_actives.csv,"
"," - (N = 290,767; hits = 446) is AID1706_binarized_sars.csv combined with the 41 validated hits from evaluation_set_v2.csv."
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/PLpro.csv,PLpro.csv,"
"," - (N = 233,891; hits = 697) Bioassay that detects activity against SARS-CoV in yeast models via PL protease inhibition. Combines PubChem data from "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/mpro_xchem.csv,​mpro_xchem.csv,"
", - (N = 880; hits = 78) Fragments screened for 3CL protease binding using crystallography techniques. Data is sourced from the 
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/amu_sars_cov_2_in_vitro.csv,amu_sars_cov_2_in_vitro.csv,"
"," - (N = 1,484; hits = 88) FDA-approved compounds screened against SARS-CoV-2 "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/ellinger.csv,ellinger.csv,"
"," - (N = 5,632; hits = 67) Compounds screened against SARS-CoV-2 "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/corona_literature_idex.csv,corona_literature_idex.csv,"
", - (N = 101) FDA-approved drugs that are mentioned in generic coronavirus literature. Drug to SMILES mapping is generated through the PubChem idex service and may contain multiple SMILES for generic drug names. These are not guaranteed to be effective against any targets; they simply appear in the literature.
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/broad_repurposing_library.csv,broad_repurposing_library.csv,"
"," - (N = 6,111) Compounds from the "
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/external_library.csv,external_library.csv,"
", - (N = 861) A set of FDA-approved drugs.
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/expanded_external_library.csv,expanded_external_library.csv,"
"," - (N = 2,661) A larger set of FDA-approved drugs, but not a strict superset of external_library.csv."
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/ecoli.csv,ecoli.csv,"
"," - (N = 2,335; hits = 120) Compounds which have been screened for inhibitory activity against "
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/splits.zip,splits.zip,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/raw_data,raw_data/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/predictions,predictions/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/plots,plots/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/AID1706_binarized_sars.csv,AID1706_binarized_sars.csv,"t-SNE plots comparing the datasets. Note that in the plots, ""sars_pos"" and ""sars_neg"" refer to any hits or non-hits, respectively, across both ", and 
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/PLpro.csv,PLpro.csv, and ,.
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/conversions,conversions/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/similarity_computations,similarity_computations/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/scripts,scripts/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/statistics,statistics/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/interpretation,interpretation/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/tree/master/old,old/,"
","
"
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/AID1706_binarized_sars.csv,AID1706_binarized_sars.csv, and should be run from the main directory in the chemprop repo. You may need to modify some paths depending on your directory structure. The commands below assume you are using , but can be modified to work with any of the datasets.
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/splits.zip,splits.zip, is seeded so that this will reproduce the same train/dev/test split as in ,.
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/mpro_xchem.csv,​mpro_xchem.csv,Experiment combining data on the 3CLpro target for SARS-CoV-2 , and SARS-CoV 
28396,https://github.com/yangkevin2/coronavirus_data/blob/master/data/AID1706_binarized_sars.csv,AID1706_binarized_sars.csv, and SARS-CoV ,.
28401,https://udata.readthedocs.io/en/latest/,full documentation,The , is hosted on Read the Docs.
28401,https://github.com/opendatateam,OpenDataTeam,It is collectively taken care of by members of the ,.
28414,https://fiware-tutorials.readthedocs.io/en/latest/linked-data,FIWARE 601: Introduction to Linked Data,"
","
"
28414,https://fiware-tutorials.readthedocs.io/en/latest/relationships-linked-data,FIWARE 602: Linked Data Relationships and Data Models,"
","
"
28416,doc/manuals/data-storage.md,Storing historical raw and aggregated time series context information,Detailed information about the API to store raw and aggregated time series context information can be found at the , section of the documentation.
28416,doc/manuals/raw-data-retrieval.md,Getting historical raw context information,Detailed information about the API to retrieve raw context information can be found at the , section of the documentation.
28416,doc/manuals/aggregated-data-retrieval.md,Getting historical aggregated time series context information,Detailed information about the API to retrieve aggregated time series context information can be found at the , section of the documentation.
28416,doc/manuals/aggregated-data-retrieval.md,Removing historical raw and aggregated time series context information,Detailed information about the API to remove raw and aggregated time series context information can be found at the , section of the documentation.
28418,https://www.argoverse.org/data.html#download-link,their website,Argoverse provides both the full dataset and the sample version of the dataset for testing purposes. Head to , to see the download option.
28426,https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html,read this,"By default, this will produce a file using the libsvm format. Scikit-learn can ",.
28430,#data-and-models,data,"Before running on RefQA, you should download/move the ", and the SQuAD 1.1 dev file 
28438,https://github.com/google-research-datasets/gap-coreference,GAP repo,The gap files have been downloaded from the ,"
"
28438,https://github.com/shtoshni92/petra/blob/master/data/num_people.tsv,data/num_people.tsv, We created the diagnostic test of counting unique people in a document for which we annotated 100 GAP validation instances. The annotation file is ,"
"
28440,https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb,finetune-to-livedoor-corpus.ipynb,"
","
"
28448,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,VoxCeleb1,. Our evaluation results on ," demonstrate  that  the  derived  CNN  architectures  from  the  proposed approach significantly outperform current speaker recognition systems  based  on  VGG-M,  ResNet-18,  and  ResNet-34  back-bones, while enjoying lower model complexity."
28448,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html,VoxCeleb1,"
",: You will need 
28451,http://xarray.pydata.org/,"
xarray
",Or use , to access the NetCDF dataset:
28451,http://xarray.pydata.org/,"
xarray
",Or use , to access the NetCDF dataset:
28464,http://minerl.io/dataset/,Download,"
", and place MineRL dataset under 
28468,#1-datasets,1. Datasets,"
","
"
28468,#11-regular-latin-datasets,1.1 Regular Latin Datasets,"
","
"
28468,#12-irregular-latin-datasets,1.2 Irregular Latin Datasets,"
","
"
28468,#13-multilingual-datasets,1.3 Multilingual Datasets,"
","
"
28468,#14-synthetic-datasets,1.4 Synthetic Datasets,"
","
"
28468,#15-comparison-of-the-benchmark-datasets,1.5 Comparison of the Benchmark Datasets,"
","
"
28468,#22-performance-comparison-on-benchmark-datasets,2.2 Performance Comparison on Benchmark Datasets,"
","
"
28468,#221-performance-comparison-of-recognition-algorithms-on-regular-latin-datasets,2.2.1 Performance Comparison of Recognition Algorithms on Regular Latin Datasets,"
","
"
28468,#222-performance-comparison-of-recognition-algorithms-on-irregular-latin-datasets,2.2.2 Performance Comparison of Recognition Algorithms on Irregular Latin Datasets,"
","
"
28468,http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset,IIIT5K-download,"
","
"
28468,http://dagdata.cvc.uab.es/icdar2013competition/?ch=2&com=downloads,IC13-download,"
","
"
28468,http://cs-chan.com/downloads_CUTE80_dataset.html,CUTE80-download,"
","
"
28468,http://rctw.vlrlab.net/dataset/,RCTW-17-download,"
","
"
28468,https://ctwdataset.github.io/,CTW-download,"
","
"
28468,http://www.robots.ox.ac.uk/~vgg/data/text/,Synth90k-download,"
","
"
28468,http://chongdata.com/ocr/,Online Chinese Recognition,             |  √   |  √   |  ×   | |   ,    |  √   |  √   |  ×   | |   
28468,http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset,dataset,"
","
"
28477,https://www.yelp.com/dataset/challenge,yelp,"
",: negative sentiment (0) <--> positive sentiment (1)
28477,https://github.com/raosudha89/GYAFC-corpus,GYAFC,"
",: informal text (0) <--> formal text (1)
28477,https://github.com/raosudha89/GYAFC-corpus,https://github.com/raosudha89/GYAFC-corpus,"). If you want to download the train and validation dataset, please follow the guidance at ",". And then, name the corpora of two styles as the yelp dataset."
28477,#extend-to-other-tasks-and-datasets,Extend to other tasks and datasets,"If you want to use your own datasets, please follow the guidance of next section ",.
28481,http://www.mustafabaydogan.com/files/viewdownload/14-multivariate-time-series-classification/29-multivariate-time-series-classification-datasets.html,Multivariate Time Series Classification Datasets,The multivariate dataset format is based on ,.
28481,http://www.cs.ucr.edu/~eamonn/time_series_data/,[LINK],In our experimental evaluation using a benchmark of time series datasets ,", TEASER is two to three times as early while keeping the same (or even a higher) level of accuracy, when compared to the state of the art."
28481,http://www.cs.ucr.edu/~eamonn/time_series_data/,The UCR Time Series Classification Archive,"
","
"
28489,#data,Data,"
","
"
28489,#metadata,Metadata,"
","
"
28489,https://github.com/rcamino/dataset-pre-processing,rcamino/dataset-pre-processing,Data is not included in the examples because of space limitations and because it does not belong to me. You can find example scripts to download the kind of data and metadata required for this project in ,.
28489,https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients,dataset repository,You can find more information about the dataset in the , or in 
28489,https://github.com/rcamino/dataset-pre-processing/tree/master/dataset_pre_processing/uci/default_of_credit_card_clients,my code repository, or in ,.
28498,./data_analysis,data_analysis, if you are using conda and want to run these notebooks from the environment. More data analysis is provided in , folder.
28498,./visdialch/data,data,"
", -- dataset reader and vocabulary defined here
28498,./released_datasets,released_datasets,We have released two subsets of Visdial val set (mentioned in our paper) in the folder ,:
28498,./evaluate_subset_data,evaluate_subset_data,"To evaluate on these subsets, use the shell scripts provided in ",.
28498,./subset_dialog_data,subset_dialog_data,We used the scripts in , to create these subsets from VisdialVal set.
28498,./released_datasets/visdialconv/README.md,README,See the , in the visdialconv folder to know more about the annotations.
28512,#data,Data,"
","
"
28512,https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data,Cam2BEV Data Repository,"We provide two synthetic datasets, which can be used to train the neural networks. The datasets are hosted in the ",. Both datasets were used to produce the results presented in our paper:
28512,https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/1_FRLR,"
Dataset 1_FRLR
","
",": images from four vehicle-mounted cameras, ground-truth BEV image centered above the ego vehicle"
28512,https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/2_F,"
Dataset 2_F
","
",: images from one frontal vehicle camera; ground-truth BEV image left-aligned with ego vehicle
28512,https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data,repository's README,"For more information regarding the data, please refer to the ",.
28513,https://github.com/haczqyf/ggc/tree/master/ggc/data,here,Data sets can be found ,". Each data set is a single csv-like file. Each row represents a sample. The first column is sample id, the last column is sample label and all the columns in the middle are features. Detailed descriptions of origins of data sets are described in the SI Appendix in our "
28517,#perturbing-data-for-invs-and-dirs,Perturbing data for INVs and DIRs,"
","
"
28517,notebooks/tutorials/1.%20Generating%20data.ipynb,Generating data,"
","
"
28517,notebooks/tutorials/2.%20Perturbing%20data.ipynb,Perturbing data,"
","
"
28517,notebooks/tutorials/1.%20Generating%20data.ipynb,1. Generating data,See , for more details.
28517,notebooks/tutorials/1.%20Generating%20data.ipynb,1. Generating data,See , for more details. In template:
28517,notebooks/other/Acquiring%20multilingual%20lexicons%20from%20wikidata.ipynb,got the data, language. We , from 
28517,https://www.wikidata.org,wikidata, from ,", so there is a bias towards names on wikipedia."
28517,notebooks/tutorials/2.%20Perturbing%20data.ipynb,2.Perturbing data,See , for more details. Custom perturbation function:
28522,https://github.com/keyonvafa/tbip/tree/master/data/senate-speeches-114,data/senate-speeches-114,Preprocessed Senate speech data for the 114th Congress is included in ,. The original data is from 
28522,https://github.com/keyonvafa/tbip/tree/master/data/candidate-tweets-2020,data/candidate-tweets-2020,. Preprocessed 2020 Democratic presidential candidate tweet data is included in ,.
28522,https://github.com/keyonvafa/tbip/tree/master/data/senate-speeches-114/clean,data/senate-speeches-114/clean,See , for an example of what the four files look like for Senate speeches. The script 
28528,dataset,dataset,"
", - consists of dataset generation codes
28540,https://www.uni-kl.de/channel-codes/channel-codes-database/bch-and-hamming/,Kaiserslautern University," format, to test the binary. This file was obtained from ",. You can test the binary by executing
28543,https://storage.googleapis.com/sfr-samson-data-research/adv_ft_nmt_enfr_transformer-big.tar.gz,Transformer-big for WMT'14 English-French,"
",: Compatible with 
28543,https://storage.googleapis.com/sfr-samson-data-research/adv_ft_mnli_BERT-base.tar.gz,BERT-base for MNLI,"
",: Compatible with 
28543,https://storage.googleapis.com/sfr-samson-data-research/adv_ft_squad2_BERT-base.tar.gz,BERT-base for SQuAD 2,"
",: Compatible with 
28549,https://datasets.vqa.mmsp-kn.de/plosone/nepl/videos.tar.gz,here," folder with the KoNViD-1k videos in the root directory, which can be downloaded as a tarball ",.
28549,https://datasets.vqa.mmsp-kn.de/plosone/nepl/frames.tar.gz,Download,: all the individual video frames are extracted here. ,"
"
28549,https://datasets.vqa.mmsp-kn.de/plosone/nepl/networks.tar.gz,Download,: fine-tuned networks are saved here. ,"
"
28549,https://datasets.vqa.mmsp-kn.de/plosone/nepl/features.tar.gz,Download,: features for all videos extracted from the networks (with and without fine-tuning) are stored here. ,"
"
28552,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/,IMDB-WIKI,"To get age information, we use an age classifier pretrained on ", dataset. We use the model released from paper 
28552,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/dex_imdb_wiki.caffemodel,caffe model,"To prepare the model, you need to download the original ", and convert it to PyTorch format. We use the converter 
28552,https://github.com/NVlabs/ffhq-dataset,FFHQ,Download , dataset and unzip it to the 
28555,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,Kitti,"
","
"
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-bm25-b8.tar,Quantized BM25,| Corpora                                                                                                                                   |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,                                           | 1.2 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil-noexp.tar,uniCOIL (noexp), | | ,                                    | 2.7 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil.tar,uniCOIL (d2q-T5), | | ,                                         | 3.4 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil-tilde-expansion.tar,uniCOIL (TILDE), | | ,                          | 3.9 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-deepimpact.tar,DeepImpact, | | ,                                            | 3.6 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-distill-splade-max.tar,SPLADEv2, | | ,                                      | 9.9 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-splade_distil_cocodenser_medium.tar,SPLADE-distill CoCodenser-medium, | | , | 4.9 GB | 
28562,https://rgw.cs.uwaterloo.ca/pyserini/data/msmarco-passage-splade-pp-ed.tar,SPLADE++ CoCondenser-EnsembleDistil, | | ,                         | 4.2 GB | 
28562,https://rgw.cs.uwaterloo.ca/pyserini/data/msmarco-passage-splade-pp-sd.tar,SPLADE++ CoCondenser-SelfDistil, | | ,                             | 4.8 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil-noexp.tar,MS MARCO V1 doc: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,                   |  11 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil.tar,MS MARCO V1 doc: uniCOIL (d2q-T5), | | ,                        |  19 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_noexp_0shot.tar,MS MARCO V2 passage: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,            |  24 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_0shot.tar,MS MARCO V2 passage: uniCOIL (d2q-T5), | | ,                 |  41 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_noexp_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (noexp),| Corpora                                                                                                                                         |   Size | Checksum                           | |:------------------------------------------------------------------------------------------------------------------------------------------------|-------:|:-----------------------------------| | ,       |  55 GB | 
28562,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (d2q-T5), | | ,            |  72 GB | 
28562,docs/regressions-beir-v1.0.0-nfcorpus-flat.md,+, | | NFCorpus   | ,       | 
28562,docs/regressions-beir-v1.0.0-nfcorpus-flat-wp.md,+,       | ,     | 
28562,docs/regressions-beir-v1.0.0-nfcorpus-multifield.md,+,     | ,       | 
28562,docs/regressions-beir-v1.0.0-nfcorpus-unicoil-noexp.md,+,       | , | 
28562,docs/regressions-beir-v1.0.0-nfcorpus-splade-distil-cocodenser-medium.md,+, | , | | NQ         | 
28577,http://nlp.stanford.edu/data/glove.840B.300d.zip,GloVe embeddings,Download the , and put it under the 
28584,(https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data),tff.simulation.datasets module,The main dataset used for these experiments is hosted by Kaggle and made available through the , in the 
28593,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ dataset, | Raw data for the ,. | └  
28593,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ repository,"For license information regarding the FFHQ dataset, please refer to the ",.
28593,https://github.com/NVlabs/ffhq-dataset,Flickr-Faces-HQ repository,"), please refer to the ",.
28593,./dataset_tool.py,dataset_tool.py,"To obtain other datasets, including LSUN, please consult their corresponding project pages. The datasets can be converted to multi-resolution TFRecords using the provided ",:
28600,#supported-datasets,Supported datasets,"
","
"
28600,./asteroid/data/musdb18_dataset.py,MUSDB18,[x] , (
28600,./asteroid/data/fuss_dataset.py,FUSS,[x] , (
28600,./asteroid/data/avspeech_dataset.py,AVSpeech,[x] , (
28600,./asteroid/data/kinect_wsj.py,Kinect-WSJ,[x] , (
28604,https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/portuguese,Chatterbot-corpus,"To create the dadaset was used public domain texts.Initially, the texts were extracted from Wikipedia articles displayed in the Highlights section. In a second phase, texts were also extracted from  ",", a corpus originally created for the construction of chatbots. We also used 20 sets of phonetically balanced phrases, each set containing 10 phrases proposed by "
28604,https://soundcloud.com/user-797601460/sets/tts-mozilla-trained-in-tts-portuguese-corpus-with-wavernn-and-griffinlim-vocoders,Link,Synthesized samples of the best model with Griffin–Lim (1.wav) and WaveRNN (1-wavernn.wav) vocoder : ,"
"
28605,https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset,The World English Bible,"
","
"
28605,https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset,Kaggle Datasets,. Kyubyong split each chapter by verse manually and aligned the segmented audio clips to the text. They are 72 hours in total. You can download them at ,.
28606,https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset,TWEB,"
","
"
28606,http://www.caito.de/2019/01/the-m-ailabs-speech-dataset/,M-AI-Labs,"
","
"
28612,#custom-datasets,custom datasets,", abstracts from arXiv, and song lyrics. By default, the scripts are configured to use the hierarchical mask function outlined in our paper. This section outlines how to train ILM models on ", and 
28616,http://nlp.stanford.edu/data/glove.42B.300d.zip,Common Crawl (42B tokens 300 dimention),We utilized the , GLOVE embedding in 
28622,https://github.com/codemayq/chinese_chatbot_corpus,chinese_chatbot_corpus,|中文闲聊语料 | 数据集地址 |语料描述| |---------|--------|--------| |常见中文闲聊|,|包含小黄鸡语料、豆瓣语料、电视剧对白语料、贴吧论坛回帖语料、微博语料、PTT八卦语料、青云语料等| |50w中文闲聊语料 | 
28624,https://data.vision.ee.ethz.ch/cvl/ntire21/#live,stream,: Our CVPR 2021 NTIRE workshop event will be available for everyone! Live ,", or "
28624,https://data.vision.ee.ethz.ch/cvl/ntire21/#schedule,here," to join on June 19 2021, starting 14:00 UTC (7:00 Pacific Standard Time). Full schedule ",.
28624,https://data.vision.ee.ethz.ch/cvl/aim20/,AIM workshop,: VIDIT is used for the relighting challenge in the ,", part of ECCV 2020. Check out the relighting competition beginning May 13th 2020, it is made up of 3 tracks for "
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track1_train.zip,Train,: [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track1_validation.zip,Validation_Input,] - [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track1_validation_gt.zip,Validation_GT,] - [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track1_test.zip,Test_Input,] - [,]
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track2_train.zip,Train,: [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track2_validation.zip,Validation_Input,] - [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track2_validation_gt.zip,Validation_GT,] - [,] - [
28624,https://datasets.epfl.ch/vidit/NTIRE2021/track2_test.zip,Test_Input,] - [,]
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track1_train.zip,Train, (1024x1024): [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track1_validation.zip,Validation_Input,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track1_validation_gt.zip,Validation_GT,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track1_test.zip,Test_Input,] - [,]
28624,https://datasets.epfl.ch/vidit/VIDIT_train.zip,Train, (1024x1024): [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track2_validation.zip,Validation_Input,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track2_validation_gt.zip,Validation_GT,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track2_test.zip,Test_Input,] - [,]
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track3_train.zip,Train, (512x512): [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track3_validation.zip,Validation_Input,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track3_validation_gt.zip,Validation_GT,] - [,] - [
28624,https://datasets.epfl.ch/vidit/AIM2020/AIM2020_track3_test.zip,Test_Input,] - [,]
28635,http://metashare.ilsp.gr:8080/repository/browse/semeval-2016-absa-mobile-phones-reviews-chinese-train-data-subtask-1/f651041268d411e59f7c842b2b6a04d77f78a1885b994740895c77b3fd15c69a/,下载地址,SE-ABSA16_PHNS是中文评价对象级情感分类数据集，主要由描述手机类别某个属性的商品用户评论构成。为方便使用demo数据中提供了完整数据，数据集,，数据集示例如下:
28635,http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools,下载地址,Sem-L数据集是英文评价对象级情感分类数据集，主要由描述笔记本电脑类别某个属性的商品用户评论构成。为方便使用demo数据中提供了完整数据，数据集,，数据集示例如下：
28635,https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/mpqa_corpus_2_0/,下载地址,MPQA数据集是英文互联网评论数据集。为方便使用demo数据中提供了完整数据，数据集,，数据集使用例子如下，其中为了方便模型使用需要将文本进行分词处理，标签用BIO标记评论内容、评论实体和实体内容表达主体。
28637,https://sites.google.com/view/cwisharedtask2018/datasets,Complex word identification Shared Task 2018 dataset,The full annotated dataset consisting of 4732 phrases extracted from the , is stored in the tab-separated file 
28637,https://github.com/ekochmar/MWE-CWI/blob/master/final_MWE_dataset.tsv,"
final_MWE_dataset.tsv
", is stored in the tab-separated file , with the following fields:
28638,tools/convert_datasets,tools, files and , for converting annotations to COCO format are provided for the following datasets:
28638,https://github.com/saic-vul/adaptis#toyv1-dataset,ToyV1,AdaptIS , and 
28638,https://github.com/saic-vul/adaptis#toyv2-dataset,ToyV2, and ,"
"
28647,http://www.bigdatalab.ac.cn/~gjf/,Homepage,"
","
"
28647,http://www.bigdatalab.ac.cn/~lanyanyan/,Homepage,"
","
"
28647,http://www.bigdatalab.ac.cn/~cxq/,Homepage,"
","
"
28648,https://www.crcv.ucf.edu/data/ucf-qnrf/,home link,UCF-QNRF: [,"], ShanghaiTech: ["
28648,http://crcv.ucf.edu/data/ucf-cc-50/,home link,"], UCF-CC-50: [",]
28650,https://dns4public.blob.core.windows.net/dns4archive/dns5-datasets-files-sha1.csv.bz2,dns5-datasets-files-sha1.csv.bz2, Personalized DNS datasets is available at: ,. The archive is 41.3MB in size and can be read in Python like this:
28652,https://mlcommons.org/en/inference-datacenter-10/,Datacenter, Once-for-All (OFA) Network is adopted by Alibaba and ranked 1st in the open division of the MLPerf Inference Benchmark (, and 
28655,https://www.flir.com/oem/adas/adas-dataset-form/,FLIR video,"This datset has 221 aligned Vis and IR image pairs containing rich scenes such as roads, vehicles, pedestrians and so on. These images are highly representative scenes from the ",". We preprocess the background thermal noise in the original IR images, accurately align the Vis and IR image pairs, and cut out the exact registration regions to form this dataset."
28657,experiments/synthetic/Localized_CNNs_on_synthetic_data.ipynb,Localized_CNNs_on_synthetic_data.ipynb,". Otherwise, ", can be executed locally by updating the path of the data files. See 
28657,https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting/blob/master/preprocessing/form_WINDS_data.py,[form_WINDS_data.py]," by using wind resource download tool on the wind toolkit data and selecting 2012 for year, wind speed for attributes, and including leap year and 5 minute interval for download options. This will download a csv for each site, that should be stored in one folder. Also, files that do not fall into the rotated rectangle will be discared. Final processed file can be acquired by running ",:
28657,https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting/blob/master/data/top10perms_GA.pkl,[here], (it can be examined visually in [2]). No further processing is required. Permutation file of the optimized embedding is located ,.
28657,https://cds.climate.copernicus.eu/cdsapp#!/dataset/sis-european-energy-sector?tab=overview,[here],", 10x10 low-resolution data ordered in a grid, part of which overlaps with the Atlantic Ocean. The data can be downloaded ",", by selecting wind speed variable, 6 hour time aggregation, 10m vertical level and no bias correction boxes. The acquired NetCDF file needs to be further processed by executing "
28657,https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting/blob/master/preprocessing/form_copernicus_data.py,[form_copernicus_data.py],", by selecting wind speed variable, 6 hour time aggregation, 10m vertical level and no bias correction boxes. The acquired NetCDF file needs to be further processed by executing ",:
28657,https://github.com/oshapio/Localized-CNNs-for-Geospatial-Wind-Forecasting/blob/master/experiments/real_world/Localized_CNNs_on_real_world_data.ipynb,[in a notebook], or locally ,", just update the locations of the data files."
28677,#dataset,Dataset,"
","
"
28677,back-end/data/dataset/2020-02-24/metadata_02242020.json,metadata_02242020.json," on 2/24/2020), we include "," file under the deep-smoke-machine/back-end/data/dataset/ folder. You need to copy, move, and rename this file to deep-smoke-machine/back-end/data/metadata.json."
28677,back-end/www/smoke_video_dataset.py,smoke_video_dataset.py,"
","
"
28677,back-end/data/dataset/2020-02-24/metadata_02242020.json,metadata_02242020.json, on 2/24/2020) ," file under the deep-smoke-machine/back-end/data/dataset/ folder. The JSON file contains an array, with each element in the array representing the metadata for a video. Each element is a dictionary with keys and values, explained below:"
28677,back-end/www/split_metadata.py,split_metadata.py,After running the ," script, the ""label_state"" and ""label_state_admin"" keys in the dictionary will be aggregated into the final label, represented by the new ""label"" key (see the JSON files in the generated deep-smoke-machine/back-end/data/split/ folder). Positive (value 1) and negative (value 0) labels mean if the video clip has smoke emissions or not, respectively."
28677,back-end/data/pretrained_models/RGB-I3D-S3.pt,RGB-I3D,We release two of our best baseline models: , and 
28677,https://github.com/CMU-CREATE-Lab/deep-smoke-machine/blob/master/back-end/data/pretrained_models/RGB-TC-S3.pt,RGB-TC, and ,", both trained and tested on split S"
28677,https://github.com/CMU-CREATE-Lab/deep-smoke-machine/blob/improve-documentation/back-end/data/production_url_list/2019-01-03.json,here,". In sum, the script takes a list of video URLs (examples can be found ","), gets their date and camera view boundary information, generates a bunch of cropped clips, and run the model on these clips to recognize smoke emissions. Here are the steps:"
28677,back-end/data/production_url_list/,back-end/data/production_url_list/,"First, for a date that you want to process, create a JSON file under the "," folder to add video URLs. The format of the file name must be ""YYYY-MM-DD.json"" (such as ""2019-01-03.json""). If the file for that date exists, just open the file and add more video URLs. Each video URL is specified using a dictionary, and you need to put the video URLs in a list in each JSON file. For example:"
28677,back-end/data/dataset/2020-02-24/dataset_1.png,this graph,". The cam_id and view_id correspond to the camera views presented in the ""Dataset"" section in this READEME. For example, if cam_id is 0 and view_id is 1, this means that the camera view is 0-1, as shown in ",". After creating the JSON files or adding video URLs to existing JSON files, run the following to perform a sanity check, which will identify problems related to the camera data and attemp to fix the problems:"
28677,back-end/data,back-end/data/,"This will create a ""production"" folder under "," to store the processed results. Then, run the following to identify events based on the probabilities of having smoke:"
28677,back-end/data,back-end/data/,"This will create an ""event"" folder under "," to store the links to the video clips that are identified as having smoke emissions. To visualize the smoke events, copy the folder (with the same folder name, ""event"") to the front-end of the "
28684,#datasets,Datasets,"
","
"
28684,http://data.nvision2.eecs.yorku.ca/PIE_dataset/,PIE,The code is trained and tested with , and 
28684,http://data.nvision2.eecs.yorku.ca/JAAD_dataset/,JAAD, and ,  datasets. We used the keras implementation of 
28690,solvatum/data/solvatum_references.bib,solvatum/data/solvatum_references.bib," contains a collection of experimentally measured partition coefficients for a large number of molecular solutes in non-aqueous solvents. The database is aimed at providing reference data for e.g. the parameter fitting of effective solvation models. The data has been compiled from the works of Abraham, Acree and co-workers and will be regularly updated as new data becomes available. Each entry is provided with the respective literature reference, all publications are listed in ",. The actual database  (
28690,solvatum/data/solvatum.sdf,solvatum/data/solvatum.sdf,. The actual database  (,) is given as a single structure data file (sdf).
28690,https://pandas.pydata.org/,pandas,Some functions will optionally use ,", "
28700,http://www.qizhexie.com/data/RACE_leaderboard.html,RACE,"
",: The 
28702,https://figshare.com/articles/dataset/SDM-Genomic-Datasets/14838342/1,SDM-Genomic-datasets,From ,", datasets including 10k, 100k, and 1M records with 25% and 75% duplicates rates, over six mapping rules with different complexities (1/4 simple object map, 2/5 object reference maps, 2/5 object join maps)"
28705,https://github.com/thunlp/DocRED/tree/master/data,here,"For the dataset and pretrained embeddings, please download it ",", which are officially provided by "
28708,https://www.cs.toronto.edu/~vmnih/data/,Massachusetts Buildings Dataset,Download the ," Training Set as the source domain, and put it "
28717,#Get-the-data,Get the data,"
","
"
28719,#data-preparation,Data Preparation,"
","
"
28719,ops/dataset_configs.py,ops/dataset_configs.py,Configure the dataset in ,.
28720,#deeplabv3-model-and-datasets,DeepLabv3+ Model and Datasets,"
","
"
28720,#how-to-add-your-own-dataset,How to add your own dataset,"
","
"
28720,https://www.audi-electronics-venture.de/aev/web/de/driving-dataset.html,A2D2 Dataset,"
","
"
28720,https://www.cityscapes-dataset.com/,Cityscapes Dataset,"
","
"
28720,https://pytorch.org/docs/1.3.1/data.html?highlight=dataset#torch.utils.data.Dataset,PyTorch dataset class,Write a , for your dataset. The 
28732,./data,data,The directory , contains sample data.
28732,./data/run_sample.py,run_sample.py,The Python script , demonstrates how to invoke CovidSim to use this data.  See the 
28732,./data/README.md,sample README, demonstrates how to invoke CovidSim to use this data.  See the , for details on how to run the samples.
28748,https://cocodataset.org/#home,COCO,", Image-Text Retrieval for ", and 
28755,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI RAW sequence, data of a ,. We choose the data 
28756,crfnet/data_processing/README.md,data_processing,"
", folder contains all functions for preprocessing before the neural network. Data fusion functions are placed in this folder. VIsualization of the generated radar augmented image (RAI) is provided by the generator script for the corresponding data set.
28763,datasets/covid_category,Read more,| Dataset name  | Num classes | Reference | | ------------- | ----------- | ----------| | COVID Category (CC)  | 2 | , | | Vaccine Sentiment (VS)  | 3 | 
28763,http://alt.qcri.org/semeval2016/task4/index.php?id=data-and-tools,See :arrow_right:, | | Twitter Sentiment SemEval (SE) | 3 | , |
28767,#training-data-preparation,Training data preparation,"
","
"
28782,https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets,here," is a directory of your choosing. Other datasets are supported in the toolbox, see ",". You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox."
28788,./data/readme.md,Data,"
","
"
28788,./data/county_data_abridged.csv,processed csv,Easily downloadable as , or full pipeline
28788,./data/list_of_columns.md,here,Extensive documentation available ,"
"
28788,./data/readme.md,./data/readme.md,"for more data details, see ","
"
28788,./data/readme.md,data readme,"Additionally, we would like to thank our sources, which can be found in the ","
"
28790,https://datashare.is.ed.ac.uk/handle/10283/2651,VCTK,"
","
"
28790,https://datashare.is.ed.ac.uk/handle/10283/2651,the VCTK dataset,Download and uncompress ,.
28791,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb I+II,The models are trained on ,", which is free for downloads (the trial lists are also there). One can easily adapt "
28805,https://nlp.stanford.edu/data/coqa/evaluate-v1.0.py,official evaluation script,Download , and save it in the folder evaluation/
28809,http://www.isle.illinois.edu/speech_web_lg/data/g2ps/,http://www.isle.illinois.edu/speech_web_lg/data/g2ps/,"The column ""FSTs"" is a trained grapheme-to-phoneme transducer for use with phonetisaurus.  If the available lexicons were large enough to test the phone error rate (PER), then it is listed in parentheses. As of this writing, PERs range from 7% to 45%.  Note: some of the trained models exceed git's file size limit, so they're not available on the github page; you can still find them at ",". Currently those are (american-english, arabic, dutch, french, german, portuguese, russian, spanish, turkish)."
28810,notebooks/prepare_datasets.ipynb,Jupyter notebook,Prepare Datasets (,)
28810,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI object detection 3D dataset,"
","
"
28810,https://www.argoverse.org/data.html,Argoverse dataset v1.1,"
","
"
28810,https://self-driving.lyft.com/level5/data/,Lyft Level 5 dataset v1.02,"
","
"
28810,https://waymo.com/open/data/,Waymo dataset v1.0,"
","
"
28810,https://level5.lyft.com/dataset/download-dataset/,Lyft, and , need to downloaded manually.
28811,http://cvlab.postech.ac.kr/research/hcngpr/data/prec_recall_raw.txt,Prec-Recall,"
","
"
28816,https://www.robots.ox.ac.uk/~vgg/data/dtd/,(DTD),:+1: The Describable Textures Dataset , for providing with a highly diverse dataset of images.
28826,docs/dataprep.md,data preparation for pretraining,"Data preparation is one of the important steps in any Machine Learning project. For BERT pretraining, document-level corpus is needed. The quality of the data used for pretraining directly impacts the quality of the trained models. To make the data preprocessing easier and for repeatability of results, data preprocessing code is included in the repo. It may be used to pre-process Wikipedia corpus or other datasets for pretraining. Refer to additional information at ", for details on that.
28828,#finetuning-pretrained-models-with-new-data,"
Finetuning pretrained models with new data
","
","
"
28830,https://github.com/snipsco/keyword-spotting-research-datasets/blob/master/LICENSE,the full License Terms,"Please note that the statistics displayed below might not remain consistent with the datasets provided. Indeed, under the GDPR and since voice recordings constitute personal data, dataset contributors have the right to opt out, see ", for more details.
28830,https://github.com/sonos/keyword-spotting-research-datasets/blob/master/LICENSE,the full License Terms,Please read , before accessing the Data Sets.
28839,http://crcv.ucf.edu/data/UCF101.php,here,Please download the UCF101 dataset ,.
28852,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,VoxCeleb 1 & 2,"
", - all
28852,https://datashare.is.ed.ac.uk/handle/10283/3443,VCTK,"
", - subsets vctk_dev and vctk_test are download from server in run.sh
28855,https://github.com/patrickwu2/Depth-Completion/blob/master/doc/data.md,Matterport3D,We provide a code for training on ,. Download Matterpord3D dataset and reorder your root folder as follows:
28855,https://github.com/patrickwu2/Depth-Completion/blob/master/doc/data.md,this order, directory is should be configured in ,. Be sure that ROOT path in 
28855,https://github.sec.samsung.net/d-senushkin/saic_depth_completion_public/blob/master/saic_depth_completion/data/datasets/matterport.py,matterport.py,. Be sure that ROOT path in , is valid. Now you can start training with the following command:
28861,https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-pressure-levels?tab=form,ERA5,"
","
"
28879,https://github.com/patrick-kidger/torchcde/blob/master/example/irregular_data.py,irregular_data.py,Also see ,", for demonstrations on how to handle variable-length inputs, irregular sampling, or missing data, all of which can be handled easily, without changing the model."
28880,https://supervise.ly/explore/projects/supervisely-person-dataset-23304/datasets,"
Supervisely Person Dataset
","-Net for human segmentation, so we trained another example model for human segemntation based on ",. 
28893,https://github.com/boxiangliu/med_translation/blob/master/data/nejm-open-access.tar.gz?raw=true,here,You can download ~ 70% of data ,. Read on if you would like the entire dataset.
28893,https://github.com/boxiangliu/med_translation/blob/master/data/manual_align_input.tar.gz?raw=true,here,"In order to compare automatic sentence alignment algorithms, we need to establish a set of ground truth alignment. Lucky for you, we have done the dirty work of aligning sentences. You can download the raw sentences (unaligned) ",. Create a directory and untar into this directory. These will be used as input to the sentence alignment algorithm below.
28893,https://raw.githubusercontent.com/boxiangliu/med_translation/master/data/align.txt,here,Next download the manual alignment result ,. Place it into the following directory.
28893,http://data.statmt.org/wmt18/translation-task/preprocessed/zh-en/,here,WMT18 preprocessed en/zh data can be downloaded ,.
28904,https://github.com/jinnovation/rainy-image-dataset,here,Download synthesized data from ,", as supervised training data. Put input images in './data/rainy_image_dataset/input' and ground truth images in './data/rainy_image_dataset/label'. Run /data/generate.m to generate HDF files as training data."
28906,https://github.com/besacier/mboshi-french-parallel-corpus,published Mboshi corpus,Here you can reproduce our experiment with ,". This dataset contains 3 speakers, and we name them A, B, and C."
28906,https://github.com/besacier/mboshi-french-parallel-corpus,here,Download mboshi dataset from ,. Then unzip it in your Downloads folder.
28918,data,data,We do not ship the corpora used in the experiments from the paper. The sample files provided in the , directory are given to illustrate the used data format (BIO). More information can be found in the 
28918,data/README.md,data/README.md, directory are given to illustrate the used data format (BIO). More information can be found in the ,.
28928,http://gradientscience.org/data_rep_bias/,blog post, (,) .
28930,http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,Breakfast,This repo includes Keras + Tensorflow implementation on , dataset. Tested with Ubuntu 16.04 + Python 2.7.
28930,./data/README,"
data/README
",Please follow the instruction in ," to obtain the data, then run"
28943,https://amath.washington.edu/research/publications/data-driven-modeling-scientific-computation-methods-complex-systems-big-data,"Data-Driven Modeling & Scientific Computation: Methods for Complex Systems & Big Data (Kutz, 2013)",": Contains a list of equations used to define the differential equation models (equations.py), a randomized initial conditions generator (ic_generator.py), a simple wrapper for SciPy's solve_ivp initial value problem solver (ivp_solver.py), and a BVP solver which encapsulates the IVP solver (bvp_solver.py). The BVP solver implements a simple shooting method which models a technique from ",.
28943,https://pandas.pydata.org/,Pandas,"The SINDyBVP.sindy_bvp() method is then called. This method loads the relevant data, randomly selects trial data to use for regression (i.e. finding the model and parameters), and sends that data to a TermBuilder object to construct ","
"
28943,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html,DataFrames,"
"," containing symbolic functions. Each trial is sent individually to the TermBuilder, which creates a DataFrame from each individual trial. The DataFrame has (p+1) columns, where (p) is the number of candidate functions to use for SINDy regression. The additional column corresponds to the regression outcome variable. After a DataFrame has been constructed for each of the trials, the data is sent to a grouper object, which takes the DataFrames and reorganizes them for regression, as described in the paper. The organized data is used by the GroupRegressor object to perform the SGTR algorithm. The results are reported, and the coefficients are computed for the operator L (rather than for the algebraically manipulated form learned by SINDy-BVP). The coefficients are returned with a configured plotter object, which can be used to visualize the results."
28945,athena/data/datasets/asr/speech_recognition_batch_bins.py,Batchbins function,2021/08/18 The , is added to Athena-v2.0
28945,athena/data/datasets/preprocess.py,SpecAugment,2021/04/09 , bug is fixed
28945,https://mispchallenge.github.io/task2_data.html,MISP, |       |  , | Model link | |:-----------------:|:---:|:-----:|:--------:|:----:|:-----------:|:----------:|:-----------:|:-----------:|:----:|:-----:|:-----:|------------| |                   |     |  CER% |   CER%   |      |     WER%    |            |             |             | WER% |       |  CER% |            | |                   |     |  dev  |    dev   | test |  dev 
28945,examples/tts/data_baker/audio_demo/,audio_demo,Traing Data | Acoustic Model | Vocoder |  Audio Demo :---------: |:-------------: | :-------------:| :------------: data_baker  |Tacotron2       | GL             |  , data_baker  |Transformer_tts | GL             |  
28945,examples/tts/data_baker/audio_demo/,audio_demo, data_baker  |Transformer_tts | GL             |  , data_baker  |Fastspeech      | GL             |  
28945,examples/tts/data_baker/audio_demo/,audio_demo, data_baker  |Fastspeech      | GL             |  , data_baker  |Fastspeech2     | GL             |  
28945,examples/tts/data_baker/audio_demo/,audio_demo, data_baker  |Fastspeech2     | GL             |  , data_baker  |Fastspeech2     | HiFiGAN        |  
28945,examples/tts/data_baker/audio_demo/,audio_demo, data_baker  |Fastspeech2     | HiFiGAN        |  , ljspeech    |Tacotron2       | GL             |  
28945,https://mispchallenge.github.io/task1_data.html,MISP2021 task1,The performances on , dataset are shown as follow:
28963,https://github.com/raghavian/lungVAE/blob/master/data/dataset.py,dataloader,Check the , to create more or to compute the masks on the fly
28965,https://commoncrawl.org/2016/10/news-dataset-available/,Common Crawl News dataset," (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the ",". For more information about the dataset and experiments, check out our ACL 2020 paper: "
28965,https://colab.research.google.com/github/complementizer/wcep-mds-dataset/blob/master/wcep_getting_started.ipynb,"
Open In Colab
","
","
"
28970,news_tls/explore_dataset.py,news_tls/explore_dataset.py,Check out , to see how to load the provided datasets.
28980,https://www.kaggle.com/benedictwilkinsai/atari-anomaly-dataset-aad,here,"Our dataset (AAD) was used to evaluate the performance of S3N, the dataset and more information can be found ",.
28998,https://s3.amazonaws.com/opennmt-trainingdata/opensub_qa_en.tgz,"
opensub_qa_en
",Extract , data in 
28999,http://numba.pydata.org/,numba,", matplotlib, numpy, ",", scipy, "
28999,http://numba.pydata.org/,numba,", numpy, ","
"
29006,https://iiscleap.github.io/coswara-blog/coswara/2020/11/23/visualize_coswara_data_metadata.html,here, to know more about the dataset. Note that the dataset size has increased since this paper came out. We also maintain a (less frequently updated) blog ,.
29007,http://jmcauley.ucsd.edu/data/amazon/index.html,Amazon-5core,Download the original data from ,", choose two relevant categories ("
29013,http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-en.tsv.gz,News Commentary v14,"
","
"
29013,http://data.statmt.org/wikititles/v1/wikititles-v1.de-en.tsv.gz,Wiki Titles v1,"
","
"
29013,http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.en-zh.tsv.gz,News Commentary v14,"
","
"
29013,http://data.statmt.org/wikititles/v1/wikititles-v1.zh-en.tsv.gz,Wiki Titles v1,"
","
"
29013,https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0-TEI.zh.tar.gz.00,UN Parallel Corpus V1.0,"
","
"
29013,http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz,Europarl v8,"
","
"
29013,http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz,Europarl v8,"
","
"
29013,http://data.statmt.org/wmt18/translation-task/rapid2016.tgz,Rapid corpus of EU press releases,"
","
"
29021,"https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015#",ECML/PKDD 2015 dataset,"
","
"
29026,https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html,BIWI Kinect,"
", :The dataset contains over 15K images of 20 people
29044,http://cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking Dataset," cameras that is able to track dynamic objects, estimate the camera poses along with the static and dynamic structure, the full SE(3) pose change of every rigid object in the scene, extract velocity information, and be demonstrable in real-world outdoor scenarios. We provide examples to run the SLAM system in the ",", and in the "
29044,https://robotic-esp.com/datasets/omd/,Oxford Multi-motion Dataset,", and in the ",.
29077,https://docs.pycom.io/datasheets/development/lopy4/,LoPy 4,"To each of the Pis, you've got a "," connected via a USB-to-Serial bridge, so it's available as "
29092,http://www.nada.kth.se/cvap/databases/kth-tips/index.html,KTH-TIPS2-b,The usage will be demonstrated with a texture classification problem using the , dataset.
29099,https://github.com/laiguokun/multivariate-time-series-data,https://github.com/laiguokun/multivariate-time-series-data,"Download Solar-Energy, Traffic, Electricity, Exchange-rate datasets from ",. Uncompress them and move them to the data folder.
29103,dataset.py,dataset.py, The processing of dataset happens inside the ,.
29108,https://kaikki.org/dictionary/rawdata.html,https://kaikki.org/dictionary/rawdata.html,"For most people, it may be easiest to just download pre-expanded data. Please see ",". The raw wiktextract data, extracted category tree, extracted templates and modules, as well as a bulk download of audio files for pronunciations in both "
29108,https://kaikki.org/dictionary/rawdata.html,https://kaikki.org/dictionary/rawdata.html,Note that Wiktionary audio files are available for bulk download at ,. Files in the download are named with the last component of the URL in 
29113,https://pypi.org/project/dataclasses/,dataclasses backport,"If you are using Python 3.6 and do not wish to upgrade, you can install a package providing the required features: the ",; see 
29113,https://docs.python.org/3/library/dataclasses.html,dataclasses module, instructions below to see how to install it. Python 3.7 provides the , automatically.
29113,https://pypi.org/project/dataclasses/,dataclasses backport module,"If it shows Python 3.6, install the ", via
29116,https://github.com/mlmed/torchxrayvision/blob/master/torchxrayvision/datasets.py,View docstrings for more detail on each dataset,"
", and 
29116,https://github.com/mlmed/torchxrayvision/blob/master/scripts/xray_datasets.ipynb,Demo notebook, and , and 
29116,https://github.com/mlmed/torchxrayvision/blob/master/scripts/dataset_utils.py,Example loading script, and ,"
"
29116,https://github.com/mlmed/torchxrayvision/blob/master/scripts/xray_datasets_views.ipynb,demo notebook,specify a subset of views (,)
29116,https://github.com/mlmed/torchxrayvision/blob/master/scripts/xray_datasets-CovariateShift.ipynb,demo notebook,Distribution shift tools (,)
29117,https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/,50 Years of Microprocessor Trend Data,"
","
"
29117,https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/,48 Years of Microprocessor Trend Data,"
","
"
29117,https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/,42 Years of Microprocessor Trend Data,"
","
"
29117,https://www.karlrupp.net/2015/06/40-years-of-microprocessor-trend-data/,40 Years of Microprocessor Trend Data,"
","
"
29118,https://github.com/google-research-datasets/boolean-questions,github, (NAACL2019) [,]
29118,https://github.com/google-research-datasets/QED,github, [,]
29118,https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data,github, (ECIR2020) [,]
29118,https://github.com/google-research-datasets/ToTTo,github, (EMNLP2020) [,]
29118,https://huggingface.co/blog/how_many_data_points/,website, (NAACL2021) [,]
29163,toydataset/README.md,toydataset_README.md,Toydataset Domain Adaptation Experiments (,)
29163,toydataset,"
toydataset
",Full details on toydataset domain adaptation including codes in , subdirectory.
29163,toydataset/git_images/plots/videos,"
toydataset/git_images/plots/videos
","
", contains videos of the training of Contradistinguisher using CUDA as the epoch progresses. We can observe the decision boundary being updated to satisfy both the domains as they are jointly trained without domain alignment.
29163,data,data,"We consider Amazon Customer Reviews Dataset with 4 domains Books, DVDs, Electronics and Kitchen Appliances located in ", folder. Each domain has 2 classes positive and negative reviews as labels of binary classification.
29166,https://github.com/JerryWei03/COVID-Q/blob/master/final_master_dataset.csv,final_master_dataset.csv,The dataset CSV file can be found at ,.
29173,https://www.cl.cam.ac.uk/research/nl/bea2019st/#data,here,All the public GEC datasets used in the paper can be downloaded from ,.
29180,data/Saldias&Roy-RTN_data.csv,data/Saldias&Roy-RTN_data.csv,", each one averaging 17.1 clauses or 62 seconds long, where each clause has on average 11 tokens. You can find it here: ","
"
29180,https://github.com/social-machines/acl-nuse-personal-narratives/tree/master/data,"
data
","This material is made available under a Creative Commons Attribution 4.0 International License. Please attribute any use of this material to Saldias, B., & Roy, D. (2020) and cite as stated in section Citation below. To download, please either go to our ", folder or use the following command:
29190,https://github.com/db758/icwsm_data_challenge/blob/master/STM/notes/final_diagnostics.png,Figure S1,"
", shows the four different diagnostics calculated for topics with 
29202,https://grouplens.org/datasets/hetrec-2011/,HetRec 2011, is downloaded from ,"
"
29219,https://travis-ci.org/openlegaldata/legal-reference-extraction,"
Build Status
","
","
"
29221,https://github.com/HRNet/HRNet-Semantic-Segmentation#data-preparation,HRNet-Semantic-Segmentation,follow the prepare instruction in ,"
"
29235,https://github.com/bigheiniu/awesome-coronavirus19-dataset,here,The repository affords researchers an easy tool and uniform tool to load the COVID-19 related dataset listed in ,.
29235,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html,pandas.Dataframe,This tool is easy to use. All the datasets are in , format which is easy for further data analysis. Researchers can run one python command to  load the dataset
29236,https://www.ncbi.nlm.nih.gov/research/coronavirus/#data-download,LitCovid,"
",: a curated literature hub for tracking up-to-date scientific information about the 2019 novel Coronavirus.
29236,https://github.com/bigheiniu/awesome-coronavirus19-dataset/blob/master/MaliciousURLS.txt,Malicious URLs,"
",: These URLs are malicious urls checked by 
29236,http://data.gdeltproject.org/blog/2020-coronavirus-narrative/live_tvnews/MASTERFILELIST.TXT, COVID-19 Television Coverage Dataset,"
",: A New Dataset For Exploring The Coronavirus Narrative On Television News.
29236,https://github.com/HLTCHKUST/covid19-misinfo-data,covid19-misinfo-data,"
",": this repository contains Covid19-scientific (CDC, WHO and  MedicalNewsToday) and Covid19-politifact (PolitiFact) fact checked claims."
29236,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/LW0BTB,Coronavirus Tweet Ids,"
",": This dataset contains the tweet ids of 51,798,932 tweets related to Coronavirus or COVID-19. They were collected between March 3, 2020 and March 19, 2020 (midnight UTC-0) from the Twitter API using Social Feed Manager."
29236,https://www.crowdbreaks.org/en/data_sharing,Crowdbreaks,"
",: Tweets with keywords that related to specific health topics. The system is designed to track trends about health and disease-related issues in real-time across different countries.
29236,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FAEZIO,Baidu Mobility Data,"
",: The data is scraped from Baidu Migration website.
29236,https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide,Geographic Distribution of COVID-19 cases worldwide,"
",: The data file is updated daily and contains the latest available public data on COVID-19.
29236,https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data,CSSE COVID-19 Dataset,"
",: Daily case reports
29236,https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset,Novel Corona Virus 2019 Dataset,"
",": This dataset has daily level information on the number of affected cases, deaths and recovery from 2019 novel coronavirus. Please note that this is a time series data and so the number of cases on any given day is the cumulative number."
29236,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRSGT3,China Health Facilities ,"
",":Health facilities POI, such as the hospital in China."
29236,https://github.com/cjvanlissa/COVID19_metadata,COVID-19 Metadata,"
",": A collection of relevant country/city level metadata about the COVID-19 pandemic, made interoperable for secondary analysis."
29236,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,NY COVID-19,"
",: The most recent information collected about people who have tested positive for COVID-19 in NYC.
29236,https://data.chhs.ca.gov/dataset/california-covid-19-hospital-data-and-case-statistics,California COVID-19 Hospital Data and Case Statistics,"
",:  California COVID-19 Hospital Data and Case Statistics.
29236,https://github.com/cmu-delphi/delphi-epidata,delphi-epidata,"
",": COVID-19 activity level across the U.S. These indicators are derived from a variety of anonymized, aggregated data sources made available by multiple partners."
29236,https://www.aminer.cn/data-covid19/,data-covid19,"
",": The scope of the data set includes the epidemic situation, scientific research, knowledge graph, media information and other aspects."
29236,https://coronavirus-disasterresponse.hub.arcgis.com/#get-data,COVID-19 GIS Hub,"
",": Get maps, datasets, applications, and more for coronavirus disease 2019 (COVID-19)."
29236,https://console.cloud.google.com/marketplace/browse?filter=solution-type:dataset&filter=category:covid19,Google COVID-19,"
",": a repository of public datasets like Johns Hopkins Center for Systems Science and Engineering (JHU CSSE), the US Census Bureau's American Community Survey (ACS), and OpenStreetMaps data."
29236,https://aws.amazon.com/blogs/big-data/a-public-data-lake-for-analysis-of-covid-19-data/,Amazon COVID-19 Data Lake,"
",": It contains COVID-19 case tracking data from Johns Hopkins and The New York Times, hospital bed availability from Definitive Healthcare, and over 45,000 research articles about COVID-19 and related coronaviruses from the Allen Institute for AI."
29236,https://data.humdata.org/event/covid-19,COVID-19 Pandemic,"
",: COVID-19 Pandemic in Locations with a Humanitarian Response Plan from WHO.
29236,https://github.com/covid19datahub/COVID19,Coronavirus COVID-19 (2019-nCoV) Epidemic Datasets ,"
",":Provide the research community with a unified data hub by collecting worldwide fine-grained data merged with demographics, air pollution, and other exogenous variables helpful for a better understanding of COVID-19."
29236,https://github.com/github/covid-19-repo-data,COVID-19 Public Repository Data,"
",: A comprehensive versioned dataset of the repositories and relevant related metadata about public projects hosted on GitHub related to the 2019 Novel Coronavirus and associated COVID-19 disease.
29236,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OAM2JK,Policies and Regulations Timeline ,"
",":Policies and regulations released by the Chinese government, global organizations, western countries, and so on."
29236,https://data.humdata.org/dataset/world-bank-indicators-of-interest-to-the-covid-19-outbreak,World Bank Indicators of Interest of the COVID-19 Outbreak,"
","
"
29236,https://www.acaps.org/covid19-government-measures-dataset,#COVID19 Government Measures Dataset,"
",": The #COVID19 Government Measures Dataset puts together all the measures implemented by governments worldwide in response to the Coronavirus pandemic. Data collection includes secondary data review. The researched information available falls into five categories: Social distancing, Movement restrictions, Public health measures, Social and economic measures, Lockdowns."
29236,https://github.com/bigheiniu/awesome-coronavirus19-dataset/pulls,pull requests,Please feel free to send me , or email (
29241,https://github.com/abin24/Magnetic-tile-defect-datasets.,Magnetic Tile Defect Dataset,"We used magetic tile defect dataset to test our proposed model, the original dataset can be found in ",". Before feeding the data into our model, we resize all the images into 100 by 100, and apply data augmentation techniques (flip and rotate) to enrich samples in classes ""crack"" and ""fray""."
29246,https://ripple.com/build/data-api-v2/,Ripple Data API, and XRP exchange rates from the ,. 
29257,https://www.javadoc.io/doc/org.interledger/jackson-datatypes,"
Javadocs
", | , |
29261,https://paperswithcode.com/sota/pill-classification-both-sides-on-epillid?p=epillid-dataset-a-low-shot-fine-grained,"
PWC
","
","
"
29274,https://databus.dbpedia.org/dbpedia/spotlight/spotlight-model/,DBpedia Databus repository,"The dbpedia/dbpedia-spotlight is a docker image to run the DBpedia Spotlight service with the most recent language models, downloaded from the ",", e.g., English (en), German (nl), Italian (it), etc."
29284,http://www.cvlibs.net/datasets/kitti/eval_tracking.php,KITTI Tracking,Download the dataset from ,.
29284,http://www.cvlibs.net/download.php?file=data_tracking_velodyne.zip,velodyne,Download ,", "
29284,http://www.cvlibs.net/download.php?file=data_tracking_calib.zip,calib,", ", and 
29284,http://www.cvlibs.net/download.php?file=data_tracking_label_2.zip,label_02, and , in the dataset and place them under the same parent folder.
29290,data/input/fig_1a.c,data/input/fig_1a.c,Example: an example program provided at ," contains errors in a for loop statement specifier. To repair this program, execute the following command"
29297,data,data,"
", - Synthetic datasets for word scramble and arithmetic tasks described in the paper.
29297,dataset_statistics,dataset_statistics,"
", - Statistics for all languages included in the training dataset mix.
29298,https://tianchi.aliyun.com/dataset/dataDetail?dataId=42,Tmall, folder. The full raw datasets are: ,", "
29298,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649,Taobao,", ", and 
29298,https://tianchi.aliyun.com/dataset/dataDetail?dataId=53,Alipay, and ,. 
29299,./latency_dataset,latency_dataset,We provide the datasets we collect in the , folder.
29299,./latency_dataset/predictors,latency_dataset/predictors, file contains the predictor's model architecture and training settings. We provide pre-trained predictors in , folder.
29317,https://grouplens.org/datasets/movielens/20m/,MovieLens 20M Dataset,Experients 7-10 are based on the ," which must be saved in as ""ratings.csv"" in the data subfolder."
29323,#downloading-the-data,Downloading, | Installing the requirements | | , | Downloading the data | | 
29323,#linked-data-fragments-endpoint,Querying, | Running RML | | , | Linked Data Fragments endpoint | | 
29323,https://query-covid19.linkeddatafragments.org/,here,We are hosting an endpoint that can be used for querying ,. The corresponding repository for this can be found 
29324,https://allenai.org/data/cord-19,COVID-19 Open Research Dataset (CORD-19),This application serves data from , dataset.
29325,https://github.com/usc-isi-i2/wikidata-semantic-similarity,here,The documentation for the KGTK Semantic Similarity API is ,"
"
29333,data/hack_zurich/hack_zurich_database.dmp,hack_zurich_database.dmp, (API only). This API is working with a deployed postgres database which can be found in ,.
29333,data/hack_zurich/hack_zurich_database.dmp,hack_zurich_database.dmp,In both cases you need to point the system to the database which contains your data. The easiest way is to install PostgreSQL locally and restore the database dump ,", which contains all necessary data, tables, views and indices."
29333,data/hack_zurich/original/tables.json,tables.json,"In case you plan to manipulate the database schema, make sure to also adapt the schema-file which is used by ValueNet at inference time (",). This file contains a high level schema of the database (some tables might be abstracted by simple views) and is the foundation from which ValueNet is synthesizing a query. The script which builds the (
29333,data/hack_zurich/original/tables.json,tables.json,). This file contains a high level schema of the database (some tables might be abstracted by simple views) and is the foundation from which ValueNet is synthesizing a query. The script which builds the (,) file can be found here (
29333,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html,nvidia container toolkit,you have installed ,"
"
29333,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html,https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html," is available (either on your notebook or in the cloud you prefer) and your Docker environment supports GPUs. To do so, you might follow the official nvidia guide: ","
"
29333,data/hack_zurich/handmade_training_data/handmade_data_train.json,handmade_data_train.json,Add your custom data to , and 
29333,data/hack_zurich/handmade_training_data/handmade_data_dev.json,handmade_data_dev.json, and ,. You see in both files one sample how to do so. Add both the SQL and the natural language representation for a question.
29333,src/tools/training_data_builder/training_data_builder.py,training_data_builder.py," split), run the ",. It will take your custom data and transform it into the Spider representation:
29333,data/hack_zurich/dev.json,data/hack_zurich/train.json,To do so merge your custom data (most probably ,") with the training data from Spider (you find them here, already pre-processed: "
29333,data/spider/train.json,data/spider/train.json,") with the training data from Spider (you find them here, already pre-processed: ",). Do the same for the 
29343,https://www.nist.gov/itl/products-and-services/emnist-dataset,EMNIST,", ",", "
29358,#vlengagement-datasets,VLEngagement Datasets,"
","
"
29358,#vlengagement-12k-dataset,VLEngagement Dataset,"
","
"
29362,https://www.dropbox.com/s/cyxgbgxvigbdw2h/cacf_data_code.zip?dl=0,https://www.dropbox.com/s/cyxgbgxvigbdw2h/cacf_data_code.zip?dl=0,This link contains a downloadable version of all data+code used in the paper: ,"
"
29364,https://github.com/DefuLian/recsys/blob/master/test/dataset/test_script.m,test/dataset/test_script.m,See , for some examples of how to use this portal
29365,#data-centric-methods,Data centric methods,"
","
"
29365,#data-selection,Data selection methods,"
","
"
29365,#political-data-identification,Political data identification,"
","
"
29365,https://github.com/academic/awesome-datascience,awesome-datascience,"
","
"
29383,./notebooks/resize_hdataset.ipynb,resize_hdataset.ipynb,Before training we resize HAdobe5k subdataset so that each side is smaller than 1024. The resizing script is provided in ,.
29394,http://datatreker.com/simulations-of-the-fast-probabilistic-consensus-protocol-fpc,Simulations of the FPC,"
", by Dr. Sebastian Mueller
29399,https://github.com/ipavlopoulos/context_toxicity/tree/master/data,data,The large dataset is included in the , folder in the form of two CSV files.
29409,https://b2drop.bsc.es/index.php/s/BIMCV-COVID19-cIter_1_2_3/download?path=%2F&files=covid19_posi_metadata.tar.gz, covid19_posi_metadata.tar.gz,); the one corresponding to the header description of the datasets (,); and another one to the sessions (
29409,https://github.com/BIMCV-CSUSP/BIMCV-COVID-19/tree/master/padchest-covid#data-sources-bimcv-padchest,"The Padchest-pneumonia dataset, here","
","
"
29418,./data/,dataset, to use the , or the 
29418,https://github.com/franklinnwren/TIMME-data-visualization,TIMME-data-visualization,"
","
"
29418,./data,data,. For the dataset options please check the , we have.
29425,https://dasci.es/transferencia/dascii-hub/open-data/covidgr-2/,More information about these datasets,Datasets of X-Ray imaging for detection of COVID-19. ,"
"
29426,https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/train-v1.1.json,here,"In order to evaluate on XQuAD, models should be trained on the SQuAD v1.1 training file. which can be downloaded from ",. Model validation similarly should be conducted on the SQuAD v1.1 validation file.
29437,https://github.com/TurkuNLP/turku-ner-corpus/archive/v1.0.zip,turku-ner-corpus-v1.0.zip,zip package: ,"
"
29437,https://github.com/TurkuNLP/turku-ner-corpus/archive/v1.0.tar.gz,turku-ner-corpus-v1.0.tar.gz,tgz package: ,"
"
29438,https://www.kaggle.com/nltkdata/conll-corpora,NER-C,"     | 98.44     | 97.10 [2]                 | 98.91 [6], 96.71 [3]           | |",  | 
29438,https://github.com/google-research-datasets/paws/tree/master/pawsx,PAWS-X,     | 95.70 [2]                 | 88.75 [4]                      | |, | 89.05         | 89.55         | 90.70 [8]                 | |
29439,https://github.com/spyysalo/yle-corpus,Yle data,][,] [
29439,https://github.com/spyysalo/ylilauta-corpus,Ylilauta data,] [,]
29439,https://github.com/mpsilfve/finer-data,data,][,]
29441,http://jmcauley.ucsd.edu/data/amazon/,Amazon review corpus, datasets from from 5-core subsets of the , by Prof. Julian McAuley.
29449,data/reddit,"In data/reddit, we provide the Reddit data set we gathered.","
","
"
29459,https://github.com/hlzhou/peers-score/tree/master/src/data_processing,README,Follow the instructions in the , in 
29460,#download-the-data,download our copy,. You can , or manually generate your own for novel data using the 
29460,#data-pipeline-reproduction,pipeline description, or manually generate your own for novel data using the , below.
29476,https://tslearn.readthedocs.io/en/stable/gen_modules/tslearn.datasets.html#module-tslearn.datasets,You can load any of the UCR datasets in the required format.,"
","
"
29476,https://tslearn.readthedocs.io/en/stable/gen_modules/tslearn.datasets.html#module-tslearn.datasets,UCR Datasets,| data                                                                                                                                                                                         | processing                                                                                                              | clustering                                                                                                                                                       | classification                                                                                                                                                                          | regression                                                                                                                                                                           | metrics                                                                                                                              | |----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------| | ,                                                                           | 
29489,https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset,the PyTorch-Geometric library,You can find the same data sets in ,". To get clean version of the data sets, use parameter "
29501,dataset_comparison/adelaideH.ipynb,notebook,The code for multiple homography fitting is available at: ,.
29501,dataset_comparison/adelaideF.ipynb,notebook,The code for multiple two-view motion fitting is available at: ,.
29514,https://data.csail.mit.edu/graphics/fivek/,here,The dataset can be found , with all explanation related to it.
29525,https://github.com/sxzrt/Instructions-of-the-PersonX-dataset#data-for-visda2020-chanllenge,PersonX,", ",", and the vehicle datasets "
29525,https://github.com/JDAI-CV/VeRidataset,VeRi-776,", ",", "
29527,https://github.com/NVlabs/ffhq-dataset,FFHQ,FFHQ: ,"
"
29539,sofc-exp-corpus/annotations,sofc-exp-corpus/annotations,The manual annotations created for the SOFC-Exp corpus located in the folder , are licensed under a 
29550,./dataproc,dataproc instructions,Create Dataproc workflow (see ,)
29550,https://console.cloud.google.com/bigquery?project=crypto-eon-164220&p=crypto-eon-164220&d=historic&page=dataset,historic dataset,Open , in BigQuery
29557,https://plotly.com/dash/big-data-for-python/,"
Big Data for Pything
","
"," Connect to Python's most popular big data back ends: Dask, Databricks, NVIDIA RAPIDS, Snowflake, Postgres, Vaex, and more."
29559,dataloaders/download_kitti_depth_rgb.py,"
dataloaders/download_kitti_depth_rgb.py
","To download the Kitti-Depth dataset, use the provided Python script ",.
29561,https://drivendata.github.io/cookiecutter-data-science/,Cookiecutter Data Science,This project uses a simplified version of , structure proposed by DrivenData.
29561,https://blog.godatadriven.com/write-less-terrible-notebook-code,here," command in the Anaconda prompt in this project directory (the dot at the end is important). This command will turn the project folder into a Python package and make scripts from src folder easily accessible. Read more about this ""trick"" ",.
29562,#included-templates-for-labeling-data-in-label-studio,Included templates for labeling data in Label Studio,"
","
"
29562,https://towardsdatascience.com/introducing-label-studio-a-swiss-army-knife-of-data-labeling-140c1be92881,introductory blog post,Have a custom dataset? You can customize Label Studio to fit your needs. Read an , to learn more.
29575,#datasets,Datasets,"
","
"
29575,#datasets,above, loads each of the datasets described , as classes with several functionalities particular to each dataset. All the data classes do have two methods: 
29575,#datasets,above,. We have also included other examples involving all the four datasets presented ,", with examples of all the architectures "
29587,https://www.deepspeed.ai/2022/12/11/data-efficiency.html,"DeepSpeed Data Efficiency: A composable library that makes better use of data, increases training efficiency, and improves model quality",[2022/12] ,"
"
29590,https://www.nist.gov/topics/data/public-access-nist-research/copyright-fair-use-and-licensing-statements-srd-data-and,National Institute for Standards and Technology (NIST) Software Disclaimer,"
","
"
29593,docs/data_preparation.md,Data Preparation,"
","
"
29596,https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition,https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition,HHAR ,"
"
29596,https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring,https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring,PAMAP2 ,"
"
29605,docs/INSTALL.md#use-custom-dataset-optional,INSTALL.md/Use Custom Dataset, Please refer to ,.
29620,#dataset-prepare,Dataset prepare,"
","
"
29623,http://snap.stanford.edu/data/index.html#communities,Stanford SNAP collection,"
","
"
29623,https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets,Benchmark datasets for graph kernels,"
","
"
29626,https://github.com/thinknew/BCINet/tree/main/testdata,testData.mat,Input Data Format: Number of EEG Channels x Number of Samples X Number of Trials for EEG data and Labels as a vector. See , for references with sampling rate of 400 Hz.
29630,./create_dataset_arrays.py,"
create_dataset_arrays.py
","
",": Processes a graph, as an edge-list file, and produces training files."
29630,./datasets,"
datasets/
","
",: Directory containing datasets used in our paper. The original datasets come from 
29630,http://snap.stanford.edu/data,Stanford SNAP,: Directory containing datasets used in our paper. The original datasets come from , and 
29630,./create_dataset_arrays.py,"
create_dataset_arrays.py
","To use, you must first create dataset files (using ","), then train the node embeddings and the edge function (using "
29630,./datasets/,datasets,". For example, if you want to simulate walks for the PPI dataset (see ","), then you can run the command:"
29631,https://github.com/pcy1302/asp2vec/tree/master/data,"
data
","For instructions regarding data, please check ", directory
29635,http://archive.ics.uci.edu/ml/datasets/communities+and+crime,Communities and Crimes,"
",: UCI Communities   and   crime   data   set.
29635,https://archive.ics.uci.edu/ml/datasets/nursery,Nursery,"
",: UCI Nursery data set.
29635,https://github.com/yromano/cqr/blob/master/get_meps_data/README.md,this explanation,The Medical Expenditure Panel Survey (MPES) data can be downloaded by following , (code provided by 
29635,https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192,MEPS_21,"
",": Medical expenditure panel survey,  panel 21."
29645,http://jmcauley.ucsd.edu/data/amazon,Source,"Amazon contains 10,166 nodes and 148,865 edges. ","
"
29645,https://snap.stanford.edu/data/higgs-twitter.html,Source,"Twitter contains 10,000 nodes and 331,899 edges. ","
"
29645,http://socialcomputing.asu.edu/datasets/YouTube,Source,"YouTube contains 2,000 nodes and 1,310,617 edges. ","
"
29668,https://archive.ics.uci.edu/ml/datasets.php,UCI,datasets from , website (the 
29669,https://numba.pydata.org,numba,", ",", "
29669,https://pandas.pydata.org,pandas,", ","
"
29683,#data,Data,"
","
"
29683,#trec-cast-2019-data,TREC CAsT 2019 Data,"
","
"
29683,#ms-marco-conversatioanl-search-corpus,MS MARCO Conversatioanl Search Corpus,"
","
"
29683,#preprocess-trec-cast-2019-data,Preprocess TREC CAsT 2019 Data,"
","
"
29683,#generate-weak-supervision-data,Generate Weak Supervision Data,"
","
"
29683,#filter-ms-marco-conversatioanl-search-corpus,Filter MS MARCO Conversatioanl Search Corpus,"
","
"
29717,https://keras.io/api/datasets/imdb/,IMDB Dataset,", ","
"
29722,CONTRIBUTING.md#1-download-data,:x: no Google Drive access,:point_right: For ,.
29730,http://www.sociopatterns.org/datasets/primary-school-temporal-network-data/,data," This data set contains the temporal network of contacts between the children and teachers used in the study published in BMC Infectious Diseases 2014, 14:695. ","
"
29738,http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset,MagnaTagATune Dataset,"
","
"
29754,src/datapoint.py,class, and contains all the information needed about the plan. Refer to the , for more information.
29767,https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz,Cora,"
","
"
29767,https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz,Citeseer,"
","
"
29767,http://leitang.net/code/social-dimension/data/blogcatalog.mat,BlogCatalog,"
","
"
29767,https://snap.stanford.edu/data/ca-HepPh.txt.gz,ca-HepPh,"
","
"
29767,https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz,PubMed,"
","
"
29784,https://motchallenge.net/data/MOT17/,MOT Challenge,Download the data from ,", and put or link it to "
29784,https://motchallenge.net/data/MOT17/,MOT Challenge,Download the data from ,", and put or link it to "
29785,http://people.ee.ethz.ch/~ihnatova/pynet-bokeh.html#dataset,EBB! dataset,Download the , and extract it into 
29786,https://atmahou.github.io/attachments/ACL2020data.zip,download, or click here: ,"
"
29791,https://cocodataset.org/#home,official,The data can be downloaded on the , website.
29792,https://stackoverflow.com/questions/37232008/how-read-common-data-formatcdf-in-python,here," to process some of the original files. If you face difficulties with the installation, you can find more elaborate instructions ",.
29792,datasets/preprocess/README.md,details for data preprocessing,"We provide code to evaluate our models on the datasets we employ for our empirical evaluation. Before continuing, please make sure that you follow the ",.
29792,https://www.di.ens.fr/willow/research/surreal/data/,SURREAL,We recently added the training and evaluation code on several datasets: ,", "
29792,http://gvv.mpi-inf.mpg.de/3dhp-dataset/,MPI-INF-3DHP, and ,. You can get the preprocess details from 
29792,datasets/preprocess/README.md,here,. You can get the preprocess details from ,.
29792,http://visiondata.cis.upenn.edu/spin/spin_fits.tar.gz,SPIN fits,We add the training code to use the , as supervision and provide the 
29792,https://www.di.ens.fr/willow/research/surreal/data/,SURREAL,We fixed the bug of the gender label in ," dataset, and retrained the model on SURREAL dataset."
29802,https://github.com/HHansi/Embed2Detect/blob/master/data_analysis/data_preprocessor.py,data_preprocessor.py, in ,.
29807,http://www.vito-eodata.be/PDF/portal/Application.html,VITO portal,"Apply the DA transformation [2] to a new Proba-V image. A level 2A 333M Proba-V image is required, it can be downloaded from the ",. By default it uses the pretrained models from the 
29807,https://landsat.usgs.gov/landsat-8-cloud-cover-assessment-validation-data,Biome dataset,Additionally it also accepts an image with its manually annotated cloud mask from the , or from the 
29812,https://s3.amazonaws.com/yaroslavvb2/data/imagenet18.tar,here,". If you run locally, you may need to download the special ImageNet dataset yourself from ",. This faster training is achieved by training on special smaller images for the first 15 epochs or so.
29814,http://odds.cs.stonybrook.edu/yelpchi-dataset/,Yelp Spam Review Datasets,"To run the code, you need the ",. Please send email with the title 
29816,https://archive.ics.uci.edu/ml/datasets/covertype,covertype dataset,Bayesian logistic regression with Gaussian prior on ," (the code is already prepared to download it). The code samples from the posterior using antithetic MLMC on the discretised Stochastic Langeving SDE and approximates E(F(X)), with F(X) = |X|^2. The code returns various plots specifying computational costs necessary to achieve different Mean Squared Errors."
29824,https://github.com/opendatacube/datacube-core,Datacube-core,"
", - Open Data Cube analyses continental scale Earth Observation data through time
29828,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object,documentation,We provide a function to help parse provided Tweet JSON objects to cascades. Tweet JSON objects are crawled from the Twitter API and one can refer to the , for more information.
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,Health Department’s COVID-19 Data webpage,You can view visualizations of these data on the ,. Additional data related to COVID-19 are available via 
29832,https://data.cityofnewyork.us/browse?category=Health&q=covid,NYC Open Data,. Additional data related to COVID-19 are available via ,.
29832,https://github.com/nychealth/coronavirus-data/blob/master/README.md#key-technical-notes,Key Technical Notes,", you can consult the documentation we have provided in the Readme files for each folder of data. To find  Readme files, just click on a folder name, above, and scroll down. Documentation is organized by file name, so you can scroll through the Readme, find the name of the file you are using for, and read documentation on it. Additionally, some universal documentation is provided in the ",.
29832,https://github.com/nychealth/coronavirus-data/issues?q=,Issues,": We will try to answer questions about the data in this repository as we are able to. If you have a question, please search the ", to see if it’s already been addressed. Please understand that we are responding to a pandemic and we might not be able to address all questions in a timely manner.  We are not able to accommodate custom data requests placed via Github.
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,COVID-19 Data webpage,"Starting the week of April 3, 2023, the Health Department will update data in this repository and on the ", weekly on Thursdays.
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,COVID-19 Data webpage,The Health Department made several changes to this repository and the ," on August 2, 2021. These include:"
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,COVID-19 Data webpage,The Health Department made several changes to this repository and the ," on June 10, 2021. These include:"
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,COVID-19 Data webpage,The Health Department made several changes to this repository and the ," on March 3, 2021. These include:"
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,COVID-19 Data webpage,The Health Department made several changes to this repository and the ," on December 7, 2020. These include:"
29832,https://github.com/nychealth/coronavirus-data#laboratory-testing,types of COVID-19 laboratory tests,"Data referenced in this repository as ""molecular tests"" correspond to data previously labeled as ""PCR tests."" Please see the technical notes for a description of the different ","
"
29832,https://github.com/nychealth/coronavirus-data#case-definitions-for-covid-19,case definitions for COVID-19,Please see the technical notes for a description of the different ,"
"
29832,https://www1.nyc.gov/site/doh/covid/covid-19-data.page,Health Department’s COVID-19 Data webpage,In order to support an update to the ," on November 9, 2020, changes were made to this repository, including revisions to some key files, filenames, and locations. These changes include:"
29832,https://github.com/nychealth/coronavirus-data/tree/master/latest#pp-by-modzctacsv,pp-by-modzcta.csv,"Most of the data in this repository include patients who reside in congregate facilities, such as correctional facilities and long-term care facilities. While data reported from these facilities may sometimes influence local trends, cases reported from these facilities do not necessarily represent community-based transmission. The only data that exclude patients in congregate facilities are in ", and 
29832,https://github.com/nychealth/coronavirus-data/tree/master/latest#last7days-by-modzctacsv,last7days-by-modzcta.csv, and ,.
29832,https://github.com/nychealth/coronavirus-data/tree/master/latest#latest,Readme,"This folder contains files with data that focus on the most recent period of the outbreak. It includes daily 7-day cumulative percent positivity for the molecular test by MODZCTA, daily 28-day counts and rates of hospitalizations and deaths by MODZCTA, and trend data that cover the most recent 90 days. See this folder’s ", for a detailed description of its contents.
29832,https://github.com/nychealth/coronavirus-data/tree/master/totals#totals,Readme,"This folder contains files with cumulative totals since the start of the COVID-19 outbreak in NYC, which the Health Department defines as the diagnosis of the first confirmed COVID-19 case on February 29, 2020. The Health Department recommends against interpreting daily changes to these files as one day’s worth of data, due to the difference between date of event and date of report. See this folder’s ", for a detailed description of its contents.
29832,https://github.com/nychealth/coronavirus-data/tree/master/trends#trends,Readme,"This folder contains files with daily, weekly, and monthly data shown across time. Note that these trend data are published by date of event, not by date of report. The Health Department recommends against interpreting daily changes to these files as one day’s worth of data, due to the difference between date of event and date of report. See this folder’s ", for a detailed description of its contents.
29832,https://github.com/nychealth/coronavirus-data/tree/master/variants#variants,Readme,"This folder contains files with data on SARS-CoV-2 variants. It includes information on the number and type of SARS-CoV-2 variants identified in NYC, over time and by MODZCTA. All tables containing variant data are updated weekly on Thursday (with data through two previous Saturdays). These files are based on a small subset of all confirmed COVID-19 cases; findings may not be representative of all confirmed COVID-19 cases citywide, and should be interpreted with caution. See this folder’s ", for a detailed description of its contents.
29832,https://github.com/nychealth/coronavirus-data/blob/master/Geography-resources/README.md,Readme,"This folder contains additional resources for data provided by MODZCTA geographies, inlcuding geographic files for MODZCTA. See this folder’s ", for a detailed description of its contents.
29837,scripts/data.md,this,data directory should look like ,"
"
29845,#datasets,Datasets,"
","
"
29851,/queries/statistics_dataset.md,statistics dataset,"After starting the server and logging in the browser interface (see Step 3 above), you can run the following SPARQL queries on the newly loaded triples: ",", "
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/DST_bert.pkl?sv=2020-10-02&st=2022-05-27T08%3A54%3A49Z&se=2024-05-28T08%3A54%3A00Z&sr=b&sp=r&sig=lpiWxhU6M%2B0zWqejCWFw%2Fa9UTs8P1yMHXWJQAfPIEBs%3D,Download,| Task        | Model       | Link                                         | | ----------- | ----------- | -------------------------------------------- | | DST         | BERT        | , | | DST         | XLM         | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/DST_xlm.pkl?sv=2020-10-02&st=2022-05-27T08%3A54%3A11Z&se=2024-05-28T08%3A54%3A00Z&sr=b&sp=r&sig=bT0%2Bo2nwgebPtYCsO9bGFjNmqf6VToDozykpErW3z0w%3D,Download, | | DST         | XLM         | , | | MLDoc       | BERT        | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/MLDoc_bert.pkl?sv=2020-10-02&st=2022-05-27T08%3A56%3A37Z&se=2024-05-28T08%3A56%3A00Z&sr=b&sp=r&sig=6awR1DhWIIG5u7WYpTcpOd96%2BrM6r0JIktiIHdxF0OU%3D,Download, | | MLDoc       | BERT        | , | | MLDoc       | XLM         | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/MLDoc_xlm.pkl?sv=2020-10-02&st=2022-05-27T09%3A03%3A17Z&se=2024-05-28T09%3A03%3A00Z&sr=b&sp=r&sig=Pu67SSYs7vI7%2BP9YrfB3gait295X2Ly9sA7xHUhVTTA%3D,Download, | | MLDoc       | XLM         | , | | SC2         | BERT        | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/SC2_bert.pkl?sv=2020-10-02&st=2022-05-27T09%3A03%3A33Z&se=2024-05-28T09%3A03%3A00Z&sr=b&sp=r&sig=8A7cgTHrY92G1DJ5%2FHB%2F8T%2BxYg5J60zK642XA%2B2M%2BEk%3D,Download, | | SC2         | BERT        | , | | SC2         | XLM         | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/SC2_xlm.pkl?sv=2020-10-02&st=2022-05-27T09%3A03%3A48Z&se=2024-05-28T09%3A03%3A00Z&sr=b&sp=r&sig=N6xRxFmyyhLubtGK879QwvEusCaJDtZ9I97%2FmibyolI%3D,Download, | | SC2         | XLM         | , | | SC4         | BERT        | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/SC4_bert.pkl?sv=2020-10-02&st=2022-05-27T09%3A04%3A08Z&se=2024-05-28T09%3A04%3A00Z&sr=b&sp=r&sig=6iWFGilS8B%2FTGcia1fNlnvSOZ58Hebz5PkXd6JrYKeU%3D,Download, | | SC4         | BERT        | , | | SC4         | XLM         | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/SC4_xlm.pkl?sv=2020-10-02&st=2022-05-27T09%3A04%3A22Z&se=2024-05-28T09%3A04%3A00Z&sr=b&sp=r&sig=TzDqq5MkddoyRFIJSoE4G6uIAnMaibLQDgyVE%2FL3v88%3D,Download, | | SC4         | XLM         | , | | XTDS        | BERT        | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/XTDS_bert.pkl?sv=2020-10-02&st=2022-05-27T09%3A04%3A36Z&se=2024-05-28T09%3A04%3A00Z&sr=b&sp=r&sig=p2YSRRXUEXEI2et9C8OoxNz8JFSHY9usTjaL1humbSg%3D,Download, | | XTDS        | BERT        | , | | XTDS        | XLM         | 
29855,https://kodeniimlworkspace.blob.core.windows.net/data/old/XTDS_xlm.pkl?sv=2020-10-02&st=2022-05-27T09%3A04%3A50Z&se=2024-05-28T09%3A04%3A00Z&sr=b&sp=r&sig=tc7CkjXLMOcGZwX3ch1y1gMwdsRdnj4OmjOooCWvEv4%3D,Download, | | XTDS        | XLM         | , |
29862,http://www.nature.com/articles/sdata20153,Multiple modality pediatric template and population study,"
", employs several aspects of ANTsR
29870,#datasets,Datasets,"
","
"
29870,https://www.cityscapes-dataset.com/file-handling/?packageID=27,here,We use the entire Frankfurt unlabeled long video sequence from the Cityscapes dataset [1]. You can download this video ,.
29870,https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/camera_lidar-20180810150607_camera_frontcenter.tar,Gaimersheim,We use the entire video sequence of front center cameras at ,", "
29870,https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/camera_lidar-20190401145936_camera_frontcenter.tar,Ingolstadt,", ",", and "
29870,https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/camera_lidar-20190401121727_camera_frontcenter.tar,Munich,", and ", from the Audi Autonomous Driving Dataset (A2D2) [2].
29870,http://graphics.cs.cmu.edu/projects/lvsdataset/,here,"For downloading the Long Videos Dataset (LVS) [1], you may check ",.
29884,#datasets,Datasets,"
","
"
29884,#creating-and-preprocessing-a-new-java-dataset,creating a new Java dataset,For , or 
29884,https://s3.amazonaws.com/code2seq/datasets/java-small.tar.gz,Java-small,"
","
"
29884,https://s3.amazonaws.com/code2seq/datasets/java-med.tar.gz,Java-med,"
","
"
29884,https://s3.amazonaws.com/code2seq/datasets/java-large.tar.gz,Java-large,"
","
"
29884,https://s3.amazonaws.com/code2seq/datasets/java-small-preprocessed.tar.gz,Java-small-preprocessed,"
","
"
29884,https://s3.amazonaws.com/code2seq/datasets/java-med-preprocessed.tar.gz,Java-med-preprocessed,"
","
"
29884,https://s3.amazonaws.com/code2seq/datasets/java-large-preprocessed.tar.gz,Java-large-preprocessed,"
","
"
29889,https://github.com/allenai/ForeCite/tree/master/forecite/topic_identification/generate_dataset.py,here,The script to generate the underlying json files with all the text and citations data is ,". Please note, this script is present only for purposes of reproducibility and clarity. It will not actually run, as it contacts Semantic Scholar internal services."
29891,instructor/oracle_data/seqgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/seqgan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/leakgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/leakgan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/maligan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/maligan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/jsdgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/jsdgan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/relgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/relgan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/dpgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/dpgan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/dgsan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/dgsan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/cot_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/cot_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/sentigan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/sentigan_instructor.py,real_data,", ","
"
29891,instructor/oracle_data/catgan_instructor.py,oracle_data,Instructors: ,", "
29891,instructor/real_data/catgan_instructor.py,real_data,", ","
"
29904,#Loading-and-scaling-data,Loading and scaling data,"
","
"
29904,#Creating-our-regressor-and-loading-data,Creating our regressor and loading data,"
","
"
29916,https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet,torchvision.datasets,We hope to make our labels easier to use by integrating them with , after the release.
29916,https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/real_labels.py,real_labels.py, repository. Have a look at , and the way it is used in 
29916,https://www.tensorflow.org/datasets/catalog/imagenet2012_real,TensorFlow Datasets,Our labels are available in the , library.
29923,https://github.com/randompeople404/health_indicator_2020/tree/master/data/data_use,Data,"
", and experiment code for replication of project health study.
29923,https://github.com/randompeople404/health_indicator_2020/tree/master/data/data_use,Data,"
",", this data is selected using the criteria in "
29927,data/input/clinical_trials.csv,"
clinical_trials.csv
",The sample input and output of the script are , and 
29927,data/output/cfg_parsed_clinical_trials.tsv,"
cfg_parsed_clinical_trials.tsv
", and ,.
29927,data/input/clinical_trials.csv,"
clinical_trials.csv
",The sample input and output of the script are , and 
29927,data/output/ie_parsed_clinical_trials.tsv,"
ie_parsed_clinical_trials.tsv
", and ,.
29952,https://towardsdatascience.com/gettingstartedwithmarathonenvs-v0-5-0a-c1054a0b540c,Blog, | -- | -- | -- | -- | , |
29962,#-sdcor-_-with-visualization---read-data-from-ram,SDCOR _ with visualization - read data from RAM,"
","
"
29962,#-sdcor-_-without-visualization---read-data-from-disk,SDCOR _ without visualization - read data from Disk,"
","
"
29968,https://github.com/ParitoshParmar/MTL-AQA/tree/master/MTL-AQA_dataset_release,here,"], where the authors provided the YouTube links of untrimmed long videos and the corresponding annotations at ",.
29968,https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/,here," (MICCAI workshop 2014), where the raw videos could be downloaded at ",. You can download our prepared JIGSAWS frames (About 500 M) at 
29973,https://sites.google.com/view/davis-driving-dataset-2020/home,DDD20,v2e can convert recordings from , and the original 
29989,https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py#L427,here, to 300 , and then install locally.
30002,https://www.cs.cornell.edu/~arb/data/genius-expertise/,here,The data can be found ,. It must be placed in the 
30004,https://cran.r-project.org/web/packages/dataMaid/index.html,dataMaid,"
",; A Suite of Checks for Identification of Potential Errors in a Data Frame as Part of the Data Screening Process
30004,https://github.com/data-cleaning/validate,validate,"
",; Professional data validation for the R environment
30004,https://github.com/data-cleaning/errorlocate,errorlocate,"
",; Find and replace erroneous fields in data using validation rules
30004,https://mi2datalab.github.io/auditor/articles/model_performance_audit.html,vigniette," model verification, validation, and error analysis ","
"
30006,https://git.uwaterloo.ca/jimmylin/hedwig-data,"
hedwig-data
","Option 2. Our school-hosted repository, ",:
30012,./examples/optimization_applications/meta_data_collection.py,data collection,makes optimization , simple
30012,https://github.com/SimonBlanke/search-data-collector,Search-Data-Collector,| Package                                                                       | Description                                                                          | |-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------| | , | Simple tool to save search-data during or after the optimization run into csv-files. | | 
30012,https://github.com/SimonBlanke/search-data-explorer,Search-Data-Explorer, | Simple tool to save search-data during or after the optimization run into csv-files. | | ,   | Visualize search-data with plotly inside a streamlit dashboard.
30016,http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,CUB-200-2011,"
","
"
30024,https://github.com/snakers4/open_stt/#dataset-composition,Dataset composition,"
","
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/radio_v4_manifest.tar.gz,opus,"| Dataset                               | GB, wav | GB, archive | Archive                                                                                                                                                                | Source                  | Manifest                                                                                                                                    | |---------------------------------------|---------|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------| | Train                                 |         |             |                                                                                                                                                                        |                         |                                                                                                                                             | | radio_v4                              | 1059    | 176         | ",", "
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/radio_v4_manifest.csv,manifest,      | Radio                   | ,                     | | public_speech                         | 257     | 47.4        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_speech_manifest.tar.gz,opus,                     | | public_speech                         | 257     | 47.4        | ,", "
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_speech_manifest.csv,manifest, | Internet + alignment    | ,                | | radio_v4_add                          | 15.7    | 2.8         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/radio_v4_add_manifest.tar.gz,opus,                | | radio_v4_add                          | 15.7    | 2.8         | ,", "
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/radio_v4_add_manifest.csv,manifest,  | Radio                   | ,                 | | 5% of radio_v4 + public_speech        | -       | 11.4        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/radio_pspeech_sample_manifest.tar.gz,opus+txt,                 | | 5% of radio_v4 + public_speech        | -       | 11.4        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/radio_pspeech_sample_manifest.csv,manifest,                                   | -                       | ,         | | audiobook_2                           | 162     | 25.8        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/private_buriy_audiobooks_2.tar.gz,opus+txt,         | | audiobook_2                           | 162     | 25.8        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/private_buriy_audiobooks_2.csv,manifest,                                    | Internet + alignment    | ,            | | radio_2                               | 154     | 24.6        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/radio_2.tar.gz,opus+txt,            | | radio_2                               | 154     | 24.6        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/radio_2.csv,manifest,                                                        | Radio                   | ,                               | | public_youtube1120                    | 237     | 19.0        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_youtube1120.tar.gz,opus+txt,                               | | public_youtube1120                    | 237     | 19.0        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_youtube1120.csv,manifest,                                             | YouTube videos          | ,                    | | asr_public_phone_calls_2              | 66      | 9.4         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/asr_public_phone_calls_2.tar.gz,opus+txt,                    | | asr_public_phone_calls_2              | 66      | 9.4         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/asr_public_phone_calls_2.csv,manifest,                                  | Internet + ASR          | ,              | | public_youtube1120_hq                 | 31      | 4.9         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_youtube1120_hq.tar.gz,opus+txt,              | | public_youtube1120_hq                 | 31      | 4.9         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_youtube1120_hq.csv,manifest,                                         | YouTube videos          | ,                 | | asr_public_stories_2                  | 9       | 1.4         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/asr_public_stories_2.tar.gz,opus+txt,                 | | asr_public_stories_2                  | 9       | 1.4         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/asr_public_stories_2.csv,manifest,                                      | Internet + alignment    | ,                  | | tts_russian_addresses_rhvoice_4voices | 80.9    | 12.9        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/tts_russian_addresses_rhvoice_4voices.tar.gz,opus+txt,                  | | tts_russian_addresses_rhvoice_4voices | 80.9    | 12.9        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/tts_russian_addresses_rhvoice_4voices.csv,manifest,                         | TTS                     | , | | public_youtube700                     | 75.0    | 12.2        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_youtube700.tar.gz,opus+txt, | | public_youtube700                     | 75.0    | 12.2        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_youtube700.csv,manifest,                                             | YouTube videos          | ,                     | | asr_public_phone_calls_1              | 22.7    | 3.2         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/asr_public_phone_calls_1.tar.gz,opus+txt,                     | | asr_public_phone_calls_1              | 22.7    | 3.2         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/asr_public_phone_calls_1.csv,manifest,                                       | Internet + ASR          | ,              | | asr_public_stories_1                  | 4.1     | 0.7         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/asr_public_stories_1.tar.gz,opus+txt,              | | asr_public_stories_1                  | 4.1     | 0.7         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/asr_public_stories_1.csv,manifest,                                        | Public stories          | ,                  | | public_series_1                       | 1.9     | 0.3         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_series_1.tar.gz,opus+txt,                  | | public_series_1                       | 1.9     | 0.3         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_series_1.csv,manifest,                                             | Public series           | ,                       | | public_lecture_1                      | 0.7     | 0.1         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_lecture_1.tar.gz,opus+txt,                       | | public_lecture_1                      | 0.7     | 0.1         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_lecture_1.csv,manifest,                                               | Internet + manual       | ,                      | | Val                                   |         |             |                                                                                                                                                                        |                         |                                                                                                                                             | | asr_calls_2_val                       | 2       | 0.8         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/asr_calls_2_val.tar.gz,wav+txt,                      | | Val                                   |         |             |                                                                                                                                                                        |                         |                                                                                                                                             | | asr_calls_2_val                       | 2       | 0.8         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/asr_calls_2_val.csv,manifest,                                                 | Internet                | ,                       | | buriy_audiobooks_2_val                | 1       | 0.5         | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/buriy_audiobooks_2_val.tar.gz,wav+txt,                       | | buriy_audiobooks_2_val                | 1       | 0.5         | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/buriy_audiobooks_2_val.csv,manifest,                                      | Books + manual          | ,                | | public_youtube700_val                 | 2       | 0.13        | 
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/archives/public_youtube700_val.tar.gz,wav+txt,                | | public_youtube700_val                 | 2       | 0.13        | ,"
"
30024,https://azureopendatastorage.blob.core.windows.net/openstt/ru_open_stt_opus/manifests/public_youtube700_val.csv,manifest,                                           | YouTube videos + manual | ,"                 | | Total                                 | 2,186   | 354         |                                                                                                                                                                        |                         |                                                                                                                                             |"
30024,https://azure.microsoft.com/en-us/services/open-datasets/,Azure Open Datasets,Newest direct download links are a courtesy of ,;
30029,http://images.cocodataset.org/annotations/annotations_trainval2014.zip,here," to download generated sentences used for our analysis. Additionally you will need MSCOCO annotations (both the instance segmentations and ground truth captions). If you do not already have them, they can be downloaded ",. You can see other python requirements in 
30031,https://urbansounddataset.weebly.com/urbansound8k.html,Urban Sound 8K,"
","
"
30031,http://marsyas.info/downloads/datasets.html,GTZAN Genre Collection,"
", and its 
30031,https://github.com/jongpillee/music_dataset_split/tree/master/GTZAN_split,fault-filtered splited version, and its ,.
30031,https://magenta.tensorflow.org/datasets/nsynth,NSynth,"
","
"
30033,./data,"
data
","
",: Simulation parameters e.g. basis sets and pseudopotentials
30045,https://aif360.readthedocs.io/en/latest/modules/datasets.html,aif360,"In this work, we introduce three new fine-tuning techniques to reduce bias in pretrained neural networks: random perturbation, layer-wise optimization, and adversarial fine-tuning. All three techniques work for any group fairness constraint. We include code that compares our three proposed methods with three popular post-processing methods, across three datasets provided by ",", and three popular bias measures."
30050,https://www.cityscapes-dataset.com/,Cityscapes,Some examples of MDEQ segmentation results on the , dataset.
30052,https://github.com/IBM/AIF360/blob/master/aif360/data/raw/compas/README.md,COMPAS,We use two real datasets: ,", "
30052,https://github.com/IBM/AIF360/blob/master/aif360/data/raw/meps/README.md,MEPS,", ",", and synthetic data that is generated by ourselves. Both real datasets are preprocessed as specified in "
30072,https://github.com/eyalbd2/PERL/tree/master/data,data directory,Make sure you download both our , and our 
30072,https://github.com/eyalbd2/PERL/tree/master/5-fold_data,5-fold-data directory, and our ,. Then run the following command to find a set the appropriate set of pivot features.
30079,https://pandas.pydata.org/,pandas,"
","
"
30079,https://seaborn.pydata.org/,seaborn,"
", - for the probability density function of the Hotel dataset presented in the article.
30079,http://numba.pydata.org/,numba,"
", - for speeding up calculations of the statistics (available automatically with Anaconda).
30083,https://magenta.tensorflow.org/datasets/nsynth,Nsynth datasaet,We use a subset of the , as described in the paper accompanying this github repository.
30085,https://github.com/TiKeil/NCD-corrected-TR-RB-approach-for-pde-opt/tree/master/fin_data,"
fin_data/
",: You can see the fin geometry and mesh in the directory ,"
"
30085,https://github.com/TiKeil/NCD-corrected-TR-RB-approach-for-pde-opt/tree/master/EXC_data,"
EXC_data/
",: The data of the blueprint is in ,. The used file for Figure 3 is 
30098,docs/dataprep.md,data preparation for pretraining,"Data preparation is one of the important steps in any Machine Learning project. For BERT pretraining, document-level corpus is needed. The quality of the data used for pretraining directly impacts the quality of the trained models. To make the data preprocessing easier and for repeatability of results, data preprocessing code is included in the repo. It may be used to pre-process Wikipedia corpus or other datasets for pretraining. Refer to additional information at ", for details on that.
30109,https://newsela.com/data/,[Newsela dataset], For the simplified outputs for the ,", please reach out to me at ddhruvkr@gmail.com. This is because the Newsela dataset is not publically available and only available via a contract with Newsela."
30120,#datasets,Datasets,"
","
"
30120,https://github.com/DavidDiazGuerra/Cross3D/tree/master/datasets/LibriSpeech,datasets/LibriSpeech," in your machine. By default, the main scripts look for it in ", but you can modify its phat with the 
30120,https://github.com/DavidDiazGuerra/Cross3D/tree/master/datasets/LOCATA,datasets/LOCATA,". By default, the main scripts look for it in ", but you can modify its phat with the 
30135,https://github.com/ankurhanda/nyuv2-meta-data,this repo,. The raw 13-class NYUv2 dataset can be directly downloaded in , with segmentation labels defined in 
30146,baal/active/dataset.py,"
ActiveLearningDataset
","
","
"
30149,#pre-training-datasets,Pre-training datasets,"
","
"
30149,#downstream-datasets,Downstream datasets,"
","
"
30152,data/,here,I have prepared a simple sample ,", you may also check "
30162,#where-to-download-the-dataset,next section,download the raw data (see ,);
30162,https://github.com/nyu-mll/spinn/tree/listops-release/python/spinn/data/listops,"
ListOps
","
","
"
30162,https://alt.qcri.org/semeval2014/task1/index.php?id=data-and-tools,"
SICK
","
","
"
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/ampersand/yawForward/maxSpeed1p0/previewVideos/,✓,                                        ||||||||| | :-----------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | | Top speed (m/s) |   0.5  |   1.0    |   2.0    |   3.0    |   4.0    |   5.0    |   6.0    |   7.0    | |  3D Figure 8  |    -    |     -   |    -    |    -    |    -    |    -    |    -    |    -    | |   Ampersand   |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/ampersand/yawForward/maxSpeed2p0/previewVideos/,✓, | , |    -    |    -    |    -    |    -    |    -    | |   Bent Dice   | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    |    -    |    -    | |   Bent Dice   | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawForward/maxSpeed3p0/previewVideos/,✓, | , |    -    |    -    |    -    |    -    | |    Clover     | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    |    -    | |    Clover     | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawForward/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    | |     Dice      |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawForward/maxSpeed1p0/previewVideos/,✓, |    -    |    -    | |     Dice      |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawForward/maxSpeed3p0/previewVideos/,✓, | , |    -    |    -    |    -    |    -    | | Flat Figure 8 |    -    |    -    |    -    |    -    |    -    |    -    |    -    |    -    | |   Half-Moon   |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawForward/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    |    -    | | Flat Figure 8 |    -    |    -    |    -    |    -    |    -    |    -    |    -    |    -    | |   Half-Moon   |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |     Mouse     | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    | |     Mouse     | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed6p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawForward/maxSpeed7p0/previewVideos/,✓, | , | |     Oval      |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawForward/maxSpeed1p0/previewVideos/,✓, | |     Oval      |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |    Patrick    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    | |    Patrick    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |    Picasso    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    | |    Picasso    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawForward/maxSpeed1p0/previewVideos/,✓, | , |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawForward/maxSpeed3p0/previewVideos/,✓, |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawForward/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    | |      Sid      | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    | |      Sid      | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |    Sphinx     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawForward/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | |    Sphinx     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |     Star      | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    |    -    | |     Star      | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawForward/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    | |    Thrice     | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    |    -    | |    Thrice     | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawForward/maxSpeed6p0/previewVideos/,✓, | , |    -    | | Tilted Thrice | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    | | Tilted Thrice | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawForward/maxSpeed6p0/previewVideos/,✓, | , |    -    | |    Winter     | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawForward/maxSpeed0p5/previewVideos/,✓, |    -    | |    Winter     | , |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawForward/maxSpeed2p0/previewVideos/,✓, |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawForward/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawForward/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    |
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/ampersand/yawConstant/maxSpeed1p0/previewVideos/,✓,                                        ||||||||| | :-----------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | | Top speed (m/s) |   0.5  |   1.0    |   2.0    |   3.0    |   4.0    |   5.0    |   6.0    |   7.0    | |  3D Figure 8  |    ✓*    |     ✓*   |    ✓*    |    ✓*    |    ✓*    |    ✓*    |    -    |    -    | |   Ampersand   |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/ampersand/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/ampersand/yawConstant/maxSpeed3p0/previewVideos/,✓, | , |    -    |    -    |    -    |    -    | |   Bent Dice   |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    |    -    | |   Bent Dice   |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/bentDice/yawConstant/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |    Clover     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | |    Clover     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/clover/yawConstant/maxSpeed6p0/previewVideos/,✓, | , |    -    | |     Dice      |    -    |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawConstant/maxSpeed2p0/previewVideos/,✓, |    -    | |     Dice      |    -    |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/dice/yawConstant/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | | Flat Figure 8 |    ✓*    |    ✓*    |    ✓*    |    ✓*    |    -    |    ✓*    |    -    |    -    | |   Half-Moon   |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | | Flat Figure 8 |    ✓*    |    ✓*    |    ✓*    |    ✓*    |    -    |    ✓*    |    -    |    -    | |   Half-Moon   |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/halfMoon/yawConstant/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |     Mouse     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | |     Mouse     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed6p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/mouse/yawConstant/maxSpeed7p0/previewVideos/,✓, | , | |     Oval      |    -    |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawConstant/maxSpeed2p0/previewVideos/,✓, | |     Oval      |    -    |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/oval/yawConstant/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |    Patrick    |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | |    Patrick    |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/patrick/yawConstant/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    | |    Picasso    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed0p5/previewVideos/,✓, |    -    |    -    | |    Picasso    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed1p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/picasso/yawConstant/maxSpeed6p0/previewVideos/,✓, | , |    -    | |      Sid      |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    | |      Sid      |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed6p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sid/yawConstant/maxSpeed7p0/previewVideos/,✓, | , | |    Sphinx     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawConstant/maxSpeed1p0/previewVideos/,✓, | |    Sphinx     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/sphinx/yawConstant/maxSpeed4p0/previewVideos/,✓, | , |    -    |    -    |    -    | |     Star      |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    |    -    | |     Star      |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/star/yawConstant/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    | |    Thrice     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed1p0/previewVideos/,✓, |    -    |    -    | |    Thrice     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed6p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/thrice/yawConstant/maxSpeed7p0/previewVideos/,✓, | , | | Tilted Thrice |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed1p0/previewVideos/,✓, | | Tilted Thrice |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed5p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed6p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/tiltedThrice/yawConstant/maxSpeed7p0/previewVideos/,✓, | , | |    Winter     |    -    | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawConstant/maxSpeed1p0/previewVideos/,✓, | |    Winter     |    -    | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawConstant/maxSpeed2p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawConstant/maxSpeed3p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawConstant/maxSpeed4p0/previewVideos/,✓, | , | 
30177,http://blackbird-dataset.mit.edu/BlackbirdDatasetData/winter/yawConstant/maxSpeed5p0/previewVideos/,✓, | , |    -    |    -    |
30179,https://www.dropbox.com/s/424wm7cp2fa4o2o/dataset.zip?dl=1,[dataset.zip],"Link to full dataset, with the extracted ROI frames and 3D reconstruction data (NMFCs, landmarks, expression, identity and camera parameters): ","
"
30184,https://ebiquity.umbc.edu/resource/html/id/351/UMBC-webbase-corpus,UMBC,", ",", and "
30185,http://nlp.stanford.edu/data/glove.6B.zip,GloVe," is the pre-trained word embeddings file, in txt format (i.e., every line consists of the word, followed by a space, and its vector. See ", for an example.)
30193,https://github.com/aikkala/O2MConverter/blob/master/tests/gait10dof18musc/output/data.pckl,data.pckl, | , | A simple leg model consisting of both legs and rotating torso. Derived from the 
30193,https://github.com/aikkala/O2MConverter/blob/master/tests/mobl_arms/output/data.pckl,data.pckl, | , | A dynamic shoulder and arm model with fixed torso. | 
30193,https://github.com/aikkala/O2MConverter/blob/master/tests/gait2392/output/data.pckl,data.pckl, | ," | A leg model consisting of both legs and a rotating/bending torso. | Distributed with OpenSim, "
30202,http://jmcauley.ucsd.edu/data/amazon/,here,Amazon datasets are derived from ,", tradesy dataset is introduced in "
30202,http://jmcauley.ucsd.edu/data/tradesy/,here,", tradesy dataset is introduced in ",. Please cite the corresponding papers if you use the datasets.
30203,https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page,raw data,Most of the , comes from the NYC Taxi & Limousine Commission.
30203,https://github.com/toddwschneider/nyc-taxi-data/tree/master/clickhouse,separate set of scripts,A , loads the Parquet files directly into a ClickHouse database
30203,https://github.com/toddwschneider/nyc-taxi-data/tree/2e805ab0f1bf362f890c6b6f227526c575f73b67,this older verion of the code,"This repo no longer works with the old CSV files provided by the TLC. Those files are no longer available to download from the TLC's website, but if you happen to have them lying around and want to use this repo, you should look at ", from before the Parquet file format change.
30203,https://github.com/toddwschneider/nyc-taxi-data/tree/master/clickhouse,"
clickhouse
",See the , directory
30203,https://www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page,Bytes of the Big Apple,Shapefile for NYC census tracts and neighborhood tabulation areas comes from ,"
"
30203,https://data.cityofnewyork.us/Transportation/FHV-Base-Aggregate-Report/2v9c-2k7f,the TLC,Mapping of FHV base numbers to names comes from ,"
"
30203,https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00094728/detail,National Climatic Data Center,Central Park weather data comes from the ,"
"
30203,https://toddwschneider.com/dashboards/nyc-taxi-ridehailing-uber-lyft-data/,NYC Taxi & Ridehailing Stats,These summary statistics are used in the , dashboard
30208,http://images.cocodataset.org/zips/train2017.zip,COCO train set,"If you do not have COCO2017 dataset, please download: ", and 
30208,http://images.cocodataset.org/zips/val2017.zip,COCO val set, and , and unzip these files and mv them under folder 
30208,http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip,COCO stuff annotations,"To train HTC models, download ", and change the name of folder 
30208,https://github.com/lvis-dataset/lvis-api,"
LVIS API
", and ,.
30209,http://www.lvisdataset.org,LVIS website,"For this release, we have annotated 159,623 images (100k train, 20k val, 20k test-dev, 20k test-challenge). Release v1.0 is publicly available at ", and will be used in the second LVIS Challenge to be held at Joint COCO and LVIS Workshop at ECCV 2020.
30209,https://github.com/cocodataset/cocoapi,COCO,The code is a re-write of PythonAPI for ,. The core functionality is the same with LVIS specific changes.
30213,https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014,Electricity (E),"
", records hourly electricity consumption transactions of 370 clients from 2011 to 2014. We use a subset of the last five weeks of 321 clients in our experiments. The tensor size is 
30213,http://databookuw.com/,data & code,] [,]
30216,https://registry.opendata.aws/terrain-tiles/,Terrain Tiles, package currently provides access to elevation data from AWS Open Data , and the Open Topography 
30231,https://mmselfsup.readthedocs.io/en/dev-1.x/user_guides/2_dataset_prepare.html,Prepare Dataset,"
","
"
30238,https://data-efficient-gans.mit.edu/,project,"
", | 
30238,https://data-efficient-gans.mit.edu/datasets/,datasets, | , | 
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2-pytorch,DiffAugment-stylegan2-pytorch, PyTorch training with , is now available!
30238,https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb,Colab tutorial, Our , is released! 
30238,https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb,"

", is released! ,"
"
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2#FFHQ,DiffAugment-stylegan2, FFHQ training is supported! See the , README.
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2/generate_gif.py,generate_gif.py, Time to generate 100-shot interpolation videos with ,!
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-imagenet,DiffAugment-biggan-imagenet, Our , repo (for TPU training) is released!
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-cifar,DiffAugment-biggan-cifar, Our , PyTorch repo is released!
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2,DiffAugment-stylegan2,This repository contains our implementation of Differentiable Augmentation (DiffAugment) in both PyTorch and TensorFlow. It can be used to significantly improve the data efficiency for GAN training. We have provided , (TensorFlow) and 
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2-pytorch,DiffAugment-stylegan2-pytorch, (TensorFlow) and ,", "
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-cifar,DiffAugment-biggan-cifar,", "," (PyTorch) for GPU training, and "
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-imagenet,DiffAugment-biggan-imagenet," (PyTorch) for GPU training, and ", (TensorFlow) for TPU training.
30238,https://colab.research.google.com/gist/zsyzzsoft/5fbb71b9bf9a3217576bebae5de46fc2/data-efficient-gans.ipynb,"

",Training and Generation with 100 Images ,"
"
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2#100-shot-generation,DiffAugment-stylegan2,", or the folder containing your own training images. Please refer to the ", README for the dependencies and details.
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2,DiffAugment-stylegan2," for unconditional generation on the 100-shot datasets, CIFAR, FFHQ, or LSUN, please refer to the ", README or 
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-stylegan2-pytorch,DiffAugment-stylegan2-pytorch, README or , for the PyTorch version.
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-cifar,DiffAugment-biggan-cifar,Please refer to the , README to run 
30238,https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-imagenet,DiffAugment-biggan-imagenet," for conditional generation on CIFAR (using GPUs), and the ", README to run on ImageNet (using TPUs).
30238,https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_tf.py,DiffAugment_tf.py,"To help you use DiffAugment in your own codebase, we provide portable DiffAugment operations of both TensorFlow and PyTorch versions in ", and 
30238,https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py,DiffAugment_pytorch.py, and ,". Generally, DiffAugment can be easily adopted in any model by substituting every "
30254,http://snap.stanford.edu/data/#signnets,SNAP,). Four example graphs (donwloaded from ,", but node ID is resorted) "
30256,https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/,here,The same code is also provided in this repo and follows the same series of commands. The Criteo dataset is found ,. Place it in the path 
30260,http://numba.pydata.org/,numba,"
","
"
30260,https://pandas.pydata.org/,pandas,"
","
"
30260,http://tseregression.org/data/ts_regression.xlsx,here, showed that a simple linear model such as Rocket performs best for the time series regression task. The full results can be obtained ,.
30263,https://www.dropbox.com/s/70d75da8h8f16ox/gazeta_data_mbart_600_160_v2.tar.gz,gazeta_data_mbart_600_160_v2.tar.gz,"v1, Preprocessed data for mBART: ","
"
30263,https://www.dropbox.com/s/gji8i1bmhg0fcfj/gazeta_v2_data_mbart_600_160_v2.tar.gz,gazeta_v2_data_mbart_600_160_v2.tar.gz,"v2, Preprocessed data for mBART: ","
"
30270,https://github.com/XifengGuo/JULE-Torch/blob/master/datasets/FRGC/data4torch.h5,FRGC source link,"
","
"
30271,https://github.com/gwgundersen/rflvm/blob/master/datasets/loader.py#L71,here," function to generate latent variables in the shape of an ""S"" and then generates data according to the data generating process described in the paper. To see the data generating process in code, see ",. See the 
30272,http://papers.nips.cc/paper/6941-gaussian-process-based-nonlinear-latent-structure-discovery-in-multivariate-spike-train-data,Wu et al 2017,As shown in Gaussian process based nonlinear latent structure discovery in multivariate spike train data ,.
30274,data/checkpoints,data/checkpoints,We provide models trained on Aff-Wild2 in , and tensorboard logs during training this models in 
30274,data/logs,data/logs, and tensorboard logs during training this models in ,". Also, all experiments such as validation of the CNN on small datasets, visualization of CNN layers, training and predicting are in "
30284,scripts/sun_data_preprocessing.py,scripts/sun_data_preprocessing.py, and then follow the instructions in ,"
"
30286,https://www.dropbox.com/s/ecem4kq0fdkver4/cityscapes-vps-dataset-1.0.zip?dl=0,Dataset,] [,] [
30286,https://www.dropbox.com/s/ecem4kq0fdkver4/cityscapes-vps-dataset-1.0.zip?dl=0,download Cityscapes-VPS here,You can ,". It provides 2500-frame panoptic labels that temporally extend the 500 Cityscapes image-panoptic labels. There are total 3000-frame panoptic labels which correspond to 5, 10, 15, 20, 25, and 30th frames of each 500 videos, where all instance ids are associated over time. "
30304,https://www.kaggle.com/chiranjivdas09/ta-feng-grocery-dataset,TaFeng,"
","
"
30304,https://tianchi.aliyun.com/dataset/dataDetail?dataId=649,TaoBao,"
","
"
30338,https://github.com/google-research/meta-dataset#user-instructions,Meta-Dataset repository,"Follow the the ""User instructions"" in the "," for ""Installation"" and ""Downloading and converting datasets""."
30340,https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset,Get ImageNet,"
"," if you don't have it yet. If you do, note that validation images need to be put in separate folders, just like train data. Follow the instructions in that link to do so easily."
30351,https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#file-formats,TFRecord,"Train, validation, and test datasets are in "," or streamed JSON (one JSON object per line). They are 45GB, 5GB, and 3GB respectively. For example test.tar.gz contains 15 files whose union is the whole test set. We split them to help speed up training/testing by parallelizing reads. Any one of the shards can be opened with a "
30357,synthetic_data,synthetic_data,"
",: You could run 
30357,synthetic_data/nonconvex.py,"
nonconvex.py
",: You could run , or 
30357,synthetic_data/nqm.py,"
nqm.py
", or , to reproduce the experiments for the nonconvex function or the Noisy Quadratic Model.
30365,http://m-selig.ae.illinois.edu/ads/coord_database.html,UIUC airfoil coordinates database,Our airfoil designs come from ,.
30372,http://pascal.inrialpes.fr/data2/dchen/pretrained/otk_checkpoint.zip,pretrained model,Download our , to 
30383,https://github.com/benywon/ReCO/blob/master/prepare_data.py#L29,"
clean(one['passage'])
","If you want to use the original doc as the context, you can set the ", in prepare_data.py line 29 to 
30412,https://data.csail.mit.edu/graphics/fivek/,MIT-Adobe FiveK dataset,As the dataset was originally rendered using raw images taken from the ,", our sRGB2XYZ dataset follows the original license of the "
30412,https://data.csail.mit.edu/graphics/fivek/,MIT-Adobe FiveK dataset,", our sRGB2XYZ dataset follows the original license of the ",.
30415,data,data,"Once the pipeline has been run, the directory structure of the "," folder should look like the following. If you get errors, a good first check would be to see if any files are missing."
30415,data/raw,"
data/raw
",Exports from Google Drive should be saved in ,. This happens by default if the 
30418,#link-of-the-dataset,Dataset Description,"
","
"
30420,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,Scene Flow Datasets,Download ,", "
30420,http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo,KITTI 2012,", ",", "
30420,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo,KITTI 2015,", ",", "
30427,http://www.frankmcsherry.org/dataflow/relational/join/2015/04/11/genericjoin.html,a blog post,"Ngo et al presented a very cool join algorithm, some details of which are described in ",". This project (a collaboration with Khaled Ammar and Semih Salihoglu) extends Ngo et al's algorithm to the case where the underlying relations change, allowing us to track the results of complex join queries as they change."
30427,https://github.com/frankmcsherry/dataflow-join/blob/master/examples/motif.rs,"
examples/motif.rs
","For an example, the "," program takes the description of a directed graph motif (to be explained) and a list of graph edges, and reports the change in the numbers of these motifs as we stream the edges in. To look for directed triangles of the form "
30438,https://grouplens.org/datasets/movielens,MovieLens Dataset,"
","
"
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Nations.html#pykeen.datasets.Nations,Nations, model on the ," dataset. By default, the training loop uses the "
30465,https://pykeen.readthedocs.io/en/latest/byo/data.html,using your own dataset," dataclass that has attributes for the trained model, the training loop, the evaluation, and more. See the tutorials on ",", "
30465,https://pykeen.readthedocs.io/en/latest/byo/data.html,Bring Your Own Dataset,"The following 36 datasets are built in to PyKEEN. The citation for each dataset corresponds to either the paper describing the dataset, the first paper published using the dataset with knowledge graph embedding models, or the URL for the dataset if neither of the first two are available. If you want to use a custom dataset, see the "," tutorial. If you have a suggestion for another dataset to include in PyKEEN, please let us know "
30465,https://github.com/pykeen/pykeen/issues/new?assignees=cthoyt&labels=New+Dataset&template=dataset-request.md&title=Add+%5BDATASET+NAME%5D,here," tutorial. If you have a suggestion for another dataset to include in PyKEEN, please let us know ",.
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.AristoV4.html,"
pykeen.datasets.AristoV4
",| Name                               | Documentation                                                                                                       | Citation                                                                                                                |   Entities |   Relations |   Triples | |------------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|------------|-------------|-----------| | Aristo-v4                          | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.BioKG.html,"
pykeen.datasets.BioKG
",                                                        |      42016 |        1593 |    279425 | | BioKG                              | ,                   | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CKG.html,"
pykeen.datasets.CKG
",                                                         |     105524 |          17 |   2067997 | | Clinical Knowledge Graph           | ,                       | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CN3l.html,"
pykeen.datasets.CN3l
",                                                      |    7617419 |          11 |  26691525 | | CN3l Family                        | ,                     | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CoDExLarge.html,"
pykeen.datasets.CoDExLarge
",                                                  |       3206 |          42 |     21777 | | CoDEx (large)                      | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CoDExMedium.html,"
pykeen.datasets.CoDExMedium
",                                                               |      77951 |          69 |    612437 | | CoDEx (medium)                     | ,       | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CoDExSmall.html,"
pykeen.datasets.CoDExSmall
",                                                               |      17050 |          51 |    206205 | | CoDEx (small)                      | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.ConceptNet.html,"
pykeen.datasets.ConceptNet
",                                                               |       2034 |          42 |     36543 | | ConceptNet                         | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Countries.html,"
pykeen.datasets.Countries
",                                                                |   28370083 |          50 |  34074917 | | Countries                          | ,           | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.CSKG.html,"
pykeen.datasets.CSKG
",                          |        271 |           2 |      1158 | | Commonsense Knowledge Graph        | ,                     | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.DB100K.html,"
pykeen.datasets.DB100K
",                                                              |    2087833 |          58 |   4598728 | | DB100K                             | ,                 | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.DBpedia50.html,"
pykeen.datasets.DBpedia50
",                                                                 |      99604 |         470 |    697479 | | DBpedia50                          | ,           | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.DRKG.html,"
pykeen.datasets.DRKG
",                                                                  |      24624 |         351 |     34421 | | Drug Repositioning Knowledge Graph | ,                     | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.FB15k.html,"
pykeen.datasets.FB15k
",                                                                         |      97238 |         107 |   5874257 | | FB15k                              | ,                   | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.FB15k237.html,"
pykeen.datasets.FB15k237
", |      14951 |        1345 |    592213 | | FB15k-237                          | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Globi.html,"
pykeen.datasets.Globi
",                                                  |      14505 |         237 |    310079 | | Global Biotic Interactions         | ,                   | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Hetionet.html,"
pykeen.datasets.Hetionet
",                                                   |     404207 |          39 |   1966385 | | Hetionet                           | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Kinships.html,"
pykeen.datasets.Kinships
",                                                       |      45158 |          24 |   2250197 | | Kinships                           | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Nations.html,"
pykeen.datasets.Nations
",                                             |        104 |          25 |     10686 | | Nations                            | ,               | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.NationsLiteral.html,"
pykeen.datasets.NationsLiteral
",                                                   |         14 |          55 |      1992 | | NationsL                           | , | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.OGBBioKG.html,"
pykeen.datasets.OGBBioKG
",                                                                     |         14 |          55 |      1992 | | OGB BioKG                          | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.OGBWikiKG2.html,"
pykeen.datasets.OGBWikiKG2
",                                                                   |      45085 |          51 |   5088433 | | OGB WikiKG2                        | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.OpenBioLink.html,"
pykeen.datasets.OpenBioLink
",                                                                   |    2500604 |         535 |  17137181 | | OpenBioLink                        | ,       | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.OpenBioLinkLQ.html,"
pykeen.datasets.OpenBioLinkLQ
",                                                  |     180992 |          28 |   4563407 | | OpenBioLink LQ                     | ,   | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.OpenEA.html,"
pykeen.datasets.OpenEA
",                                                  |     480876 |          32 |  27320889 | | OpenEA Family                      | ,                 | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.PharmKG.html,"
pykeen.datasets.PharmKG
",                                                     |      15000 |         248 |     38265 | | PharmKG                            | ,               | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.PharmKG8k.html,"
pykeen.datasets.PharmKG8k
",                                                             |     188296 |          39 |   1093236 | | PharmKG8k                          | ,           | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.PrimeKG.html,"
pykeen.datasets.PrimeKG
",                                                             |       7247 |          28 |    485787 | | PrimeKG                            | ,               | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.UMLS.html,"
pykeen.datasets.UMLS
",                                                     |     129375 |          30 |   8100498 | | Unified Medical Language System    | ,                     | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.WD50KT.html,"
pykeen.datasets.WD50KT
",                                                   |        135 |          46 |      6529 | | WD50K (triples)                    | ,                 | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.Wikidata5M.html,"
pykeen.datasets.Wikidata5M
",                                          |      40107 |         473 |    232344 | | Wikidata5M                         | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.WK3l120k.html,"
pykeen.datasets.WK3l120k
",                                                                 |    4594149 |         822 |  20624239 | | WK3l-120k Family                   | ,             | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.WK3l15k.html,"
pykeen.datasets.WK3l15k
",                                                  |     119748 |        3109 |   1375406 | | WK3l-15k Family                    | ,               | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.WN18.html,"
pykeen.datasets.WN18
",                                                  |      15126 |        1841 |    209041 | | WordNet-18                         | ,                     | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.WN18RR.html,"
pykeen.datasets.WN18RR
",                                                                |      40943 |          18 |    151442 | | WordNet-18 (RR)                    | ,                 | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.YAGO310.html,"
pykeen.datasets.YAGO310
",                                                  |      40559 |          11 |     92583 | | YAGO3-10                           | ,               | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.ILPC2022Large.html,"
pykeen.datasets.ILPC2022Large
",| Name            | Documentation                                                                                                             | Citation                                                  | |-----------------|---------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------| | ILPC2022 Large  | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.ILPC2022Small.html,"
pykeen.datasets.ILPC2022Small
", | | ILPC2022 Small  | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.InductiveFB15k237.html,"
pykeen.datasets.InductiveFB15k237
", | | FB15k-237       | , | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.InductiveNELL.html,"
pykeen.datasets.InductiveNELL
",   | | NELL            | ,         | 
30465,https://pykeen.readthedocs.io/en/latest/api/pykeen.datasets.InductiveWN18RR.html,"
pykeen.datasets.InductiveWN18RR
",   | | WordNet-18 (RR) | ,     | 
30477,https://github.com/rochesterxugroup/HAM_dataset/tree/master/figures,figures,Images of the generated mapping are given in the ,. These images were generated using 
30496,http://www.select.cs.cmu.edu/code/graphlab/datasets/netflix_mm,netflix_mm,The netflix_mm and netflix_mme are original data files downloaded from , and 
30496,http://www.select.cs.cmu.edu/code/graphlab/datasets/netflix_mme,netflix_mme, and ,". As the download link no longer works, we put them on the above google drive link. If you are interested on how to transform ""netflix_train/netflix_test"" to ""netflix_train.bin/netflix_test.bin"", please check out ./data/netflix/prepare.sh"
30502,https://data.vision.ee.ethz.ch/reyang/model.zip,Download link,Pre-trained models (,)
30506,https://tianchi.aliyun.com/dataset/dataDetail?dataId=75173,Tianchi, the original google drive download link. Dataset now can be downloaded from ,.
30520,http://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/,Sunnybrook Health Sciences Centre,Many thanks to the ," for providing a set of CMR data with associated contours. Unfortunately, in the latest release the filenames have become a little mangled, and don't match up with the contours. I have gone through the files and matched them up; exported the DICOMS as PNGs and converted the list of coordinates of the contours to PNGs as well."
30531,https://data.pyg.org/whl,here,"We alternatively provide pip wheels for all major OS/PyTorch/CUDA combinations, see ",.
30531,https://data.pyg.org/whl,here, in order to prevent a manual installation from source. You can look up the latest supported version number ,.
30532,https://pandas.pydata.org/,Panda,". Furthermore, the analysis of the results is based on ",", "
30532,https://seaborn.pydata.org/index.html,Seaborn, and , libraries.
30533,https://download.visinf.tu-darmstadt.de/data/from_games/,here,: Please follow the instructions , to download images and semantic segmentation annotations. The GTA5 dataset directory should have this basic structure:
30533,https://www.cityscapes-dataset.com/,Cityscape,: Please follow the instructions in , to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:
30535,https://towardsdatascience.com/autogluon-deep-learning-automl-5cdb4e2388ec?source=friends_link&sk=e3d17d06880ac714e47f07f39178fdf2,AutoGluon overview & example applications,"
", (
30536,https://archive.ics.uci.edu/ml/datasets/adult,Adult Census Dataset,In this example we load the ,* which is a built-in demo dataset. We use CTGAN to learn from the real data and then generate some synthetic data.
30536,https://datacebo.com,DataCebo," in 2016. After 4 years of research and traction with enterprise, we created "," in 2020 with the goal of growing the project. Today, DataCebo is the proud developer of SDV, the largest ecosystem for synthetic data generation & evaluation. It is home to multiple libraries that support synthetic data, including:"
30537,data,"
data folder",Benchmark data of the Ginkgo library on different hardware in the ,;
30537,data,"
data/ folder",In the ,", you can find the actual benchmark data listed. The data is organized in a hierarchy of folders, with the following levels:"
30542,http://b2.cvl.iis.u-tokyo.ac.jp/~roxas/data_iros2019_open.zip,dataset,Our dataset can be accessed from: ,". There are 56 image pairs with ground truth depth, simulated LIDAR data, semantic segmentation results from "
30557,dataset/,dataset,"For details and instructions for downloading the dataset used in this work, please see ",.
30564,https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md,mmaction2,"First, please follow the ", to prepare data. Note that our codebase only supports the 
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-3d-dve/2019-08-08_17-54-21/config.json,config,| Embed. Dim | Model | Same Identity | Different Identity | Params | Links | | :-----------: | :-: | :----: | :----: | :----: | :----: | |  3 | smallnet | 1.36 | 3.03 | 334.9k | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-16d-dve/2019-08-02_06-20-13/config.json,config, | |  16 | smallnet | 1.28 | 2.79 | 338.2k | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-32d-dve/2019-08-02_06-19-59/config.json,config, | |  32 | smallnet | 1.29 | 2.79 | 342.3k | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-64d-dve/2019-08-02_06-20-28/config.json,config, | |  64 | smallnet | 1.28 | 2.77 | 350.6k | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-hourglass-64d-dve/0618_103501/config.json,config, | |  64 | hourglass | 0.93 | 2.37 | 12.6M | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-3d-dve/2019-08-11_08-33-22/config.json,config,| Embed. Dim | Model | Error (%IOD) | Links | | :-----------: | :-: | :----: | :----: | |  3 | smallnet | 4.17 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-16d-dve/2019-08-11_08-29-31/config.json,config, | |  16 | smallnet | 3.97 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-32d-dve/2019-08-11_08-29-53/config.json,config, | |  32 | smallnet | 3.82 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-64d-dve/2019-08-11_08-40-48/config.json,config, | |  64 | smallnet | 3.42 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-hourglass-64d-dve/2019-08-11_14-30-53/config.json,config, | |  64 | hourglass | 2.86 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-3d-dve/2019-08-11_14-50-50/config.json,config,| Embed. Dim | Model | Error (%IOD) | Links | | :-----------: | :--: | :----: | :----: | |  3 | smallnet | 7.66 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-16d-dve/2019-08-11_14-50-52/config.json,config, | |  16 | smallnet | 6.29 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-32d-dve/2019-08-11_14-50-53/config.json,config, | |  32 | smallnet | 6.13 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-64d-dve/2019-08-11_14-50-54/config.json,config, | |  64 | smallnet | 5.75 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-hourglass-64d-dve/2019-08-11_18-50-33/config.json,config, | |  64 | hourglass | 4.65 | ,", "
30584,http://mmlab.ie.cuhk.edu.hk/projects/TCDCN/data/MTFL.zip,here," introduced a test subset of almost 3K faces (for convenience, we include a mirror version of these images, but you can obtain the originals ",)
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-3d-dve/2019-08-12_05-50-54/config.json,config,| Embed. Dim | Model | Error (%IOD) | Links | | :-----------: | :--: | :----: | :----: | |  3 | smallnet | 10.13 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_07-57-00/config.json,config, | |  16 | smallnet | 8.40 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_07-56-57/config.json,config, | |  32 | smallnet | 8.18 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-53-30/config.json,config, | |  64 | smallnet | 7.79 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-hourglass-64d-dve/2019-08-12_06-00-52/config.json,config, | |  64 | hourglass | 6.54 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-3d-dve/2019-08-11_18-43-20/config.json,config,| Embed. Dim | Model | Error (%IOD) | Links | | :-----------: | :--: | :----: | :----: | |  3 | smallnet | 11.12 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_18-43-24/config.json,config, | |  16 | smallnet | 9.15 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_18-43-30/config.json,config, | |  32 | smallnet | 9.17 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-43-35/config.json,config, | |  64 | smallnet | 8.60 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-hourglass-64d-dve/2019-08-12_06-00-59/config.json,config, | |  64 | hourglass | 7.53 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-3d/2019-08-04_17-55-48/config.json,config,| Embed. Dim | Model | DVE | Same Identity | Different Identity | Links | | :-----------: | :--:  | :-: | :----: | :----: | :--: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark:  | 1.33 / 1.36| 2.89 / 3.03 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-3d-dve/2019-08-08_17-54-21/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-16d/2019-08-04_17-55-52/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark:  | 1.25 / 1.28| 5.65 / 2.79 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-16d-dve/2019-08-02_06-20-13/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-32d/2019-08-04_17-55-57/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark:  | 1.26 / 1.29| 5.81 / 2.79 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-32d-dve/2019-08-02_06-19-59/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-64d/2019-08-04_17-56-04/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark:  | 1.25 / 1.28| 5.68 / 2.77 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/celeba-smallnet-64d-dve/2019-08-02_06-20-28/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-3d/2019-08-11_08-24-51/config.json,config,| Embed. Dim | Model | DVE | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 4.02/4.17 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-3d-dve/2019-08-11_08-33-22/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-16d/2019-08-11_08-29-08/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 5.31/3.97 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-16d-dve/2019-08-11_08-29-31/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-32d/2019-08-11_08-25-14/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 5.36/3.82 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-32d-dve/2019-08-11_08-29-53/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-64d/2019-08-08_17-56-47/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 4.99/3.42 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/mafl-keypoints-celeba-smallnet-64d-dve/2019-08-11_08-40-48/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-3d/2019-08-11_14-50-45/config.json,config,| Embed. Dim | Model | DVE | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 8.23/7.66 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-3d-dve/2019-08-11_14-50-50/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-16d/2019-08-11_14-50-46/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.66/6.29 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-16d-dve/2019-08-11_14-50-52/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-32d/2019-08-11_14-50-47/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.33/6.13 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-32d-dve/2019-08-11_14-50-53/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-64d/2019-08-11_14-50-48/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 9.33/5.75 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-64d-dve/2019-08-11_14-50-54/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-3d/2019-08-11_18-42-45/config.json,config,| Embed. Dim | Model | DVE | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.99/11.12 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-3d-dve/2019-08-11_18-43-20/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-16d/2019-08-11_18-43-03/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 12.22/9.15 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_18-43-24/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-32d/2019-08-11_18-43-09/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 12.60/9.17 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_18-43-30/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-64d/2019-08-11_18-43-14/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 12.92/8.60 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-43-35/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-3d/2019-08-11_07-56-34/config.json,config,| Embed. Dim | Model | DVE | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.14/10.13 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-3d-dve/2019-08-12_05-50-54/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-16d/2019-08-11_07-56-41/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.73/8.40 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_07-57-00/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-32d/2019-08-11_07-56-46/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 11.05/8.18 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_07-56-57/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-64d/2019-08-11_07-56-52/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 11.43/7.79 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-53-30/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-keypoints-celeba-smallnet-3d-dve/2019-08-11_14-02-25/config.json,config,| Embed. Dim | Model | Finetune | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 11.82/11.12 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-3d-dve/2019-08-11_18-43-20/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-keypoints-celeba-smallnet-16d-dve/2019-08-11_14-02-40/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 10.22/9.15 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_18-43-24/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-keypoints-celeba-smallnet-32d-dve/2019-08-11_14-02-56/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 9.80/9.17 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_18-43-30/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-keypoints-celeba-smallnet-64d-dve/2019-08-11_14-03-14/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 9.28/8.60 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-43-35/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-keypoints-celeba-hourglass-64d-dve/2019-08-11_18-49-34/config.json,config,) | |  64 | hourglass | :heavy_multiplication_x: / :heavy_check_mark: | 8.15/7.53 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-keypoints-celeba-hourglass-64d-dve/2019-08-12_06-00-59/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-keypoints-celeba-smallnet-3d-dve/2019-08-11_08-42-33/config.json,config,| Embed. Dim | Model | Finetune | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 9.65/10.13 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-3d-dve/2019-08-12_05-50-54/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-keypoints-celeba-smallnet-16d-dve/2019-08-10_12-53-44/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 8.91/8.40 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-16d-dve/2019-08-11_07-57-00/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-keypoints-celeba-smallnet-32d-dve/2019-08-10_09-20-02/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 8.73/8.18 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-32d-dve/2019-08-11_07-56-57/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-keypoints-celeba-smallnet-64d-dve/2019-08-10_12-55-24/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 8.14/7.79 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-smallnet-64d-dve/2019-08-11_18-53-30/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-keypoints-celeba-hourglass-64d-dve/2019-08-09_14-29-19/config.json,config,) | |  64 | hourglass | :heavy_multiplication_x: / :heavy_check_mark: | 6.88/6.54 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-keypoints-celeba-hourglass-64d-dve/2019-08-12_06-00-52/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-3d-dve/2019-08-11_14-50-50/config.json,config,| Embed. Dim | Model | Finetune | Error (%IOD) | Links | | :-----------: | :--:  | :-: | :----: | :----: | |  3 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 7.66/7.20 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-keypoints-celeba-smallnet-3d-dve/2019-08-12_05-21-09/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-16d-dve/2019-08-11_14-50-52/config.json,config,) | |  16 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 6.29/5.90 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-keypoints-celeba-smallnet-16d-dve/2019-08-12_05-21-17/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-32d-dve/2019-08-11_14-50-53/config.json,config,) | |  32 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 6.13/5.75 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-keypoints-celeba-smallnet-32d-dve/2019-08-12_05-21-27/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-smallnet-64d-dve/2019-08-11_14-50-54/config.json,config,) | |  64 | smallnet | :heavy_multiplication_x: / :heavy_check_mark: | 5.75/5.58 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-keypoints-celeba-smallnet-64d-dve/2019-08-12_05-21-35/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-keypoints-celeba-hourglass-64d-dve/2019-08-11_18-50-33/config.json,config,) | |  64 | hourglass | :heavy_multiplication_x: / :heavy_check_mark: | 4.65/4.65 | (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-keypoints-celeba-hourglass-64d-dve/2019-08-11_18-42-44/config.json,config,) / (,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-celeba-smallnet-3d-dve/2019-08-11_08-20-03/config.json,config,| Embed. Dim | Model | Same Identity | Different Identity | Links | | :-----------: | :-: | :----: | :----: | :----: | |  3 | smallnet | 5.99 | 7.16 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-celeba-smallnet-16d-dve/2019-08-11_08-19-58/config.json,config, | |  16 | smallnet | 4.72 | 7.11 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-celeba-smallnet-32d-dve/2019-08-11_08-19-55/config.json,config, | |  32 | smallnet | 6.42 | 8.71 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-celeba-smallnet-64d-dve/2019-08-11_08-19-54/config.json,config, | |  64 | smallnet | 8.07 | 10.09 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-ft-celeba-hourglass-64d-dve/2019-08-11_16-40-28/config.json,config, | |  64 | hourglass | 1.53 | 3.65 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-celeba-smallnet-3d-dve/2019-08-11_18-51-40/config.json,config,| Embed. Dim | Model | Same Identity | Different Identity | Links | | :-----------: | :-: | :----: | :----: | :----: | |  3 | smallnet | 6.36 | 7.69 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-celeba-smallnet-16d-dve/2019-08-10_12-50-30/config.json,config, | |  16 | smallnet | 6.34 | 8.62 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-celeba-smallnet-32d-dve/2019-08-10_12-50-31/config.json,config, | |  32 | smallnet | 8.10 | 10.11 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-celeba-smallnet-64d-dve/2019-08-10_12-50-32/config.json,config, | |  64 | smallnet | 4.08 | 5.21 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-ft-celeba-hourglass-64d-dve/2019-08-11_14-43-34/config.json,config, | |  64 | hourglass | 1.17 | 4.04 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-celeba-smallnet-3d-dve/2019-08-11_18-11-57/config.json,config,| Embed. Dim | Model | Same Identity | Different Identity | Links | | :-----------: | :-: | :----: | :----: | :----: | |  3 | smallnet | 5.21 | 6.51 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-celeba-smallnet-16d-dve/2019-08-11_18-12-04/config.json,config, | |  16 | smallnet | 5.55 | 7.30 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-celeba-smallnet-32d-dve/2019-08-11_18-12-15/config.json,config, | |  32 | smallnet | 5.85 | 7.47 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-celeba-smallnet-64d-dve/2019-08-11_18-12-24/config.json,config, | |  64 | smallnet | 6.58 | 8.19 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/300w-ft-celeba-hourglass-64d-dve/2019-08-11_12-57-08/config.json,config, | |  64 | hourglass | 1.63 | 3.82 | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-1-annos-celeba-smallnet-3d/2019-08-13_12-06-42/config.json,config,| Embed. Dim | Model | DVE | Num annos. | Error (%IOD) | Links | | :-----------: | :--: | :--: | :--: | :----: | :----: | |  3 | smallnet | 1 | :heavy_multiplication_x: | 19.87 (+/- 3.10) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-5-annos-celeba-smallnet-3d/2019-08-13_12-07-37/config.json,config, | |  3 | smallnet | 5 | :heavy_multiplication_x: | 16.90 (+/- 1.04) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-10-annos-celeba-smallnet-3d/2019-08-13_12-07-49/config.json,config, | |  3 | smallnet | 10 | :heavy_multiplication_x: | 16.12 (+/- 1.07) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-20-annos-celeba-smallnet-3d/2019-08-13_12-08-03/config.json,config, | |  3 | smallnet | 20 | :heavy_multiplication_x: | 15.30 (+/- 0.59) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-1-annos-celeba-smallnet-64d-dve/2019-08-13_12-08-14/config.json,config, | |  64 | smallnet | 1 | :heavy_check_mark: | 17.13 (+/- 1.78) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-5-annos-celeba-smallnet-64d-dve/2019-08-13_12-08-23/config.json,config, | |  64 | smallnet | 5 | :heavy_check_mark: | 13.57 (+/- 2.08) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-10-annos-celeba-smallnet-64d-dve/2019-08-13_12-08-34/config.json,config, | |  64 | smallnet | 10 | :heavy_check_mark: | 12.97 (+/- 2.36) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-20-annos-celeba-smallnet-64d-dve/2019-08-13_12-08-55/config.json,config, | |  64 | smallnet | 20 | :heavy_check_mark: | 11.26 (+/- 0.93) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-1-annos-celeba-hourglass-64d-dve/2019-08-13_14-14-23/config.json,config, | |  64 | hourglass | 1 | :heavy_check_mark: | 14.23 (+/- 1.54) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-5-annos-celeba-hourglass-64d-dve/2019-08-13_14-14-49/config.json,config, | |  64 | hourglass | 5 | :heavy_check_mark: | 12.04 (+/- 2.03) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-10-annos-celeba-hourglass-64d-dve/2019-08-13_14-15-11/config.json,config, | |  64 | hourglass | 10 | :heavy_check_mark: | 12.25 (+/- 2.42) | ,", "
30584,http:/www.robots.ox.ac.uk/~vgg/research/DVE/data/models/aflw-mtfl-limit-annos-ft-keypoints-20-annos-celeba-hourglass-64d-dve/2019-08-13_14-32-35/config.json,config, | |  64 | hourglass | 20 | :heavy_check_mark: | 11.46 (+/- 0.83) | ,", "
30584,misc/sync_datasets.py,utility script,"For each dataset used in the paper, we provide a preprocessed copy to allow the results described above to be reproduced directly.  These can be downloaded and unpacked with a ",", which will store them in the locations expected by the training code. Each dataset has a brief README, which also provides the citations for use with each dataset, together with a link from which it can be downloaded directly."
30584,misc/datasets/celeba/README.md,README,| Dataset   | Details and links | Archive size | sha1sum | |:-------------:|:-----:|:----:|:---:| | CelebA (+ MAFL) | ,| 9.0 GiB | 
30584,misc/datasets/300w/README.md,README, | | 300w | ,| 3.0 GiB | 
30584,misc/datasets/aflw-mtfl/README.md,README, | ,| 252 MiB | 
30584,misc/datasets/aflw-recrop/README.md,README, | ,| 1.1 GiB | 
30584,misc/sync_datasets.py,data fetching script, (this will be done automatically by the ,", or can be done manually)."
30584,misc/sync_datasets.py,data fetching script, (this will be done automatically by the ,", or can be done manually)."
30584,misc/sync_datasets.py,data fetching script, (this will be done automatically by the ,", or can be done manually)."
30588,#data,Data,"
","
"
30596,https://github.com/deepmind/dsprites-dataset,dSprites,This is a dataset based on ,". Each image consists of multiple oval, heart, or square-shaped sprites (with some occlusions) set against a uniformly colored background."
30596,https://github.com/facebookresearch/clevr-dataset-gen,open-source script,We adapted the ," provided by Johnson et al. to produce ground-truth segmentation masks for CLEVR [6] scenes. These were generated afresh, so images in this dataset are not identical to those in the original CLEVR dataset. We ignore the original question-answering task."
30596,https://console.cloud.google.com/storage/browser/multi-object-datasets,Google Cloud Storage,The datasets can be downloaded from ,. Each dataset is a single 
30596,https://www.tensorflow.org/tutorials/load_data/tf_records,TFRecords,. Each dataset is a single ," file. To download a particular dataset, use the web interface, or run "
30596,https://www.tensorflow.org/api_docs/python/tf/data/Dataset,"
tf.data.Dataset
","After downloading the dataset files, you can read them as ", instances with the readers provided. The example below shows how to read the colored-sprites-and-background version of Multi-dSprites:
30603,https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz,Textures,"
",: download it and place it in the folder of 
30603,http://data.csail.mit.edu/places/places365/test_256.tar,Places365,"
",: download it and place it in the folder of 
30607,#datasets,Datasets,"
","
"
30619,https://ieee-dataport.org/open-access/mqtt-internet-things-intrusion-detection-dataset,IEEE DataPort,The used dataset is published in ,"
"
30632,https://github.com/cocodataset/cocoapi,COCOAPI,Install ,:
30632,http://cocodataset.org/#download,COCO download,", please download from ",", 2017 Train/Val is needed for COCO keypoints training and validation. Download and extract them under {POSE_ROOT}/data, and make them look like this:"
30632,https://github.com/Jeff-sjtu/CrowdPose#dataset,CrowdPose download,", please download from ",", Train/Val is needed for CrowdPose keypoints training. Download and extract them under {POSE_ROOT}/data, and make them look like this:"
30633,./data/,data/,. Unpack the dataset in to , directory.
30633,./data/,data/,. Unpack this dataset in , directory. Then run the following script to generate split files.
30633,http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz,here,Download and unpack the CUB 200-2011 from , in 
30633,./data/,data/, in , directory. Then run the following script to generate split files.
30633,./data/iNat,data/iNat,", to ", directory from 
30638,dataset,dataset,We release five open-domain distantly/weakly labeled NER datasets here: ,". For gazetteers information and distant label generation code, please directly email cliang73@gatech.edu."
30657,http://crcv.ucf.edu/data/UCF101.php,UCF101,Download and pre-process , and 
30657,http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,HMDB51, and , datasets as follows.
30668,https://www.dropbox.com/s/ar9cupb18hv96gj/retro_data.zip?dl=0,link,Download and unzip the files from this ,", and put all the folders ("
30670,http://nlp.dmis.korea.edu/projects/covidask-lee-et-al-2020/covidask_data.tgz,covidask_data.tgz (109MB),"We provide pre-processed CORD-19 datasets, pre-trained QA models, and their phrase dumps. Download required files from here: ",", "
30671,http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip,Download link, (,") (82G). After downloading, please run the following codes to generate ""folder.npy"" which contains the directories of all training samples."
30673,https://github.com/LUMII-Syslab/Matrix-SE/tree/master/data,data folder,"), and represent graphs as an adjacency matrix. Dataset generators for algorithmic tasks are available in ",.
30682,https://github.com/A2Zadeh/CMU-MultimodalSDK/blob/master/mmsdk/mmdatasdk/dataset/standard_datasets/CMU_MOSEI/README.md,here, 7-class and 2-class sentiment and emotion models have been train according to the instructions ,.
30683,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,Kitti Object Detection Dataset,Download the , (
30683,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,image, (,", "
30683,http://www.cvlibs.net/download.php?file=data_object_calib.zip,calib,", ", and 
30683,http://www.cvlibs.net/download.php?file=data_object_label_2.zip,label, and ,) and place it into 
30684,#families-in-the-wild-database,Families In the Wild Database,"
","
"
30691,https://www.dropbox.com/s/p5rseuzntkhos7f/data.zip?dl=1,datasets, and , as described in 
30695,https://pytorch.org/vision/stable/datasets.html#imagefolder,"
datasets.ImageFolder
", train and val images. The directory structure is the standard layout for the torchvision , as follows:
30708,https://github.com/kohpangwei/data-poisoning-journal-release,original github repository,This repository maintains code for the model-targeted poisoning attacks. The KKT attack is adapted from its ,. Our experiments on deep neural networks are in a separate folder 
30726,https://data.mendeley.com/datasets/cd2rtzm23r/1,Mendeley,Sample trajectories from each organ is publicly available in ,.
30730,https://sites.google.com/site/pydatalog/,pyDatalog,"
", that handles back-end Datalog deductible database.
30734,https://github.com/waymo-research/waymo-open-dataset/blob/master/docs/quick_start.md,official guide,Note: the following command assumes TensorFlow version 2.1.0. You may modify the command according to your TensorFlow version. See , for additional details.
30734,https://github.com/waymo-research/waymo-open-dataset/blob/master/docs/quick_start.md,official guide,The output file of the conversion tool is a single .bin. Waymo Open Dataset provides methods for creating submission (create_submission) or local evaluation (compute_detection_metrics_main). See the , for details.
30734,https://github.com/waymo-research/waymo-open-dataset,waymo-open-dataset,"
","
"
30750,datasets/datasets.md#top,Datasets,"
","
"
30750,datasets/alphabetical/alphabetical_datasets.md#top,Alphabetical,"
","
"
30750,datasets/year/year_datasets.md#top,Year,"
","
"
30750,datasets/application/application_datasets.md#top,Application,"
","
"
30750,datasets/task/task_datasets.md#top,Task,"
","
"
30750,datasets/annotation_datasets.md#top,Annotation,"
","
"
30754,https://seaborn.pydata.org/,seaborn,"
","
"
30754,https://pandas.pydata.org/,pandas,"
","
"
30760,http://participants-area.bioasq.org/datasets/,BioASQ,We provide a pre-processed version of the , Task 8b-Phase B
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,first part of the train set, --  ,", "
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/train_data/train_part_2.zip,second part of the train set, -- ,  is released
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/train_data/train_part_3.zip,third part of the train set, -- , is released
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,the test set, -- ," is released, the first evaluation phase (for tasks 1 and 3) starts "
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/test_data/test_ner_only.zip,NER markup for test set,. , is released.
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/test_data/test_full.zip,full test data markup, -- the results are announced and , is released.
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,train set,Date format in each is the same as in , i. e. for every text there should be .ann file containing BRAT markup of the text (.txt file for each text with text itself is optional). The name of each .ann file should be the same as the name of .txt file from 
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,test set, i. e. for every text there should be .ann file containing BRAT markup of the text (.txt file for each text with text itself is optional). The name of each .ann file should be the same as the name of .txt file from , provided by organizers
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,train set,"For phase 2 the submission format is as follows: zip archive containing 1 folders named ""set_2"". This folder should contain data for task 2. Date format is the same as in ", i. e. for every text there should be .ann file containing BRAT markup of the text (.txt file for each text with text itself is optional). The name of each .ann file should be the same as the name of .txt file from 
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,test set, i. e. for every text there should be .ann file containing BRAT markup of the text (.txt file for each text with text itself is optional). The name of each .ann file should be the same as the name of .txt file from , provided by organizers.
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,here,A train set with manually annotated named entities and relations. First and second parts of train set are avaliable ,"
"
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,здесь,"(i) Обучающая выборка, размеченная вручную сущностями и отношениями; первая часть обучающей выборки доступна ","
"
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,первой части собучающей выборки, - выдача ,", "
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/train_data/train_part_2.zip,второй части обучающей выборки, - выдача ,"
"
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/train_data/train_part_3.zip,третьей части обучающей выборки, - выдача ,"
"
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,тестовой выборки, - выдача , начало первой фазы тестирования (для задач 1 и 3) 
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/test_data/test_ner_only.zip,разметка NER тестовых данных,. Опубликована , for test is released.
30764,https://github.com/dialogue-evaluation/RuREBus/blob/master/test_data/test_full.zip,полной разметки теста, - объявление официальных результатов и публикация ,"
"
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,обучающей выборки,Формат данных в каждой папке такой же как для ," т. е. для каждого текста должен иметься в наличии .ann файл, содержащий аннотацию текста в формате BRAT (включение .txt файлов опционально). Имя каждого .ann файла должно совпадать с именем .txt файла из "
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,тестовой выборки," т. е. для каждого текста должен иметься в наличии .ann файл, содержащий аннотацию текста в формате BRAT (включение .txt файлов опционально). Имя каждого .ann файла должно совпадать с именем .txt файла из ",", предоставленной организаторами."
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/train_data,обучающей выборки,Формат данных в папке такой же как для ," т. е. для каждого текста должен иметься в наличии .ann файл, содержащий аннотацию текста в формате BRAT (включение .txt файлов опционально). Имя каждого .ann файла должно совпадать с именем .txt файла из "
30764,https://github.com/dialogue-evaluation/RuREBus/tree/master/test_data,тестовой выборки," т. е. для каждого текста должен иметься в наличии .ann файл, содержащий аннотацию текста в формате BRAT (включение .txt файлов опционально). Имя каждого .ann файла должно совпадать с именем .txt файла из ",", предоставленной организаторами."
30766,#prepare-lidar-data,"
Prepare lidar data
","
", (must read)
30766,#prepare-imu-data,"
Prepare IMU data
","
", (must read)
30766,#sample-datasets,"
Sample datasets
","
","
"
30766,http://docs.ros.org/en/melodic/api/robot_localization/html/preparing_sensor_data.html,here,": it is generally caused due to unavailable transform between message frame_ids and robot frame_id (for example: transform should be available from ""imu_frame_id"" and ""gps_frame_id"" to ""base_link"" frame. Please read the Robot Localization documentation found ",.
30775,https://www.cityscapes-dataset.com/,Cityscapes,Download , and 
30783,http://images.cocodataset.org/zips/train2014.zip,MS-COCO 2014,"
"," (T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3-5.) is utilized to train our auto-encoder network."
30786,https://www.dropbox.com/s/c2ekl15tb81fzjr/pairrec_data_code.zip?dl=0,https://www.dropbox.com/s/c2ekl15tb81fzjr/pairrec_data_code.zip?dl=0,You can download the datasets+code from: ,"
"
30793,data.md,click here,Sequences: ,"
"
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/18-layers/,data.dmlc.ml,| Network       | Top-1 error | Top-5 error | Traind Model | | :------------ | :---------: | :---------: | :-------------: | | ResNet-18     | 30.48       | 10.92       |, | | ResNet-34     | 27.20      | 8.86        | 
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/34-layers/,data.dmlc.ml, | | ResNet-34     | 27.20      | 8.86        | , | | ResNet-50     | 24.39   | 7.24   | 
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/50-layers/,data.dmlc.ml, | | ResNet-50     | 24.39   | 7.24   | , | | ResNet-101    | 22.68      | 6.58        | 
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/101-layers/,data.dmlc.ml, | | ResNet-101    | 22.68      | 6.58        | , | | ResNet-152    | 22.25       | 6.42        | 
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/152-layers/,data.dmlc.ml, | | ResNet-152    | 22.25       | 6.42        | , | | ResNet-200    | 22.14       | 6.16        | 
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/200-layers/,data.dmlc.ml, | | ResNet-200    | 22.14       | 6.16        | , |
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/,here,", and do not forget to shuffle the list files!), or just download the provided version from ",.
30800,http://data.dmlc.ml/mxnet/models/imagenet/resnet/,data.dml.ml,download the pre-trained model form , and put it into the predict directory.
30801,https://github.com/MadryLab/spatial-pytorch/blob/0c9b3c608047989d72d6fed7ec3235c360e88857/robustness/data_augmentation.py#L34,here,". This data augmentation is nearly identical to that used in this repository, except that the resize/cropping algorithms are slightly different, and the lighting applied has slightly different parameters. The data augmentation routine used can be found in detail in the repository ",.
30803,https://bokeh.pydata.org/en/latest,Bokeh,. The examples also depend on , and 
30804,https://www.drivendata.org/competitions/78/overhead-geopose-challenge/,Overhead Geopose Challenge,We developed the , in collaboration with DrivenData for the National Geospatial Intelligence Agency (NGA). Developers from DrivenData contributed software improvements for reducing file sizes and run times for submission evaluation and documentation below for working with the competition data. They produced a 
30804,https://www.drivendata.co/blog/overhead-geopose-benchmark/,blog post, in collaboration with DrivenData for the National Geospatial Intelligence Agency (NGA). Developers from DrivenData contributed software improvements for reducing file sizes and run times for submission evaluation and documentation below for working with the competition data. They produced a , with a tutorial describing how to use our code as a benchmark for the contest. They also produced a 
30804,https://www.drivendata.co/blog/overhead-geopose-challenge-winners/,blog post, with a tutorial describing how to use our code as a benchmark for the contest. They also produced a , summarizing results from the competition. Winning solutions are provided in this DrivenData 
30804,https://github.com/drivendataorg/overhead-geopose-challenge,open source repository, summarizing results from the competition. Winning solutions are provided in this DrivenData ,", and model weights are archived on "
30804,https://ieee-dataport.org/open-access/urban-semantic-3d-dataset,DataPort,", and model weights are archived on ",.
30804,https://ieee-dataport.org/open-access/urban-semantic-3d-dataset,DataPort,The original Urban Semantic 3D (US3D) data used in our CVPR 2020 paper is available on ,.
30804,https://www.drivendata.org/competitions/78/overhead-geopose-challenge/,Overhead Geopose Challenge,Competition data associated with our CVPR EarthVision 2021 paper is available for download at the ," web site and will be archived in the US3D DataPort repository when the contest is complete. The original data includes RGB images in TIFF format and above ground level (AGL) height images in floating point TIFF format with units of meters. To reduce file sizes, the competition RGB images are J2K, and the AGL images are integer TIFF format with units of centimeters. The competition data is now also available on is available on "
30804,https://ieee-dataport.org/open-access/urban-semantic-3d-dataset,DataPort," web site and will be archived in the US3D DataPort repository when the contest is complete. The original data includes RGB images in TIFF format and above ground level (AGL) height images in floating point TIFF format with units of meters. To reduce file sizes, the competition RGB images are J2K, and the AGL images are integer TIFF format with units of centimeters. The competition data is now also available on is available on ",.
30809,https://people.tamu.edu/~xinli/DocReassembly/data/DocDataset.zip,here,Click , to download the 
30822,data/v1.6,"
v1.6
","
",: Correction related to entities and templates. Thanks to 
30822,data/v1.5,"
v1.5
","
",": English Lexicalization templates, introduced in the EMNLP 2019 paper ""Neural data-to-text generation: A comparison between pipeline and end-to-end architectures"". (August 22th, 2019)"
30822,data/v1.4,"
v1.4
","
",": full revision of the delexicalized templates. (April 1st, 2019)"
30822,data/v1.3,"
v1.3
","
",: tokenization by 
30822,data/v1.2,"
v1.2
","
",: annotation of the test part of the corpus. See Issue 
30822,data/v1.1,"
v1.1
","
",: fix on some annotation mistakes. See Issue 
30822,data/v1.0,"
v1.0
","
",: first version with annotation of the train and development parts of the corpus and German translation.
30822,data/v2.0,"
v2.0 (BETA)
","
",": Tree templates. (April 1st, 2019)"
30822,http://data.statmt.org/wmt17_systems,here,", publicly available ",.
30822,https://gitlab.com/shimorina/webnlg-dataset,here,. The original version of the dataset can be found ,.
30825,/input-dataset-example.json,Click here to download,Here is an example of input dataset: ,"
"
30846,http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip,[glove.840B.300d.zip],Download the pretrained Glove vectors ,. Decompress the zip file and put the txt file in the root directory.
30854,http://cocodataset.org/#home,COCO dataset,"For the needs of our application we utilized YOLOv3 detector, trained on the ",. You can download this detector from 
30858,https://github.com/deepmind/multi-object-datasets/,Multi-Object Datasets,. A few steps are required for setting up each individual dataset. We also provide a PyTorch wrapper around the , used for the experiments on the 
30858,https://github.com/deepmind/dsprites-dataset,dSprites dataset,Generate coloured Multi-dSprites from the original , with:
30858,https://github.com/deepmind/gqn-datasets,GQN datasets,The , are quite large. The 
30858,https://github.com/deepmind/multi_object_datasets,Multi-Object Datasets,The repository contains a wrapper around the ,", returning an iterable which behaves similarly to a PyTorch DataLoader object. The default config assumes that any datasets you wish to use have been downloaded to "
30858,https://github.com/deepmind/multi_object_datasets,multi_object_datasets,"
", (Apache v2 license)
30860,http://buildingparser.stanford.edu/dataset.html,S3DIS,", ", and 
30863,http://andrewilyas.com/datasets.tar,this link,The datasets can be downloaded from , and loaded via the following code:
30886,http://www.cvlibs.net/datasets/kitti/eval_mots.php,KITTI leader-board, as the segmentation sub-network. The current ranking of PointTrack is available in ,". Until now (07/03/2020), PointTrack++ still ranks first for both cars and pedestrians. The detailed task description of MOTS is avaliable in "
30886,http://www.cvlibs.net/download.php?file=data_tracking_image_2.zip,KITTI Images,"
", + 
30886,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,KITTI dataset,Note that the training of SpatialEmbedding needs KITTI object detection left color images as well as the KINS annotations. Please download images from ,", and unzip the zip file under "
30887,http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow,leaderboard,This correspondens to the entry on the , (Fl-all=6.30%).
30887,https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html,flying chairs,Make sure you have downloaded , and 
30887,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,"flying things subset
", and ,", and placed them under the same folder, say /ssd/."
30891,https://data.kb.se/datasets/2020/01/tf/bert_base_swedish_cased-v1.1.tar,TF checkpoint,", ", | | 
30891,https://data.kb.se/datasets/2020/01/tf/albert_base_swedish_cased.tar,TF checkpoint,", ", | | 
30894,https://medium.com/towards-artificial-intelligence/the-50-best-public-datasets-for-machine-learning-d80e9f030279,The Best Public Datasets for Machine Learning and Data Science,"
", On TowardsDataScience.com
30894,https://www.reddit.com/r/MachineLearning/comments/b5idqk/p_dataset_480000_rotten_tomatoes_reviews_for_nlp/,Reddit/r/MachineLearning,One of the top posts on ,"
"
30894,https://www.reddit.com/r/datasets/comments/b4yy6p/480000_rotten_tomato_critic_reviews/,Reddit/r/Datasets,6th post of all-time on ,"
"
30895,http://indra.readthedocs.io/en/latest/modules/tools/index.html#module-indra.tools.assemble_corpus,indra.tools.assemble_corpus,"The internal assembly steps of INDRA including the ones listed above, and also a large collection of filters (filter by source, belief, the presence of grounding information, semantic filters by entity role, etc.) are exposed in the ", submodule. This submodule contains functions that take Statements as input and produce processed Statements as output. They can be composed to form an assembly pipeline connecting knowledge collected from sources with an output model.
30895,https://indra.readthedocs.io/en/latest/modules/databases/index.html,"
indra.databases
",", and access ontological information and convert between identifiers (e.g., UniProt, HGNC), available in ",. A full list of further INDRA modules is available in the 
30896,https://github.com/hehaodele/CIDA#intra-dataset-results-on-real-world-medical-datasets,domain interpolation,". For example, instead of adapting from domain A to domain B, we would like to simultaneously adapt across infintely many domains in a manifold. This allows us to go beyond domain adaption and perform both ", and 
30896,https://github.com/hehaodele/CIDA#intra-dataset-results-on-real-world-medical-datasets,domain extrapolation, and ,. See the following toy example.
30898,#tfdata-example,TF.Data Example,"
","
"
30903,https://archive.org/download/armancohan-long-summarization-paper-code/arxiv-dataset.zip,mirror, (,) PubMed dataset: 
30903,https://archive.org/download/armancohan-long-summarization-paper-code/pubmed-dataset.zip,mirror, (,)
30903,https://www.tensorflow.org/datasets/catalog/scientific_papers,Tensorflow Datasets,The dataset is also available on , which makes it easy to use within Tensorflow or colab.
30914,https://www.researchgate.net/publication/337376844_Is_your_clock-face_cozie_A_smartwatch_methodology_for_the_in-situ_collection_of_occupant_comfort_data,Is your clock-face cozie? A smartwatch methodology for the in-situ collection of occupant comfort data,"
","
"
30920,https://github.com/fractalego/pynsett/blob/master/pynsett/rules/wikidata.rules,this file,A working example of pynsett's rules is in ,.
30921,https://database.lichess.org/,Lichess Database, from the ,", as this provides virtually limitless positions from human games. To perform artificial dataset expansion (as mentioned in the paper), you may use "
30945,data/drugbank.tsv,tsv file, -- extracts information from the DrugBank xml download into a , where each row represents a drug. A 
30945,data/drugbank-slim.tsv,subset, where each row represents a drug. A ," referred to as slim contains only drugs that are approved, small molecules, and contain an InChI structure ("
30945,data/proteins.tsv,interacting proteins,). We also extract the ," for each drug, which include targets, enzymes, transporters, and carriers ("
30945,data/similarity-slim.tsv.gz,on github,. The subset of similarities for slim compounds is ,.
30945,data/mapping.tsv.gz,bulk download,. The mapping is based on atomic connectivity and ignores differences in small molecular details. Mappings are available in a , or for 
30945,data/mapping,individual resources, or for ,. Summary statistics are also 
30945,data/mapping-counts.tsv,available,. Summary statistics are also , (
30945,data/pubchem-mapping.tsv,tsv file, based on exact InChi string matches. The mapping is available as a ,.
30945,data/drugbank_halflife.tsv,tsv file, -- extracts half-life and other structural information from the Drugbank xml download into a , where each row represents a drug. The half-life information was listed as free text in Drugbank. We manually extract the numeric value from free text into a 
30945,data/drugbank_halflife_curated.xlsx,xlsx file, where each row represents a drug. The half-life information was listed as free text in Drugbank. We manually extract the numeric value from free text into a ,". All values were converted to hours. If the value was listed as time range (e.g. a ~ b) in DrugBank, average was calculated (e.g. (a + b)/2)."
30945,data/drugbank_subset_halflife_curated.tsv,tsv file, -- extracts subset of drugs with curated half-life into a , where each row represents a drug.
30946,https://github.com/thunlp/FewRel/blob/master/data/pid2name.json,pid2name.json,We also provide ," to show the Wikidata PID, name and description for each relation."
30952,https://easyreg-unc-biag.readthedocs.io/en/latest/notes/walk_through_demos.html#demos-on-data-augmentation,Demos on Data augmentation,Related demos can be found at , section.
30967,https://data.world/vmarkovtsev/github-word-2-vec-120-k,large corpus of sub-token sequences,"Firstly, we took a ", and trained their embeddings with 
30974,https://github.com/facebookresearch/MUSE/blob/master/data/get_evaluation.sh,Muse,"For anchor training, you only need to replace words in monolingual data with dictionaries from ", using 
30975,https://github.com/audio-captioning/clotho-dataloader,Clotho data loader repository, in the ,.
30975,#set-up-of-data-and-code,Set up of data and code,"
","
"
30988,https://github.com/coseal/aslib_data,Github,"Obviously, the code requires access to the ASLib scenarios in order to run the requested evaluations. It expects the ASLib scenarios (which can be downloaded from ",) to be located in a folder 
30989,https://crossminds.ai/video/counterfactual-data-augmentation-using-locally-factored-dynamics-5fb82261890833803bc7e7ef/,3 minute presentation,Counterfactual Data Augmentation using Locally Factored Dynamics (NeurIPS 2020 (,", "
30989,https://crossminds.ai/video/counterfactual-data-augmentation-using-locally-factored-dynamics-606f4312072e523d7b7806e6/,5 minute presentation,", ","), "
30991,https://github.com/Davidham3/ASTGCN/tree/master/data,ASTGCN,"data: including PEMSD4 and PEMSD8 dataset used in our experiments, which are released by and available at  ",.
30993,http://cocodataset.org/#download,COCO download,", please download from ",", 2017 Train/Val is needed for COCO detection/instance segmentation/keypoints training and validation. Download and extract them under {ROOT}/data, and make them look like this:"
31002,https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/wikics.html#WikiCS,"
torch_geometric.datasets.WikiCS
",You can load the dataset easily using the , class in PyTorch Geometric. Note that the 
31004,data/CTD_DDA,CTD DDA,"
", : a drug-disease association graph extracted from 
31004,data/NDFRT_DDA,NDFRT DDA,"
", : a drug-disease association graph extracted from 
31004,data/DrugBank_DDI,DrugBank DDi,"
", : a drug-drug interaction graph extracted from 
31004,data/STRING_PPI,STRING PPI,"
", : a protein-protein interaction graph extracted from 
31004,data/Clin_Term_COOC,Clin Term COOC,"
"," : a medical term-term co-occurrence graph from (Finlayson et al., 2014) "
31004,https://datadryad.org//resource/doi:10.5061/dryad.jp917,[source data]," : a medical term-term co-occurrence graph from (Finlayson et al., 2014) ",", "
31004,https://doi.org/10.1038/sdata.2014.32,[paper],", ","
"
31004,data/node2vec_PPI,node2vec PPI,"
",: a PPI graph with functional annotations used in 
31004,data/Mashup_PPI,Mashup PPI,"
",: a experimental PPI graph with functional annotations used in 
31006,https://github.com/albertqjiang/INT/blob/2ec739c94b2feb5f7f80b3d5e71e8b751dbd9ef3/data_generation/generate_problems.py#L268,INT/data_generation/generate_problems.py:268,"
","
"
31019,https://kelvins.esa.int/proba-v-super-resolution/data/,dataset,Give easy access to a unique ,", introduced by ESA in 2019, to work on the very challenging task of multi-frame super-resolution. If you've never heard about this computer vision task, "
31019,https://github.com/EscVM/RAMS/blob/master/preprocessing_dataset.ipynb,jupyter notebook," could help you. Anyway, in a few words, its aim is pretty intuitive and straightforward: reconstruct a high-resolution image from a set of low-resolution frames. So, with a practical and easy to use ", we give you the possibility to preprocess this 
31019,https://kelvins.esa.int/proba-v-super-resolution/data/,dataset, we give you the possibility to preprocess this ," and dive directly into the design of your methodology. It's a very flexible pipeline where all steps can be easily omitted. All data are included with this repository, already split in train validation and testing. At the end of the process you will have three primary tensors: "
31019,https://github.com/EscVM/RAMS/blob/master/preprocessing_dataset.ipynb,pre-processing,Use the , notebook to process the Proba-V 
31019,https://kelvins.esa.int/proba-v-super-resolution/data/,original dataset, notebook to process the Proba-V , and obtain the training/validation/testing arrays ready to be used.
31020,https://www.nytimes.com/2023/03/22/us/covid-data-cdc.html,this story," to use data from the federal government for cases and deaths. This GitHub repo will serve as an archive of the virus data reporting from The Times since 2020. For more information about this change, please see ",.
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv,Raw CSV, (,) | 
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv,Raw CSV, (,) | 
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv,Raw CSV, (,) ]
31020,https://github.com/nytimes/covid-19-data/blob/master/PROBABLE-CASES-NOTE.md,read here, that were developed by states and the federal government. Not all geographies are reporting probable cases and yet others are providing confirmed and probable as a single total. Please , for a full discussion of this issue.
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv,Raw CSV file here., file.  (,)
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv,Raw CSV file here., file. (,)
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-2020.csv,Raw CSV file here., file. (,)
31020,https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-recent.csv,Raw CSV file here., file.  (,)
31022,http://snap.stanford.edu/data/soc-Pokec.html,soc-Pokec,"
","
"
31022,http://snap.stanford.edu/data/com-LiveJournal.html,com-LiveJournal,"
","
"
31022,http://snap.stanford.edu/data/com-Orkut.html,com-Orkut,"
","
"
31022,http://snap.stanford.edu/data/web-NotreDame.html,web-NotreDame,"
","
"
31022,http://snap.stanford.edu/data/web-Stanford.html,web-Stanford,"
","
"
31022,http://snap.stanford.edu/data/web-Google.html,web-Google,"
","
"
31022,http://snap.stanford.edu/data/web-BerkStan.html,web-BerkStan,"
","
"
31032,https://grouplens.org/datasets/movielens/,MovieLens,"
","
"
31068,https://wyqdatabase.s3-us-west-1.amazonaws.com/bedroom.zip,"
bedroom
", | | :----------: |  :-----: | :-------: | :-------: | | , | 
31068,https://wyqdatabase.s3-us-west-1.amazonaws.com/chess.zip,"
chess
", | , | 
31068,https://wyqdatabase.s3-us-west-1.amazonaws.com/robot.zip,"
robot
", | , | 
31068,https://wyqdatabase.s3-us-west-1.amazonaws.com/study.zip,"
study
", | , |
31075,https://luna16.grand-challenge.org/data/,LUNA16,This model was was pre-trained on the ," dataset to segment lung regions. We then reset the output layer of the model and extend it to 5 classes, and then train the model on our train split outlined above. The weights provided come from the best model selected using our validation set, and score an opacity IOU of 0.76 on our test set for groups 3,4, and 5 combined. We combine these groups in order to evalaute the ability of the model to segment the volume of opacification. We evaluate the accuracy of our model to differentiate between the opacification types using a confusion matrix. We see there is often confusion between the different opacity types. An example of this is shown in the figure above, where the confusion between Group 1 and Group 2 leads to poor qualitative results, however a radiologist has confirmed that this segmentation is qualitatively valid due to the fact that the classes are not mutually exclusive. We believe there is still signifigant room for improvement as we collect more data and train more sophisticated models."
31079,#requesting-dataset,Instructions to request our full dataset.,"
","
"
31079,#dataset-contents,Documentation on our dataset structure and contents.,"
","
"
31083,#your-own-data,Your own data,"
", (portrait images or segmaps)
31097,https://github.com/googlecreativelab/quickdraw-dataset,Google QuickDraw dataset,". In particular, these 345 categories are corresponding to the 345 free-hand sketch categories of ",. Please see 
31100,create_datasets.py,create_dataset.py,The 50% dataset has 4GB in size and the 20% dataset has 16.5GB in size. Use the , to create the two datasets used from the original dataset.
31110,https://github.com/zswvivi/icdm_pqa/blob/master/data/ICDM_Annotated_dataset.csv,Annotated_Data,"
","
"
31110,http://cseweb.ucsd.edu/~jmcauley/datasets.html,dataset link,The original dataset are from ,", including Amazon QA and reviews data."
31111,http://cseweb.ucsd.edu/~jmcauley/datasets.html,dataset link,The Amazon PQA dataset (including reviews and QA data) can be downloaded from ,.
31111,https://github.com/zswvivi/ecml_pqa/blob/master/data/Annotated_Data.txt,Annotated_Data,"
","
"
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/reentrancy,Reentrancy,| Vulnerability                                                                                                     | Description                                                                        | Level      | | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ---------- | | ,                               | Reentrant function calls make a contract to behave in an unexpected way            | Solidity   | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/access_control,Access Control,                               | Reentrant function calls make a contract to behave in an unexpected way            | Solidity   | | ,                       | Failure to use function modifiers or use of tx.origin                              | Solidity   | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/arithmetic,Arithmetic,                       | Failure to use function modifiers or use of tx.origin                              | Solidity   | | ,                               | Integer over/underflows                                                            | Solidity   | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/unchecked_low_level_calls,Unchecked Low Level Calls,                               | Integer over/underflows                                                            | Solidity   | | ," | call(), callcode(), delegatecall() or send() fails and it is not checked           | Solidity   | | "
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/denial_of_service,Denial Of Service," | call(), callcode(), delegatecall() or send() fails and it is not checked           | Solidity   | | ",                 | The contract is overwhelmed with time-consuming computations                       | Solidity   | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/bad_randomness,Bad Randomness,                 | The contract is overwhelmed with time-consuming computations                       | Solidity   | | ,                       | Malicious miner biases the outcome                                                 | Blockchain | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/front_running,Front Running,                       | Malicious miner biases the outcome                                                 | Blockchain | | ,                         | Two dependent transactions that invoke the same contract are included in one block | Blockchain | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/time_manipulation,Time Manipulation,                         | Two dependent transactions that invoke the same contract are included in one block | Blockchain | | ,                 | The timestamp of the block is manipulated by the miner                             | Blockchain | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/short_addresses,Short Addresses,                 | The timestamp of the block is manipulated by the miner                             | Blockchain | | ,                     | EVM itself accepts incorrectly padded arguments                                    | EVM        | | 
31113,https://github.com/smartbugs/smartbugs/blob/master/dataset/other,Unknown Unknowns,                     | EVM itself accepts incorrectly padded arguments                                    | EVM        | | ,                              | Vulnerabilities not identified in DASP 10                                          | N.A        |
31118,https://github.com/google-research/sound-separation/tree/master/datasets/fuss,fuss_scripts, baseline_model and ,)
31118,https://github.com/google-research/sound-separation/tree/master/datasets/fuss,fuss_scripts, baseline_model and ,)
31118,https://github.com/google-research/sound-separation/tree/master/datasets/fuss,fuss_scripts, baseline_model and ,)
31118,#scripts-to-generate-the-dataset,scripts,Note: the reverberated data (see ,) are not computed for the baseline
31119,https://github.com/google-research/sound-separation/blob/master/datasets/fuss/FUSS_license_doc/README.md,Free Universal Sound Separation (FUSS),"
",.
31119,https://github.com/google-research/sound-separation/blob/master/datasets/yfcc100m/README.md,Audio from YFCC100M videos for mixture-invariant training (MixIT),"
",.
31119,https://github.com/google-research/sound-separation/blob/master/datasets/audioscope/README.md,Audio-visual YFCC100M with annotations for on-screen sound separation with AudioScope,"
",.
31119,https://github.com/google-research/sound-separation/blob/master/datasets/audioscope-v2/README.md,Audio-visual YFCC100M with annotations for on-screen sound separation with AudioScopeV2,"
",.
31119,https://github.com/google-research/sound-separation/blob/master/datasets/synthetic_ami/README.md,Synthetic AMI for speech separation in meeting room scenarios,"
",.
31121,https://iohprofiler.github.io/IOHanalyzer/data/,this page,. The supported data format is specified in ,. Please follow the instruction there to convert your data sets.
31128,https://github.com/davidsandberg/facenet/blob/master/data/pairs.txt,here, can be seen in ,.
31131,#1-our-dataset-contribution,1. Our dataset contribution,"
","
"
31131,#the-dataset-splits,The dataset splits,"
","
"
31131,#2-dataset-downloading-and-license,2. Dataset downloading and license,"
","
"
31131,#dataset-statistics,Dataset statistics,"
","
"
31131,#prepare-evaluation-data,Prepare evaluation data,"
","
"
31131,#prepare-traineval-datasets,Prepare train+eval datasets,"
","
"
31131,#prepare-evaluation-data,Evaluation only,"
","
"
31131,#prepare-traineval-datasets,Training + evaluation,"
","
"
31131,metadata/OpenImages/,here,. Corresponding metadata can be found in ,. The annotations are licensed by Google LLC under 
31131,dataset,dataset, images at ,. Metadata and box annotations already exist in this repository under 
31131,metadata,metadata,. Metadata and box annotations already exist in this repository under ,". OpenImages mask annotations are also downloaded by the above script, and will be saved under "
31131,dataset,dataset,". OpenImages mask annotations are also downloaded by the above script, and will be saved under ", with the images.
31131,metadata/ILSVRC,here,Corresponding annotation files can be found in ,.
31131,metadata/CUB,here,Corresponding annotation files can be found in ,.
31131,metadata/OpenImages,here,Corresponding annotation files can be found in ,.
31140,http://groups.csail.mit.edu/vision/datasets/ADE20K/,ADK20K, |    11355     |      2857      |      ✘      | | , |    20210     |      2000      |      ✘      | | 
31140,https://www.cityscapes-dataset.com/downloads/,Cityscapes, |    20210     |      2000      |      ✘      | | ,  |     2975     |      500       |      ✘      | | 
31140,http://cocodataset.org/#download,COCO,  |     2975     |      500       |      ✘      | | ,           |              |                |             | | 
31140,http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip,SBU-shadow,           |              |                |             | | , |     4085     |      638       |      ✘      | | 
31143,https://github.com/slryou41/reaction-gcnn/blob/master/data/data_processing_example.ipynb,instruction,Download the dataset by following the , from Reaxys®
31158,https://pandas.pydata.org/,Pandas,"
","
"
31166,https://github.com/audio-captioning/clotho-dataloader,here,. Code for dataset pre-processing/feature extraction for Clotho dataset can also be found ,". Finally, code for handling the Clotho data (i.e. extracted features and one-hot encoded words) for PyTorch library (i.e. PyTorch DataLoader for Clotho data) can also be found "
31166,https://github.com/audio-captioning/clotho-dataloader,here,". Finally, code for handling the Clotho data (i.e. extracted features and one-hot encoded words) for PyTorch library (i.e. PyTorch DataLoader for Clotho data) can also be found ",.
31166,#preparing-the-data,Preparing the data,"
","
"
31166,#getting-the-data-from-zenodo,Getting the data from Zenodo,"
","
"
31166,#setting-up-the-data,Settings up the data,"
","
"
31166,#create-the-dataset,Create the dataset,"
","
"
31166,#settings-for-the-creation-of-the-dataset,Settings for the creation of the dataset,"
","
"
31166,#create-the-dataset,create the dataset,"The above commands will start the process of optimizing the baseline DNN, using the data that were created in the ", section.
31166,#setting-up-the-data,setting up the data,) directory mentioned in section ,.
31187,Documentation/lessons/core_concepts/output_data.md,Output data,"
","
"
31187,Documentation/lessons/read_write/output_data_writer.md,The OutputDataWriter add-on,"
","
"
31187,https://github.com/alters-mit/tdw_image_dataset,tdw_image_dataset,High-level API: ,"
"
31187,Documentation/lessons/flex/output_data.md,"
FlexParticles output data","
","
"
31187,Documentation/lessons/replicants/output_data.md,Output data,"
","
"
31187,Documentation/api/output_data.md,Output Data,"
","
"
31187,Documentation/python/add_ons/output_data_writer.md,OutputDataWriter,"
","
"
31187,Documentation/python/collision_data/collision_base.md,CollisionBase,"
","
"
31187,Documentation/python/collision_data/collision_obj_env.md,CollisionObjEnv,"
","
"
31187,Documentation/python/collision_data/collision_obj_obj.md,CollisionObjObj,"
","
"
31187,Documentation/python/collision_data/trigger_collider_shape.md,TriggerColliderShape,"
","
"
31187,Documentation/python/collision_data/trigger_collision_event.md,TriggerCollisionEvent,"
","
"
31187,Documentation/python/container_data/box_container.md,BoxContainer,"
","
"
31187,Documentation/python/container_data/container_shape.md,ContainerShape,"
","
"
31187,Documentation/python/container_data/container_tag.md,ContainerTag,"
","
"
31187,Documentation/python/container_data/containment_event.md,ContainmentEvent,"
","
"
31187,Documentation/python/container_data/cylinder_container.md,CylinderContainer,"
","
"
31187,Documentation/python/container_data/sphere_container.md,SphereContainer,"
","
"
31187,Documentation/python/flex_data/fluid_type.md,FluidType,"
","
"
31187,Documentation/python/lisdf_data/lisdf_robot_metadata.md,LisdfRobotMetadata,"
","
"
31187,Documentation/python/obi_data/force_mode.md,ForceMode,"
","
"
31187,Documentation/python/obi_data/obi_actor.md,ObiActor,"
","
"
31187,Documentation/python/obi_data/cloth/cloth_material.md,ClothMaterial,"
","
"
31187,Documentation/python/obi_data/cloth/sheet_type.md,SheetType,"
","
"
31187,Documentation/python/obi_data/cloth/tether_particle_group.md,TetherParticleGroup,"
","
"
31187,Documentation/python/obi_data/cloth/tether_type.md,TetherType,"
","
"
31187,Documentation/python/obi_data/cloth/volume_type.md,VolumeType,"
","
"
31187,Documentation/python/obi_data/collision_materials/collision_material.md,CollisionMaterial,"
","
"
31187,Documentation/python/obi_data/collision_materials/material_combine_mode.md,MaterialCombineMode,"
","
"
31187,Documentation/python/obi_data/fluids/cube_emitter.md,CubeEmitter,"
","
"
31187,Documentation/python/obi_data/fluids/disk_emitter.md,DiskEmitter,"
","
"
31187,Documentation/python/obi_data/fluids/edge_emitter.md,EdgeEmitter,"
","
"
31187,Documentation/python/obi_data/fluids/emitter_sampling_method.md,EmitterSamplingMethod,"
","
"
31187,Documentation/python/obi_data/fluids/emitter_shape.md,EmitterShape,"
","
"
31187,Documentation/python/obi_data/fluids/fluid.md,Fluid,"
","
"
31187,Documentation/python/obi_data/fluids/fluid_base.md,FluidBase,"
","
"
31187,Documentation/python/obi_data/fluids/granular_fluid.md,GranularFluid,"
","
"
31187,Documentation/python/obi_data/fluids/sphere_emitter.md,SphereEmitter,"
","
"
31187,Documentation/python/object_data/bound.md,Bound,"
","
"
31187,Documentation/python/object_data/object_static.md,ObjectStatic,"
","
"
31187,Documentation/python/object_data/rigidbody.md,Rigidbody,"
","
"
31187,Documentation/python/object_data/transform.md,Transform,"
","
"
31187,Documentation/python/object_data/composite_object/composite_object_dynamic.md,CompositeObjectDynamic,"
","
"
31187,Documentation/python/object_data/composite_object/composite_object_static.md,CompositeObjectStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/hinge_dynamic.md,HingeDynamic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/hinge_static.md,HingeStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/hinge_static_base.md,HingeStaticBase,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/light_dynamic.md,LightDynamic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/light_static.md,LightStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/motor_static.md,MotorStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/non_machine_static.md,NonMachineStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/prismatic_joint_static.md,PrismaticJointStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/spring_static.md,SpringStatic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/sub_object_dynamic.md,SubObjectDynamic,"
","
"
31187,Documentation/python/object_data/composite_object/sub_object/sub_object_static.md,SubObjectStatic,"
","
"
31187,Documentation/python/robot_data/drive.md,Drive,"
","
"
31187,Documentation/python/robot_data/joint_dynamic.md,JointDynamic,"
","
"
31187,Documentation/python/robot_data/joint_static.md,JointStatic,"
","
"
31187,Documentation/python/robot_data/joint_type.md,JointType,"
","
"
31187,Documentation/python/robot_data/non_moving.md,NonMoving,"
","
"
31187,Documentation/python/robot_data/robot_dynamic.md,RobotDynamic,"
","
"
31187,Documentation/python/robot_data/robot_static.md,RobotStatic,"
","
"
31187,Documentation/python/scene_data/interior_region.md,InteriorRegion,"
","
"
31187,Documentation/python/scene_data/region_bounds.md,RegionBounds,"
","
"
31187,Documentation/python/scene_data/room.md,Room,"
","
"
31187,Documentation/python/scene_data/scene_bounds.md,SceneBounds,"
","
"
31187,Documentation/python/vr_data/oculus_touch_button.md,OculusTouchButton,"
","
"
31187,Documentation/python/vr_data/rig_type.md,RigType,"
","
"
31187,Documentation/benchmark/object_data.md,Object data,"
","
"
31213,https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html,the official CDC COVID-19 Forecasting page,", which is the data source for ",.
31213,https://github.com/reichlab/covid19-forecast-hub/blob/master/data-processed/README.md,technical README with detailed submission instructions,"If you are a modeling team interested in submitting to the Hub, please visit our ",.
31213,https://github.com/reichlab/covid19-forecast-hub/tree/master/data-processed,forecasts, to access the data through an API. Participating teams provide their , in a 
31213,https://github.com/reichlab/covid19-forecast-hub/blob/master/data-processed/README.md#Data-formatting,quantile-based format, in a ,. Please also follow the data license and citation guidelines below.
31213,./data-processed/,data-processed,We are grateful to the teams who have generated these and made their data publicly available under different terms and licenses. You will find the licenses (when provided) within the model-specific folders in the , directory. Please consult these licenses before using these data to ensure that you follow the terms under which these data were released.
31218,https://github.com/FnTm/latvian-tweet-sentiment-corpus,latvian-tweet-sentiment-corpus,"
","
"
31222,http://www.cs.columbia.edu/CAVE/databases/multispectral,CAVE dataset,) of the , (Using 
31235,doc/datasets/datasets.md,here," command to see the available datasets, or check ",.
31236,https://huggingface.co/datasets,HuggingFace Datasets Hub," major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the ",. With a simple command like 
31236,https://huggingface.co/docs/datasets/,"🎓 Documentation
","
","
"
31236,https://colab.research.google.com/github/huggingface/datasets/blob/main/notebooks/Overview.ipynb,"🕹 Colab tutorial
","
","
"
31236,https://huggingface.co/datasets,"🔎 Find a dataset in the Hub
","
","
"
31236,https://huggingface.co/docs/datasets/share.html,"🌟 Add a new dataset to the Hub
","
","
"
31236,https://github.com/tensorflow/datasets,TensorFlow Datasets,🤗 Datasets originated from a fork of the awesome , and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library. More details on the differences between 🤗 Datasets and 
31236,#main-differences-between--datasets-and-tfds,"Main differences between 🤗 Datasets and tfds
", can be found in the section ,.
31236,https://colab.research.google.com/github/huggingface/datasets/blob/main/notebooks/Overview.ipynb,"
Open In Colab
",Another introduction to 🤗 Datasets is the tutorial on Google Colab here: ,"
"
31236,https://huggingface.co/datasets,HuggingFace Datasets Hub, datasets already provided on the ,.
31236,https://huggingface.co/docs/datasets/upload_dataset,how to upload a dataset to the Hub using your web browser or Python,"
", and also
31236,https://huggingface.co/docs/datasets/share,how to upload it using Git,"
",.
31242,https://github.com/ryan112358/nist-synthetic-data-2021,Adaptive Grid,"
", - Second place solution to the 2020 NIST synthetic data challenge.
31245,https://github.com/piergiaj/AViD/tree/master/dataset,dataset,"The annotations are provided in this repository, in the ", directory. This contains the 
31245,https://github.com/piergiaj/AViD/blob/master/dataset/avid_full.json,full dataset, directory. This contains the , with labels and weak tags as well as classification-only 
31245,https://github.com/piergiaj/AViD/blob/master/dataset/avid_train.json,train, with labels and weak tags as well as classification-only , and 
31245,https://github.com/piergiaj/AViD/blob/master/dataset/avid_val.json,validation, and , sets.
31246,data/README.md,data/README.md,Check the instructions detailed in ,"
"
31251,https://sparse.pydata.org/en/stable/install.html,here," is the PyData Sparse library, documented ","
"
31254,https://github.com/doonny/PipeCNN/blob/master/project/data/README.md,here," files, as described ",.
31256,./docs/en/developer/datasets.md,Dataset Guide,", ",", "
31259,https://github.com/griffbr/ODMD/tree/main/data/odms_detection,now available here, An object detection-based version of the ODMS benchmark is ,!
31272,https://github.com/CodeandoMexico/terremoto-cdmx/issues?utf8=%E2%9C%93&q=is%3Aissue%20label%3Adata%20,Data," | Tareas de comunicación, monitoreo de redes y vinculación con otras organizaciones. | | ", | 
31272,https://github.com/dataquito/juridica19s,:octocat:, | Node | , |
31280,#2-dataset,2. Dataset,"
","
"
31280,data/TCGA@Focus.txt,"
data/TCGA@Focus.txt
", dataset proposed and used in the paper. The specific testing images (14371 images) can be found in ,"
"
31280,data/FocusPath_full_split1.txt,"
data/FocusPath_full_split1.txt
", dataset used in the paper. The specific training images (5200 images) in one of the ten folds can be found in ,"
"
31284,https://www.eecs.yorku.ca/~kamel/sidd/dataset.php,SIDD Medium Dataset,The DANet model was trained on ,", and tested on SIDD "
31285,#limiting-requestresponse-data,Limiting request/response data,"
","
"
31285,#clearing-logged-data,Clearing logged data,"
","
"
31297,https://figshare.com/articles/PHEME_dataset_of_rumours_and_non-rumours/4010619,figshare,In the 'dataset/PHEME_data' folder we provide the pre-processed data files used for our experiments. The raw data can be downloaded at ,.
31306,https://www.robots.ox.ac.uk/~vgg/data/vgg_face/,Oxford's VGGFace Dataset,Each row / column is one of 2622 classes (celebrities) from ,". Male vs. female celebrities define the major two blocks in the matrix. Prominent subgroups in these blocks are based on wrinkles and hair / skin color. Chromaticity defines two of these sub-groups (split marked in red), and indicates that the CNN uses unreliable features to recognize the corresponding celebrities."
31309,http://cocodataset.org/#download,official COCO dataset website,Please follow the , to download the dataset. After downloading the dataset you should have the following directory structure:
31324,https://cocodataset.org,MS COCO 2017, on the , benchmark.
31339,https://github.com/googlecreativelab/quickdraw-dataset,"Quick, Draw! Dataset", trained on ,. Each time any of categories from the dataset appears on the page I generate and add random picture somewhere arround.
31342,https://dataguide.nlm.nih.gov/edirect/install.html,EDirect, or ,", the commandline tools requesting the PubMed infrastructure"
31342,https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/10026,WikiWarsDe Corpus,"
","
"
31346,#a--data-preparation-,A) Data preparation:,"
","
"
31346,#1--retrieve-the-data-,1) Retrieve the data:,"
","
"
31354,https://github.com/lpsmlgeobr/Landslide_segmentation_with_unet/blob/master/notebooks/5_data_augmentation.ipynb,Data Augmentation,"
","
"
31374,./data/Matlab/generate_structure_images.m,data/Matlab/generate_structre_images.m,.Run generation function ," in your matlab. For example, if you want to generate smooth images for Places2, you can run the following code:"
31375,https://github.com/buds-lab/building-data-genome-project-2,Building Data Genome 2 project,The data set from the competition is now opened as the , that is outlined in a publication in 
31377,/notebooks/00_All-meters-dataset.ipynb,this,To join all meters raw data into one dataset follow , notebook
31377,notebooks/01_EDA-metadata.ipynb,Exploratory Data Analysis of metadata,"
","
"
31377,https://github.com/buds-lab/building-data-genome-project-2/wiki,repository's wiki,The detailed documentation of how this data set was created can be found in the , and in the following publication:
31377,https://www.researchgate.net/publication/341895125_The_Building_Data_Genome_Project_2_Hourly_energy_meter_data_from_the_ASHRAE_Great_Energy_Predictor_III_competition,ResearchGate,"
","
"
31383,https://download.visinf.tu-darmstadt.de/data/from_games/,GTA5 dataset,Download ,.
31383,https://www.cityscapes-dataset.com/file-handling/?packageID=3,leftImg8bit_trainvaltest.zip,Download the , and 
31383,https://www.cityscapes-dataset.com/file-handling/?packageID=1,gtFine_trainvaltest.zip, and , from the Cityscapes.
31383,tools/datasets/cityscapes/,file of image list,Put the , into where you save the dataset.
31384,http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data,GISCO EU website,Data is taken from the ,.
31384,http://opendatacommons.org/licenses/pddl/1.0/,Public Domain Dedication and License (PDDL),This Data Package is licensed by its maintainers under the ,.
31384,http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units,Copyright notice,Refer to the , of the source dataset for any specific restrictions on using these data in a public or commercial product.
31388,data/citations_separated.parquet,citations_separated,"
",: Dataset containing all citations from Wikipedia with each of the column keys separated and compress in parquet format 
31388,data/citations_ids.csv,citations_ids,"
"," Subset of the above dataset but containing all citation which have a valid identifier such as DOI, ISBN, PMC, PMID or ArXIV."
31388,data/top300_templates.csv,top300_templates,"
", A CSV file which contains the TOP 300 csv templates as calculated by DLAB-EPFL.
31397,https://www.cityscapes-dataset.com/downloads/,Cityscapes,Download Cityscapes dataset from ,.
31399,#multiviewx-dataset,MultiviewX dataset,"
","
"
31399,#data-preparation,Data Preparation,"
","
"
31399,#multiviewx-dataset,MultiviewX,. We use , and 
31399,https://www.epfl.ch/labs/cvlab/data/data-wildtrack/,Wildtrack, and , in this project.
31404,https://icb-anndata.readthedocs-hosted.com/en/stable/anndata.AnnData.html,anndata,"
","
"
31404,http://snap.stanford.edu/comet/data/tabula-muris-comet,http://snap.stanford.edu/comet/data/tabula-muris-comet.zip,"If you would like to test your algorithm on the new benchmark dataset introduced in our work, you can download the data as described above or directly at ",.
31404,https://github.com/snap-stanford/comet/blob/master/TM/data/preprocess.py,preprocess.py,Dataset needs to be preprocessed using ,. Train/test/validation splits are available in 
31404,https://github.com/snap-stanford/comet/blob/master/TM/data/dataset.py,load_tabula_muris,. Train/test/validation splits are available in ,.
31404,https://icb-anndata.readthedocs-hosted.com/en/stable/anndata.AnnData.html,anndata,Running this code requires , and 
31412,#data-download,Data Download,"
","
"
31412,https://data.vision.ee.ethz.ch/cvl/DIV2K,DIV2K,"
","
"
31417,datasets/README.md,Download Datasets,"
","
"
31426,https://github.com/erictzeng/mldata,Here,"For digits dataset, the code is modified from ",". For real-world dataset, the code is based on "
31428,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,NYU Depth V2,First download , on the official website and unzip the raw data to DATA_PATH.
31432,#datasets,Datasets,"
","
"
31432,#datasets,next section, column. The , explains data format in detail.
31432,dataset/,this page,"A list of supported datasets, data preprocessing scripts and details are documented on ",.
31442,https://www.cityscapes-dataset.com/,Link,Download the dataset from the Cityscapes dataset server(,"). Download the files named 'gtFine_trainvaltest.zip', 'leftImg8bit_trainvaltest.zip' and extract in ../data/CityScapes/"
31445,https://pyterrier.readthedocs.io/en/latest/datamodel.html,PyTerrier data model," show other common use cases. For more information, see the ",.
31445,https://pyterrier.readthedocs.io/en/latest/datasets.html,dataset API,PyTerrier allows simple access to standard information retrieval test collections through its ,", which can download the topics, qrels, corpus or, for some test collections, a ready-made Terrier index."
31445,https://github.com/allenai/ir_datasets,ir_datasets package,All datasets from the , are available under the 
31445,https://ir-datasets.com/all.html,here, to get the TREC Genomics 2004 dataset. A full catalogue of ir_datasets is available ,.
31446,#data-format,Data Format,"
","
"
31446,data,data,Main folder: ,"
"
31446,data/v2.zip,v2,Subfolder: ,"
"
31446,data/v2.zip,verified_claims.docs.tsv,"
","
"
31446,data/v2.zip,/train,Subfolder ,"
"
31446,data/v2.zip,tweets,"
","
"
31446,data/v2.zip,tweet-vclaim-pairs.qrels,"
","
"
31446,data/v2.zip,/dev,Subfolder ,"
"
31446,data/v2.zip,/train, Contains dev data released with the version 2.0. Has the same structure as ,.
31446,data/v1.zip,tweets,"
","
"
31446,data/v2.zip,tweet-vclaim-pairs.qrels,"
","
"
31446,data/v3.zip,v3,Subfolder: ,"
"
31446,data/v3.zip,v2, Similar structure to subfolder ,"
"
31447,#data-format,Data Format,"
","
"
31447,#input-dataset,Input Dataset,"
","
"
31447,#data-annotation-process,Data Annotation Process,"
","
"
31447,data,data,Main folder: ,"
"
31447,data/v1.zip,v1,Subfolder: ,"
"
31447,data/v1.zip,training.tsv,"
","
"
31447,data/v1.zip,dev.tsv,"
","
"
31447,data/v2.zip,v2,Subfolder: ,"
"
31447,data/v2.zip,training_v2.tsv,"
","
"
31447,data/v2.zip,dev_v2.tsv,"
","
"
31447,data/v2.zip,training_v2.json,"
","
"
31447,https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object,twitter object, Contains the , for the tweets included in the v2 training dataset
31447,data/v2.zip,dev_v2.json,"
","
"
31447,data/training.tsv,training.tsv,Both baselines will be trained on the training tweets from , and the performace of the model was was evaluated on the dev tweets from 
31447,data/dev.tsv,dev.tsv, and the performace of the model was was evaluated on the dev tweets from , The performance of both baselines will be displayed:
31448,#data-format,Data Format,"
","
"
31448,data,data,Main folder: ,"
"
31448,data/v1.zip,v1,Subfolder: ,"
"
31448,data/v1.zip,/training,Subfolder ,"
"
31448,data/v2.zip,v2,Subfolder: ,"
"
31448,data/v2.zip,/training,Subfolder ,"
"
31451,https://dafny-lang.github.io/dafny/DafnyRef/DafnyRef#181-inductive-datatypes,inductive datatypes,", "," that can have methods and are suitable for pattern matching, "
31451,https://dafny-lang.github.io/dafny/DafnyRef/DafnyRef#182-co-inductive-datatypes,lazily unbounded datatypes," that can have methods and are suitable for pattern matching, ",", "
31457,https://pandas.pydata.org/,pandas,"
", (tested with version 0.24.2)
31458,https://www.ibm.com/products/marketscan-research-databases,MarketScan claims data,The real world patient data used in this paper is ,. Interested parties may contact IBM for acquiring the data access at this 
31458,https://www.ibm.com/products/marketscan-research-databases,link,. Interested parties may contact IBM for acquiring the data access at this ,.
31458,data/,data,The demo of the input data can be found in the ," folder, where the data structures and a "
31458,data/synthetic,synthetic demo," folder, where the data structures and a "," of the inputs are provided. Before running the preprocessing codes, make sure the input data format is same to the provided input demo."
31458,data/synthetic/Cohort.csv,Cohort,"
","
"
31458,data/synthetic/Cohort.csv,cohort table,The data structure for ," is as follows,"
31458,data/synthetic/drug,Drug table,"
","
"
31458,data/synthetic/drug/drug12.csv,drug table,The data structure for the ," is as follows,"
31458,https://www.fda.gov/drugs/drug-approvals-and-databases/national-drug-code-directory,NDC,| Column Name | Description                                                                  | Note                                                                                                                    | |-------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------| | ENROLID     | Patient enroll ID                                                            | Unique identifier for each patient                                                                                      | | NDCNUM      | National drug code (,)                                                     | We map NDC to observational medical
31458,data/synthetic/inpatient,Inpatient table,"
","
"
31458,data/synthetic/inpatient/inpat12.csv,inpatient table,The data structure for the ," is as follows,"
31458,http://www.icd9data.com/2015/Volume1/default.htm,ICD-9,") codes                                | 57,089 ICD-9/10 codes considered in the dataset. Dictionary for ", and 
31458,https://www.icd10data.com/,ICD-10, and ," codes.| | DXVER        | Flag to denote ICD-9/10 codes                     | “9” = ICD-9-CM and “0” = ICD-10-CM                                                                                                                              |                                                                                                                                                            | | ADMDATE      | Admission date for this inpatient visit           | M/D/Y, e.g., 03/25/2732                                                                                                                                         | | Days         | The number of days stay in the inpatient hospital | Day, e.g., 28                                                                                                                                                   |"
31458,data/synthetic/outpatient,Outpatient table,"
","
"
31458,data/synthetic/outpatient/outpat12.csv,outpatient table,The data structure for the ," is as follows,"
31458,http://www.icd9data.com/2015/Volume1/default.htm,ICD-9,") codes                                | 57,089 ICD-9/10 codes considered in the dataset. Dictionary for ", and 
31458,https://www.icd10data.com/,ICD-10, and ," codes.| | DXVER        | Flag to denote ICD-9/10 codes                     | “9” = ICD-9-CM and “0” = ICD-10-CM                                                                                                                              |                                                                                                                                                             | | SVCDATE      | Service date for this outpatient visit           | M/D/Y, e.g., 03/25/2732                                                                                                                                         |                                                                                                                                                |"
31458,data/synthetic/demo.csv,Demographics,"
","
"
31458,data/synthetic/demo.csv,demo table,The data structure for ," is as follows,"
31468,https://www.robots.ox.ac.uk/~vgg/data/pets/,Oxford Pets,", ",", "
31468,https://www.robots.ox.ac.uk/~vgg/data/flowers/,Oxford flowers,", ",", "
31468,http://data.csail.mit.edu/places/places365/val_256.tar,Places-365,", ",", "
31468,https://www.robots.ox.ac.uk/~vgg/data/dtd/,DTD,", ","
"
31481,https://dm.kaist.ac.kr/datasets/animal-10n,link," | | ANIMAL-10N (noisy) | 50,000           | 5,000            | 10       |    64x64   | ", |
31494,https://vipl.ict.ac.cn/view_database.php?id=15,this link,"For the VIPL-HR database, please refer to ",. An extended version of the VIPl-HR database (VIPL-HR-V2) can be accessed using 
31494,https://vipl.ict.ac.cn/view_database.php?id=17,this link.,. An extended version of the VIPl-HR database (VIPL-HR-V2) can be accessed using ,". For the OBF database, please contact "
31502,#synthetic-dataset,Comparisons of Reconstruction on the synthetic dataset,"
","
"
31502,#real-dataset,Comparisons of Reconstruction on the real dataset,"
","
"
31503,data_processing/README.md,data processing,Please see the directory , for instructions on downloading and using data.
31505,https://pytorch.org/docs/stable/nn.html#dataparallel,nn.DataParallel, faster than the multi-thread parallel method(,"), we use the multi-processing parallel method."
31506,https://bdd-data.berkeley.edu/wad-2018.html,WAD Drivable Area Segmentation Challenge 2018 @CVPR18, and ,. Sample experimented datasets are 
31506,https://www.cityscapes-dataset.com,Cityscapes, and ,.
31506,https://pytorch.org/docs/stable/nn.html#dataparallel,nn.DataParallel,", both multithreading training (",) and multiprocessing training (
31515,http://snap.stanford.edu/data/web-Amazon.html,SNAP,The original datasets can be found at , and 
31515,https://www.datafountain.cn/competitions/350,News, and ,.
31518,best_params_per_dataset_per_arch.md,here,2- Run the following script (best parameters for each dataset and architecture can be found ,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/fgvc-aircraft-2013b.tar.gz?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,aircraft (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/birdsnap.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,birds (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/caltech101.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,caltech101 (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/caltech256.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,caltech256 (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/dtd.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,dtd (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/flowers.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,flowers (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/food.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,food (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/pets.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,pets (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/stanford_cars.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,stanford_cars (,)
31518,"https://robustnessws4285631339.blob.core.windows.net/public-datasets/SUN397.tar?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D",Download,SUN397 (,)
31522,http://www.robots.ox.ac.uk/~vgg/data/voxceleb/,Voxceleb,", ",", "
31522,http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html,Lrs3,", ", dataset.
31538,https://github.com/Ha0Tang/SelectionGAN/tree/master/person_transfer#data-preperation,SelectionGAN,Please follow , to directly download both Market-1501 and DeepFashion datasets.
31538,https://github.com/Ha0Tang/SelectionGAN/tree/master/person_transfer#data-preperation,SelectionGAN,This repository uses the same dataset format as , and 
31550,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI 3D object detection,Please download the official , dataset and organize the downloaded files as follows:
31575,https://download.visinf.tu-darmstadt.de/data/from_games/,The GTA5 Dataset,Download ,"
"
31575,http://synthia-dataset.net/download/808/,The SYNTHIA Dataset,Download ,"
"
31575,https://www.cityscapes-dataset.com/,The Cityscapes Dataset,Download ,"
"
31575,https://github.com/JDAI-CV/FADA/blob/98336a61f0fde633c6d504972fd782688fb8bd3a/core/datasets/dataset_path_catalog.py#L25,here,"We provide the training script using 4 Tesla P40 GPUs. Note that when generating pseudo labels for self distillation, the link to the pseudo label directory should be updated ",.
31578,https://github.com/firehose-dataset/congrad/tree/master/data/Firehose10M,Firehose10M,"To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, ", and 
31578,https://github.com/firehose-dataset/congrad/tree/master/data/Firehose100M,Firehose100M, and ,", comprise "
31578,https://github.com/hexiang-hu/congrad/blob/master/data/README.md,data/README.md," as CL method, and with OnlineGD and ConGraD as optimizers. Please refer to ", for details in preparing the data and downloading the pre-trained user-agnostic language model.
31585,https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/data/README.md#coco-minival-annotations,Detectron, sets from , (
31601,https://github.com/zhou13/lcnn#downloading-the-processed-dataset,LCNN,** Recommend** You can also download the pre-processed dataset directly from ,. Details are as follows:
31606,datasets/nyt/,New York Times annotated corpus,"We provide two example datasets, the ", and the 
31606,datasets/arxiv/,arXiv abstract corpus, and the ,", which are used in the paper. We also provide a shell script "
31611,https://github.com/LuoweiZhou/VLP#-data-preparation,link,Prepare MSCOCO data follow ,.
31613,https://travis-ci.org/cylondata/cylon,"
Build Status
","
","
"
31613,https://github.com/cylondata/cylon/blob/master/LICENSE,"
License
","
","
"
31613,https://cylondata.org,https://cylondata.org,The documentation can be found at ,"
"
31613,mailto:cylondata@googlegroups.com,cylondata@googlegroups.com,Email - ,"
"
31613,https://groups.google.com/forum/#!forum/cylondata/join,Join,Mailing List - ,"
"
31613,https://cylondata.org/docs/,Compiling on Linux,"
","
"
31618,http://vis.computer.org/vis2004contest/data.html,Hurricane ISABEL,                         | 2D   | images from the LCLS instrument                              | | ,    | 3D   | weather simulation                                           | | 
31618,script/sh.download-sdrb-data,"
script/sh.download-sdrb-data
"," by executing the script there. To download more SDRBench datasets, please use ",.
31623,https://cocodataset.org/,MS COCO,The long-tail multi-label datasets we use in the paper are created from , 2017 and 
31648,http://data.airc.aist.go.jp/HS-SOD/HS-SOD.zip,here,The dataset can be downloaded from , (5.6GB). Or type the following in the terminal.
31658,https://github.com/hpatches/hpatches-dataset,HPatches dataset repository, and HPSequences dataset published along with it (,). Thanks to the authors for providing the dataset and the evaluation details.
31664,https://pandas.pydata.org/,Pandas,"
", (v1.0.1 or later)
31665,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,official website,Note that the original full NYUv2 dataset is available at the ,.
31665,https://github.com/XinJCheng/CSPN/tree/master/cspn_pytorch/datalist,CSPN repository,Note that data lists for NYUv2 are borrowed from the ,.
31665,http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion,KITTI DC Website,KITTI DC dataset is available at the ,.
31665,http://www.cvlibs.net/datasets/kitti/raw_data.php,KITTI Raw Website,"For color images, KITTI Raw dataset is also needed, which is available at the ",.
31680,https://competitions.codalab.org/competitions/19544#participate-get-data,here, The dataset can be downloaded from ,". In particular, we follow the standard train/validation/test split (3,471/474/508). The dataset should be arranged in the same directory structure as"
31687,#dataset-preparation,Dataset Preparation,"
","
"
31690,http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/,FGVC Aircraft,"* |  | 5,000 | 8,000 | 10 | | ","* | fine-grained | 6,667 | 3,333 | 100 | | "
31690,https://www.robots.ox.ac.uk/~vgg/data/dtd/,DTD,"* | fine-grained | 6,667 | 3,333 | 100 | | "," |  | 3,760 | 1,880 | 47 | | "
31690,https://www.robots.ox.ac.uk/~vgg/data/pets/,Oxford-IIIT Pets," |  | 3,760 | 1,880 | 47 | | "," |  | 3,680 | 3,369 | 37 | | "
31690,https://www.robots.ox.ac.uk/~vgg/data/flowers/102/,Oxford Flowers102," |  | 3,680 | 3,369 | 37 | | "," |  | 2,040 | 6,149 | 102 |"
31690,https://github.com/mikelzc1990/nsganetv2/blob/master/data/i7-8700K_lut.yaml,this," for your own device, like ",.
31699,#drkg-dataset,DRKG dataset,"We present an example of using pretrained DRKG model for drug repurposing for COVID-19. In the example, we directly use the pretrained model provided at ", and proposed 100 drugs for COVID-19. The following notebook provides the details:
31726,#data-manipulation,Data Manipulation,"
","
"
31726,#database-management,Database Management,"
","
"
31726,#data-packages,Data Packages,"
","
"
31726,https://github.com/Rdatatable/data.table,"data.table <img class=""emoji"" alt=""heart"" src=""https://cdn.jsdelivr.net/gh/qinwf/awesome-R@3c66da6e291bcc0520b1649125b0bed750896a9a/heart.png"" height=""20"" align=""absmiddle"" width=""20"">
","
", - Fast data manipulation in a short and flexible syntax.
31726,http://bokeh.pydata.org/en/latest/,Bokeh, - R Interface to ,.
31726,https://github.com/datastorm-open/visNetwork,visNetwork,"
", - Using vis.js library for network visualization.
31726,https://github.com/datastorm-open/visNetwork,visNetwork,"
", - Using vis.js library for network visualization.
31726,https://github.com/jalapic/engsoccerdata,engsoccerdata,"
", - English and European soccer results 1871-2016.
31726,https://leanpub.com/exdata,"
Exploratory Data Analysis with R by Roger D. Peng (2016)","
", - Basic analytical skills for all sorts of data in R.
31726,https://www.packtpub.com/big-data-and-business-intelligence/learning-r-programming,Learning R Programming,"
", - Learning R as a programming language from basics to advanced topics.
31726,https://www.coursera.org/specialization/jhudatascience/1,Johns Hopkins University Data Science Specialization,"
"," - 9 courses including: Introduction to R, literate analysis tools, Shiny and some more."
31726,http://simplystatistics.org/2014/11/25/harvardx-biomedical-data-science-open-online-training-curriculum-launches-on-january-19/,HarvardX Biomedical Data Science,"
", - Introduction to R for the Life Sciences.
31726,https://github.com/ropensci/opendata,Open Data,"
"," - Using R to obtain, parse, manipulate, create, and share open data."
31727,https://github.com/MadryLab/backdoor_data_poisoning,[code],"
","
"
31727,https://github.com/data61/MP-SPDZ,[code],"
","
"
31727,https://github.com/INK-USC/data-poisoning,[code],"
","
"
31735,http://roboimagedata.compute.dtu.dk/?page_id=36,DTU evaluation code,Evaluate the point clouds using the ,.
31739,https://github.com/evidencebp/commit-classification/tree/master/data,Data sets,"
","
"
31739,https://console.cloud.google.com/bigquery?d=github_repos&p=bigquery-public-data&page=dataset,BigQuery GitHub scheme,Our main data source is , and the 
31756,http://captain.whu.edu.cn/DOTAweb/dataset.html,DOTA,"We provide the config files, TFRecord files and label_map file used in training "," with ssd and rfcn, and the trained models have been uploaded to Baidu Drive. Notice that our code is tested on official "
31756,http://captain.whu.edu.cn/DOTAweb/dataset.html,here,Tensorflow Object Detection API reads data using the TFRecord file format. The raw DOTA data set is located ,". To download, extract and convert it to TFRecords, run the following commands below:"
31760,https://captain-whu.github.io/DOTA/dataset.html,DOTA,Please download ," dataset, use the "
31766,https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html,FlyingThings3D website,"FlyingThings3D: Download and unzip the ""Disparity"", ""Disparity Occlusions"", ""Disparity change"", ""Optical flow"", ""Flow Occlusions"" for DispNet/FlowNet2.0 dataset subsets from the ", (we used the paths from 
31766,https://lmb.informatik.uni-freiburg.de/data/FlyingThings3D_subset/FlyingThings3D_subset_all_download_paths.txt,this file, (we used the paths from ,", now they added torrent downloads) . They will be upzipped into the same directory, "
31766,http://www.cvlibs.net/download.php?file=data_scene_flow.zip,KITTI Scene Flow Evaluation 2015,KITTI Scene Flow 2015 Download and unzip , to directory 
31769,http://jmcauley.ucsd.edu/data/amazon,Source,"Amazon contains 10,166 nodes and 148,865 edges. ","
"
31769,https://snap.stanford.edu/data/higgs-twitter.html,Source,"Twitter contains 10,000 nodes and 331,899 edges. ","
"
31769,http://socialcomputing.asu.edu/datasets/YouTube,Source,"YouTube contains 2,000 nodes and 1,310,617 edges. ","
"
31782,https://github.com/JDAI-CV/lapa-dataset,https://github.com/JDAI-CV/lapa-dataset, : ,"
"
31784,#dataloader,Dataloader,December 2020: Added raw SVG dataloader (see , section).
31802,https://github.com/jiwei0921/RGBD-SOD-datasets,here,The web link is ,.
31807,https://mmpose.readthedocs.io/en/latest/topics/wholebody.html#coco-wholebody-dataset,MMPose,". The repo contains COCO-WholeBody annotations proposed in this paper. Note that in our ECCV paper, all experiments are conducted on COCO-WholeBody V0.5. We further improve the annotation of the proposed dataset from V0.5 to V1.0. The benchmark results for COCO-WholeBody V1.0 can be found in ",. More introduction of COCO-WholeBody V1.0 is summarized in our recent 
31807,https://cocodataset.org/#keypoints-2017,COCO 2017 dataset,COCO-WholeBody dataset is the first large-scale benchmark for whole-body pose estimation. It is an extension of , with the same train/val split as COCO.
31807,https://cocodataset.org/#keypoints-2017,COCO 2017 website,Images can be downloaded from ,.
31807,data_format.md,DATA_FORMAT,The data format is defined in ,.
31807,https://github.com/cocodataset/cocoapi,@cocodataset/cocoapi,We provide evaluation tools for COCO-WholeBody dataset. Our evaluation tools is developed based on ,.
31808,http://cocodataset.org/#download,here,"Next, set up the COCO dataset. You can download it from ",", and update the paths in "
31808,https://github.com/cocodataset/cocoapi,here," to the correct directories for both images and annotations. After that, make sure to install the COCO PythonAPI from ",.
31810,https://github.com/EricArazo/PseudoLabeling/blob/master/miniImagenet/dataset/create_dataset.txt,create_dataset.txt,"To run the CIFAR experiments download the corresponding dataset in the folder ./CIFAR10/data or ./CIFAR100/data. To run the MiniImageNet experiments download the ImageNet dataset, pre-process it (see ","), and place it in ./miniImagenet/data."
31814,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,leaderboard,"This work proposes a weakly supervised approach for 3D object detection, only requiring a small set of weakly annotated scenes, associated with a few precisely labeled object instances. This is achieved by a two-stage architecture design. Using only 500 weakly annotated scenes and 534 precisely labeled vehicle instances, our method achieves 85−95% the performance of current top-leading, fully supervised detectors (which require 3, 712 exhaustively and precisely annotated scenes with 15, 654 instances) on KITTI 3D object detection ",". More importantly, our trained model can be applied as a 3D object annotator, generating annotations which can be used to train 3D object detectors with over 94% of their original performance (under manually labeled data). Above designs make our approach highly practical and introduce new opportunities for learning 3D object detection with reduced annotation burden."
31814,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,KITTI 3D object detection,Please download the official , dataset and organize the downloaded files as follows:
31820,https://www.lvisdataset.org/dataset,LVIS,For ," dataset, please arrange the data as:"
31821,#dataset-and-pre-processing,Dataset and Pre-processing,"
","
"
31822,http://wellyzhang.github.io/project/raven.html#dataset,our project page,". To download the dataset, please check ",.
31827,https://bdd-data.berkeley.edu/,official website, on their ,. We have provided the classified 
31827,./datasets/bdd100k_lists,lists,. We have provided the classified ," of different weathers and times. After downloading, you only need to run:"
31827,https://www.cityscapes-dataset.com/,official website,. Please follow the standard download and preparation guidelines on the ,. We recommend to symlink its root folder 
31827,http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip,here,. The dataset can be downloaded ,", which is from "
31829,https://amsterdammedicaldatascience.nl/,Amsterdam Medical Data Science,"The database, although de-identified, still contains detailed information regarding the clinical care of patients, so must be treated with appropriate care and respect and cannot be shared without permission. To request access, go to the ", website.
31829,https://www.icudata.nl/index-en.html,The Dutch ICU Data Warehouse, project in preparation for the , a number of commonly used items have been mapped to 
31831,docs/zh_CN/training/config_description/data_augmentation.md,数据增强,"
","
"
31832,docs/tutorials/data/README.md,数据准备,"
","
"
31837,#data-for-demo,Data for Demo,"
","
"
31866,https://data.vision.ee.ethz.ch/kanakism/RI.zip,Download RI (RI subdirectory),"
","
"
31866,https://data.vision.ee.ethz.ch/kanakism/pretrained_models.zip,Download pretrained resnet18 models (models subdirectory),"
","
"
31866,https://data.vision.ee.ethz.ch/kanakism/results.zip,Download sample results for the provided configs (results subdirectory),"
","
"
31866,https://data.vision.ee.ethz.ch/kanakism/RI.zip,here,Required information for RI have been pre-computed and can be downloaded ,", but the subspace can also be generated using:"
31870,data.py,data.py,"
",: Loading the dataset and applying the synthetic transformations
31871,#data,Data,"
","
"
31873,#data,Data,"
","
"
31873,#data-preparation,here, To recreate the full dataset please follow the instructions listed ,.
31876,https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A223390/datastream/PDF_01/view,[Paper],"
","
"
31876,https://www.researchgate.net/publication/220907047_A_general_framework_for_estimating_similarity_of_datasets_and_decision_trees_exploring_semantic_similarity_of_decision_trees,[Paper],"
","
"
31876,https://papers.nips.cc/paper/1359-data-dependent-structural-risk-minimization-for-perceptron-decision-trees,[Paper],"
","
"
31877,https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz,link,"for the Higgs dataset, directly use this "," or go to the UCI repository (link in the References section of the paper) and download HIGGS.csv.gz,"
31877,https://github.com/alexandre-bayle/cvci/blob/master/processing_dataset.py,processing_dataset.py,"
",": performs the initial processing of the dataset; takes as inputs the task (""Clf"" or ""Reg"") and the path to the folder where you saved the downloaded dataset (Higgs dataset for ""Clf"" and FlightDelays dataset for ""Reg""),"
31902,dataset_train.txt,dataset_train.txt,Download the source file ,"
"
31904,data/original_vimeo_links.txt,here, and all video links are ,.
31904,http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip,zip (1.7GB),Test set only: ,.
31904,http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip,zip (33GB),Both training and test set: ,.
31904,http://data.csail.mit.edu/tofu/testset/vimeo_denoising_test.zip,zip (16GB),The test set for video denoising: ,.
31904,http://data.csail.mit.edu/tofu/testset/vimeo_sep_block.zip,zip (11GB),The test set for video deblocking: ,.
31904,http://data.csail.mit.edu/tofu/testset/vimeo_super_resolution_test.zip,zip (6GB),The test set for video super-resolution: ,.
31904,http://data.csail.mit.edu/tofu/testset/vimeo_test_clean.zip,zip (15GB),The original test set (not downsampled or downgraded by noise): ,.
31904,http://data.csail.mit.edu/tofu/dataset/vimeo_septuplet.zip,zip (82GB),"The original training + test set (consists of 91701 sequences, which are not downsampled or downgraded by noise): ",.
31906,#hd-dataset-results,HD Dataset Results,"
","
"
31918,https://data.princeton.edu/pop509/recid1,here,"(Not part of my MLHC paper) There's also a recidivism dataset by Chung, Schmidt, and Witte (1991) that is included; some information is available ","
"
31936,https://oscar-corpus.com/,OSCAR,Arabic version of , (unshuffled version of the corpus) - filtered from 
31936,https://www.sites.google.com/a/mohamedaly.info/www/datasets/astd,ASTD,    | | ," |  4 Classes, MSA and Egyptian dialects | 0.670     | 0.677    | "
31957,https://hpai.bsc.es/MAMe-dataset,official website,The MAMe dataset can be downloaded from its ,.
31957,https://github.com/HPAI-BSC/MAMe-baselines/tree/master/dataset/get_metadata.sh,download metadata,Metadata: ,"
"
31957,https://github.com/HPAI-BSC/MAMe-baselines/tree/master/dataset/get_data.sh,download data,Image data: ,"
"
31958,https://towardsdatascience.com/how-to-properly-use-the-gpu-within-a-docker-container-4c699c78c6d1,this link, in newer versions of Docker. It may be different based on your machine’s operating system and the kind of NVIDIA GPU that your machine has. See ,.
31962,https://github.com/rbgirshick/py-faster-rcnn/tree/master/data,Faster R-CNN, (originally from ,)
31962,http://cocodataset.org/#download,here,"Download the images (2014 Train, 2014 Val, 2017 Test) from ","
"
31968,http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/,Oxford5K,"The table below contains the pre-trained models that we provide with this library, together with their mAP performance on some of the most well-know image retrieval benchmakrs: ",", "
31968,http://www.robots.ox.ac.uk/~vgg/data/parisbuildings/,Paris6K,", ",", and their Revisited versions ("
31968,https://www.kaggle.com/google/google-landmarks-dataset,Google-Landmarks Dataset,", which has been trained on the ",", a large dataset consisting of more than 1M images and 15K classes."
31968,https://github.com/naver/kapture/blob/master/doc/datasets.adoc,here,"If you want to convert your own dataset into kapture, please find some examples ",.
31969,https://github.com/naver/kapture/blob/master/doc/datasets.adoc,here,"If you want to convert your own dataset into kapture, please find some examples ",.
31976,http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d,Kitti Object Detection Dataset,Download the , (
31976,http://www.cvlibs.net/download.php?file=data_object_image_2.zip,image, (,", "
31976,http://www.cvlibs.net/download.php?file=data_object_calib.zip,calib,", ", and 
31976,http://www.cvlibs.net/download.php?file=data_object_label_2.zip,label, and ,) and place them into 
32005,https://datagen.tech/blog/fully-simulated-training-data-how-were-using-synthetic-expo-markers-to-train-a-network/,Blog Post, | ,"
"
32005,https://datagen.tech/blog/fully-simulated-training-data-how-were-using-synthetic-expo-markers-to-train-a-network/,blog post,The data can be downloaded from our ,.
32005,https://www.datagen.tech/data-license-agreement/,Non-Commercial Data License Agreement,The Datagen expo dataset is released under ,.
32006,https://pandas.pydata.org/,Pandas,", "," for federated analytics, or even raw "
32025,#nisqa-corpus,NISQA Corpus,"
","
"
32035,http://cocodataset.org/#home,MS COCO,Datasets: , for image instance segmentation and 
32035,https://youtube-vos.org/dataset/vis/,YouTube-VIS, for image instance segmentation and , for video instance segmentation.
32037,[/meta/dataset/kg/raw/,/meta/dataset/kg/raw/,", the fact file could be found in directory ",". Before, next two tasks, query registration and maintaining query results, set few parameters in "
32046,https://github.com/MacOS/blockchain-oracles-data-collection/tree/master/apps,apps,"
","
"
32046,https://github.com/MacOS/blockchain-oracles-data-collection/tree/master/evaluation,evaluation,"
","
"
32046,https://github.com/MacOS/blockchain-oracles-data-collection/tree/master/oracles,oracles,"
","
"
32046,https://github.com/MacOS/blockchain-oracles-data-collection/tree/master/solidity,solidity,"
","
"
32046,https://github.com/MacOS/blockchain-oracles-data-collection/tree/master/test,test,"
","
"
32054,https://github.com/deepmind/dsprites-dataset,dSprites,"Over the last few years, there has been significant research attention on representation learning focused on disentangling the underlying factors of variation in given data. However, most of the current/previous studies rely on datasets from the image/computer vision domain (such as the ", dataset). The purpose of this work is to be able to create a standardized dataset for conducting disentanglement studies on symbolic music data. The key motivation is that such a dataset would help researchers working on disentanglement problems demonstrate their algorithm on diverse domains.
32055,https://github.com/ashispati/dmelodies_dataset,dMelodies,This repository contains the source code for running the benchmarking experiments for disentanglement studies using the , dataset.
32060,https://www.cityscapes-dataset.com/,Cityscapes,| Dataset (with Link) | Content | Resolution (pixels) | Number of Classes | | :--: | :--: | :--: | :--: | | , | urban scenes |  2048x1024 | 19  | | 
32060,https://www.cityscapes-dataset.com/,Cityscapes,Segmentation performance measured in IoU/mIoU (%) on ,". For class names: All referred to all classes average, and the rest single class codes are as follows: DeepGlobe: Ro.-road, Sw.-sidewalk, Bu.-building, W.-wall, F.-fence, P.-pole, T.L.-traffic light, R.S.-raffic sign, V.-vegetation, Te.-terrain, Sk.-sky, P.-person, RI.-rider, C.-car, Tru.-truck, Tra.-train, M.-motorcycle, Bi.-bicycle."
32065,https://github.com/jason9693/MusicTransformer-tensorflow2.0/blob/master/dataset/scripts/ecomp_piano_downloader.sh,Piano e-Competition,Download the , dataset and 
32068,https://github.com/lawy623/LSTM_Pose_Machines/blob/master/dataset/JHMDB/utils/getBox.m,get bbox,please refer , to generate bbox for later use
32070,#dataset,Dataset,"
","
"
32075,http://universal.grew.fr/validator.html?corpus=meta/valid_SUD/SUD_Beja-NSC@latest.json&top=http://universal.grew.fr/,SUD_Beja-NSC,"
","
"
32075,http://universal.grew.fr/validator.html?corpus=meta/valid_SUD/SUD_French-GSD@latest.json&top=http://universal.grew.fr,SUD_French-GSD,"
","
"
32075,http://universal.grew.fr/validator.html?corpus=meta/valid_SUD/SUD_French-Rhapsodie@latest.json&top=http://universal.grew.fr/,SUD_French-Rhapsodie,"
","
"
32075,http://universal.grew.fr/validator.html?corpus=meta/valid_SUD/SUD_French-ParisStories@latest.json&top=http://universal.grew.fr/,SUD_French-ParisStories,"
","
"
32075,http://universal.grew.fr/validator.html?corpus=meta/valid_SUD/SUD_Naija-NSC@latest.json&top=http://universal.grew.fr/,SUD_Naija-NSC,"
","
"
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil-noexp.tar,MS MARCO V1 passage: uniCOIL (noexp),| Corpora | Size | Checksum | |:--------|-----:|:---------| | , | 2.7 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-passage-unicoil.tar,MS MARCO V1 passage: uniCOIL (d2q-T5), | | , | 3.4 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil-noexp.tar,MS MARCO V1 doc: uniCOIL (noexp), | | , | 11 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco-doc-segmented-unicoil.tar,MS MARCO V1 doc: uniCOIL (d2q-T5), | | , | 19 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_noexp_0shot.tar,MS MARCO V2 passage: uniCOIL (noexp), | | , | 24 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_passage_unicoil_0shot.tar,MS MARCO V2 passage: uniCOIL (d2q-T5), | | , | 41 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_noexp_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (noexp), | | , | 55 GB | 
32076,https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/data/msmarco_v2_doc_segmented_unicoil_0shot_v2.tar,MS MARCO V2 doc: uniCOIL (d2q-T5), | | , | 72 GB | 
32076,https://tripdatabase.github.io/tripclick/,TripClick, for ,: a large-scale dataset of click logs in the health domain
32077,https://grouplens.org/datasets/movielens/25m/,ML25M,|| Recommendation | Search | Conversational Recommendation | |-------------|-------------|------------|------------| |Movies | ,: 25m movie ratings | Reviews crawled from IMDB | Conversations crawled from 
32088,https://github.com/twitter/meta-learning-lstm/tree/master/data/miniImagenet,here,Mini-Imagenet as described ,"
"
32090,data,"
data
",All datasets should be placed in the ," folder. In order to run the Omniglot experiment, download the dataset from "
32090,data/omniglot,"
data/omniglot
",] in the , folder.
32101,data/s3dis/prepare_data.py,"
data/s3dis/
",. The code for preprocessing the S3DIS dataset is located in ,. One should first download the dataset from 
32101,http://buildingparser.stanford.edu/dataset.html,here,. One should first download the dataset from ,", then run"
32101,http://www.cvlibs.net/download.php?file=data_object_label_2.zip,here,. One should first download the ground truth labels from ,", then run"
32101,data/s3dis/prepare_data.py,pre-processing,The code for data , and 
32101,datasets/kitti/frustum.py,pre-processing,The code for data , and 
32104,./docs/data/pre_data.md,Prepare Public Dataset,"
","
"
32104,./docs/data/marker/marker.md,Prepare Customized Dataset,"
","
"
32115,https://nyu.databrary.org/,Databrary,The dataset is hosted on the ," repository for behavioral science. Unfortunately, we are unable to publicly share the SAYCam dataset here due to the terms of use. However, interested researchers can apply for access to the dataset with approval from their institution's IRB."
32116,#data-preparation,Data preparation,"
","
"
32116,#training-data-preparation,Training data preparation,"
","
"
32116,#dataset-for-data-augmentation,Dataset for data augmentation,"
","
"
